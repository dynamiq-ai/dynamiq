"""
End-to-end test for agents-as-tools under XML mode with parallel execution.

This test verifies:
- Wrapping agents as tools and per-call cloning (isolation for parallelism)
- Parallel execution via Map (fan-out over two inputs) and Manager runs
- Streaming events are received (provider-agnostic, with tracing fallback)
- Tracing captures node runs with metadata, executions, and hierarchy
- Memory sharing path remains functional across follow-up run
- Robust tool input handling (e.g., None sanitization for metadata)
"""

import os
import uuid

import pytest

from dynamiq import Workflow, connections, flows
from dynamiq.callbacks import TracingCallbackHandler
from dynamiq.callbacks.streaming import StreamingIteratorCallbackHandler
from dynamiq.memory import Memory
from dynamiq.memory.backends import InMemory
from dynamiq.nodes.agents.react import ReActAgent
from dynamiq.nodes.llms import OpenAI
from dynamiq.nodes.operators.operators import Map
from dynamiq.nodes.tools.python import Python
from dynamiq.nodes.types import InferenceMode
from dynamiq.runnables import RunnableConfig, RunnableStatus
from dynamiq.types.streaming import StreamingConfig, StreamingMode


@pytest.mark.integration
def test_agents_as_tools_end_to_end_parallel_streaming_tracing_memory_with_creds():
    """Exercise managerâ†’sub-agent tools with parallel Map, streaming, and tracing.

    Arrange: Build sub-agents (Researcher, Writer), wrap as tools for a Manager.
    Act: Run Manager inside a Map over two inputs with streaming + tracing callbacks.
    Assert: Map and Manager node runs exist with executions; streaming events observed;
            sub-agent activity validated via tracing fallback; follow-up run succeeds
            with same user/session (memory continuity).
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        pytest.skip("OPENAI_API_KEY is not set; skipping credentials-required test.")

    # Shared memory for manager/sub-agents when share_memory=True
    memory = Memory(backend=InMemory())

    openai_node = OpenAI(model="gpt-4.1", connection=connections.OpenAI(api_key=api_key))

    # Sub-agent: Researcher (has a simple tool)
    research_tool = Python(
        name="NoOpResearch",
        description="Returns static research snippet",
        code="""
def run(input_data):
    q = input_data.get("query", "")
    return {"content": f"research:{q}"}
""",
    )
    researcher = ReActAgent(
        name="Researcher", llm=openai_node, tools=[research_tool], inference_mode=InferenceMode.XML, max_loops=5
    )

    # Sub-agent: Writer (no tools)
    writer = ReActAgent(name="Writer", llm=openai_node, tools=[], inference_mode=InferenceMode.XML, max_loops=5)

    # Manager agent uses sub-agents as tools (share_memory=True to test memory propagation)
    manager = ReActAgent(
        name="Manager",
        llm=openai_node,
        tools=[
            researcher.as_tool(description="Perform research", share_memory=True),
            writer.as_tool(description="Draft final output", share_memory=True),
        ],
        inference_mode=InferenceMode.XML,
        parallel_tool_calls_enabled=True,
        memory=memory,
        max_loops=10,
    )

    # Enable token streaming for manager and sub-agents to ensure source tagging
    stream_cfg = StreamingConfig(enabled=True, mode=StreamingMode.ALL)
    manager.streaming = stream_cfg
    researcher.streaming = stream_cfg
    writer.streaming = stream_cfg

    # Place manager inside Map to validate cloning/parallel isolation
    map_node = Map(node=manager, max_workers=2)
    wf = Workflow(flow=flows.Flow(nodes=[map_node]))

    user_id = str(uuid.uuid4())
    session_id = str(uuid.uuid4())

    inputs = {
        "input": [
            {"input": "find facts about cats and write a brief"},
            {"input": "find facts about dogs and write a brief"},
        ]
    }

    streaming = StreamingIteratorCallbackHandler()
    tracing = TracingCallbackHandler()

    result = wf.run(input_data=inputs, config=RunnableConfig(callbacks=[streaming, tracing]))
    assert result.status == RunnableStatus.SUCCESS

    # Workflow/Flow presence in tracing
    has_workflow_run = any(
        isinstance(getattr(run, "metadata", None), dict) and "workflow" in run.metadata for run in tracing.runs.values()
    )
    has_flow_run = any(
        isinstance(getattr(run, "metadata", None), dict) and "flow" in run.metadata for run in tracing.runs.values()
    )
    assert has_workflow_run and has_flow_run

    # Map-specific assertions: Map node exists and Manager ran once per input
    map_present = any(
        isinstance(getattr(run, "metadata", None), dict) and run.metadata.get("node", {}).get("name") == "Map"
        for run in tracing.runs.values()
    )
    assert map_present

    # Verify Map run has executions recorded
    map_runs = [
        run
        for run in tracing.runs.values()
        if isinstance(getattr(run, "metadata", None), dict) and run.metadata.get("node", {}).get("name") == "Map"
    ]
    for r in map_runs:
        assert getattr(r, "executions", None) is not None and len(r.executions) > 0

    manager_runs = [
        run
        for run in tracing.runs.values()
        if isinstance(getattr(run, "metadata", None), dict) and run.metadata.get("node", {}).get("name") == "Manager"
    ]
    # Output cardinality via Manager runs: count matches inputs and each output is non-empty
    assert len(manager_runs) == len(inputs["input"])  # same cardinality
    assert all(bool(str(getattr(r, "output", "")).strip()) for r in manager_runs)

    # Each Manager run should have executions and a parent (Map)
    for r in manager_runs:
        assert getattr(r, "executions", None) is not None and len(r.executions) > 0
        assert getattr(r, "parent_run_id", None) is not None

    # Sub-agent tracing: if sub-agents were invoked, ensure correct parentage and at least one has executions
    subagent_runs = [
        run
        for run in tracing.runs.values()
        if isinstance(getattr(run, "metadata", None), dict)
        and run.metadata.get("node", {}).get("name") in {"Researcher", "Writer"}
    ]
    if len(subagent_runs) > 0:
        manager_parent_ids = {m.id for m in manager_runs}
        # All sub-agent runs should be children of a Manager run
        assert all(getattr(r, "parent_run_id", None) in manager_parent_ids for r in subagent_runs)
        # At least one sub-agent run should have recorded executions
        assert any((getattr(r, "executions", None) is not None and len(r.executions) > 0) for r in subagent_runs)

    # Clone isolation: distinct node ids for different Researcher runs
    researcher_ids = {run.id for run in subagent_runs if run.metadata.get("node", {}).get("name") == "Researcher"}
    # if multiple Researcher runs occurred, ensure distinct ids (clone isolation)
    if len(researcher_ids) >= 2:
        assert len(researcher_ids) == len({*researcher_ids})

    # Streaming: ensure we saw streams and iterator exhausts cleanly
    sources = set()
    steps = set()
    received = 0
    try:
        for event in streaming:
            received += 1
            # Prefer structured source metadata
            src = getattr(event, "source", None)
            name = getattr(src, "name", None) if src else None
            step = None
            data = event.data if isinstance(event.data, dict) else {}
            choices = data.get("choices") if isinstance(data, dict) else None
            if choices and isinstance(choices, list):
                delta = choices[0].get("delta", {})
                if isinstance(delta, dict):
                    # Some providers include step in delta
                    step = delta.get("step")
            if name:
                sources.add(name)
            if step:
                steps.add(step)
    except Exception as e:
        pytest.fail(f"Streaming iterator did not exhaust cleanly: {e}")
    assert received > 0
    assert any("Manager" in s for s in sources) or any("Agent Manager" in s for s in sources)
    # Sub-agent/tool names may not appear in streaming metadata across providers; fall back to tracing
    if not any(("Researcher" in s) or ("Writer" in s) or ("NoOpResearch" in s) for s in sources):
        node_runs = [
            run
            for run in tracing.runs.values()
            if isinstance(getattr(run, "metadata", None), dict) and "node" in run.metadata
        ]
        node_names = [getattr(run, "name", "") for run in node_runs]
        assert any(("Researcher" in n) or ("Writer" in n) for n in node_names)

    # Tracing: ensure multiple node runs were recorded (avoid RunType access on handler)
    has_node_runs = any(
        isinstance(getattr(run, "metadata", None), dict) and "node" in run.metadata for run in tracing.runs.values()
    )
    assert has_node_runs or len(tracing.runs) > 0

    # Memory sharing: follow-up under same user/session to ensure context continuity
    followup_inputs = {
        "input": [
            {
                "input": "continue with more details on cats",
                "user_id": user_id,
                "session_id": session_id,
            },
            {
                "input": "continue with more details on dogs",
                "user_id": user_id,
                "session_id": session_id,
            },
        ]
    }
    followup_result = wf.run(input_data=followup_inputs, config=RunnableConfig(callbacks=[streaming, tracing]))
    assert followup_result.status == RunnableStatus.SUCCESS

    # Tools in sub-agents executed (trace presence for NoOpResearch)
    def is_tool_run(run):
        node_meta = getattr(run, "metadata", {}).get("node", {})
        group = node_meta.get("group") if isinstance(node_meta, dict) else None
        if hasattr(group, "value"):
            group = group.value
        return group == "tools"

    tool_names = [getattr(run, "name", "") for run in tracing.runs.values() if is_tool_run(run)]
    assert (
        "NoOpResearch" in tool_names
        or any("NoOpResearch" in getattr(r, "name", "") for r in tracing.runs.values())
        or any("NoOpResearch" in str(r) for r in tracing.runs.values())
    )

    # Memory sharing: verify user_id/session_id propagated in follow-up Manager and sub-agent runs
    # Capture new runs that appeared after the follow-up invocation
    # (We use the known user_id/session_id values for equality checks.)
    # Note: Since the tracing handler accumulates runs, select runs with latest start_time heuristic
    new_runs = [run for run in tracing.runs.values() if isinstance(getattr(run, "metadata", None), dict)]
    manager_with_ids = [
        r
        for r in new_runs
        if r.metadata.get("node", {}).get("name") == "Manager"
        and isinstance(getattr(r, "input", None), dict)
        and r.input.get("user_id") == user_id
        and r.input.get("session_id") == session_id
    ]
    assert len(manager_with_ids) >= 1

    # For sub-agents, validate execution and parentage under Manager rather than requiring id fields in inputs
    subagent_runs = [r for r in new_runs if r.metadata.get("node", {}).get("name") in {"Researcher", "Writer"}]
    assert len(subagent_runs) >= 1
    manager_ids = {r.id for r in manager_with_ids}
    assert any(getattr(r, "parent_run_id", None) in manager_ids for r in subagent_runs)
