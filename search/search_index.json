{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Dynamiq is an orchestration framework for agentic AI and LLM applications </p> <p> </p> <p>Welcome to Dynamiq! \ud83e\udd16</p> <p>Dynamiq is your all-in-one Gen AI framework, designed to streamline the development of AI-powered applications. Dynamiq specializes in orchestrating retrieval-augmented generation (RAG) and large language model (LLM) agents.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's how you can get started with Dynamiq:</p>"},{"location":"#installation","title":"Installation","text":"<p>First, let's get Dynamiq installed. You'll need Python, so make sure that's set up on your machine. Then run:</p> <pre><code>pip install dynamiq\n</code></pre> <p>Or build the Python package from the source code: <pre><code>git clone https://github.com/dynamiq-ai/dynamiq.git\ncd dynamiq\npoetry install\n</code></pre></p>"},{"location":"#documentation","title":"Documentation","text":"<p>For more examples and detailed guides, please refer to our documentation.</p>"},{"location":"#examples","title":"Examples","text":""},{"location":"#simple-llm-flow","title":"Simple LLM Flow","text":"<p>Here's a simple example to get you started with Dynamiq:</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.prompts import Prompt, Message\n\n# Define the prompt template for translation\nprompt_template = \"\"\"\nTranslate the following text into English: {{ text }}\n\"\"\"\n\n# Create a Prompt object with the defined template\nprompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\n# Setup your LLM (Large Language Model) Node\nllm = OpenAI(\n    id=\"openai\",  # Unique identifier for the node\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),  # Connection using API key\n    model=\"gpt-4o\",  # Model to be used\n    temperature=0.3,  # Sampling temperature for the model\n    max_tokens=1000,  # Maximum number of tokens in the output\n    prompt=prompt  # Prompt to be used for the model\n)\n\n# Run the LLM node with the input data\nresult = llm.run(\n    input_data={\n        \"text\": \"Hola Mundo!\"  # Text to be translated\n    }\n)\n\n# Print the result of the translation\nprint(result.output)\n</code></pre>"},{"location":"#simple-react-agent-with-asynchronous-execution","title":"Simple ReAct Agent with asynchronous execution","text":"<p>An agent that has the access to E2B Code Interpreter and is capable of solving complex coding tasks.</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection, E2B as E2BConnection\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\n\n# Initialize the E2B tool\ne2b_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"E2B_API_KEY\")\n)\n\n# Setup your LLM\nllm = OpenAI(\n    id=\"openai\",\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.3,\n    max_tokens=1000,\n)\n\n# Create the agent\nagent = Agent(\n    name=\"react-agent\",\n    llm=llm, # Language model instance\n    tools=[e2b_tool],  # List of tools that the agent can use\n    role=\"Senior Data Scientist\",  # Role of the agent\n    max_loops=10, # Limit on the number of processing loops\n)\n\nasync def run_async_agent():\n    # Run the agent asynchronously with an input\n    result = await agent.run(\n        input_data={\n            \"input\": \"Add the first 10 numbers and tell if the result is prime.\",\n        }\n    )\n\n    print(result.output.get(\"content\"))\n\n\n# Execute the async function\nif __name__ == \"__main__\":\n    asyncio.run(run_async_agent())\n</code></pre>"},{"location":"#configuring-two-parallel-agents-with-workflow","title":"Configuring Two Parallel Agents with WorkFlow","text":"<pre><code>from dynamiq import Workflow\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.nodes.agents import Agent\n\n# Setup your LLM\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\n# Define the first agent: a question answering agent\nfirst_agent = Agent(\n    name=\"Expert Agent\",\n    llm=llm,\n    role=\"Professional writer with the goal of producing well-written and informative responses.\",\n    id=\"agent_1\",\n    max_loops=5\n)\n\n# Define the second agent: a poetic writer\nsecond_agent = Agent(\n    name=\"Poetic Rewriter Agent\",\n    llm=llm,\n    role=\"Professional writer with the goal of rewriting user input as a poem without changing its meaning.\",\n    id=\"agent_2\",\n    max_loops=5\n)\n\n\n# Create a workflow to run both agents with the same input\n# The `Workflow` class simplifies setting up and executing a series of nodes in a pipeline.\n# It automatically handles running the agents in parallel where possible.\nwf = Workflow()\nwf.flow.add_nodes(first_agent)\nwf.flow.add_nodes(second_agent)\n\n# Equivalent alternative way to define the workflow:\n# from dynamiq.flows import Flow\n# wf = Workflow(flow=Flow(nodes=[agent_first, agent_second]))\n\n# Run the workflow with an input\nresult = wf.run(\n    input_data={\"input\": \"How are sin(x) and cos(x) connected in electrodynamics?\"},\n)\n\n# Print the input and output for both agents\nprint('--- Agent 1: Input ---\\n', result.output[first_agent.id].get(\"input\").get('input'))\nprint('--- Agent 1: Output ---\\n', result.output[first_agent.id].get(\"output\").get('content'))\nprint('--- Agent 2: Input ---\\n', result.output[second_agent.id].get(\"input\").get('input'))\nprint('--- Agent 2: Output ---\\n', result.output[second_agent.id].get(\"output\").get('content'))\n</code></pre>"},{"location":"#configuring-two-sequential-agents-with-workflow","title":"Configuring Two Sequential Agents with WorkFlow","text":"<pre><code>from dynamiq import Workflow\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.nodes.agents import Agent\n\nfrom dynamiq.nodes.node import InputTransformer, NodeDependency\n\n# Setup your LLM\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\nfirst_agent = Agent(\n    name=\"Expert Agent\",\n    llm=llm,\n    role=\"Professional writer with the goal of producing well-written and informative responses.\",  # Role of the agent\n    id=\"agent_1\",\n    max_loops=5\n)\n\nsecond_agent = Agent(\n    name=\"Poetic Rewriter Agent\",\n    llm=llm,\n    role=\"Professional writer with the goal of rewriting user input as a poem without changing its meaning.\",  # Role of the agent\n    id=\"agent_2\",\n    depends=[NodeDependency(first_agent)],  # Set dependency on the first agent\n    input_transformer=InputTransformer(\n        selector={\"input\": f\"${[first_agent.id]}.output.content\"}  # Extract the output of the first agent as input\n    ),\n    max_loops=5\n)\n\n# Create a workflow to run the agents sequentially based on dependencies.\n# Without a workflow, you would need to run `first_agent`, collect its output,\n# and then manually pass that output as input to `second_agent`. The workflow automates this process.\nwf = Workflow()\nwf.flow.add_nodes(first_agent)\nwf.flow.add_nodes(second_agent)\n\n# Equivalent alternative way to define the workflow:\n# from dynamiq.flows import Flow\n# wf = Workflow(flow=Flow(nodes=[agent_first, agent_second]))\n\n# Run the workflow with an input\nresult = wf.run(\n    input_data={\"input\": \"How are sin(x) and cos(x) connected in electrodynamics?\"},\n)\n\n# Print the input and output for both agents\nprint('--- Agent 1: Input ---\\n', result.output[first_agent.id].get(\"input\").get('input'))\nprint('--- Agent 1: Output ---\\n', result.output[first_agent.id].get(\"output\").get('content'))\nprint('--- Agent 2: Input ---\\n', result.output[second_agent.id].get(\"input\").get('input'))\nprint('--- Agent 2: Output ---\\n', result.output[second_agent.id].get(\"output\").get('content'))\n</code></pre>"},{"location":"#multi-agent-orchestration","title":"Multi-agent orchestration","text":"<pre><code>from dynamiq import Workflow\nfrom dynamiq.connections import OpenAI as OpenAIConnection, ScaleSerp as ScaleSerpConnection\nfrom dynamiq.flows import Flow\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.nodes.tools.scale_serp import ScaleSerpTool\nfrom dynamiq.nodes.types import Behavior, InferenceMode\n\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\nsearch_tool = ScaleSerpTool(connection=ScaleSerpConnection(api_key=\"SCALESERP_API_KEY\"))\n\nresearch_agent = Agent(\n    name=\"Research Analyst\",\n    role=\"Find recent market news and provide referenced highlights.\",\n    llm=llm,\n    tools=[search_tool],\n    inference_mode=InferenceMode.XML,\n    max_loops=6,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n\nwriter_agent = Agent(\n    name=\"Brief Writer\",\n    role=\"Turn research highlights into a concise executive brief.\",\n    llm=llm,\n    inference_mode=InferenceMode.XML,\n    max_loops=4,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n\nmanager_agent = Agent(\n    name=\"Manager\",\n    role=(\n        \"Delegate research and writing to sub-agents.\\n\"\n        \"Always call tools with {'input': '&lt;task&gt;'} payloads and assemble the final brief.\"\n    ),\n    llm=llm,\n    tools=[research_agent, writer_agent],\n    inference_mode=InferenceMode.XML,\n    parallel_tool_calls_enabled=True,\n    max_loops=8,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n\nworkflow = Workflow(flow=Flow(nodes=[manager_agent]))\n\nresult = workflow.run(\n    input_data={\"input\": \"Summarize the latest developments in battery technology for investors.\"},\n)\n\nprint(result.output[manager_agent.id][\"output\"][\"content\"])\n</code></pre>"},{"location":"#rag-document-indexing-flow","title":"RAG - document indexing flow","text":"<p>This workflow takes input PDF files, pre-processes them, converts them to vector embeddings, and stores them in the Pinecone vector database. The example provided is for an existing index in Pinecone. You can find examples for index creation on the <code>docs/tutorials/rag</code> page.</p> <pre><code>from io import BytesIO\n\nfrom dynamiq import Workflow\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.converters import PyPDFConverter\nfrom dynamiq.nodes.splitters.document import DocumentSplitter\nfrom dynamiq.nodes.embedders import OpenAIDocumentEmbedder\nfrom dynamiq.nodes.writers import PineconeDocumentWriter\n\nrag_wf = Workflow()\n\n# PyPDF document converter\nconverter = PyPDFConverter(document_creation_mode=\"one-doc-per-page\")\nrag_wf.flow.add_nodes(converter)  # add node to the DAG\n\n# Document splitter\ndocument_splitter = (\n    DocumentSplitter(\n        split_by=\"sentence\",\n        split_length=10,\n        split_overlap=1,\n    )\n    .inputs(documents=converter.outputs.documents)  # map converter node output to the expected input of the current node\n    .depends_on(converter)\n)\nrag_wf.flow.add_nodes(document_splitter)\n\n# OpenAI vector embeddings\nembedder = (\n    OpenAIDocumentEmbedder(\n        connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n        model=\"text-embedding-3-small\",\n    )\n    .inputs(documents=document_splitter.outputs.documents)\n    .depends_on(document_splitter)\n)\nrag_wf.flow.add_nodes(embedder)\n\n# Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(api_key=\"PINECONE_API_KEY\"),\n        index_name=\"default\",\n        dimension=1536,\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\nrag_wf.flow.add_nodes(vector_store)\n\n# Prepare input PDF files\nfile_paths = [\"example.pdf\"]\ninput_data = {\n    \"files\": [\n        BytesIO(open(path, \"rb\").read()) for path in file_paths\n    ],\n    \"metadata\": [\n        {\"filename\": path} for path in file_paths\n    ],\n}\n\n# Run RAG indexing flow\nrag_wf.run(input_data=input_data)\n</code></pre>"},{"location":"#rag-document-retrieval-flow","title":"RAG - document retrieval flow","text":"<p>Simple retrieval RAG flow that searches for relevant documents and answers the original user question using retrieved documents.</p> <pre><code>from dynamiq import Workflow\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.embedders import OpenAITextEmbedder\nfrom dynamiq.nodes.retrievers import PineconeDocumentRetriever\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.prompts import Message, Prompt\n\n# Initialize the RAG retrieval workflow\nretrieval_wf = Workflow()\n\n# Shared OpenAI connection\nopenai_connection = OpenAIConnection(api_key=\"OPENAI_API_KEY\")\n\n# OpenAI text embedder for query embedding\nembedder = OpenAITextEmbedder(\n    connection=openai_connection,\n    model=\"text-embedding-3-small\",\n)\nretrieval_wf.flow.add_nodes(embedder)\n\n# Pinecone document retriever\ndocument_retriever = (\n    PineconeDocumentRetriever(\n        connection=PineconeConnection(api_key=\"PINECONE_API_KEY\"),\n        index_name=\"default\",\n        dimension=1536,\n        top_k=5,\n    )\n    .inputs(embedding=embedder.outputs.embedding)\n    .depends_on(embedder)\n)\nretrieval_wf.flow.add_nodes(document_retriever)\n\n# Define the prompt template\nprompt_template = \"\"\"\nPlease answer the question based on the provided context.\n\nQuestion: {{ query }}\n\nContext:\n{% for document in documents %}\n- {{ document.content }}\n{% endfor %}\n\n\"\"\"\n\n# OpenAI LLM for answer generation\nprompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\nanswer_generator = (\n    OpenAI(\n        connection=openai_connection,\n        model=\"gpt-4o\",\n        prompt=prompt,\n    )\n    .inputs(\n        documents=document_retriever.outputs.documents,\n        query=embedder.outputs.query,\n    )  # take documents from the vector store node and query from the embedder\n    .depends_on([document_retriever, embedder])\n)\nretrieval_wf.flow.add_nodes(answer_generator)\n\n# Run the RAG retrieval flow\nquestion = \"What are the line intems provided in the invoice?\"\nresult = retrieval_wf.run(input_data={\"query\": question})\n\nanswer = result.output.get(answer_generator.id).get(\"output\", {}).get(\"content\")\nprint(answer)\n</code></pre>"},{"location":"#simple-chatbot-with-memory","title":"Simple Chatbot with Memory","text":"<p>A simple chatbot that uses the <code>Memory</code> module to store and retrieve conversation history.</p> <pre><code>from dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.memory import Memory\nfrom dynamiq.memory.backends.in_memory import InMemory\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.llms import OpenAI\n\nAGENT_ROLE = \"helpful assistant, goal is to provide useful information and answer questions\"\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\nmemory = Memory(backend=InMemory())\n\nagent = Agent(\n    name=\"Agent\",\n    llm=llm,\n    role=AGENT_ROLE,\n    id=\"agent\",\n    memory=memory,\n)\n\n\ndef main():\n    print(\"Welcome to the AI Chat! (Type 'exit' to end)\")\n    while True:\n        user_input = input(\"You: \")\n        user_id = \"user\"\n        session_id = \"session\"\n        if user_input.lower() == \"exit\":\n            break\n\n        response = agent.run({\"input\": user_input, \"user_id\": user_id, \"session_id\": session_id})\n        response_content = response.output.get(\"content\")\n        print(f\"AI: {response_content}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"#graph-orchestrator","title":"Graph Orchestrator","text":"<p>Graph Orchestrator allows to create any architecture tailored to specific use cases. Example of simple workflow that manages iterative process of feedback and refinement of email.</p> <pre><code>from typing import Any\n\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.nodes.agents.orchestrators.graph import END, START, GraphOrchestrator\nfrom dynamiq.nodes.agents.orchestrators.graph_manager import GraphAgentManager\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.llms import OpenAI\n\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\nemail_writer = Agent(\n    name=\"email-writer-agent\",\n    llm=llm,\n    role=\"Write personalized emails taking into account feedback.\",\n)\n\n\ndef gather_feedback(context: dict[str, Any], **kwargs):\n    \"\"\"Gather feedback about email draft.\"\"\"\n    feedback = input(\n        f\"Email draft:\\n\"\n        f\"{context.get('history', [{}])[-1].get('content', 'No draft')}\\n\"\n        f\"Type in SEND to send email, CANCEL to exit, or provide feedback to refine email: \\n\"\n    )\n\n    reiterate = True\n\n    result = f\"Gathered feedback: {feedback}\"\n\n    feedback = feedback.strip().lower()\n    if feedback == \"send\":\n        print(\"####### Email was sent! #######\")\n        result = \"Email was sent!\"\n        reiterate = False\n    elif feedback == \"cancel\":\n        print(\"####### Email was canceled! #######\")\n        result = \"Email was canceled!\"\n        reiterate = False\n\n    return {\"result\": result, \"reiterate\": reiterate}\n\n\ndef router(context: dict[str, Any], **kwargs):\n    \"\"\"Determines next state based on provided feedback.\"\"\"\n    if context.get(\"reiterate\", False):\n        return \"generate_sketch\"\n\n    return END\n\n\norchestrator = GraphOrchestrator(\n    name=\"Graph orchestrator\",\n    manager=GraphAgentManager(llm=llm),\n)\n\n# Attach tasks to the states. These tasks will be executed when the respective state is triggered.\norchestrator.add_state_by_tasks(\"generate_sketch\", [email_writer])\norchestrator.add_state_by_tasks(\"gather_feedback\", [gather_feedback])\n\n# Define the flow between states by adding edges.\n# This configuration creates the sequence of states from START -&gt; \"generate_sketch\" -&gt; \"gather_feedback\".\norchestrator.add_edge(START, \"generate_sketch\")\norchestrator.add_edge(\"generate_sketch\", \"gather_feedback\")\n\n# Add a conditional edge to the \"gather_feedback\" state, allowing the flow to branch based on a condition.\n# The router function will determine whether the flow should go to \"generate_sketch\" (reiterate) or END (finish the process).\norchestrator.add_conditional_edge(\"gather_feedback\", [\"generate_sketch\", END], router)\n\n\nif __name__ == \"__main__\":\n    print(\"Welcome to email writer.\")\n    email_details = input(\"Provide email details: \")\n    orchestrator.run(input_data={\"input\": f\"Write and post email, provide feedback about status of email: {email_details}\"})\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We love contributions! Whether it's bug reports, feature requests, or pull requests, head over to our CONTRIBUTING.md to see how you can help.</p>"},{"location":"#license","title":"License","text":"<p>Dynamiq is open-source and available under the Apache 2 License.</p> <p>Happy coding! \ud83d\ude80</p>"},{"location":"dynamiq/cache/codecs/","title":"Codecs","text":""},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec","title":"<code>Base64Codec</code>","text":"<p>               Bases: <code>BaseCodec</code></p> <p>Base64 encoding and decoding implementation.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>class Base64Codec(BaseCodec):\n    \"\"\"Base64 encoding and decoding implementation.\"\"\"\n\n    def encode(self, value: str) -&gt; str:\n        \"\"\"Encode a string using Base64.\n\n        Args:\n            value (str): The string to encode.\n\n        Returns:\n            str: The Base64 encoded string.\n        \"\"\"\n        return base64.b64encode(value.encode()).decode()\n\n    def decode(self, value: str | bytes) -&gt; str:\n        \"\"\"Decode a Base64 encoded string or bytes.\n\n        Args:\n            value (str | bytes): The value to decode.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        return base64.b64decode(value).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec.decode","title":"<code>decode(value)</code>","text":"<p>Decode a Base64 encoded string or bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | bytes</code> <p>The value to decode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def decode(self, value: str | bytes) -&gt; str:\n    \"\"\"Decode a Base64 encoded string or bytes.\n\n    Args:\n        value (str | bytes): The value to decode.\n\n    Returns:\n        str: The decoded string.\n    \"\"\"\n    return base64.b64decode(value).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec.encode","title":"<code>encode(value)</code>","text":"<p>Encode a string using Base64.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to encode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Base64 encoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def encode(self, value: str) -&gt; str:\n    \"\"\"Encode a string using Base64.\n\n    Args:\n        value (str): The string to encode.\n\n    Returns:\n        str: The Base64 encoded string.\n    \"\"\"\n    return base64.b64encode(value.encode()).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec","title":"<code>BaseCodec</code>","text":"<p>Abstract base class for encoding and decoding.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>class BaseCodec:\n    \"\"\"Abstract base class for encoding and decoding.\"\"\"\n\n    def encode(self, value: str) -&gt; str:\n        \"\"\"Encode a string value.\n\n        Args:\n            value (str): The string to encode.\n\n        Returns:\n            str: The encoded string.\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, value: str | bytes) -&gt; str:\n        \"\"\"Decode a string or bytes value.\n\n        Args:\n            value (str | bytes): The value to decode.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec.decode","title":"<code>decode(value)</code>","text":"<p>Decode a string or bytes value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | bytes</code> <p>The value to decode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def decode(self, value: str | bytes) -&gt; str:\n    \"\"\"Decode a string or bytes value.\n\n    Args:\n        value (str | bytes): The value to decode.\n\n    Returns:\n        str: The decoded string.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec.encode","title":"<code>encode(value)</code>","text":"<p>Encode a string value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to encode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The encoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def encode(self, value: str) -&gt; str:\n    \"\"\"Encode a string value.\n\n    Args:\n        value (str): The string to encode.\n\n    Returns:\n        str: The encoded string.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/config/","title":"Config","text":""},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheBackend","title":"<code>CacheBackend</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for cache backends.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class CacheBackend(str, enum.Enum):\n    \"\"\"Enumeration for cache backends.\"\"\"\n    Redis = \"Redis\"\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheConfig","title":"<code>CacheConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for cache settings.</p> <p>Attributes:</p> Name Type Description <code>backend</code> <code>CacheBackend</code> <p>The cache backend to use.</p> <code>namespace</code> <code>str | None</code> <p>Optional namespace for cache keys.</p> <code>ttl</code> <code>int | None</code> <p>Optional time-to-live for cache entries.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class CacheConfig(BaseModel):\n    \"\"\"Configuration for cache settings.\n\n    Attributes:\n        backend (CacheBackend): The cache backend to use.\n        namespace (str | None): Optional namespace for cache keys.\n        ttl (int | None): Optional time-to-live for cache entries.\n    \"\"\"\n    backend: CacheBackend\n    namespace: str | None = None\n    ttl: int | None = None\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert config to dictionary.\n\n        Args:\n            **kwargs: Additional arguments.\n\n        Returns:\n            dict: Configuration as dictionary.\n        \"\"\"\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheConfig.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert config to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Configuration as dictionary.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert config to dictionary.\n\n    Args:\n        **kwargs: Additional arguments.\n\n    Returns:\n        dict: Configuration as dictionary.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>               Bases: <code>CacheConfig</code>, <code>RedisConnection</code></p> <p>Configuration for Redis cache.</p> <p>Attributes:</p> Name Type Description <code>backend</code> <code>Literal[Redis]</code> <p>The Redis cache backend.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class RedisCacheConfig(CacheConfig, RedisConnection):\n    \"\"\"Configuration for Redis cache.\n\n    Attributes:\n        backend (Literal[CacheBackend.Redis]): The Redis cache backend.\n    \"\"\"\n    backend: Literal[CacheBackend.Redis] = CacheBackend.Redis\n</code></pre>"},{"location":"dynamiq/cache/utils/","title":"Utils","text":""},{"location":"dynamiq/cache/utils/#dynamiq.cache.utils.cache_wf_entity","title":"<code>cache_wf_entity(entity_id, cache_enabled=False, cache_manager_cls=WorkflowCacheManager, cache_config=None, func_kwargs_to_remove=FUNC_KWARGS_TO_REMOVE)</code>","text":"<p>Decorator to cache workflow entity outputs.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Identifier for the entity.</p> required <code>cache_enabled</code> <code>bool</code> <p>Flag to enable caching.</p> <code>False</code> <code>cache_manager_cls</code> <code>type[WorkflowCacheManager]</code> <p>Cache manager class.</p> <code>WorkflowCacheManager</code> <code>cache_config</code> <code>CacheConfig | None</code> <p>Cache configuration.</p> <code>None</code> <code>func_kwargs_to_remove</code> <code>tuple[str]</code> <p>List of params to remove from callable function kwargs.</p> <code>FUNC_KWARGS_TO_REMOVE</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Wrapped function with caching.</p> Source code in <code>dynamiq/cache/utils.py</code> <pre><code>def cache_wf_entity(\n    entity_id: str,\n    cache_enabled: bool = False,\n    cache_manager_cls: type[WorkflowCacheManager] = WorkflowCacheManager,\n    cache_config: CacheConfig | None = None,\n    func_kwargs_to_remove: tuple[str] = FUNC_KWARGS_TO_REMOVE,\n) -&gt; Callable:\n    \"\"\"Decorator to cache workflow entity outputs.\n\n    Args:\n        entity_id (str): Identifier for the entity.\n        cache_enabled (bool): Flag to enable caching.\n        cache_manager_cls (type[WorkflowCacheManager]): Cache manager class.\n        cache_config (CacheConfig | None): Cache configuration.\n        func_kwargs_to_remove (tuple[str]): List of params to remove from callable function kwargs.\n\n    Returns:\n        Callable: Wrapped function with caching.\n    \"\"\"\n    def _cache(func: Callable) -&gt; Callable:\n        \"\"\"Inner cache decorator.\n\n        Args:\n            func (Callable): Function to wrap.\n\n        Returns:\n            Callable: Wrapped function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; tuple[Any, bool]:\n            \"\"\"Wrapper function to handle caching.\n\n            Args:\n                *args (Any): Positional arguments.\n                **kwargs (Any): Keyword arguments.\n\n            Returns:\n                tuple[Any, bool]: Function output and cache status.\n            \"\"\"\n            cache_manager = None\n            from_cache = False\n            input_data = kwargs.pop(\"input_data\", args[0] if args else {})\n            input_data = dict(input_data) if isinstance(input_data, BaseModel) else input_data\n\n            cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in func_kwargs_to_remove}\n            if cache_enabled and cache_config:\n                logger.debug(f\"Entity_id {entity_id}: cache used\")\n                cache_manager = cache_manager_cls(config=cache_config)\n                if output := cache_manager.get_entity_output(\n                    entity_id=entity_id, input_data=input_data, **cleaned_kwargs\n                ):\n                    from_cache = True\n                    return output, from_cache\n\n            output = func(*args, **kwargs)\n\n            if cache_manager:\n                cache_manager.set_entity_output(\n                    entity_id=entity_id, input_data=input_data, output_data=output, **cleaned_kwargs\n                )\n\n            return output, from_cache\n\n        return wrapper\n\n    return _cache\n</code></pre>"},{"location":"dynamiq/cache/backends/base/","title":"Base","text":""},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for cache backends.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>class BaseCache(ABC):\n    \"\"\"Abstract base class for cache backends.\n\n    Attributes:\n        client (CacheClient): Cache client instance.\n    \"\"\"\n\n    def __init__(self, client: CacheClient):\n        \"\"\"Initialize BaseCache.\n\n        Args:\n            client (CacheClient): Cache client instance.\n        \"\"\"\n        self.client = client\n\n    @classmethod\n    def from_client(cls, client: CacheClient):\n        \"\"\"Create cache instance from client.\n\n        Args:\n            client (CacheClient): Cache client instance.\n\n        Returns:\n            BaseCache: Cache instance.\n        \"\"\"\n        return cls(client=client)\n\n    @classmethod\n    @abstractmethod\n    def from_config(cls, config: CacheConfig):\n        \"\"\"Create cache instance from configuration.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get(self, key: str):\n        \"\"\"Retrieve value from cache.\n\n        Args:\n            key (str): Cache key.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def set(self, key: str, value: dict):\n        \"\"\"Set value in cache.\n\n        Args:\n            key (str): Cache key.\n            value (dict): Value to cache.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def delete(self, key: str):\n        \"\"\"Delete value from cache.\n\n        Args:\n            key (str): Cache key.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.__init__","title":"<code>__init__(client)</code>","text":"<p>Initialize BaseCache.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> required Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>def __init__(self, client: CacheClient):\n    \"\"\"Initialize BaseCache.\n\n    Args:\n        client (CacheClient): Cache client instance.\n    \"\"\"\n    self.client = client\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.delete","title":"<code>delete(key)</code>  <code>abstractmethod</code>","text":"<p>Delete value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef delete(self, key: str):\n    \"\"\"Delete value from cache.\n\n    Args:\n        key (str): Cache key.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.from_client","title":"<code>from_client(client)</code>  <code>classmethod</code>","text":"<p>Create cache instance from client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> required <p>Returns:</p> Name Type Description <code>BaseCache</code> <p>Cache instance.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@classmethod\ndef from_client(cls, client: CacheClient):\n    \"\"\"Create cache instance from client.\n\n    Args:\n        client (CacheClient): Cache client instance.\n\n    Returns:\n        BaseCache: Cache instance.\n    \"\"\"\n    return cls(client=client)\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.from_config","title":"<code>from_config(config)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create cache instance from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(cls, config: CacheConfig):\n    \"\"\"Create cache instance from configuration.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.get","title":"<code>get(key)</code>  <code>abstractmethod</code>","text":"<p>Retrieve value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef get(self, key: str):\n    \"\"\"Retrieve value from cache.\n\n    Args:\n        key (str): Cache key.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.set","title":"<code>set(key, value)</code>  <code>abstractmethod</code>","text":"<p>Set value in cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>dict</code> <p>Value to cache.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef set(self, key: str, value: dict):\n    \"\"\"Set value in cache.\n\n    Args:\n        key (str): Cache key.\n        value (dict): Value to cache.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/","title":"Redis","text":""},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache","title":"<code>RedisCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>Redis cache backend implementation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>class RedisCache(BaseCache):\n    \"\"\"Redis cache backend implementation.\"\"\"\n\n    @classmethod\n    def from_config(cls, config: RedisCacheConfig):\n        \"\"\"Create RedisCache instance from configuration.\n\n        Args:\n            config (RedisCacheConfig): Redis cache configuration.\n\n        Returns:\n            RedisCache: Redis cache instance.\n        \"\"\"\n        from redis import Redis\n\n        return cls(client=Redis(**config.to_dict()))\n\n    def get(self, key: str) -&gt; Any:\n        \"\"\"Retrieve value from Redis cache.\n\n        Args:\n            key (str): Cache key.\n\n        Returns:\n            Any: Cached value.\n        \"\"\"\n        return self.client.get(key)\n\n    def set(self, key: str, value: dict, ttl: int | None = None) -&gt; Any:\n        \"\"\"Set value in Redis cache.\n\n        Args:\n            key (str): Cache key.\n            value (dict): Value to cache.\n            ttl (int | None): Time-to-live for cache entry.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        if ttl is None:\n            return self.client.set(key, value)\n        return self.client.setex(key, ttl, value)\n\n    def delete(self, key: str) -&gt; Any:\n        \"\"\"Delete value from Redis cache.\n\n        Args:\n            key (str): Cache key.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        return self.client.delete(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.delete","title":"<code>delete(key)</code>","text":"<p>Delete value from Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def delete(self, key: str) -&gt; Any:\n    \"\"\"Delete value from Redis cache.\n\n    Args:\n        key (str): Cache key.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    return self.client.delete(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create RedisCache instance from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>Redis cache configuration.</p> required <p>Returns:</p> Name Type Description <code>RedisCache</code> <p>Redis cache instance.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>@classmethod\ndef from_config(cls, config: RedisCacheConfig):\n    \"\"\"Create RedisCache instance from configuration.\n\n    Args:\n        config (RedisCacheConfig): Redis cache configuration.\n\n    Returns:\n        RedisCache: Redis cache instance.\n    \"\"\"\n    from redis import Redis\n\n    return cls(client=Redis(**config.to_dict()))\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve value from Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached value.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def get(self, key: str) -&gt; Any:\n    \"\"\"Retrieve value from Redis cache.\n\n    Args:\n        key (str): Cache key.\n\n    Returns:\n        Any: Cached value.\n    \"\"\"\n    return self.client.get(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.set","title":"<code>set(key, value, ttl=None)</code>","text":"<p>Set value in Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>dict</code> <p>Value to cache.</p> required <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entry.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def set(self, key: str, value: dict, ttl: int | None = None) -&gt; Any:\n    \"\"\"Set value in Redis cache.\n\n    Args:\n        key (str): Cache key.\n        value (dict): Value to cache.\n        ttl (int | None): Time-to-live for cache entry.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    if ttl is None:\n        return self.client.set(key, value)\n    return self.client.setex(key, ttl, value)\n</code></pre>"},{"location":"dynamiq/cache/managers/base/","title":"Base","text":""},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager","title":"<code>CacheManager</code>","text":"<p>Manager for handling cache operations.</p> <p>Attributes:</p> Name Type Description <code>CACHE_BACKENDS_BY_TYPE</code> <code>dict[CacheBackend, BaseCache]</code> <p>Mapping of backends.</p> <code>cache_backend</code> <code>BaseCache</code> <p>Selected cache backend.</p> <code>cache</code> <code>BaseCache</code> <p>Cache instance.</p> <code>serializer</code> <code>Any</code> <p>Serializer instance.</p> <code>codec</code> <code>Any</code> <p>Codec instance.</p> <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entries.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>class CacheManager:\n    \"\"\"Manager for handling cache operations.\n\n    Attributes:\n        CACHE_BACKENDS_BY_TYPE (dict[CacheBackend, BaseCache]): Mapping of backends.\n        cache_backend (BaseCache): Selected cache backend.\n        cache (BaseCache): Cache instance.\n        serializer (Any): Serializer instance.\n        codec (Any): Codec instance.\n        namespace (str | None): Cache namespace.\n        ttl (int | None): Time-to-live for cache entries.\n    \"\"\"\n    CACHE_BACKENDS_BY_TYPE: dict[CacheBackend, BaseCache] = {\n        CacheBackend.Redis: RedisCache,\n    }\n\n    def __init__(\n        self,\n        config: CacheConfig,\n        serializer: Any | None = None,\n        codec: Any | None = None,\n    ):\n        \"\"\"Initialize CacheManager.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n            serializer (Any | None): Serializer instance.\n            codec (Any | None): Codec instance.\n        \"\"\"\n        self.cache_backend = self.CACHE_BACKENDS_BY_TYPE.get(config.backend)\n        self.cache = self.cache_backend.from_config(config)\n        self.serializer = serializer or JsonSerializer()\n        self.codec = codec or Base64Codec()\n        self.namespace = config.namespace\n        self.ttl = config.ttl\n\n    def get(\n        self,\n        key: str,\n        namespace: str | None = None,\n        loads_func: Callable[[Any], Any] | None = None,\n        decode_func: Callable[[Any], Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Retrieve value from cache.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n            loads_func (Callable[[Any], Any] | None): Function to deserialize.\n            decode_func (Callable[[Any], Any] | None): Function to decode.\n\n        Returns:\n            Any: Cached value.\n        \"\"\"\n        loads = loads_func or self.serializer.loads\n        decode = decode_func or self.codec.decode\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n\n        if (res := self.cache.get(ns_key)) is not None:\n            res = loads(decode(self.cache.get(key=ns_key)))\n\n        return res\n\n    def set(\n        self,\n        key: str,\n        value: Any,\n        ttl: int | None = None,\n        namespace: str | None = None,\n        dumps_func: Callable[[Any], Any] | None = None,\n        encode_func: Callable[[Any], Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Set value in cache.\n\n        Args:\n            key (str): Cache key.\n            value (Any): Value to cache.\n            ttl (int | None): Time-to-live for cache entry.\n            namespace (str | None): Cache namespace.\n            dumps_func (Callable[[Any], Any] | None): Function to serialize.\n            encode_func (Callable[[Any], Any] | None): Function to encode.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        dumps = dumps_func or self.serializer.dumps\n        encode = encode_func or self.codec.encode\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n        ttl = ttl or self.ttl\n\n        res = self.cache.set(key=ns_key, value=encode(dumps(value)), ttl=ttl)\n\n        return res\n\n    def delete(\n        self,\n        key: str,\n        namespace: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Delete value from cache.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n        res = self.cache.delete(ns_key)\n\n        return res\n\n    def _get_namespace(self, namespace: str | None = None) -&gt; str | None:\n        \"\"\"Get effective namespace.\n\n        Args:\n            namespace (str | None): Provided namespace.\n\n        Returns:\n            str | None: Effective namespace.\n        \"\"\"\n        return namespace if namespace is not None else self.namespace\n\n    @staticmethod\n    def _get_key(key: str, namespace: str | None = None) -&gt; str:\n        \"\"\"Construct cache key with namespace.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n\n        Returns:\n            str: Namespaced cache key.\n        \"\"\"\n        if namespace is not None:\n            return f\"{namespace}:{key}\"\n        return key\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.__init__","title":"<code>__init__(config, serializer=None, codec=None)</code>","text":"<p>Initialize CacheManager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <code>serializer</code> <code>Any | None</code> <p>Serializer instance.</p> <code>None</code> <code>codec</code> <code>Any | None</code> <p>Codec instance.</p> <code>None</code> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def __init__(\n    self,\n    config: CacheConfig,\n    serializer: Any | None = None,\n    codec: Any | None = None,\n):\n    \"\"\"Initialize CacheManager.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n        serializer (Any | None): Serializer instance.\n        codec (Any | None): Codec instance.\n    \"\"\"\n    self.cache_backend = self.CACHE_BACKENDS_BY_TYPE.get(config.backend)\n    self.cache = self.cache_backend.from_config(config)\n    self.serializer = serializer or JsonSerializer()\n    self.codec = codec or Base64Codec()\n    self.namespace = config.namespace\n    self.ttl = config.ttl\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.delete","title":"<code>delete(key, namespace=None)</code>","text":"<p>Delete value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def delete(\n    self,\n    key: str,\n    namespace: str | None = None,\n) -&gt; Any:\n    \"\"\"Delete value from cache.\n\n    Args:\n        key (str): Cache key.\n        namespace (str | None): Cache namespace.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n    res = self.cache.delete(ns_key)\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.get","title":"<code>get(key, namespace=None, loads_func=None, decode_func=None)</code>","text":"<p>Retrieve value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <code>loads_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to deserialize.</p> <code>None</code> <code>decode_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to decode.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached value.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def get(\n    self,\n    key: str,\n    namespace: str | None = None,\n    loads_func: Callable[[Any], Any] | None = None,\n    decode_func: Callable[[Any], Any] | None = None,\n) -&gt; Any:\n    \"\"\"Retrieve value from cache.\n\n    Args:\n        key (str): Cache key.\n        namespace (str | None): Cache namespace.\n        loads_func (Callable[[Any], Any] | None): Function to deserialize.\n        decode_func (Callable[[Any], Any] | None): Function to decode.\n\n    Returns:\n        Any: Cached value.\n    \"\"\"\n    loads = loads_func or self.serializer.loads\n    decode = decode_func or self.codec.decode\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n\n    if (res := self.cache.get(ns_key)) is not None:\n        res = loads(decode(self.cache.get(key=ns_key)))\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.set","title":"<code>set(key, value, ttl=None, namespace=None, dumps_func=None, encode_func=None)</code>","text":"<p>Set value in cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>Any</code> <p>Value to cache.</p> required <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entry.</p> <code>None</code> <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <code>dumps_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to serialize.</p> <code>None</code> <code>encode_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to encode.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def set(\n    self,\n    key: str,\n    value: Any,\n    ttl: int | None = None,\n    namespace: str | None = None,\n    dumps_func: Callable[[Any], Any] | None = None,\n    encode_func: Callable[[Any], Any] | None = None,\n) -&gt; Any:\n    \"\"\"Set value in cache.\n\n    Args:\n        key (str): Cache key.\n        value (Any): Value to cache.\n        ttl (int | None): Time-to-live for cache entry.\n        namespace (str | None): Cache namespace.\n        dumps_func (Callable[[Any], Any] | None): Function to serialize.\n        encode_func (Callable[[Any], Any] | None): Function to encode.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    dumps = dumps_func or self.serializer.dumps\n    encode = encode_func or self.codec.encode\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n    ttl = ttl or self.ttl\n\n    res = self.cache.set(key=ns_key, value=encode(dumps(value)), ttl=ttl)\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/","title":"Workflow","text":""},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager","title":"<code>WorkflowCacheManager</code>","text":"<p>               Bases: <code>CacheManager</code></p> <p>Manager for caching workflow entity outputs.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> <code>serializer</code> <code>Any</code> <p>Serializer instance.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>class WorkflowCacheManager(CacheManager):\n    \"\"\"Manager for caching workflow entity outputs.\n\n    Attributes:\n        config (CacheConfig): Cache configuration.\n        serializer (Any): Serializer instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: CacheConfig,\n        serializer: Any | None = None,\n    ):\n        \"\"\"Initialize WorkflowCacheManager.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n            serializer (Any | None): Serializer instance.\n        \"\"\"\n        super().__init__(\n            config=config,\n            serializer=serializer,\n        )\n\n    def get_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n        \"\"\"Retrieve cached entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Cached output data.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().get(key=key)\n\n    def set_entity_output(self, entity_id: str, input_data: dict, output_data: Any, **kwargs) -&gt; Any:\n        \"\"\"Cache entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            output_data (Any): Output data to cache.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().set(key=key, value=output_data)\n\n    def delete_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n        \"\"\"Delete cached entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().delete(key=key)\n\n    def get_key(self, entity_id: str, input_data: dict, **kwargs) -&gt; str:\n        \"\"\"Generate cache key for entity.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            str: Generated cache key.\n        \"\"\"\n        input_data_formatted = format_value(self._sort_dict(input_data))\n        input_data_hash = self.hash(self.serializer.dumps(input_data_formatted))\n        kwargs_formatted = format_value(self._sort_dict(kwargs))\n        kwargs_hash = self.hash(self.serializer.dumps(kwargs_formatted))\n        return f\"{entity_id}:{input_data_hash}:{kwargs_hash}\"\n\n    @staticmethod\n    def hash(data: str) -&gt; str:\n        \"\"\"Generate SHA-256 hash of data.\n\n        Args:\n            data (str): Data to hash.\n\n        Returns:\n            str: SHA-256 hash.\n        \"\"\"\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def _sort_dict(self, d: dict) -&gt; dict:\n        \"\"\"Recursively sort dictionary keys, including nested dictionaries.\n\n        Args:\n            d (dict): Dictionary to sort.\n\n        Returns:\n            dict: Sorted dictionary.\n        \"\"\"\n        return {k: self._sort_dict(v) if isinstance(v, dict) else v for k, v in sorted(d.items())}\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.__init__","title":"<code>__init__(config, serializer=None)</code>","text":"<p>Initialize WorkflowCacheManager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <code>serializer</code> <code>Any | None</code> <p>Serializer instance.</p> <code>None</code> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def __init__(\n    self,\n    config: CacheConfig,\n    serializer: Any | None = None,\n):\n    \"\"\"Initialize WorkflowCacheManager.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n        serializer (Any | None): Serializer instance.\n    \"\"\"\n    super().__init__(\n        config=config,\n        serializer=serializer,\n    )\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.delete_entity_output","title":"<code>delete_entity_output(entity_id, input_data, **kwargs)</code>","text":"<p>Delete cached entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def delete_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n    \"\"\"Delete cached entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().delete(key=key)\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.get_entity_output","title":"<code>get_entity_output(entity_id, input_data, **kwargs)</code>","text":"<p>Retrieve cached entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached output data.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def get_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n    \"\"\"Retrieve cached entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Cached output data.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().get(key=key)\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.get_key","title":"<code>get_key(entity_id, input_data, **kwargs)</code>","text":"<p>Generate cache key for entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated cache key.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def get_key(self, entity_id: str, input_data: dict, **kwargs) -&gt; str:\n    \"\"\"Generate cache key for entity.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        str: Generated cache key.\n    \"\"\"\n    input_data_formatted = format_value(self._sort_dict(input_data))\n    input_data_hash = self.hash(self.serializer.dumps(input_data_formatted))\n    kwargs_formatted = format_value(self._sort_dict(kwargs))\n    kwargs_hash = self.hash(self.serializer.dumps(kwargs_formatted))\n    return f\"{entity_id}:{input_data_hash}:{kwargs_hash}\"\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.hash","title":"<code>hash(data)</code>  <code>staticmethod</code>","text":"<p>Generate SHA-256 hash of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Data to hash.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>SHA-256 hash.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>@staticmethod\ndef hash(data: str) -&gt; str:\n    \"\"\"Generate SHA-256 hash of data.\n\n    Args:\n        data (str): Data to hash.\n\n    Returns:\n        str: SHA-256 hash.\n    \"\"\"\n    return hashlib.sha256(data.encode()).hexdigest()\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.set_entity_output","title":"<code>set_entity_output(entity_id, input_data, output_data, **kwargs)</code>","text":"<p>Cache entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>output_data</code> <code>Any</code> <p>Output data to cache.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def set_entity_output(self, entity_id: str, input_data: dict, output_data: Any, **kwargs) -&gt; Any:\n    \"\"\"Cache entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        output_data (Any): Output data to cache.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().set(key=key, value=output_data)\n</code></pre>"},{"location":"dynamiq/callbacks/base/","title":"Base","text":""},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler","title":"<code>BaseCallbackHandler</code>","text":"<p>               Bases: <code>NodeCallbackHandler</code>, <code>ABC</code></p> <p>Abstract base class for general callback handlers.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>class BaseCallbackHandler(NodeCallbackHandler, ABC):\n    \"\"\"Abstract base class for general callback handlers.\"\"\"\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            input_data (dict[str, Any]): Input data for the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            input_data (dict[str, Any]): Input data for the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            output_data (dict[str, Any]): Output data from the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the flow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_end","title":"<code>on_flow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        output_data (dict[str, Any]): Output data from the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_error","title":"<code>on_flow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the flow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the flow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_start","title":"<code>on_flow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        input_data (dict[str, Any]): Input data for the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        input_data (dict[str, Any]): Input data for the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler","title":"<code>NodeCallbackHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for node callback handlers.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>class NodeCallbackHandler(ABC):\n    \"\"\"Abstract class for node callback handlers.\"\"\"\n\n    def on_node_start(self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_end(self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_error(self, serialized: dict[str, Any], error: BaseException, **kwargs: Any):\n        \"\"\"Called when the node errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_start(self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_end(self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_error(self, serialized: dict[str, Any], error: BaseException, **kwargs: Any):\n        \"\"\"Called when the node execute errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute runs.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_stream(self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any):\n        \"\"\"Called when the node execute streams.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            chunk (dict[str, Any] | None): Stream chunk data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_skip(\n        self, serialized: dict[str, Any], skip_data: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node skips.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_end","title":"<code>on_node_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_end(self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_error","title":"<code>on_node_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_error(self, serialized: dict[str, Any], error: BaseException, **kwargs: Any):\n    \"\"\"Called when the node errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_execute_end","title":"<code>on_node_execute_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node execute ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_end(self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_execute_error","title":"<code>on_node_execute_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node execute errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_error(self, serialized: dict[str, Any], error: BaseException, **kwargs: Any):\n    \"\"\"Called when the node execute errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_execute_run","title":"<code>on_node_execute_run(serialized, **kwargs)</code>","text":"<p>Called when the node execute runs.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute runs.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_execute_start","title":"<code>on_node_execute_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node execute starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_start(self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_execute_stream","title":"<code>on_node_execute_stream(serialized, chunk=None, **kwargs)</code>","text":"<p>Called when the node execute streams.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>chunk</code> <code>dict[str, Any] | None</code> <p>Stream chunk data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_stream(self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any):\n    \"\"\"Called when the node execute streams.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        chunk (dict[str, Any] | None): Stream chunk data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_skip","title":"<code>on_node_skip(serialized, skip_data, input_data, **kwargs)</code>","text":"<p>Called when the node skips.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_skip(\n    self, serialized: dict[str, Any], skip_data: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node skips.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.NodeCallbackHandler.on_node_start","title":"<code>on_node_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_start(self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_entity_id","title":"<code>get_entity_id(entity_name, kwargs)</code>","text":"<p>Retrieve entity ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>entity_name</code> <code>str</code> <p>Name of the entity.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Entity ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity ID is not found or invalid.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_entity_id(entity_name: str, kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve entity ID from kwargs.\n\n    Args:\n        entity_name (str): Name of the entity.\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Entity ID.\n\n    Raises:\n        ValueError: If entity ID is not found or invalid.\n    \"\"\"\n    entity_id = kwargs.get(entity_name)\n    if entity_name == \"parent_run_id\" and not entity_id:\n        return entity_id\n    if not entity_id:\n        raise ValueError(f\"{entity_name} not found\")\n\n    if isinstance(entity_id, UUID):\n        return entity_id\n    elif isinstance(entity_id, str):\n        return UUID(entity_id)\n\n    raise ValueError(f\"{entity_name} is not UUID or str\")\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_execution_run_id","title":"<code>get_execution_run_id(kwargs)</code>","text":"<p>Retrieve execution run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Execution run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_execution_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve execution run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Execution run ID.\n    \"\"\"\n    return get_entity_id(\"execution_run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_parent_run_id","title":"<code>get_parent_run_id(kwargs)</code>","text":"<p>Retrieve parent run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Parent run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_parent_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve parent run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Parent run ID.\n    \"\"\"\n    return get_entity_id(\"parent_run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_run_id","title":"<code>get_run_id(kwargs)</code>","text":"<p>Retrieve run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Run ID.\n    \"\"\"\n    return get_entity_id(\"run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/","title":"Streaming","text":""},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AgentStreamingParserCallback","title":"<code>AgentStreamingParserCallback</code>","text":"<p>               Bases: <code>BaseStreamingCallbackHandler</code></p> <p>Agent callback that parses LLM streaming output in real time and streams structured chunks.</p> <p>This callback attaches to the underlying LLM node (group == 'llms'), incrementally parses the provider streaming chunks to detect logical sections like reasoning and final answer based on the selected inference mode, and forwards the relevant segments via the <code>stream_content()</code> API with appropriate <code>step</code> labels (reasoning/answer) included.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class AgentStreamingParserCallback(BaseStreamingCallbackHandler):\n    \"\"\"Agent callback that parses LLM streaming output in real time and streams structured chunks.\n\n    This callback attaches to the underlying LLM node (group == 'llms'), incrementally parses the\n    provider streaming chunks to detect logical sections like reasoning and final answer based on\n    the selected inference mode, and forwards the relevant segments via the `stream_content()` API\n    with appropriate `step` labels (reasoning/answer) included.\n    \"\"\"\n\n    def __init__(self, agent: \"Agent\", config, loop_num: int, **kwargs):\n        self.agent = agent\n        self.config = config\n        self.loop_num = loop_num\n        self.kwargs = kwargs\n\n        # Aggregate streamed text from the LLM in the current loop for proper tracing inside the agent\n        self.accumulated_content: str = \"\"\n\n        self._buffer: str = \"\"\n        self._current_state: str | None = None\n        self._state_start_index: int = 0\n        self._state_last_emit_index: int = 0\n        self._answer_started: bool = False\n        self._state_has_emitted: dict[str, bool] = {\n            StreamingState.REASONING: False,\n            StreamingState.ANSWER: False,\n        }\n\n        # Set a tail guard to avoid streaming parts of the next tag that may arrive in next chunk\n        self._tail_guard: int = TAIL_GUARD_SIZE\n\n        self.mode_name = getattr(self.agent.inference_mode, \"name\", str(self.agent.inference_mode)).upper()\n\n    def on_node_execute_stream(self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any):\n        if not chunk or not self.agent.streaming.enabled:\n            return\n\n        # Only process the chunks from the LLM node\n        if serialized.get(\"group\") != \"llms\":\n            return\n\n        agent_run_id = kwargs.get(\"run_id\") or kwargs.get(\"parent_run_id\")\n        if agent_run_id and serialized.get(\"id\") != getattr(self.agent, \"llm\", object()).id:\n            return\n\n        if self.mode_name == InferenceMode.FUNCTION_CALLING.value:\n            text_delta, function_name = self._extract_function_calling_text(chunk)\n\n            if function_name and function_name == FINAL_ANSWER_FUNCTION_NAME:\n                self._answer_started = True\n        else:\n            text_delta = self._extract_text_delta(chunk)\n\n        if not text_delta:\n            return\n\n        self.accumulated_content += text_delta\n        self._buffer += text_delta\n\n        final_answer_only = self.agent.streaming.mode == StreamingMode.FINAL\n\n        if self.mode_name == InferenceMode.DEFAULT.value:\n            self._process_default_mode(final_answer_only)\n        elif self.mode_name == InferenceMode.XML.value:\n            self._process_xml_mode(final_answer_only)\n        elif self.mode_name == InferenceMode.STRUCTURED_OUTPUT.value:\n            self._process_structured_output_mode(final_answer_only)\n        elif self.mode_name == InferenceMode.FUNCTION_CALLING.value:\n            self._process_function_calling_mode(final_answer_only)\n\n        self._trim_buffer()\n\n    def on_node_execute_end(self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any):\n        # Clear the remaining buffer content when the LLM streaming ends\n        if serialized.get(\"group\") != \"llms\":\n            return\n\n        if not self.agent.streaming.enabled:\n            return\n\n        self._flush_buffer()\n        self._trim_buffer(force=True)\n\n    def _flush_buffer(self) -&gt; None:\n        \"\"\"Flush the remaining buffer content by streaming it as one chunk.\"\"\"\n        if not self._buffer or len(self._buffer) &lt;= self._state_last_emit_index:\n            return\n\n        if self._current_state in (StreamingState.REASONING, StreamingState.ANSWER):\n            remaining_content = self._buffer[self._state_last_emit_index :]\n            if remaining_content.strip():\n                self._emit(remaining_content, step=self._current_state)\n                self._state_last_emit_index = len(self._buffer)\n\n    def _extract_text_delta(self, chunk: dict[str, Any]) -&gt; str:\n        \"\"\"Extract textual content from streaming chunk received from the LLM.\n\n        Returns:\n            str: The extracted content.\n        \"\"\"\n        extracted_content = \"\"\n\n        choices = chunk.get(\"choices\") or []\n        if choices:\n            delta = choices[0].get(\"delta\", {})\n            content = delta.get(\"content\")\n            if isinstance(content, str):\n                extracted_content = content\n\n        if not extracted_content and isinstance(chunk.get(\"content\"), str):\n            extracted_content = chunk.get(\"content\")\n\n        return extracted_content\n\n    def _extract_function_calling_text(self, chunk: dict[str, Any]) -&gt; tuple[str, str | None]:\n        \"\"\"\n        Extract incremental JSON values (arguments) and function name\n        from the LLM streaming chunks in FUNCTION_CALLING inference mode.\n\n        Returns:\n            tuple[str, str | None]: (arguments_text, function_name)\n        \"\"\"\n        arguments_text = \"\"\n        function_name = None\n\n        choices = chunk.get(\"choices\") or []\n        if choices:\n            delta = choices[0].get(\"delta\", {})\n            tool_calls = delta.get(\"tool_calls\")\n\n            if tool_calls and len(tool_calls) &gt; 0:\n                tool_call = tool_calls[0]\n                if tool_call.get(\"type\") == \"function\" and \"function\" in tool_call:\n                    function_data = tool_call[\"function\"]\n\n                    if function_data.get(\"name\"):\n                        function_name = function_data[\"name\"]\n\n                    if function_data.get(\"arguments\"):\n                        arguments_text = function_data[\"arguments\"]\n\n        return arguments_text, function_name\n\n    def _emit(self, content: str, step: str) -&gt; None:\n        \"\"\"Emit the parsed content using the agent's stream_content method.\n\n        Args:\n            content (str): The content to stream.\n            step (str): The step to stream the content to.\n        \"\"\"\n        if not content:\n            return\n\n        # Skip streaming if in FINAL mode and not in answer step\n        if self.agent.streaming.mode == StreamingMode.FINAL and step != StreamingState.ANSWER:\n            return\n\n        if step in self._state_has_emitted:\n            if not self._state_has_emitted[step]:\n                trimmed = content.lstrip(\"\\r\\n \")\n                if not trimmed:\n                    return\n                content = trimmed\n\n        # Format content based on the step type\n        if step == StreamingState.REASONING:\n            thought_model = StreamingThought(thought=content, loop_num=self.loop_num)\n            content_to_stream = thought_model.to_dict()\n        elif step == StreamingState.ANSWER:\n            content_to_stream = content\n\n        self.agent.stream_content(\n            content=content_to_stream,\n            source=self.agent.name,\n            step=step,\n            config=self.config,\n            **(self.kwargs | {\"loop_num\": self.loop_num}),\n        )\n        if step in self._state_has_emitted:\n            self._state_has_emitted[step] = True\n\n    def _process_default_mode(self, final_answer_only: bool) -&gt; None:\n        if self._current_state is None:\n            start = self._state_last_emit_index\n            idx_thought = self._buffer.find(DefaultModeTag.THOUGHT, start) if not final_answer_only else -1\n            idx_answer = self._buffer.find(DefaultModeTag.ANSWER, start)\n\n            if not final_answer_only and idx_thought != -1 and (idx_answer == -1 or idx_thought &lt; idx_answer):\n                self._current_state = StreamingState.REASONING\n                self._state_start_index = idx_thought + len(DefaultModeTag.THOUGHT)\n                self._state_last_emit_index = self._state_start_index\n            elif idx_answer != -1:\n                self._current_state = StreamingState.ANSWER\n                self._answer_started = True\n                self._state_start_index = idx_answer + len(DefaultModeTag.ANSWER)\n                self._state_last_emit_index = self._state_start_index\n\n        # If the state was not detected, nothing to emit yet\n        if self._current_state is None:\n            return\n\n        search_start = self._state_last_emit_index\n\n        if self._current_state == StreamingState.REASONING:\n            # Check if there is a transition to Action or Answer\n            next_tag_pos = -1\n            next_tag_name = None\n            for tag_text, name in ((DefaultModeTag.ACTION, \"action\"), (DefaultModeTag.ANSWER, \"answer\")):\n                pos = self._buffer.find(tag_text, search_start)\n                if pos != -1 and (next_tag_pos == -1 or pos &lt; next_tag_pos):\n                    next_tag_pos, next_tag_name = pos, name\n\n            if next_tag_pos != -1:\n                # If a complete next tag is found, emit everything up to it\n                if next_tag_pos &gt; self._state_last_emit_index:\n                    self._emit(self._buffer[self._state_last_emit_index : next_tag_pos], step=StreamingState.REASONING)\n                if next_tag_name == \"answer\":\n                    self._current_state = StreamingState.ANSWER\n                    self._answer_started = True\n                    self._state_start_index = next_tag_pos + len(DefaultModeTag.ANSWER)\n                    self._state_last_emit_index = self._state_start_index\n                else:\n                    # Wait for the next state after `action`\n                    self._current_state = None\n                    self._state_last_emit_index = next_tag_pos + len(DefaultModeTag.ACTION)\n                return\n\n            # If there is no next tag yet, emit incrementally using a tail guard\n            safe_end = max(self._state_last_emit_index, len(self._buffer) - self._tail_guard)\n            if safe_end &gt; self._state_last_emit_index:\n                self._emit(self._buffer[self._state_last_emit_index : safe_end], step=StreamingState.REASONING)\n                self._state_last_emit_index = safe_end\n            return\n\n        # If the current state is 'answer', stream up to the end\n        safe_end = max(self._state_last_emit_index, len(self._buffer) - self._tail_guard)\n        if safe_end &gt; self._state_last_emit_index:\n            self._emit(self._buffer[self._state_last_emit_index : safe_end], step=StreamingState.ANSWER)\n            self._state_last_emit_index = safe_end\n\n    def _process_xml_mode(self, final_answer_only: bool) -&gt; None:\n        if self._current_state is None:\n            start = self._state_last_emit_index\n            idx_thought = self._buffer.find(XMLModeTag.OPEN_THOUGHT, start) if not final_answer_only else -1\n            idx_answer = self._buffer.find(XMLModeTag.OPEN_ANSWER, start)\n\n            if not final_answer_only and idx_thought != -1 and (idx_answer == -1 or idx_thought &lt; idx_answer):\n                self._current_state = StreamingState.REASONING\n                self._state_start_index = idx_thought + len(XMLModeTag.OPEN_THOUGHT)\n                self._state_last_emit_index = self._state_start_index\n            elif idx_answer != -1:\n                self._current_state = StreamingState.ANSWER\n                self._answer_started = True\n                self._state_start_index = idx_answer + len(XMLModeTag.OPEN_ANSWER)\n                self._state_last_emit_index = self._state_start_index\n\n        if self._current_state is None:\n            return\n\n        search_start = self._state_last_emit_index\n\n        if self._current_state == StreamingState.REASONING:\n            # Check for the next boundary: either &lt;/thought&gt;, &lt;action&gt;, or &lt;answer&gt;\n            next_pos = -1\n            next_tag = None\n            for tag in (XMLModeTag.CLOSE_THOUGHT, XMLModeTag.OPEN_ACTION, XMLModeTag.OPEN_ANSWER):\n                pos = self._buffer.find(tag, search_start)\n                if pos != -1 and (next_pos == -1 or pos &lt; next_pos):\n                    next_pos, next_tag = pos, tag\n\n            if next_pos != -1:\n                # Emit everything up to the next tag\n                if next_pos &gt; self._state_last_emit_index:\n                    self._emit(self._buffer[self._state_last_emit_index : next_pos], step=StreamingState.REASONING)\n\n                if next_tag == XMLModeTag.OPEN_ANSWER:\n                    self._current_state = StreamingState.ANSWER\n                    self._answer_started = True\n                    self._state_start_index = next_pos + len(XMLModeTag.OPEN_ANSWER)\n                    self._state_last_emit_index = self._state_start_index\n                else:\n                    # Stop reasoning stream due to either &lt;/thought&gt; or &lt;action&gt;\n                    self._current_state = None\n                    self._state_last_emit_index = next_pos + len(next_tag)\n                return\n\n            # If there is no next tag yet, emit incrementally using a tail guard\n            safe_end = max(self._state_last_emit_index, len(self._buffer) - self._tail_guard)\n            if safe_end &gt; self._state_last_emit_index:\n                self._emit(self._buffer[self._state_last_emit_index : safe_end], step=StreamingState.REASONING)\n                self._state_last_emit_index = safe_end\n            return\n\n        # If the current state is 'answer', stream up to the &lt;/answer&gt; tag\n        end_pos = self._buffer.find(XMLModeTag.CLOSE_ANSWER, search_start)\n        if end_pos != -1:\n            if end_pos &gt; self._state_last_emit_index:\n                self._emit(self._buffer[self._state_last_emit_index : end_pos], step=StreamingState.ANSWER)\n            # Close the answer\n            self._current_state = None\n            self._state_last_emit_index = end_pos + len(XMLModeTag.CLOSE_ANSWER)\n            return\n\n        safe_end = max(self._state_last_emit_index, len(self._buffer) - self._tail_guard)\n        if safe_end &gt; self._state_last_emit_index:\n            self._emit(self._buffer[self._state_last_emit_index : safe_end], step=StreamingState.ANSWER)\n            self._state_last_emit_index = safe_end\n\n    def _process_structured_output_mode(self, final_answer_only: bool) -&gt; None:\n        \"\"\"Process structured output mode.\"\"\"\n        self._process_json_mode(final_answer_only, is_function_calling=False)\n\n    def _process_function_calling_mode(self, final_answer_only: bool) -&gt; None:\n        \"\"\"Process function calling mode.\"\"\"\n        self._process_json_mode(final_answer_only, is_function_calling=True)\n\n    def _find_unescaped_quote_end(self, input_string: str, start_quote_index: int) -&gt; int:\n        \"\"\"\n        Return index of the next unescaped '\"' after start_quote_index, or -1 if not complete yet.\n\n        Args:\n            input_string (str): The string to search in.\n            start_quote_index (int): The index of the starting quote.\n\n        Returns:\n            int: The index of the next unescaped quote, or -1 if not found.\n        \"\"\"\n        current_index = start_quote_index + 1\n        while current_index &lt; len(input_string):\n            if input_string[current_index] == '\"':\n                # Count preceding backslashes\n                backslash_count = 0\n                previous_index = current_index - 1\n                while previous_index &gt;= 0 and input_string[previous_index] == \"\\\\\":\n                    backslash_count += 1\n                    previous_index -= 1\n                if backslash_count % 2 == 0:\n                    return current_index\n            current_index += 1\n        return -1\n\n    def _find_field_string_value_start(self, input_string: str, field_name: str, start_index: int = 0) -&gt; int:\n        \"\"\"\n        Find the index of the first character inside the opening quote of a string field value.\n        Returns -1 if field or opening quote is not fully present yet.\n\n        Args:\n            input_string (str): The string to search in.\n            field_name (str): The name of the field to search for.\n            start_index (int): The index to start searching from.\n\n        Returns:\n            int: The index of the first character inside the opening quote of the string field value,\n                 or -1 if not found.\n        \"\"\"\n        key = f'\"{field_name}\"'\n        position = input_string.find(key, start_index)\n        if position == -1:\n            return -1\n\n        colon_index = input_string.find(\":\", position + len(key))\n        if colon_index == -1:\n            return -1\n\n        # Skip the whitespace after the colon\n        index = colon_index + 1\n        while index &lt; len(input_string) and input_string[index] in WHITESPACE_PATTERNS:\n            index += 1\n        if index &gt;= len(input_string) or input_string[index] != '\"':\n            return -1\n        return index + 1\n\n    def _initialize_json_field_state(\n        self, buf: str, field_name: str, state: str, final_answer_only: bool = False\n    ) -&gt; bool:\n        \"\"\"\n        Initialize streaming state for a JSON field if not already set.\n\n        Args:\n            buf: Buffer containing JSON content\n            field_name: Name of the JSON field to look for\n            state: State to set if field is found (\"reasoning\" or \"answer\")\n            final_answer_only: Whether we're in final answer only mode\n\n        Returns:\n            bool: True if state was initialized, False otherwise\n        \"\"\"\n        if self._current_state is not None:\n            return False\n\n        # Skip reasoning fields in final answer only mode\n        if final_answer_only and state == StreamingState.REASONING:\n            return False\n\n        field_start = self._find_field_string_value_start(\n            buf, field_name, max(0, self._state_last_emit_index - FIND_JSON_FIELD_MAX_OFFSET)\n        )\n\n        # If the field is found, set the state and indices\n        if field_start != -1:\n            self._current_state = state\n            self._state_start_index = field_start\n            self._state_last_emit_index = max(self._state_last_emit_index, field_start)\n            return True\n        return False\n\n    def _process_json_mode(self, final_answer_only: bool, is_function_calling: bool = False) -&gt; None:\n        \"\"\"\n        Unified processing for JSON-like modes (structured output and function calling).\n\n        Args:\n            final_answer_only: Whether to stream only final answers\n            is_function_calling: Whether this is function calling mode (vs structured output)\n        \"\"\"\n        buf = self._buffer\n\n        if not is_function_calling and not self._answer_started:\n            # If there is a \"finish\" action, enable answer streaming\n            action_key_pos = buf.find(\n                f'\"{JSONStreamingField.ACTION.value}\"', max(0, self._state_last_emit_index - FIND_JSON_FIELD_MAX_OFFSET)\n            )\n            if action_key_pos != -1:\n                colon_pos = buf.find(\":\", action_key_pos)\n                if colon_pos != -1:\n                    v_start = self._skip_whitespace(buf, colon_pos + 1)\n                    if v_start &lt; len(buf) and buf[v_start] == '\"':\n                        end_quote = self._find_unescaped_quote_end(buf, v_start)\n                        if end_quote != -1:\n                            action_value = buf[v_start + 1 : end_quote]\n                            if action_value.strip().lower() == \"finish\":\n                                self._answer_started = True\n                                # Try to find the action_input field\n                                action_input_start = self._find_field_string_value_start(\n                                    buf, JSONStreamingField.ACTION_INPUT.value, end_quote + 1\n                                )\n                                if action_input_start != -1:\n                                    self._current_state = StreamingState.ANSWER\n                                    self._state_start_index = action_input_start\n                                    self._state_last_emit_index = max(self._state_last_emit_index, action_input_start)\n\n        self._initialize_json_field_state(\n            buf, JSONStreamingField.THOUGHT.value, StreamingState.REASONING, final_answer_only\n        )\n\n        if self._answer_started:\n            answer_field = (\n                JSONStreamingField.ANSWER.value if is_function_calling else JSONStreamingField.ACTION_INPUT.value\n            )\n            self._initialize_json_field_state(buf, answer_field, StreamingState.ANSWER)\n\n        if self._current_state == StreamingState.REASONING:\n            self._emit_json_field_content(buf, StreamingState.REASONING)\n        elif self._current_state == StreamingState.ANSWER:\n            self._emit_json_field_content(buf, StreamingState.ANSWER)\n\n    def _skip_whitespace(self, text: str, start: int) -&gt; int:\n        \"\"\"Skip whitespace characters starting from the given position.\"\"\"\n        while start &lt; len(text) and text[start] in WHITESPACE_PATTERNS:\n            start += 1\n        return start\n\n    def _emit_json_field_content(self, buf: str, step: str) -&gt; bool:\n        \"\"\"\n        Emit JSON field content in segments, handling complete and partial values.\n\n        Args:\n            buf: Buffer containing the JSON content\n            step: The streaming step (\"reasoning\" or \"answer\")\n\n        Returns:\n            bool: True if field is complete and state should be reset, False otherwise\n        \"\"\"\n        # Find the closing quote of the current JSON field\n        end_quote = self._find_unescaped_quote_end(buf, self._state_start_index - 1)\n        if end_quote != -1:\n            # If the field is complete, emit it\n            if end_quote &gt; self._state_last_emit_index:\n                segment_start = self._state_last_emit_index\n                while segment_start &lt; end_quote:\n                    segment_end = min(end_quote, segment_start + STREAMING_SEGMENT_SIZE)\n                    self._emit(buf[segment_start:segment_end], step=step)\n                    segment_start = segment_end\n                self._state_last_emit_index = end_quote\n            # Reset the state\n            self._current_state = None\n            return True\n\n        # Emit incrementally if the field is not complete\n        if len(buf) &gt; self._state_last_emit_index:\n            segment_start = self._state_last_emit_index\n            segment_end_target = len(buf)\n            while segment_start &lt; segment_end_target:\n                segment_end = min(segment_end_target, segment_start + STREAMING_SEGMENT_SIZE)\n                self._emit(buf[segment_start:segment_end], step=step)\n                segment_start = segment_end\n            self._state_last_emit_index = segment_end_target\n        return False\n\n    def _trim_buffer(self, force: bool = False) -&gt; None:\n        \"\"\"Trim already-emitted prefix of buffer to prevent re-detection.\"\"\"\n        if not self._buffer:\n            return\n\n        if self.mode_name == InferenceMode.STRUCTURED_OUTPUT.value:\n            return\n\n        if force:\n            keep_from = self._state_last_emit_index\n        else:\n            if (\n                self._current_state in (StreamingState.REASONING, StreamingState.ANSWER)\n                and self._state_start_index != -1\n            ):\n                keep_from = max(0, min(self._state_last_emit_index - self._tail_guard, self._state_start_index - 1))\n            else:\n                keep_from = max(0, self._state_last_emit_index - self._tail_guard)\n        if keep_from &lt;= 0:\n            return\n        self._buffer = self._buffer[keep_from:]\n\n        # Rebase the indices\n        self._state_start_index = max(0, self._state_start_index - keep_from)\n        self._state_last_emit_index = max(0, self._state_last_emit_index - keep_from)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler","title":"<code>AsyncStreamingIteratorCallbackHandler</code>","text":"<p>               Bases: <code>StreamingQueueCallbackHandler</code></p> <p>Callback handler for streaming events using an async iterator.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class AsyncStreamingIteratorCallbackHandler(StreamingQueueCallbackHandler):\n    \"\"\"Callback handler for streaming events using an async iterator.\"\"\"\n\n    def __init__(\n        self,\n        queue: asyncio.Queue | None = None,\n        done_event: asyncio.Event | None = None,\n        loop: asyncio.AbstractEventLoop | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize AsyncStreamingIteratorCallbackHandler.\n\n        Args:\n            queue (asyncio.Queue | None): Queue for streaming events.\n            done_event (asyncio.Event | None): Event to signal completion.\n            loop (asyncio.AbstractEventLoop | None): Event loop.\n        \"\"\"\n        if queue is None:\n            queue = asyncio.Queue()\n        if done_event is None:\n            done_event = asyncio.Event()\n        super().__init__(queue, done_event)\n        self._iterator = self._iter_queue_events()\n        self.loop = loop or asyncio.get_event_loop()\n\n    def send_to_queue(self, event: StreamingEventMessage):\n        \"\"\"Send the event to the queue.\"\"\"\n        asyncio.run_coroutine_threadsafe(self.queue.put(event), self.loop)\n\n    async def _iter_queue_events(self) -&gt; AsyncIterator[StreamingEventMessage]:\n        \"\"\"Async iterate over queue events.\n\n        Returns:\n            AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n        \"\"\"\n        try:\n            while not self.queue.empty() or not self.done_event.is_set():\n                event = await self.queue.get()\n                yield event\n        except Exception as e:\n            logger.error(f\"Event streaming failed. Error: {e}\")\n\n    async def __anext__(self) -&gt; StreamingEventMessage:\n        \"\"\"Get the next async streaming event.\n\n        Returns:\n            StreamingEventMessage: Next async streaming event.\n        \"\"\"\n        return await self._iterator.__anext__()\n\n    async def __aiter__(self) -&gt; AsyncIterator[StreamingEventMessage]:\n        \"\"\"Get the async iterator for streaming events.\n\n        Returns:\n            AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n        \"\"\"\n        async for item in self._iterator:\n            yield item\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":"<p>Get the async iterator for streaming events.</p> <p>Returns:</p> Type Description <code>AsyncIterator[StreamingEventMessage]</code> <p>AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>async def __aiter__(self) -&gt; AsyncIterator[StreamingEventMessage]:\n    \"\"\"Get the async iterator for streaming events.\n\n    Returns:\n        AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n    \"\"\"\n    async for item in self._iterator:\n        yield item\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__anext__","title":"<code>__anext__()</code>  <code>async</code>","text":"<p>Get the next async streaming event.</p> <p>Returns:</p> Name Type Description <code>StreamingEventMessage</code> <code>StreamingEventMessage</code> <p>Next async streaming event.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>async def __anext__(self) -&gt; StreamingEventMessage:\n    \"\"\"Get the next async streaming event.\n\n    Returns:\n        StreamingEventMessage: Next async streaming event.\n    \"\"\"\n    return await self._iterator.__anext__()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None, loop=None)</code>","text":"<p>Initialize AsyncStreamingIteratorCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | None</code> <p>Event to signal completion.</p> <code>None</code> <code>loop</code> <code>AbstractEventLoop | None</code> <p>Event loop.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: asyncio.Queue | None = None,\n    done_event: asyncio.Event | None = None,\n    loop: asyncio.AbstractEventLoop | None = None,\n) -&gt; None:\n    \"\"\"Initialize AsyncStreamingIteratorCallbackHandler.\n\n    Args:\n        queue (asyncio.Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | None): Event to signal completion.\n        loop (asyncio.AbstractEventLoop | None): Event loop.\n    \"\"\"\n    if queue is None:\n        queue = asyncio.Queue()\n    if done_event is None:\n        done_event = asyncio.Event()\n    super().__init__(queue, done_event)\n    self._iterator = self._iter_queue_events()\n    self.loop = loop or asyncio.get_event_loop()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.send_to_queue","title":"<code>send_to_queue(event)</code>","text":"<p>Send the event to the queue.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def send_to_queue(self, event: StreamingEventMessage):\n    \"\"\"Send the event to the queue.\"\"\"\n    asyncio.run_coroutine_threadsafe(self.queue.put(event), self.loop)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.BaseStreamingCallbackHandler","title":"<code>BaseStreamingCallbackHandler</code>","text":"<p>               Bases: <code>BaseCallbackHandler</code></p> <p>Base callback handler for streaming events.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class BaseStreamingCallbackHandler(BaseCallbackHandler):\n    \"\"\"Base callback handler for streaming events.\"\"\"\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.DefaultModeTag","title":"<code>DefaultModeTag</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of default mode tags.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class DefaultModeTag(str, Enum):\n    \"\"\"\n    Enumeration of default mode tags.\n    \"\"\"\n\n    THOUGHT = \"Thought:\"\n    ACTION = \"Action:\"\n    ANSWER = \"Answer:\"\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.InferenceMode","title":"<code>InferenceMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of inference types.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class InferenceMode(str, Enum):\n    \"\"\"\n    Enumeration of inference types.\n    \"\"\"\n\n    DEFAULT = \"DEFAULT\"\n    XML = \"XML\"\n    FUNCTION_CALLING = \"FUNCTION_CALLING\"\n    STRUCTURED_OUTPUT = \"STRUCTURED_OUTPUT\"\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.JSONStreamingField","title":"<code>JSONStreamingField</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of JSON streaming fields in FUNCTION_CALLING mode.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class JSONStreamingField(str, Enum):\n    \"\"\"\n    Enumeration of JSON streaming fields in FUNCTION_CALLING mode.\n    \"\"\"\n\n    THOUGHT = \"thought\"\n    ACTION = \"action\"\n    ACTION_INPUT = \"action_input\"\n    ANSWER = \"answer\"\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler","title":"<code>StreamingIteratorCallbackHandler</code>","text":"<p>               Bases: <code>StreamingQueueCallbackHandler</code></p> <p>Callback handler for streaming events using an iterator.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class StreamingIteratorCallbackHandler(StreamingQueueCallbackHandler):\n    \"\"\"Callback handler for streaming events using an iterator.\"\"\"\n\n    def __init__(\n        self,\n        queue: Queue | None = None,\n        done_event: threading.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize StreamingIteratorCallbackHandler.\n\n        Args:\n            queue (Queue | None): Queue for streaming events.\n            done_event (threading.Event | None): Event to signal completion.\n        \"\"\"\n        if queue is None:\n            queue = Queue()\n        if done_event is None:\n            done_event = threading.Event()\n        super().__init__(queue, done_event)\n        self._iterator = self._iter_queue_events()\n\n    def _iter_queue_events(self) -&gt; Iterator[StreamingEventMessage]:\n        \"\"\"Iterate over queue events.\n\n        Returns:\n            Iterator[StreamingEventMessage]: Iterator for streaming events.\n        \"\"\"\n        try:\n            while not self.queue.empty() or not self.done_event.is_set():\n                event = self.queue.get()\n                yield event\n        except Exception as e:\n            logger.error(f\"Event streaming failed. Error: {e}\")\n\n    def __next__(self) -&gt; StreamingEventMessage:\n        \"\"\"Get the next streaming event.\n\n        Returns:\n            StreamingEventMessage: Next streaming event.\n        \"\"\"\n        return self._iterator.__next__()\n\n    def __iter__(self) -&gt; Iterator[StreamingEventMessage]:\n        \"\"\"Get the iterator for streaming events.\n\n        Returns:\n            Iterator[StreamingEventMessage]: Iterator for streaming events.\n        \"\"\"\n        yield from self._iterator\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None)</code>","text":"<p>Initialize StreamingIteratorCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | None</code> <p>Event to signal completion.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: Queue | None = None,\n    done_event: threading.Event | None = None,\n) -&gt; None:\n    \"\"\"Initialize StreamingIteratorCallbackHandler.\n\n    Args:\n        queue (Queue | None): Queue for streaming events.\n        done_event (threading.Event | None): Event to signal completion.\n    \"\"\"\n    if queue is None:\n        queue = Queue()\n    if done_event is None:\n        done_event = threading.Event()\n    super().__init__(queue, done_event)\n    self._iterator = self._iter_queue_events()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__iter__","title":"<code>__iter__()</code>","text":"<p>Get the iterator for streaming events.</p> <p>Returns:</p> Type Description <code>Iterator[StreamingEventMessage]</code> <p>Iterator[StreamingEventMessage]: Iterator for streaming events.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __iter__(self) -&gt; Iterator[StreamingEventMessage]:\n    \"\"\"Get the iterator for streaming events.\n\n    Returns:\n        Iterator[StreamingEventMessage]: Iterator for streaming events.\n    \"\"\"\n    yield from self._iterator\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__next__","title":"<code>__next__()</code>","text":"<p>Get the next streaming event.</p> <p>Returns:</p> Name Type Description <code>StreamingEventMessage</code> <code>StreamingEventMessage</code> <p>Next streaming event.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __next__(self) -&gt; StreamingEventMessage:\n    \"\"\"Get the next streaming event.\n\n    Returns:\n        StreamingEventMessage: Next streaming event.\n    \"\"\"\n    return self._iterator.__next__()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler","title":"<code>StreamingQueueCallbackHandler</code>","text":"<p>               Bases: <code>BaseStreamingCallbackHandler</code></p> <p>Callback handler for streaming events to a queue.</p> <p>Attributes:</p> Name Type Description <code>queue</code> <code>Queue | Queue | None</code> <p>Queue for streaming events.</p> <code>done_event</code> <code>Event | Event | None</code> <p>Event to signal completion.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class StreamingQueueCallbackHandler(BaseStreamingCallbackHandler):\n    \"\"\"Callback handler for streaming events to a queue.\n\n    Attributes:\n        queue (asyncio.Queue | Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n    \"\"\"\n\n    def __init__(\n        self,\n        queue: asyncio.Queue | Queue | None = None,\n        done_event: asyncio.Event | threading.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize StreamingQueueCallbackHandler.\n\n        Args:\n            queue (asyncio.Queue | Queue | None): Queue for streaming events.\n            done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n        \"\"\"\n        self.queue = queue\n        self.done_event = done_event\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            prompts (list[str]): List of prompts.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        self.done_event.clear()\n\n    def on_node_execute_stream(\n        self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the node execute streams.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            chunk (dict[str, Any] | None): Stream chunk data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        event = kwargs.get(\"event\") or StreamingEventMessage(\n            run_id=str(get_run_id(kwargs)),\n            wf_run_id=kwargs.get(\"wf_run_id\"),\n            entity_id=serialized.get(\"id\"),\n            data=format_value(chunk),\n            event=serialized.get(\"streaming\", {}).get(\"event\"),\n            source=StreamingEntitySource(\n                id=serialized.get(\"id\"),\n                name=serialized.get(\"name\", None),\n                group=serialized.get(\"group\", None),\n                type=serialized.get(\"type\", None),\n            ),\n        )\n        self.send_to_queue(event)\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        event = StreamingEventMessage(\n            run_id=str(get_run_id(kwargs)),\n            wf_run_id=kwargs.get(\"wf_run_id\"),\n            entity_id=serialized.get(\"id\"),\n            data=format_value(output_data),\n            event=serialized.get(\"streaming\", {}).get(\"event\"),\n            source=StreamingEntitySource(\n                id=serialized.get(\"id\"),\n                name=serialized.get(\"name\", None),\n                group=serialized.get(\"group\", None),\n                type=serialized.get(\"type\", None),\n            ),\n        )\n        self.send_to_queue(event)\n        self.done_event.set()\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        event = StreamingEventMessage(\n            run_id=str(get_run_id(kwargs)),\n            wf_run_id=kwargs.get(\"wf_run_id\"),\n            entity_id=serialized.get(\"id\"),\n            data={\"error\": str(error)},\n            event=serialized.get(\"streaming\", {}).get(\"event\"),\n            source=StreamingEntitySource(\n                id=serialized.get(\"id\"),\n                name=serialized.get(\"name\", None),\n                group=serialized.get(\"group\", None),\n                type=serialized.get(\"type\", None),\n            ),\n        )\n        self.send_to_queue(event)\n        self.done_event.set()\n\n    def send_to_queue(self, event: StreamingEventMessage):\n        \"\"\"Send the event to the queue.\"\"\"\n        self.queue.put_nowait(event)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None)</code>","text":"<p>Initialize StreamingQueueCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | Event | None</code> <p>Event to signal completion.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: asyncio.Queue | Queue | None = None,\n    done_event: asyncio.Event | threading.Event | None = None,\n) -&gt; None:\n    \"\"\"Initialize StreamingQueueCallbackHandler.\n\n    Args:\n        queue (asyncio.Queue | Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n    \"\"\"\n    self.queue = queue\n    self.done_event = done_event\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_node_execute_stream","title":"<code>on_node_execute_stream(serialized, chunk=None, **kwargs)</code>","text":"<p>Called when the node execute streams.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>chunk</code> <code>dict[str, Any] | None</code> <p>Stream chunk data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_node_execute_stream(\n    self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the node execute streams.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        chunk (dict[str, Any] | None): Stream chunk data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    event = kwargs.get(\"event\") or StreamingEventMessage(\n        run_id=str(get_run_id(kwargs)),\n        wf_run_id=kwargs.get(\"wf_run_id\"),\n        entity_id=serialized.get(\"id\"),\n        data=format_value(chunk),\n        event=serialized.get(\"streaming\", {}).get(\"event\"),\n        source=StreamingEntitySource(\n            id=serialized.get(\"id\"),\n            name=serialized.get(\"name\", None),\n            group=serialized.get(\"group\", None),\n            type=serialized.get(\"type\", None),\n        ),\n    )\n    self.send_to_queue(event)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    event = StreamingEventMessage(\n        run_id=str(get_run_id(kwargs)),\n        wf_run_id=kwargs.get(\"wf_run_id\"),\n        entity_id=serialized.get(\"id\"),\n        data=format_value(output_data),\n        event=serialized.get(\"streaming\", {}).get(\"event\"),\n        source=StreamingEntitySource(\n            id=serialized.get(\"id\"),\n            name=serialized.get(\"name\", None),\n            group=serialized.get(\"group\", None),\n            type=serialized.get(\"type\", None),\n        ),\n    )\n    self.send_to_queue(event)\n    self.done_event.set()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    event = StreamingEventMessage(\n        run_id=str(get_run_id(kwargs)),\n        wf_run_id=kwargs.get(\"wf_run_id\"),\n        entity_id=serialized.get(\"id\"),\n        data={\"error\": str(error)},\n        event=serialized.get(\"streaming\", {}).get(\"event\"),\n        source=StreamingEntitySource(\n            id=serialized.get(\"id\"),\n            name=serialized.get(\"name\", None),\n            group=serialized.get(\"group\", None),\n            type=serialized.get(\"type\", None),\n        ),\n    )\n    self.send_to_queue(event)\n    self.done_event.set()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, prompts, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>prompts</code> <code>list[str]</code> <p>List of prompts.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        prompts (list[str]): List of prompts.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    self.done_event.clear()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.send_to_queue","title":"<code>send_to_queue(event)</code>","text":"<p>Send the event to the queue.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def send_to_queue(self, event: StreamingEventMessage):\n    \"\"\"Send the event to the queue.\"\"\"\n    self.queue.put_nowait(event)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingState","title":"<code>StreamingState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of streaming states.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class StreamingState(str, Enum):\n    \"\"\"\n    Enumeration of streaming states.\n    \"\"\"\n\n    REASONING = \"reasoning\"\n    ANSWER = \"answer\"\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.XMLModeTag","title":"<code>XMLModeTag</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of XML mode tags.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class XMLModeTag(str, Enum):\n    \"\"\"\n    Enumeration of XML mode tags.\n    \"\"\"\n\n    OPEN_THOUGHT = \"&lt;thought&gt;\"\n    CLOSE_THOUGHT = \"&lt;/thought&gt;\"\n    OPEN_ACTION = \"&lt;action&gt;\"\n    OPEN_ANSWER = \"&lt;answer&gt;\"\n    CLOSE_ANSWER = \"&lt;/answer&gt;\"\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/","title":"Tracing","text":""},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ExecutionRun","title":"<code>ExecutionRun</code>  <code>dataclass</code>","text":"<p>Data class for execution run details.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>UUID</code> <p>Execution run ID.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the execution.</p> <code>end_time</code> <code>datetime | None</code> <p>End time of the execution.</p> <code>status</code> <code>RunStatus | None</code> <p>Status of the execution.</p> <code>input</code> <code>Any | None</code> <p>Input data for the execution.</p> <code>output</code> <code>Any | None</code> <p>Output data from the execution.</p> <code>error</code> <code>Any | None</code> <p>Error details if any.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>@dataclass\nclass ExecutionRun:\n    \"\"\"Data class for execution run details.\n\n    Attributes:\n        id (UUID): Execution run ID.\n        start_time (datetime): Start time of the execution.\n        end_time (datetime | None): End time of the execution.\n        status (RunStatus | None): Status of the execution.\n        input (Any | None): Input data for the execution.\n        output (Any | None): Output data from the execution.\n        error (Any | None): Error details if any.\n        metadata (dict): Additional metadata.\n    \"\"\"\n    id: UUID\n    start_time: datetime\n    end_time: datetime | None = None\n    status: RunStatus | None = None\n    input: Any | None = None\n    output: Any | None = None\n    error: Any | None = None\n    metadata: dict = field(default_factory=dict)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert ExecutionRun to dictionary.\n\n        Returns:\n            dict: Dictionary representation of ExecutionRun.\n        \"\"\"\n        return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ExecutionRun.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert ExecutionRun to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of ExecutionRun.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert ExecutionRun to dictionary.\n\n    Returns:\n        dict: Dictionary representation of ExecutionRun.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run","title":"<code>Run</code>  <code>dataclass</code>","text":"<p>Data class for run details.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>UUID</code> <p>Run ID.</p> <code>name</code> <code>str</code> <p>Name of the run.</p> <code>type</code> <code>RunType</code> <p>Type of the run.</p> <code>trace_id</code> <code>UUID | str</code> <p>Trace ID.</p> <code>source_id</code> <code>UUID | str</code> <p>Source ID.</p> <code>session_id</code> <code>UUID | str</code> <p>Session ID.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the run.</p> <code>end_time</code> <code>datetime</code> <p>End time of the run.</p> <code>parent_run_id</code> <code>UUID</code> <p>Parent run ID.</p> <code>status</code> <code>RunStatus</code> <p>Status of the run.</p> <code>input</code> <code>Any</code> <p>Input data for the run.</p> <code>output</code> <code>Any</code> <p>Output data from the run.</p> <code>metadata</code> <code>Any</code> <p>Additional metadata.</p> <code>error</code> <code>Any</code> <p>Error details if any.</p> <code>executions</code> <code>list[ExecutionRun]</code> <p>List of execution runs.</p> <code>tags</code> <code>list[str]</code> <p>List of tags.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>@dataclass\nclass Run:\n    \"\"\"Data class for run details.\n\n    Attributes:\n        id (UUID): Run ID.\n        name (str): Name of the run.\n        type (RunType): Type of the run.\n        trace_id (UUID | str): Trace ID.\n        source_id (UUID | str): Source ID.\n        session_id (UUID | str): Session ID.\n        start_time (datetime): Start time of the run.\n        end_time (datetime): End time of the run.\n        parent_run_id (UUID): Parent run ID.\n        status (RunStatus): Status of the run.\n        input (Any): Input data for the run.\n        output (Any): Output data from the run.\n        metadata (Any): Additional metadata.\n        error (Any): Error details if any.\n        executions (list[ExecutionRun]): List of execution runs.\n        tags (list[str]): List of tags.\n    \"\"\"\n    id: UUID\n    name: str\n    type: RunType\n    trace_id: UUID | str\n    source_id: UUID | str\n    session_id: UUID | str\n    start_time: datetime\n    end_time: datetime = None\n    parent_run_id: UUID = None\n    status: RunStatus = None\n    input: Any = None\n    output: Any = None\n    metadata: Any = None\n    error: Any = None\n    executions: list[ExecutionRun] = field(default_factory=list)\n    tags: list[str] = field(default_factory=list)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert Run to dictionary.\n\n        Returns:\n            dict: Dictionary representation of Run.\n        \"\"\"\n        return asdict(self)\n\n    def to_json(self) -&gt; str:\n        \"\"\"Convert Run to JSON string.\n\n        Returns:\n            str: JSON string representation of Run.\n        \"\"\"\n        return json.dumps(self.to_dict(), cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Run to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of Run.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert Run to dictionary.\n\n    Returns:\n        dict: Dictionary representation of Run.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run.to_json","title":"<code>to_json()</code>","text":"<p>Convert Run to JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string representation of Run.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert Run to JSON string.\n\n    Returns:\n        str: JSON string representation of Run.\n    \"\"\"\n    return json.dumps(self.to_dict(), cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for run statuses.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class RunStatus(str, Enum):\n    \"\"\"Enumeration for run statuses.\"\"\"\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.RunType","title":"<code>RunType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for run types.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class RunType(str, Enum):\n    \"\"\"Enumeration for run types.\"\"\"\n    WORKFLOW = \"workflow\"\n    FLOW = \"flow\"\n    NODE = \"node\"\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler","title":"<code>TracingCallbackHandler</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>BaseCallbackHandler</code></p> <p>Callback handler for tracing workflow events.</p> <p>Attributes:</p> Name Type Description <code>source_id</code> <code>str | None</code> <p>Source ID.</p> <code>trace_id</code> <code>str | None</code> <p>Trace ID.</p> <code>session_id</code> <code>str | None</code> <p>Session ID.</p> <code>client</code> <code>BaseTracingClient | None</code> <p>Tracing client.</p> <code>runs</code> <code>dict[UUID, Run]</code> <p>Dictionary of runs.</p> <code>tags</code> <code>list[str]</code> <p>List of tags.</p> <code>installed_pkgs</code> <code>list[str]</code> <p>List of installed packages.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class TracingCallbackHandler(BaseModel, BaseCallbackHandler):\n    \"\"\"Callback handler for tracing workflow events.\n\n    Attributes:\n        source_id (str | None): Source ID.\n        trace_id (str | None): Trace ID.\n        session_id (str | None): Session ID.\n        client (BaseTracingClient | None): Tracing client.\n        runs (dict[UUID, Run]): Dictionary of runs.\n        tags (list[str]): List of tags.\n        installed_pkgs (list[str]): List of installed packages.\n    \"\"\"\n    source_id: str | None = Field(default_factory=generate_uuid)\n    trace_id: str | None = Field(default_factory=generate_uuid)\n    session_id: str | None = Field(default_factory=generate_uuid)\n    client: BaseTracingClient | None = None\n    runs: dict[UUID, Run] = {}\n    tags: list[str] = []\n    metadata: dict = {}\n\n    installed_pkgs: list[str] = Field(\n        [\"dynamiq\"],\n        description=\"List of installed packages to include in the host information.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @cached_property\n    def host(self) -&gt; dict:\n        \"\"\"Get host information.\n\n        Returns:\n            dict: Host information including installed packages.\n        \"\"\"\n        return {\n            \"installed_pkgs\": [\n                {\"name\": dist.metadata[\"Name\"], \"version\": dist.version}\n                for dist in distributions()\n                if dist.metadata.get(\"Name\") in self.installed_pkgs\n            ],\n        }\n\n    def _get_node_base_run(self, serialized: dict[str, Any], **kwargs: Any) -&gt; Run:\n        \"\"\"Get base run details for a node.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n\n        Returns:\n            Run: Base run details for the node.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        parent_run_id = get_parent_run_id(kwargs)\n\n        from dynamiq.nodes import NodeGroup\n\n        # Handle runtime LLM prompt override\n        if serialized.get(\"group\") == NodeGroup.LLMS:\n            prompt = kwargs.get(\"prompt\") or serialized.get(\"prompt\")\n            if isinstance(prompt, BaseModel):\n                prompt = prompt.model_dump()\n            serialized[\"prompt\"] = prompt\n\n        run = Run(\n            id=run_id,\n            name=serialized.get(\"name\"),\n            type=RunType.NODE,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            parent_run_id=parent_run_id,\n            metadata={\n                \"node\": serialized,\n                \"run_depends\": kwargs.get(\"run_depends\", []),\n                **self.metadata,\n            },\n            tags=self.tags,\n            input=format_value(kwargs.get(\"input_data\"), for_tracing=True),\n        )\n        return run\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            input_data (dict[str, Any]): Input data for the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n\n        self.runs[run_id] = Run(\n            id=run_id,\n            name=\"Workflow\",\n            type=RunType.WORKFLOW,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            input=format_value(input_data, for_tracing=True),\n            metadata={\n                \"workflow\": {\"id\": serialized.get(\"id\"), \"version\": serialized.get(\"version\")},\n                **self.metadata,\n            },\n            tags=self.tags,\n        )\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data, for_tracing=True)\n        run.status = RunStatus.SUCCEEDED\n\n        self.flush()\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n\n        failed_nodes = kwargs.get(\"failed_nodes\", [])\n        failed_nodes_info = [node.to_dict() if hasattr(node, \"to_dict\") else node for node in failed_nodes]\n        if failed_nodes_info:\n            run.metadata[\"failed_nodes\"] = failed_nodes_info\n\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n        self.flush()\n\n    def on_flow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            input_data (dict[str, Any]): Input data for the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        parent_run_id = get_parent_run_id(kwargs)\n\n        self.runs[run_id] = Run(\n            id=run_id,\n            name=\"Flow\",\n            type=RunType.FLOW,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            parent_run_id=parent_run_id,\n            input=format_value(input_data, for_tracing=True),\n            metadata={\"flow\": {\"id\": serialized.get(\"id\")}, **self.metadata},\n            tags=self.tags,\n        )\n\n    def on_flow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            output_data (dict[str, Any]): Output data from the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data, for_tracing=True)\n        run.status = RunStatus.SUCCEEDED\n        # If parent_run_id is None, the run is the highest in the execution tree\n        if run.parent_run_id is None:\n            self.flush()\n\n    def on_flow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the flow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n\n        failed_nodes = kwargs.get(\"failed_nodes\", [])\n        failed_nodes_info = [node.to_dict() if hasattr(node, \"to_dict\") else node for node in failed_nodes]\n        if failed_nodes_info:\n            run.metadata[\"failed_nodes\"] = failed_nodes_info\n\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n        # If parent_run_id is None, the run is the highest in the execution tree\n        if run.parent_run_id is None:\n            self.flush()\n\n    def on_node_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        run = self._get_node_base_run(serialized, **kwargs)\n        run.input = format_value(input_data, for_tracing=True)\n        self.runs[run_id] = run\n\n    def on_node_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data, for_tracing=True)\n        run.status = RunStatus.SUCCEEDED\n        run.metadata[\"is_output_from_cache\"] = kwargs.get(\"is_output_from_cache\", False)\n        # If parent_run_id is None, the run is the highest in the execution tree\n        if run.parent_run_id is None:\n            self.flush()\n\n    def on_node_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        if (run := self.runs.get(run_id)) is None:\n            run = self._get_node_base_run(serialized, **kwargs)\n            self.runs[run_id] = run\n\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n        # If parent_run_id is None, the run is the highest in the execution tree\n        if run.parent_run_id is None:\n            self.flush()\n\n    def on_node_skip(\n        self,\n        serialized: dict[str, Any],\n        skip_data: dict[str, Any],\n        input_data: dict[str, Any],\n        human_feedback: str | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Called when the node skips.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            human_feedback (str | None): Human feedback.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        if (run := self.runs.get(run_id)) is None:\n            run = self._get_node_base_run(serialized, **kwargs)\n            self.runs[run_id] = run\n\n        run.input = format_value(input_data, for_tracing=True)\n        run.end_time = run.start_time\n        run.status = RunStatus.SKIPPED\n        run.metadata[\"skip\"] = format_value(skip_data)\n        if human_feedback:\n            run.metadata[\"human_feedback\"] = human_feedback\n\n        # If parent_run_id is None, the run is the highest in the execution tree\n        if run.parent_run_id is None:\n            self.flush()\n\n    def on_node_execute_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution_run_id = get_execution_run_id(kwargs)\n        execution = ExecutionRun(\n            id=execution_run_id,\n            start_time=datetime.now(UTC),\n            metadata={},\n        )\n        run.executions.append(execution)\n\n    def on_node_execute_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n        execution.end_time = datetime.now(UTC)\n        execution.status = RunStatus.SUCCEEDED\n\n    def on_node_execute_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node execute errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n        execution.end_time = datetime.now(UTC)\n        execution.status = RunStatus.FAILED\n        execution.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n    def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute runs.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        if usage := kwargs.get(\"usage_data\"):\n            run.metadata[\"usage\"] = usage\n\n        if prompt_messages := kwargs.get(\"prompt_messages\"):\n            run.metadata[\"node\"][\"prompt\"][\"messages\"] = prompt_messages\n\n        if tool_data := kwargs.get(\"tool_data\"):\n            run.metadata[\"tool_data\"] = tool_data\n\n    def flush(self):\n        \"\"\"Flush the runs to the tracing client.\"\"\"\n        if self.client:\n            self.client.trace([run for run in self.runs.values()])\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.host","title":"<code>host: dict</code>  <code>cached</code> <code>property</code>","text":"<p>Get host information.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Host information including installed packages.</p>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.flush","title":"<code>flush()</code>","text":"<p>Flush the runs to the tracing client.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def flush(self):\n    \"\"\"Flush the runs to the tracing client.\"\"\"\n    if self.client:\n        self.client.trace([run for run in self.runs.values()])\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_end","title":"<code>on_flow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        output_data (dict[str, Any]): Output data from the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data, for_tracing=True)\n    run.status = RunStatus.SUCCEEDED\n    # If parent_run_id is None, the run is the highest in the execution tree\n    if run.parent_run_id is None:\n        self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_error","title":"<code>on_flow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the flow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the flow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n\n    failed_nodes = kwargs.get(\"failed_nodes\", [])\n    failed_nodes_info = [node.to_dict() if hasattr(node, \"to_dict\") else node for node in failed_nodes]\n    if failed_nodes_info:\n        run.metadata[\"failed_nodes\"] = failed_nodes_info\n\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n\n    # If parent_run_id is None, the run is the highest in the execution tree\n    if run.parent_run_id is None:\n        self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_start","title":"<code>on_flow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        input_data (dict[str, Any]): Input data for the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    parent_run_id = get_parent_run_id(kwargs)\n\n    self.runs[run_id] = Run(\n        id=run_id,\n        name=\"Flow\",\n        type=RunType.FLOW,\n        trace_id=self.trace_id,\n        source_id=self.source_id,\n        session_id=self.session_id,\n        start_time=datetime.now(UTC),\n        parent_run_id=parent_run_id,\n        input=format_value(input_data, for_tracing=True),\n        metadata={\"flow\": {\"id\": serialized.get(\"id\")}, **self.metadata},\n        tags=self.tags,\n    )\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_end","title":"<code>on_node_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data, for_tracing=True)\n    run.status = RunStatus.SUCCEEDED\n    run.metadata[\"is_output_from_cache\"] = kwargs.get(\"is_output_from_cache\", False)\n    # If parent_run_id is None, the run is the highest in the execution tree\n    if run.parent_run_id is None:\n        self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_error","title":"<code>on_node_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    if (run := self.runs.get(run_id)) is None:\n        run = self._get_node_base_run(serialized, **kwargs)\n        self.runs[run_id] = run\n\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n\n    # If parent_run_id is None, the run is the highest in the execution tree\n    if run.parent_run_id is None:\n        self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_end","title":"<code>on_node_execute_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node execute ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n    execution.end_time = datetime.now(UTC)\n    execution.status = RunStatus.SUCCEEDED\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_error","title":"<code>on_node_execute_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node execute errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node execute errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n    execution.end_time = datetime.now(UTC)\n    execution.status = RunStatus.FAILED\n    execution.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_run","title":"<code>on_node_execute_run(serialized, **kwargs)</code>","text":"<p>Called when the node execute runs.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute runs.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    if usage := kwargs.get(\"usage_data\"):\n        run.metadata[\"usage\"] = usage\n\n    if prompt_messages := kwargs.get(\"prompt_messages\"):\n        run.metadata[\"node\"][\"prompt\"][\"messages\"] = prompt_messages\n\n    if tool_data := kwargs.get(\"tool_data\"):\n        run.metadata[\"tool_data\"] = tool_data\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_start","title":"<code>on_node_execute_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node execute starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution_run_id = get_execution_run_id(kwargs)\n    execution = ExecutionRun(\n        id=execution_run_id,\n        start_time=datetime.now(UTC),\n        metadata={},\n    )\n    run.executions.append(execution)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_skip","title":"<code>on_node_skip(serialized, skip_data, input_data, human_feedback=None, **kwargs)</code>","text":"<p>Called when the node skips.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>human_feedback</code> <code>str | None</code> <p>Human feedback.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_skip(\n    self,\n    serialized: dict[str, Any],\n    skip_data: dict[str, Any],\n    input_data: dict[str, Any],\n    human_feedback: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Called when the node skips.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        human_feedback (str | None): Human feedback.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    if (run := self.runs.get(run_id)) is None:\n        run = self._get_node_base_run(serialized, **kwargs)\n        self.runs[run_id] = run\n\n    run.input = format_value(input_data, for_tracing=True)\n    run.end_time = run.start_time\n    run.status = RunStatus.SKIPPED\n    run.metadata[\"skip\"] = format_value(skip_data)\n    if human_feedback:\n        run.metadata[\"human_feedback\"] = human_feedback\n\n    # If parent_run_id is None, the run is the highest in the execution tree\n    if run.parent_run_id is None:\n        self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_start","title":"<code>on_node_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    run = self._get_node_base_run(serialized, **kwargs)\n    run.input = format_value(input_data, for_tracing=True)\n    self.runs[run_id] = run\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data, for_tracing=True)\n    run.status = RunStatus.SUCCEEDED\n\n    self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n\n    failed_nodes = kwargs.get(\"failed_nodes\", [])\n    failed_nodes_info = [node.to_dict() if hasattr(node, \"to_dict\") else node for node in failed_nodes]\n    if failed_nodes_info:\n        run.metadata[\"failed_nodes\"] = failed_nodes_info\n\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n\n    self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        input_data (dict[str, Any]): Input data for the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n\n    self.runs[run_id] = Run(\n        id=run_id,\n        name=\"Workflow\",\n        type=RunType.WORKFLOW,\n        trace_id=self.trace_id,\n        source_id=self.source_id,\n        session_id=self.session_id,\n        start_time=datetime.now(UTC),\n        input=format_value(input_data, for_tracing=True),\n        metadata={\n            \"workflow\": {\"id\": serialized.get(\"id\"), \"version\": serialized.get(\"version\")},\n            **self.metadata,\n        },\n        tags=self.tags,\n    )\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ensure_execution_run","title":"<code>ensure_execution_run(execution_run_id, executions)</code>","text":"<p>Ensure the execution run exists in the executions list.</p> <p>Parameters:</p> Name Type Description Default <code>execution_run_id</code> <code>UUID</code> <p>Execution run ID.</p> required <code>executions</code> <code>list[ExecutionRun]</code> <p>List of execution runs.</p> required <p>Returns:</p> Name Type Description <code>ExecutionRun</code> <code>ExecutionRun</code> <p>The execution run corresponding to the execution run ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the execution run is not found.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def ensure_execution_run(execution_run_id: UUID, executions: list[ExecutionRun]) -&gt; ExecutionRun:\n    \"\"\"Ensure the execution run exists in the executions list.\n\n    Args:\n        execution_run_id (UUID): Execution run ID.\n        executions (list[ExecutionRun]): List of execution runs.\n\n    Returns:\n        ExecutionRun: The execution run corresponding to the execution run ID.\n\n    Raises:\n        ValueError: If the execution run is not found.\n    \"\"\"\n    for execution in executions:\n        if execution.id == execution_run_id:\n            return execution\n\n    raise ValueError(f\"execution run {execution_run_id} not found\")\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ensure_run","title":"<code>ensure_run(run_id, runs)</code>","text":"<p>Ensure the run exists in the runs dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>UUID</code> <p>Run ID.</p> required <code>runs</code> <code>dict[UUID, Run]</code> <p>Dictionary of runs.</p> required <p>Returns:</p> Name Type Description <code>Run</code> <code>Run</code> <p>The run corresponding to the run ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the run is not found.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def ensure_run(run_id: UUID, runs: dict[UUID, Run]) -&gt; Run:\n    \"\"\"Ensure the run exists in the runs dictionary.\n\n    Args:\n        run_id (UUID): Run ID.\n        runs (dict[UUID, Run]): Dictionary of runs.\n\n    Returns:\n        Run: The run corresponding to the run ID.\n\n    Raises:\n        ValueError: If the run is not found.\n    \"\"\"\n    run = runs.get(run_id)\n    if not run:\n        raise ValueError(f\"run {run_id} not found\")\n\n    return runs[run_id]\n</code></pre>"},{"location":"dynamiq/cli/client/","title":"Client","text":""},{"location":"dynamiq/cli/client/#dynamiq.cli.client.HTTPError","title":"<code>HTTPError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised for non-2xx responses after retries.</p> Source code in <code>dynamiq/cli/client.py</code> <pre><code>class HTTPError(RuntimeError):\n    \"\"\"Raised for non-2xx responses after retries.\"\"\"\n</code></pre>"},{"location":"dynamiq/cli/config/","title":"Config","text":""},{"location":"dynamiq/cli/config/#dynamiq.cli.config.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/cli/config.py</code> <pre><code>class Settings(BaseModel):\n    org_id: str | None = Field(default=None)\n    project_id: str | None = Field(default=None)\n\n    api_host: str | None = Field(default=DYNAMIQ_BASE_URL)\n    api_key: str | None = Field(default=None)\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    @property\n    def base_url(self) -&gt; str:\n        return self.api_host.rstrip(\"/\")\n\n    def __str__(self) -&gt; str:\n        return f\"Settings(org={self.org_id}, project={self.project_id}, host={self.api_host})\"\n\n    @classmethod\n    def _from_env(cls) -&gt; dict[str, Any]:\n        \"\"\"Pick just the env-vars we care about.\"\"\"\n        return {\n            k: v\n            for k, v in {\n                \"api_host\": os.getenv(\"DYNAMIQ_API_HOST\"),\n                \"api_key\": os.getenv(\"DYNAMIQ_API_KEY\"),\n            }.items()\n            if v is not None\n        }\n\n    @classmethod\n    def load_settings(cls):\n        disk: dict[str, Any] = {}\n        env: dict = cls._from_env()\n        if _CONFIG_FILE_PATH.exists():\n            try:\n                disk = json.loads(_CONFIG_FILE_PATH.read_text())\n            except json.JSONDecodeError as exc:\n                raise SystemExit(f\"\u274c Corrupted config file at {_CONFIG_FILE_PATH}: {exc}\") from exc\n        if _CREDS_FILE_PATH.exists():\n            try:\n                env = json.loads(_CREDS_FILE_PATH.read_text())\n            except json.JSONDecodeError as exc:\n                raise SystemExit(f\"\u274c Corrupted credentials file at {_CREDS_FILE_PATH}: {exc}\") from exc\n\n        merged = {**disk, **env}\n        try:\n            return cls.model_validate(merged)\n        except ValidationError as exc:\n            raise SystemExit(f\"\u274c Invalid configuration: {exc}\") from exc\n\n    def save_settings(self) -&gt; None:\n        _CONFIG_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)\n        payload = self.model_dump(include={\"org_id\", \"project_id\"})\n        _CONFIG_FILE_PATH.write_text(json.dumps(payload, indent=2))\n\n        _CREDS_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)\n        payload = self.model_dump(include={\"api_key\", \"api_host\"})\n        _CREDS_FILE_PATH.write_text(json.dumps(payload, indent=2))\n</code></pre>"},{"location":"dynamiq/cli/commands/config/","title":"Config","text":""},{"location":"dynamiq/cli/commands/context/","title":"Context","text":""},{"location":"dynamiq/cli/commands/context/#dynamiq.cli.commands.context.with_api_and_settings","title":"<code>with_api_and_settings(fn)</code>","text":"<p>Decorator to inject <code>api</code> kwarg after verifying settings.</p> Source code in <code>dynamiq/cli/commands/context.py</code> <pre><code>def with_api_and_settings(fn):\n    \"\"\"Decorator to inject `api` kwarg after verifying settings.\"\"\"\n\n    @pass_dctx\n    def _wrapper(dctx: DynamiqCtx, *args, **kwargs):\n        if dctx.settings is None:\n            dctx.settings = Settings.load_settings()\n            dctx.api = ApiClient(dctx.settings)\n        return fn(*args, api=dctx.api, settings=dctx.settings, **kwargs)\n\n    return _wrapper\n</code></pre>"},{"location":"dynamiq/cli/commands/org/","title":"Org","text":""},{"location":"dynamiq/cli/commands/project/","title":"Project","text":""},{"location":"dynamiq/cli/commands/resource_profiles/","title":"Resource profiles","text":""},{"location":"dynamiq/cli/commands/service/","title":"Service","text":""},{"location":"dynamiq/cli/commands/utils/","title":"Utils","text":""},{"location":"dynamiq/clients/base/","title":"Base","text":""},{"location":"dynamiq/clients/base/#dynamiq.clients.base.BaseTracingClient","title":"<code>BaseTracingClient</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tracing clients.</p> Source code in <code>dynamiq/clients/base.py</code> <pre><code>class BaseTracingClient(abc.ABC):\n    \"\"\"Abstract base class for tracing clients.\"\"\"\n\n    @abc.abstractmethod\n    def trace(self, runs: list[\"Run\"]) -&gt; None:\n        \"\"\"Trace the given runs.\n\n        Args:\n            runs (list[\"Run\"]): List of runs to trace.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/clients/base/#dynamiq.clients.base.BaseTracingClient.trace","title":"<code>trace(runs)</code>  <code>abstractmethod</code>","text":"<p>Trace the given runs.</p> <p>Parameters:</p> Name Type Description Default <code>runs</code> <code>list[Run]</code> <p>List of runs to trace.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/clients/base.py</code> <pre><code>@abc.abstractmethod\ndef trace(self, runs: list[\"Run\"]) -&gt; None:\n    \"\"\"Trace the given runs.\n\n    Args:\n        runs (list[\"Run\"]): List of runs to trace.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/clients/dynamiq/","title":"Dynamiq","text":""},{"location":"dynamiq/clients/dynamiq/#dynamiq.clients.dynamiq.DynamiqTracingClient","title":"<code>DynamiqTracingClient</code>","text":"<p>               Bases: <code>BaseTracingClient</code></p> Source code in <code>dynamiq/clients/dynamiq.py</code> <pre><code>class DynamiqTracingClient(BaseTracingClient):\n\n    def __init__(self, base_url: str | None = None, access_key: str | None = None, timeout: float = 60.0):\n        self.base_url = base_url or DYNAMIQ_BASE_URL\n        self.access_key = access_key or get_env_var(\"DYNAMIQ_ACCESS_KEY\") or get_env_var(\"DYNAMIQ_SERVICE_TOKEN\")\n        if self.access_key is None:\n            raise ValueError(\"No API key provided\")\n        self.timeout = timeout\n\n    def _send_traces_sync(self, runs: list[\"Run\"]) -&gt; None:\n        \"\"\"Synchronous method to send traces using requests\"\"\"\n        try:\n            trace_data = orjson.dumps(\n                {\"runs\": [run.to_dict() for run in runs]},\n                default=orjson_encode,\n                option=orjson.OPT_NON_STR_KEYS,\n            )\n            response = requests.post(  # nosec\n                urljoin(self.base_url, \"/v1/traces\"),\n                data=trace_data,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": f\"Bearer {self.access_key}\",\n                },\n                timeout=self.timeout,\n            )\n            response.raise_for_status()\n        except Exception as e:\n            logger.error(f\"Failed to send traces (sync). Error: {e}\")\n\n    async def request(self, method: str, url_path: URLTypes, **kwargs: Any) -&gt; Response:\n        logger.debug(f'[{self.__class__.__name__}] REQ \"{method} {url_path}\". Kwargs: {kwargs}')\n        url = f\"{self.base_url}/{str(url_path).lstrip('/')}\" if self.base_url else str(url_path).lstrip(\"/\")\n        try:\n            async with httpx.AsyncClient() as client:  # nosec B113\n                response = await client.request(method, url=url, timeout=self.timeout, **kwargs)\n        except (httpx.TimeoutException, httpx.NetworkError) as e:\n            raise HttpConnectionError(e) from e\n\n        try:\n            response.raise_for_status()\n        except httpx.HTTPError as e:\n            if httpx.codes.is_client_error(response.status_code):\n                raise HttpClientError(e, response) from e\n            else:\n                raise HttpServerError(e, response) from e\n\n        return response\n\n    async def _send_traces_async(self, runs: list[\"Run\"]) -&gt; None:\n        \"\"\"Async method to send traces using httpx\"\"\"\n        try:\n            trace_data = orjson.dumps(\n                {\"runs\": [run.to_dict() for run in runs]},\n                default=orjson_encode,\n                option=orjson.OPT_NON_STR_KEYS,\n            )\n            await self.request(\n                method=\"POST\",\n                url_path=\"/v1/traces\",\n                content=trace_data,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": f\"Bearer {self.access_key}\",\n                },\n            )\n        except Exception as e:\n            logger.error(f\"Failed to send traces (async). Error: {e}\")\n\n    def trace(self, runs: list[\"Run\"]) -&gt; None:\n        \"\"\"Sync method required by BaseTracingClient interface\"\"\"\n        if not runs:\n            return\n\n        try:\n            if is_called_from_async_context():\n                loop = asyncio.get_running_loop()\n                loop.create_task(self._send_traces_async(runs))\n            else:\n                self._send_traces_sync(runs)\n        except Exception as e:\n            logger.error(f\"Failed to send traces. Error: {e}\")\n</code></pre>"},{"location":"dynamiq/clients/dynamiq/#dynamiq.clients.dynamiq.DynamiqTracingClient.trace","title":"<code>trace(runs)</code>","text":"<p>Sync method required by BaseTracingClient interface</p> Source code in <code>dynamiq/clients/dynamiq.py</code> <pre><code>def trace(self, runs: list[\"Run\"]) -&gt; None:\n    \"\"\"Sync method required by BaseTracingClient interface\"\"\"\n    if not runs:\n        return\n\n    try:\n        if is_called_from_async_context():\n            loop = asyncio.get_running_loop()\n            loop.create_task(self._send_traces_async(runs))\n        else:\n            self._send_traces_sync(runs)\n    except Exception as e:\n        logger.error(f\"Failed to send traces. Error: {e}\")\n</code></pre>"},{"location":"dynamiq/components/serializers/","title":"Serializers","text":""},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer","title":"<code>BaseSerializer</code>","text":"<p>Base class for serializers providing interface for dumps and loads methods.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class BaseSerializer:\n    \"\"\"\n    Base class for serializers providing interface for dumps and loads methods.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a string.\n\n        Args:\n            value (Any): The value to be serialized.\n\n        Returns:\n            str: The serialized string representation of the value.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n\n    def loads(self, value: Any) -&gt; Any:\n        \"\"\"\n        Deserialize the given value from a string.\n\n        Args:\n            value (Any): The serialized string to be deserialized.\n\n        Returns:\n            Any: The deserialized value.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized string representation of the value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be implemented by subclasses.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a string.\n\n    Args:\n        value (Any): The value to be serialized.\n\n    Returns:\n        str: The serialized string representation of the value.\n\n    Raises:\n        NotImplementedError: This method should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given value from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The serialized string to be deserialized.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be implemented by subclasses.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: Any) -&gt; Any:\n    \"\"\"\n    Deserialize the given value from a string.\n\n    Args:\n        value (Any): The serialized string to be deserialized.\n\n    Returns:\n        Any: The deserialized value.\n\n    Raises:\n        NotImplementedError: This method should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer","title":"<code>JsonPickleSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that uses jsonpickle to convert complex Python objects to and from JSON format.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class JsonPickleSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that uses jsonpickle to convert complex Python objects to and from JSON format.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a JSON string using jsonpickle.\n\n        Args:\n            value (Any): The value to be serialized.\n\n        Returns:\n            str: The JSON string representation of the value.\n        \"\"\"\n        import jsonpickle\n\n        return jsonpickle.encode(value)\n\n    def loads(self, value: str) -&gt; Any:\n        \"\"\"\n        Deserialize the given JSON string to a Python object using jsonpickle.\n\n        Args:\n            value (str): The JSON string to be deserialized.\n\n        Returns:\n            Any: The deserialized Python object.\n        \"\"\"\n        import jsonpickle\n\n        return jsonpickle.decode(value)  # nosec\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a JSON string using jsonpickle.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a JSON string using jsonpickle.\n\n    Args:\n        value (Any): The value to be serialized.\n\n    Returns:\n        str: The JSON string representation of the value.\n    \"\"\"\n    import jsonpickle\n\n    return jsonpickle.encode(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given JSON string to a Python object using jsonpickle.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The JSON string to be deserialized.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: str) -&gt; Any:\n    \"\"\"\n    Deserialize the given JSON string to a Python object using jsonpickle.\n\n    Args:\n        value (str): The JSON string to be deserialized.\n\n    Returns:\n        Any: The deserialized Python object.\n    \"\"\"\n    import jsonpickle\n\n    return jsonpickle.decode(value)  # nosec\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer","title":"<code>JsonSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that converts values to and from JSON format.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class JsonSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that converts values to and from JSON format.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a JSON string.\n\n        Args:\n            value (Any): The value to be serialized to JSON.\n\n        Returns:\n            str: The JSON string representation of the value.\n        \"\"\"\n        return json.dumps(value, cls=JsonWorkflowEncoder)\n\n    def loads(self, value: str | None) -&gt; Any:\n        \"\"\"\n        Deserialize the given JSON string to a Python object.\n\n        Args:\n            value (str | None): The JSON string to be deserialized, or None.\n\n        Returns:\n            Any: The deserialized Python object, or None if the input is None.\n        \"\"\"\n        if value is None:\n            return None\n        return json.loads(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized to JSON.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a JSON string.\n\n    Args:\n        value (Any): The value to be serialized to JSON.\n\n    Returns:\n        str: The JSON string representation of the value.\n    \"\"\"\n    return json.dumps(value, cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given JSON string to a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>The JSON string to be deserialized, or None.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object, or None if the input is None.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: str | None) -&gt; Any:\n    \"\"\"\n    Deserialize the given JSON string to a Python object.\n\n    Args:\n        value (str | None): The JSON string to be deserialized, or None.\n\n    Returns:\n        Any: The deserialized Python object, or None if the input is None.\n    \"\"\"\n    if value is None:\n        return None\n    return json.loads(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer","title":"<code>StringSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that converts values to and from strings.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class StringSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that converts values to and from strings.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Convert the given value to a string.\n\n        Args:\n            value (Any): The value to be converted to a string.\n\n        Returns:\n            str: The string representation of the value.\n        \"\"\"\n        return str(value)\n\n    def loads(self, value: Any) -&gt; Any:\n        \"\"\"\n        Return the input value as is, since it's already a string.\n\n        Args:\n            value (Any): The value to be deserialized (expected to be a string).\n\n        Returns:\n            Any: The input value, unchanged.\n        \"\"\"\n        return value\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Convert the given value to a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be converted to a string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Convert the given value to a string.\n\n    Args:\n        value (Any): The value to be converted to a string.\n\n    Returns:\n        str: The string representation of the value.\n    \"\"\"\n    return str(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Return the input value as is, since it's already a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be deserialized (expected to be a string).</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The input value, unchanged.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: Any) -&gt; Any:\n    \"\"\"\n    Return the input value as is, since it's already a string.\n\n    Args:\n        value (Any): The value to be deserialized (expected to be a string).\n\n    Returns:\n        Any: The input value, unchanged.\n    \"\"\"\n    return value\n</code></pre>"},{"location":"dynamiq/components/converters/base/","title":"Base","text":""},{"location":"dynamiq/components/converters/base/#dynamiq.components.converters.base.BaseConverter","title":"<code>BaseConverter</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/components/converters/base.py</code> <pre><code>class BaseConverter(BaseModel):\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(**kwargs)\n\n    def run(\n        self,\n        file_paths: list[str] | list[os.PathLike] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Converts files to Documents.\n\n        Processes file paths or BytesIO objects into Documents. Handles directories and files.\n\n        Args:\n            file_paths: List of file or directory paths to convert.\n            files: List of BytesIO objects to convert.\n            metadata: Metadata for documents. Can be a dict for all or a list of dicts for each.\n\n        Returns:\n            Dict with 'documents' key containing a list of created Documents.\n\n        Raises:\n            ValueError: If neither paths nor files provided, or if metadata is a list with\n                directory paths, or if files cannot be processed properly.\n            FileNotFoundError: If any file path does not exist.\n            IOError: If any file cannot be read.\n            Exception: Any other exception that may occur during processing.\n        \"\"\"\n        if file_paths is None and files is None:\n            raise ValueError(\"No input provided. Please provide either file_paths or files.\")\n\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath for path in paths_obj if path.is_dir() for filepath in path.glob(\"*.*\") if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = set(filepaths + filepaths_in_directories)\n\n            if not all_filepaths:\n                raise FileNotFoundError(f\"No files found in the provided paths: {file_paths}\")\n\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for filepath, meta in zip(all_filepaths, meta_list):\n                documents.extend(self._process_file(filepath, meta))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n            for file, meta in zip(files, meta_list):\n                documents.extend(self._process_file(file, meta))\n\n        if len(documents) == 0 and (file_paths is not None or files is not None):\n            raise ValueError(\n                \"No documents were created from the provided inputs. Please check your files and try again.\"\n            )\n\n        return {\"documents\": documents}\n\n    @staticmethod\n    def _normalize_metadata(\n        metadata: dict[str, Any] | list[dict[str, Any]] | None, sources_count: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Normalizes metadata input for a converter.\n\n        Given all possible values of the metadata input for a converter (None, dictionary, or list of\n        dicts), ensures to return a list of dictionaries of the correct length for the converter to use.\n\n        Args:\n            metadata: The meta input of the converter, as-is. Can be None, a dictionary, or a list of\n                dictionaries.\n            sources_count: The number of sources the converter received.\n\n        Returns:\n            A list of dictionaries of the same length as the sources list.\n\n        Raises:\n            ValueError: If metadata is not None, a dictionary, or a list of dictionaries, or if the length\n                of the metadata list doesn't match the number of sources.\n        \"\"\"\n        if metadata is None:\n            return [{} for _ in range(sources_count)]\n        if isinstance(metadata, dict):\n            return [copy.deepcopy(metadata) for _ in range(sources_count)]\n        if isinstance(metadata, list):\n            metadata_count = len(metadata)\n            if sources_count != metadata_count:\n                raise ValueError(\n                    f\"The length of the metadata list [{metadata_count}] \"\n                    f\"must match the number of sources [{sources_count}].\"\n                )\n            return metadata\n        raise ValueError(\"metadata must be either None, a dictionary or a list of dictionaries.\")\n\n    @abstractmethod\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: Any,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        pass\n\n    @abstractmethod\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        pass\n</code></pre>"},{"location":"dynamiq/components/converters/base/#dynamiq.components.converters.base.BaseConverter.run","title":"<code>run(file_paths=None, files=None, metadata=None)</code>","text":"<p>Converts files to Documents.</p> <p>Processes file paths or BytesIO objects into Documents. Handles directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str] | list[PathLike] | None</code> <p>List of file or directory paths to convert.</p> <code>None</code> <code>files</code> <code>list[BytesIO] | None</code> <p>List of BytesIO objects to convert.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]] | None</code> <p>Metadata for documents. Can be a dict for all or a list of dicts for each.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of created Documents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither paths nor files provided, or if metadata is a list with directory paths, or if files cannot be processed properly.</p> <code>FileNotFoundError</code> <p>If any file path does not exist.</p> <code>IOError</code> <p>If any file cannot be read.</p> <code>Exception</code> <p>Any other exception that may occur during processing.</p> Source code in <code>dynamiq/components/converters/base.py</code> <pre><code>def run(\n    self,\n    file_paths: list[str] | list[os.PathLike] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Converts files to Documents.\n\n    Processes file paths or BytesIO objects into Documents. Handles directories and files.\n\n    Args:\n        file_paths: List of file or directory paths to convert.\n        files: List of BytesIO objects to convert.\n        metadata: Metadata for documents. Can be a dict for all or a list of dicts for each.\n\n    Returns:\n        Dict with 'documents' key containing a list of created Documents.\n\n    Raises:\n        ValueError: If neither paths nor files provided, or if metadata is a list with\n            directory paths, or if files cannot be processed properly.\n        FileNotFoundError: If any file path does not exist.\n        IOError: If any file cannot be read.\n        Exception: Any other exception that may occur during processing.\n    \"\"\"\n    if file_paths is None and files is None:\n        raise ValueError(\"No input provided. Please provide either file_paths or files.\")\n\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath for path in paths_obj if path.is_dir() for filepath in path.glob(\"*.*\") if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = set(filepaths + filepaths_in_directories)\n\n        if not all_filepaths:\n            raise FileNotFoundError(f\"No files found in the provided paths: {file_paths}\")\n\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for filepath, meta in zip(all_filepaths, meta_list):\n            documents.extend(self._process_file(filepath, meta))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n        for file, meta in zip(files, meta_list):\n            documents.extend(self._process_file(file, meta))\n\n    if len(documents) == 0 and (file_paths is not None or files is not None):\n        raise ValueError(\n            \"No documents were created from the provided inputs. Please check your files and try again.\"\n        )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/components/converters/docx/","title":"Docx","text":""},{"location":"dynamiq/components/converters/docx/#dynamiq.components.converters.docx.DOCXConverter","title":"<code>DOCXConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting DOCX files to Documents using the python-docx library.</p> <p>Initializes the object with the configuration for converting documents using python-docx.</p> <p>Parameters:</p> Name Type Description Default <code>document_creation_mode</code> <code>Literal['one-doc-per-file', 'one-doc-per-page']</code> <p>Determines how to create Documents from the elements of the Word document. Options are: - <code>\"one-doc-per-file\"</code>: Creates one Document per file.     All elements are concatenated into one text field. - <code>\"one-doc-per-page\"</code>: Creates one Document per page.     All elements on a page are concatenated into one text field. Defaults to <code>\"one-doc-per-file\"</code>.</p> required Usage example <pre><code>from dynamiq.components.converters.docx import DOCXConverter\n\nconverter = DOCXConverter()\ndocuments = converter.run(paths=[\"a/file/path.docx\", \"a/directory/path\"])[\"documents\"]\n</code></pre> Source code in <code>dynamiq/components/converters/docx.py</code> <pre><code>class DOCXConverter(BaseConverter):\n    \"\"\"\n    A component for converting DOCX files to Documents using the python-docx library.\n\n    Initializes the object with the configuration for converting documents using\n    python-docx.\n\n    Args:\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\"], optional):\n            Determines how to create Documents from the elements of the Word document. Options are:\n            - `\"one-doc-per-file\"`: Creates one Document per file.\n                All elements are concatenated into one text field.\n            - `\"one-doc-per-page\"`: Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            Defaults to `\"one-doc-per-file\"`.\n\n    Usage example:\n        ```python\n        from dynamiq.components.converters.docx import DOCXConverter\n\n        converter = DOCXConverter()\n        documents = converter.run(paths=[\"a/file/path.docx\", \"a/directory/path\"])[\"documents\"]\n        ```\n    \"\"\"\n\n    document_creation_mode: Literal[DocumentCreationMode.ONE_DOC_PER_FILE, DocumentCreationMode.ONE_DOC_PER_PAGE] = (\n        DocumentCreationMode.ONE_DOC_PER_FILE\n    )\n\n    xml_key: str = \"{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val\"\n    xml_namespaces: dict[str, str] = {\n        \"w\": \"http://schemas.microsoft.com/office/word/2003/wordml\",\n        \"a\": \"http://schemas.openxmlformats.org/drawingml/2006/main\",\n        \"r\": \"http://schemas.openxmlformats.org/officeDocument/2006/relationships\",\n    }\n    image_placeholder: str = \"&lt;!-- image --&gt;\"\n    blip_tag: str = \"{http://schemas.openxmlformats.org/drawingml/2006/main}blip\"\n\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a single Word document and create documents.\n\n        Args:\n            file (Union[Path, BytesIO]): The file to process.\n            metadata (Dict[str, Any]): Metadata to attach to the documents.\n\n        Returns:\n            List[Any]: A list of created documents.\n\n        Raises:\n            TypeError: If the file argument is neither a Path nor a BytesIO object.\n        \"\"\"\n        if isinstance(file, Path):\n            with open(file, \"rb\") as upload_file:\n                file_content = BytesIO(upload_file.read())\n                file_path = upload_file.name\n        elif isinstance(file, BytesIO):\n            file_path = get_filename_for_bytesio(file)\n            file_content = file\n        else:\n            raise TypeError(\"Expected a Path object or a BytesIO object.\")\n\n        file_content.seek(0)\n        elements = DocxDocument(file_content)\n        return self._create_documents(\n            filepath=file_path,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: DocxDocument,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the elements of the Word document.\n        \"\"\"\n        docs = []\n        if document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            text_content = []\n\n            for element in elements.element.body:\n                tag_name = element.tag.split(\"}\", 1)[-1] if \"}\" in element.tag else element.tag\n\n                # Check for inline images\n                contains_blip = any(child.tag == self.blip_tag for child in element.iter())\n\n                if element.tag.endswith(\"p\"):\n                    text = self._process_paragraph(element, elements)\n                    if text.strip():\n                        text_content.append(text)\n                elif element.tag.endswith(\"tbl\"):\n                    text = self._process_table(element, elements)\n                    if text.strip():\n                        text_content.append(text)\n                elif contains_blip:\n                    text_content.append(self.image_placeholder)\n                elif tag_name in [\"sdt\"]:\n                    sdt_content = element.find(\".//w:sdtContent\", namespaces=self.xml_namespaces)\n                    if sdt_content is not None:\n                        paragraphs = sdt_content.findall(\".//w:p\", namespaces=self.xml_namespaces)\n                        for p in paragraphs:\n                            text = self._process_paragraph(p, elements)\n                            if text.strip():\n                                text_content.append(text)\n\n            full_text = \"\\n\\n\".join(text_content)\n\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n            docs = [Document(content=full_text, metadata=metadata)]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_PAGE:\n            sections = self._split_into_sections(elements)\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n\n            for idx, section_content in enumerate(sections, start=1):\n                section_metadata = copy.deepcopy(metadata)\n                section_metadata[\"page_number\"] = idx\n                docs.append(Document(content=section_content, metadata=section_metadata))\n\n        return docs\n\n    def _process_paragraph(self, paragraph_element, docx_obj) -&gt; str:\n        \"\"\"\n        Process a paragraph element with formatting and hyperlinks.\n        \"\"\"\n        paragraph = Paragraph(paragraph_element, docx_obj)\n        text = paragraph.text\n\n        paragraph_elements = self._get_paragraph_elements(paragraph)\n        is_list_item, list_marker, list_level = self._check_for_list_item(paragraph)\n        is_header, header_level = self._check_for_header(paragraph)\n\n        formatted_text = \"\"\n\n        if is_header:\n            header_prefix = \"#\" * header_level\n            formatted_text = f\"{header_prefix} {text}\"\n        elif is_list_item:\n            indent = \"  \" * (list_level - 1) if list_level &gt; 1 else \"\"\n            formatted_text = f\"{indent}{list_marker} {text}\"\n        else:\n            formatted_parts = []\n            for txt, format_info, hyperlink in paragraph_elements:\n                if not txt.strip():\n                    continue\n\n                if format_info:\n                    if format_info.get(\"bold\", False):\n                        txt = f\"**{txt}**\"\n                    if format_info.get(\"italic\", False):\n                        txt = f\"*{txt}*\"\n                    if format_info.get(\"underline\", False):\n                        txt = f\"_{txt}_\"\n\n                if hyperlink:\n                    txt = f\"[{txt}]({hyperlink})\"\n\n                formatted_parts.append(txt)\n\n            formatted_text = \" \".join(formatted_parts)\n\n        return formatted_text\n\n    def _process_table(self, table_element, docx_obj) -&gt; str:\n        \"\"\"\n        Process a table element with enhanced features like cell spanning and formatting.\n        \"\"\"\n        table = Table(table_element, docx_obj)\n\n        # Check for single-cell tables\n        if len(table.rows) == 1 and len(table.columns) == 1:\n            cell_text = table.rows[0].cells[0].text\n            if cell_text.strip():\n                return cell_text\n\n        table_rows = []\n\n        if table.rows:\n            header_cells = []\n            for cell in table.rows[0].cells:\n                header_cells.append(cell.text.strip() or \"\")\n            table_rows.append(\"| \" + \" | \".join(header_cells) + \" |\")\n            table_rows.append(\"| \" + \" | \".join([\"---\"] * len(header_cells)) + \" |\")\n\n        cell_set = set()\n        for row_idx, row in enumerate(table.rows):\n            # Skip the header row\n            if row_idx == 0:\n                continue\n\n            row_cells = []\n            for cell in row.cells:\n                if cell._tc in cell_set:\n                    continue\n                cell_set.add(cell._tc)\n\n                cell_text = cell.text.strip() or \"\"\n                row_cells.append(cell_text)\n\n            if row_cells:\n                table_rows.append(\"| \" + \" | \".join(row_cells) + \" |\")\n\n        return \"\\n\".join(table_rows)\n\n    def _get_paragraph_elements(self, paragraph):\n        \"\"\"\n        Extract paragraph elements (with the formatting and hyperlinks).\n        \"\"\"\n        if paragraph.text.strip() == \"\":\n            return [(\"\", None, None)]\n\n        paragraph_elements = []\n\n        for content in paragraph.iter_inner_content():\n            if isinstance(content, Hyperlink):\n                text = content.text\n                hyperlink = content.address if hasattr(content, \"address\") else None\n                format_info = self._get_format_from_run(content.runs[0] if content.runs else None)\n                if text.strip():\n                    paragraph_elements.append((text, format_info, hyperlink))\n            elif isinstance(content, Run):\n                text = content.text\n                format_info = self._get_format_from_run(content)\n                if text.strip():\n                    paragraph_elements.append((text, format_info, None))\n\n        if not paragraph_elements and paragraph.text.strip():\n            paragraph_elements.append((paragraph.text.strip(), None, None))\n\n        return paragraph_elements\n\n    def _get_format_from_run(self, run):\n        \"\"\"\n        Extract formatting information from a run.\n        \"\"\"\n        if not run:\n            return None\n\n        format_info = {}\n        if hasattr(run, \"bold\") and run.bold:\n            format_info[\"bold\"] = True\n        if hasattr(run, \"italic\") and run.italic:\n            format_info[\"italic\"] = True\n        if hasattr(run, \"underline\") and run.underline:\n            format_info[\"underline\"] = True\n\n        return format_info if format_info else None\n\n    def _check_for_list_item(self, paragraph):\n        \"\"\"\n        Check if a paragraph is a list item and return relevant information.\n        \"\"\"\n        numbering_properties = paragraph._element.find(\".//w:numPr\", namespaces=paragraph._element.nsmap)\n        if numbering_properties is not None:\n            numbering_id_elem = numbering_properties.find(\"w:numId\", namespaces=paragraph._element.nsmap)\n            indexing_level_elem = numbering_properties.find(\"w:ilvl\", namespaces=paragraph._element.nsmap)\n\n            numbering_id = numbering_id_elem.get(self.xml_key) if numbering_id_elem is not None else None\n            indexing_level = indexing_level_elem.get(self.xml_key) if indexing_level_elem is not None else None\n\n            if numbering_id and numbering_id != \"0\":\n                level = int(indexing_level) + 1 if indexing_level else 1\n                # Check if the paragraph is a numbered list item or a bullet list item\n                is_numbered = \"Number\" in paragraph.style.name if paragraph.style and paragraph.style.name else False\n                marker = f\"{level}.\" if is_numbered else \"\u2022\"\n                return True, marker, level\n\n        return False, \"\", 0\n\n    def _check_for_header(self, paragraph):\n        \"\"\"\n        Check if a paragraph is a header and return the level.\n        \"\"\"\n        if not paragraph.style:\n            return False, 0\n\n        style_id = paragraph.style.name.lower() if paragraph.style.name else \"\"\n\n        heading_string = \"heading\"\n        heading_pattern = r\"heading\\s*(\\d+)\"\n\n        if heading_string in style_id:\n            match = re.search(heading_pattern, style_id)\n            if match:\n                level = int(match.group(1))\n                return True, level\n\n        return False, 0\n\n    def _split_into_sections(self, doc: DocxDocument) -&gt; list[str]:\n        \"\"\"\n        Split the document into sections based on section breaks.\n        \"\"\"\n        sections = []\n        current_section = []\n\n        for element in doc.element.body:\n            if element.tag.endswith(\"sectPr\"):\n                if current_section:\n                    sections.append(\"\\n\\n\".join(current_section))\n                    current_section = []\n            elif element.tag.endswith(\"p\"):\n                text = self._process_paragraph(element, doc)\n                if text.strip():\n                    current_section.append(text)\n            elif element.tag.endswith(\"tbl\"):\n                text = self._process_table(element, doc)\n                if text.strip():\n                    current_section.append(text)\n\n        if current_section:\n            sections.append(\"\\n\\n\".join(current_section))\n        if not sections:\n            return [\"\"]\n\n        return sections\n</code></pre>"},{"location":"dynamiq/components/converters/html/","title":"Html","text":""},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.HTMLConverter","title":"<code>HTMLConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting HTML files to Documents using lxml.</p> <p>Initializes the object with the configuration for converting documents using lxml HTML parser.</p> <p>Parameters:</p> Name Type Description Default <code>document_creation_mode</code> <code>Literal['one-doc-per-file']</code> <p>Determines how to create Documents from the HTML content. Currently only supports: - <code>\"one-doc-per-file\"</code>: Creates one Document per file.     All content is converted to markdown format. Defaults to <code>\"one-doc-per-file\"</code>.</p> required Usage example <pre><code>from dynamiq.components.converters.html import HTMLConverter\n\nconverter = HTMLConverter()\ndocuments = converter.run(paths=[\"a/file/path.html\", \"a/directory/path\"])[\"documents\"]\n</code></pre> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>class HTMLConverter(BaseConverter):\n    \"\"\"\n    A component for converting HTML files to Documents using lxml.\n\n    Initializes the object with the configuration for converting documents using\n    lxml HTML parser.\n\n    Args:\n        document_creation_mode (Literal[\"one-doc-per-file\"], optional):\n            Determines how to create Documents from the HTML content. Currently only supports:\n            - `\"one-doc-per-file\"`: Creates one Document per file.\n                All content is converted to markdown format.\n            Defaults to `\"one-doc-per-file\"`.\n\n    Usage example:\n        ```python\n        from dynamiq.components.converters.html import HTMLConverter\n\n        converter = HTMLConverter()\n        documents = converter.run(paths=[\"a/file/path.html\", \"a/directory/path\"])[\"documents\"]\n        ```\n    \"\"\"\n\n    document_creation_mode: Literal[DocumentCreationMode.ONE_DOC_PER_FILE] = DocumentCreationMode.ONE_DOC_PER_FILE\n\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a file and return a list of Documents.\n\n        Args:\n            file: Path to a file or BytesIO object\n            metadata: Metadata to attach to the documents\n\n        Returns:\n            List of Documents\n        \"\"\"\n        # Get the file path or name for BytesIO\n        if isinstance(file, BytesIO):\n            filepath = get_filename_for_bytesio(file)\n        else:\n            filepath = str(file)\n\n        # Read the file content\n        if isinstance(file, BytesIO):\n            file.seek(0)\n            html_content = file.read().decode(\"utf-8\")\n        else:\n            with open(file, encoding=\"utf-8\") as f:\n                html_content = f.read()\n\n        # Parse the HTML content\n        elements = self._parse_html_content(html_content)\n\n        # Create documents from the HTML elements\n        return self._create_documents(\n            filepath=filepath,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _parse_html_content(self, html_content: str) -&gt; lxml_html.HtmlElement:\n        \"\"\"\n        Parse HTML content using lxml.\n\n        Args:\n            html_content: HTML content to parse\n\n        Returns:\n            lxml HTML element\n        \"\"\"\n        try:\n            return lxml_html.fromstring(html_content)\n        except etree.ParserError:\n            # Handle malformed HTML by cleaning it first\n            html_content = self._clean_html(html_content)\n            return lxml_html.fromstring(html_content)\n\n    @staticmethod\n    def _clean_html(html_content: str) -&gt; str:\n        \"\"\"\n        Clean malformed HTML.\n\n        Args:\n            html_content: HTML content to clean\n\n        Returns:\n            Cleaned HTML content\n        \"\"\"\n        parser = html.parser.HTMLParser()\n        try:\n            return parser.unescape(html_content)\n        except AttributeError:\n            # For Python 3.9+, HTMLParser.unescape is deprecated\n            return html_module.unescape(html_content)\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: lxml_html.HtmlElement,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the HTML elements.\n        \"\"\"\n        if document_creation_mode != DocumentCreationMode.ONE_DOC_PER_FILE:\n            raise ValueError(\"HTMLConverter only supports one-doc-per-file mode\")\n\n        markdown_content = self._convert_to_markdown(elements)\n\n        # Clean up excessive newlines\n        markdown_content = re.sub(r\"\\n{4,}\", \"\\n\\n\", markdown_content)\n        markdown_content = markdown_content.strip()\n\n        metadata = copy.deepcopy(metadata)\n        metadata[\"file_path\"] = filepath\n\n        return [Document(content=markdown_content, metadata=metadata)]\n\n    def _convert_to_markdown(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"\n        Convert HTML element to Markdown.\n\n        Args:\n            element: lxml HTML element\n\n        Returns:\n            Markdown string\n        \"\"\"\n        markdown = MarkdownConverter()\n        return markdown.convert_element(element)\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter","title":"<code>MarkdownConverter</code>","text":"<p>Helper class to convert HTML elements to Markdown format</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>class MarkdownConverter:\n    \"\"\"Helper class to convert HTML elements to Markdown format\"\"\"\n\n    def __init__(self):\n        self.tag_handlers = {\n            \"h1\": self.handle_heading,\n            \"h2\": self.handle_heading,\n            \"h3\": self.handle_heading,\n            \"h4\": self.handle_heading,\n            \"h5\": self.handle_heading,\n            \"h6\": self.handle_heading,\n            \"p\": self.handle_paragraph,\n            \"a\": self.handle_link,\n            \"img\": self.handle_image,\n            \"strong\": self.handle_strong,\n            \"b\": self.handle_strong,\n            \"em\": self.handle_emphasis,\n            \"i\": self.handle_emphasis,\n            \"code\": self.handle_code,\n            \"pre\": self.handle_preformatted,\n            \"ul\": self.handle_unordered_list,\n            \"ol\": self.handle_ordered_list,\n            \"li\": self.handle_list_item,\n            \"br\": self.handle_linebreak,\n            \"hr\": self.handle_horizontal_rule,\n            \"table\": self.handle_table,\n            \"tr\": self.handle_table_row,\n            \"th\": self.handle_table_cell,\n            \"td\": self.handle_table_cell,\n            \"blockquote\": self.handle_blockquote,\n            \"div\": self.handle_block,\n            \"span\": self.handle_inline,\n        }\n\n        self.list_state = []\n        self.in_table = False\n        self.table_headers = []\n        self.table_rows = []\n        self.current_row = []\n\n    def convert_element(self, element: lxml_html.HtmlElement | str, parent_tag=None) -&gt; str:\n        \"\"\"Convert an HTML element to Markdown recursively\"\"\"\n        if element is None:\n            return \"\"\n\n        if isinstance(element, str) or element.tag == \"text\":\n            text = element if isinstance(element, str) else element.text_content().strip()\n            if text and parent_tag not in [\"pre\", \"code\"]:\n                text = \" \".join(text.split())\n            return text\n\n        tag = element.tag\n\n        if tag is etree.Comment or tag in [\"script\", \"style\"]:\n            return \"\"\n\n        if tag == \"html\" or tag == \"body\" or tag == \"document\":\n            return self.handle_document(element)\n\n        if tag in self.tag_handlers:\n            return self.tag_handlers[tag](element)\n\n        return self.process_children(element)\n\n    def process_children(self, element: lxml_html.HtmlElement, add_linebreaks=False) -&gt; str:\n        \"\"\"Process all children of an element and join their markdown\"\"\"\n        result = []\n\n        if element.text and element.text.strip():\n            result.append(element.text)\n\n        for child in element:\n            child_md = self.convert_element(child)\n            if child_md:\n                result.append(child_md)\n            if child.tail and child.tail.strip():\n                result.append(child.tail)\n\n        separator = \"\\n\" if add_linebreaks else \"\"\n        return separator.join([r for r in result if r and r.strip()])\n\n    def handle_document(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle document element (html or body)\"\"\"\n        return self.process_children(element, True)\n\n    def handle_heading(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle heading elements (h1-h6)\"\"\"\n        level = int(element.tag[1])\n        content = element.text_content().strip()\n        return f\"\\n\\n{'#' * level} {content}\\n\\n\"\n\n    def handle_paragraph(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle paragraph elements\"\"\"\n        content = element.text or \"\"\n\n        for child in element:\n            if child.tag in self.tag_handlers:\n                content += self.tag_handlers[child.tag](child)\n            else:\n                content += self.process_children(child)\n\n            if child.tail:\n                content += child.tail\n\n        if not content.strip():\n            return \"\"\n\n        return f\"\\n\\n{content}\\n\\n\"\n\n    def handle_link(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle anchor elements\"\"\"\n        href = element.get(\"href\", \"\")\n        content = element.text_content() or href\n\n        if href.startswith(\"#\"):\n            anchor = href[1:]\n            return f\"[{content}](#{anchor})\"\n\n        return f\"[{content}]({href})\"\n\n    def handle_image(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle image elements\"\"\"\n        src = element.get(\"src\", \"\")\n        alt = element.get(\"alt\", \"\")\n        title = element.get(\"title\", \"\")\n        if title:\n            return f'![{alt}]({src} \"{title}\")'\n        return f\"![{alt}]({src})\"\n\n    def handle_strong(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle strong/bold elements\"\"\"\n        content = element.text_content().strip()\n        return f\"**{content}**\"\n\n    def handle_emphasis(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle emphasis/italic elements\"\"\"\n        content = element.text_content().strip()\n        return f\"*{content}*\"\n\n    def handle_code(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle inline code elements\"\"\"\n        if element.getparent() is not None and element.getparent().tag == \"pre\":\n            return element.text_content()\n\n        content = element.text_content()\n        return f\"`{content}`\"\n\n    def handle_preformatted(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle preformatted code blocks\"\"\"\n        code_element = element.find(\"code\")\n        if code_element is not None:\n            content = code_element.text_content()\n            language = \"\"\n            for cls in code_element.get(\"class\", \"\").split():\n                if cls.startswith(\"language-\"):\n                    language = cls[9:]\n                    break\n            return f\"\\n```{language}\\n{content}\\n```\\n\"\n\n        content = element.text_content()\n        return f\"\\n```\\n{content}\\n```\\n\"\n\n    def handle_unordered_list(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle unordered list elements\"\"\"\n        self.list_state.append({\"type\": \"ul\", \"index\": 0})\n        content = self.process_children(element, True)\n        self.list_state.pop()\n        return f\"\\n{content}\\n\"\n\n    def handle_ordered_list(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle ordered list elements\"\"\"\n        start = element.get(\"start\", \"1\")\n        try:\n            start = int(start)\n        except ValueError:\n            start = 1\n\n        self.list_state.append({\"type\": \"ol\", \"index\": start - 1})\n        content = self.process_children(element, True)\n        self.list_state.pop()\n        return f\"\\n{content}\\n\"\n\n    def handle_list_item(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle list item elements\"\"\"\n        if self.list_state and self.list_state[-1][\"type\"] == \"ol\":\n            self.list_state[-1][\"index\"] += 1\n\n        indent = \"  \" * (len(self.list_state) - 1)\n        if self.list_state and self.list_state[-1][\"type\"] == \"ul\":\n            bullet = \"*\"\n        else:\n            bullet = f\"{self.list_state[-1]['index']}.\"\n\n        content = element.text or \"\"\n\n        for child in element:\n            if child.tag in self.tag_handlers:\n                content += self.tag_handlers[child.tag](child)\n            else:\n                content += self.process_children(child)\n\n            if child.tail:\n                content += child.tail\n\n        return f\"{indent}{bullet} {content}\"\n\n    def handle_linebreak(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle line break elements\"\"\"\n        return \"\\n\"\n\n    def handle_horizontal_rule(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle horizontal rule elements\"\"\"\n        return \"\\n\\n---\\n\\n\"\n\n    def handle_table(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle table elements\"\"\"\n        self.in_table = True\n        self.table_headers = []\n        self.table_rows = []\n\n        self.process_children(element)\n\n        if not self.table_headers and self.table_rows:\n            self.table_headers = self.table_rows[0]\n            self.table_rows = self.table_rows[1:]\n\n        if not self.table_headers:\n            self.in_table = False\n            return \"\"\n\n        result = []\n        result.append(\"| \" + \" | \".join(self.table_headers) + \" |\")\n        result.append(\"| \" + \" | \".join([\"---\"] * len(self.table_headers)) + \" |\")\n\n        for row in self.table_rows:\n            padded_row = row + [\"\"] * (len(self.table_headers) - len(row))\n            result.append(\"| \" + \" | \".join(padded_row) + \" |\")\n\n        self.in_table = False\n        return \"\\n\\n\" + \"\\n\".join(result) + \"\\n\\n\"\n\n    def handle_table_row(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle table row elements\"\"\"\n        if not self.in_table:\n            return \"\"\n\n        self.current_row = []\n        self.process_children(element)\n\n        if element.findall(\"th\"):\n            self.table_headers = self.current_row\n        else:\n            self.table_rows.append(self.current_row)\n\n        return \"\"\n\n    def handle_table_cell(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle table cell elements\"\"\"\n        if not self.in_table:\n            return \"\"\n\n        content = element.text_content()\n        self.current_row.append(content.strip())\n        return \"\"\n\n    def handle_blockquote(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle blockquote elements\"\"\"\n        content = element.text_content()\n        lines = content.split(\"\\n\")\n        quoted_lines = [f\"&gt; {line}\" if line.strip() else \"&gt;\" for line in lines]\n        return \"\\n\\n\" + \"\\n\".join(quoted_lines) + \"\\n\\n\"\n\n    def handle_block(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle block-level elements like div\"\"\"\n        content = self.process_children(element)\n        return f\"\\n\\n{content}\\n\\n\"\n\n    def handle_inline(self, element: lxml_html.HtmlElement) -&gt; str:\n        \"\"\"Handle inline elements like span\"\"\"\n        return self.process_children(element)\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.convert_element","title":"<code>convert_element(element, parent_tag=None)</code>","text":"<p>Convert an HTML element to Markdown recursively</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def convert_element(self, element: lxml_html.HtmlElement | str, parent_tag=None) -&gt; str:\n    \"\"\"Convert an HTML element to Markdown recursively\"\"\"\n    if element is None:\n        return \"\"\n\n    if isinstance(element, str) or element.tag == \"text\":\n        text = element if isinstance(element, str) else element.text_content().strip()\n        if text and parent_tag not in [\"pre\", \"code\"]:\n            text = \" \".join(text.split())\n        return text\n\n    tag = element.tag\n\n    if tag is etree.Comment or tag in [\"script\", \"style\"]:\n        return \"\"\n\n    if tag == \"html\" or tag == \"body\" or tag == \"document\":\n        return self.handle_document(element)\n\n    if tag in self.tag_handlers:\n        return self.tag_handlers[tag](element)\n\n    return self.process_children(element)\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_block","title":"<code>handle_block(element)</code>","text":"<p>Handle block-level elements like div</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_block(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle block-level elements like div\"\"\"\n    content = self.process_children(element)\n    return f\"\\n\\n{content}\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_blockquote","title":"<code>handle_blockquote(element)</code>","text":"<p>Handle blockquote elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_blockquote(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle blockquote elements\"\"\"\n    content = element.text_content()\n    lines = content.split(\"\\n\")\n    quoted_lines = [f\"&gt; {line}\" if line.strip() else \"&gt;\" for line in lines]\n    return \"\\n\\n\" + \"\\n\".join(quoted_lines) + \"\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_code","title":"<code>handle_code(element)</code>","text":"<p>Handle inline code elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_code(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle inline code elements\"\"\"\n    if element.getparent() is not None and element.getparent().tag == \"pre\":\n        return element.text_content()\n\n    content = element.text_content()\n    return f\"`{content}`\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_document","title":"<code>handle_document(element)</code>","text":"<p>Handle document element (html or body)</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_document(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle document element (html or body)\"\"\"\n    return self.process_children(element, True)\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_emphasis","title":"<code>handle_emphasis(element)</code>","text":"<p>Handle emphasis/italic elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_emphasis(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle emphasis/italic elements\"\"\"\n    content = element.text_content().strip()\n    return f\"*{content}*\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_heading","title":"<code>handle_heading(element)</code>","text":"<p>Handle heading elements (h1-h6)</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_heading(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle heading elements (h1-h6)\"\"\"\n    level = int(element.tag[1])\n    content = element.text_content().strip()\n    return f\"\\n\\n{'#' * level} {content}\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_horizontal_rule","title":"<code>handle_horizontal_rule(element)</code>","text":"<p>Handle horizontal rule elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_horizontal_rule(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle horizontal rule elements\"\"\"\n    return \"\\n\\n---\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_image","title":"<code>handle_image(element)</code>","text":"<p>Handle image elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_image(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle image elements\"\"\"\n    src = element.get(\"src\", \"\")\n    alt = element.get(\"alt\", \"\")\n    title = element.get(\"title\", \"\")\n    if title:\n        return f'![{alt}]({src} \"{title}\")'\n    return f\"![{alt}]({src})\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_inline","title":"<code>handle_inline(element)</code>","text":"<p>Handle inline elements like span</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_inline(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle inline elements like span\"\"\"\n    return self.process_children(element)\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_linebreak","title":"<code>handle_linebreak(element)</code>","text":"<p>Handle line break elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_linebreak(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle line break elements\"\"\"\n    return \"\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_link","title":"<code>handle_link(element)</code>","text":"<p>Handle anchor elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_link(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle anchor elements\"\"\"\n    href = element.get(\"href\", \"\")\n    content = element.text_content() or href\n\n    if href.startswith(\"#\"):\n        anchor = href[1:]\n        return f\"[{content}](#{anchor})\"\n\n    return f\"[{content}]({href})\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_list_item","title":"<code>handle_list_item(element)</code>","text":"<p>Handle list item elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_list_item(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle list item elements\"\"\"\n    if self.list_state and self.list_state[-1][\"type\"] == \"ol\":\n        self.list_state[-1][\"index\"] += 1\n\n    indent = \"  \" * (len(self.list_state) - 1)\n    if self.list_state and self.list_state[-1][\"type\"] == \"ul\":\n        bullet = \"*\"\n    else:\n        bullet = f\"{self.list_state[-1]['index']}.\"\n\n    content = element.text or \"\"\n\n    for child in element:\n        if child.tag in self.tag_handlers:\n            content += self.tag_handlers[child.tag](child)\n        else:\n            content += self.process_children(child)\n\n        if child.tail:\n            content += child.tail\n\n    return f\"{indent}{bullet} {content}\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_ordered_list","title":"<code>handle_ordered_list(element)</code>","text":"<p>Handle ordered list elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_ordered_list(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle ordered list elements\"\"\"\n    start = element.get(\"start\", \"1\")\n    try:\n        start = int(start)\n    except ValueError:\n        start = 1\n\n    self.list_state.append({\"type\": \"ol\", \"index\": start - 1})\n    content = self.process_children(element, True)\n    self.list_state.pop()\n    return f\"\\n{content}\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_paragraph","title":"<code>handle_paragraph(element)</code>","text":"<p>Handle paragraph elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_paragraph(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle paragraph elements\"\"\"\n    content = element.text or \"\"\n\n    for child in element:\n        if child.tag in self.tag_handlers:\n            content += self.tag_handlers[child.tag](child)\n        else:\n            content += self.process_children(child)\n\n        if child.tail:\n            content += child.tail\n\n    if not content.strip():\n        return \"\"\n\n    return f\"\\n\\n{content}\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_preformatted","title":"<code>handle_preformatted(element)</code>","text":"<p>Handle preformatted code blocks</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_preformatted(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle preformatted code blocks\"\"\"\n    code_element = element.find(\"code\")\n    if code_element is not None:\n        content = code_element.text_content()\n        language = \"\"\n        for cls in code_element.get(\"class\", \"\").split():\n            if cls.startswith(\"language-\"):\n                language = cls[9:]\n                break\n        return f\"\\n```{language}\\n{content}\\n```\\n\"\n\n    content = element.text_content()\n    return f\"\\n```\\n{content}\\n```\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_strong","title":"<code>handle_strong(element)</code>","text":"<p>Handle strong/bold elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_strong(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle strong/bold elements\"\"\"\n    content = element.text_content().strip()\n    return f\"**{content}**\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_table","title":"<code>handle_table(element)</code>","text":"<p>Handle table elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_table(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle table elements\"\"\"\n    self.in_table = True\n    self.table_headers = []\n    self.table_rows = []\n\n    self.process_children(element)\n\n    if not self.table_headers and self.table_rows:\n        self.table_headers = self.table_rows[0]\n        self.table_rows = self.table_rows[1:]\n\n    if not self.table_headers:\n        self.in_table = False\n        return \"\"\n\n    result = []\n    result.append(\"| \" + \" | \".join(self.table_headers) + \" |\")\n    result.append(\"| \" + \" | \".join([\"---\"] * len(self.table_headers)) + \" |\")\n\n    for row in self.table_rows:\n        padded_row = row + [\"\"] * (len(self.table_headers) - len(row))\n        result.append(\"| \" + \" | \".join(padded_row) + \" |\")\n\n    self.in_table = False\n    return \"\\n\\n\" + \"\\n\".join(result) + \"\\n\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_table_cell","title":"<code>handle_table_cell(element)</code>","text":"<p>Handle table cell elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_table_cell(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle table cell elements\"\"\"\n    if not self.in_table:\n        return \"\"\n\n    content = element.text_content()\n    self.current_row.append(content.strip())\n    return \"\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_table_row","title":"<code>handle_table_row(element)</code>","text":"<p>Handle table row elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_table_row(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle table row elements\"\"\"\n    if not self.in_table:\n        return \"\"\n\n    self.current_row = []\n    self.process_children(element)\n\n    if element.findall(\"th\"):\n        self.table_headers = self.current_row\n    else:\n        self.table_rows.append(self.current_row)\n\n    return \"\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.handle_unordered_list","title":"<code>handle_unordered_list(element)</code>","text":"<p>Handle unordered list elements</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def handle_unordered_list(self, element: lxml_html.HtmlElement) -&gt; str:\n    \"\"\"Handle unordered list elements\"\"\"\n    self.list_state.append({\"type\": \"ul\", \"index\": 0})\n    content = self.process_children(element, True)\n    self.list_state.pop()\n    return f\"\\n{content}\\n\"\n</code></pre>"},{"location":"dynamiq/components/converters/html/#dynamiq.components.converters.html.MarkdownConverter.process_children","title":"<code>process_children(element, add_linebreaks=False)</code>","text":"<p>Process all children of an element and join their markdown</p> Source code in <code>dynamiq/components/converters/html.py</code> <pre><code>def process_children(self, element: lxml_html.HtmlElement, add_linebreaks=False) -&gt; str:\n    \"\"\"Process all children of an element and join their markdown\"\"\"\n    result = []\n\n    if element.text and element.text.strip():\n        result.append(element.text)\n\n    for child in element:\n        child_md = self.convert_element(child)\n        if child_md:\n            result.append(child_md)\n        if child.tail and child.tail.strip():\n            result.append(child.tail)\n\n    separator = \"\\n\" if add_linebreaks else \"\"\n    return separator.join([r for r in result if r and r.strip()])\n</code></pre>"},{"location":"dynamiq/components/converters/pptx/","title":"Pptx","text":""},{"location":"dynamiq/components/converters/pptx/#dynamiq.components.converters.pptx.PPTXConverter","title":"<code>PPTXConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting files to Documents using the pptx library.</p> <pre><code>  Initializes the object with the configuration for converting documents using\n  the python-pptx.\n\n  Args:\n  document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n      Determines how to create Documents from the elements of presentation. Options are:\n      - `\"one-doc-per-file\"`: Creates one Document per file.\n          All elements are concatenated into one text field.\n      - `\"one-doc-per-page\"`: Creates one Document per page.\n          All elements on a page are concatenated into one text field.\n      Defaults to `\"one-doc-per-file\"`.\n  extraction_mode(ExtractionMode): Type of text extraction format.\n\nUsage example:\n    ```python\n    from dynamiq.components.converters.pptx import PPTXConverter\n\n    converter = PPTXConverter()\n    documents = converter.run(paths=[\"a/file/path.pptx\", \"a/directory/path\"])[\"documents\"]\n    ```\n</code></pre> Source code in <code>dynamiq/components/converters/pptx.py</code> <pre><code>class PPTXConverter(BaseConverter):\n    \"\"\"\n    A component for converting files to Documents using the pptx library.\n\n          Initializes the object with the configuration for converting documents using\n          the python-pptx.\n\n          Args:\n          document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n              Determines how to create Documents from the elements of presentation. Options are:\n              - `\"one-doc-per-file\"`: Creates one Document per file.\n                  All elements are concatenated into one text field.\n              - `\"one-doc-per-page\"`: Creates one Document per page.\n                  All elements on a page are concatenated into one text field.\n              Defaults to `\"one-doc-per-file\"`.\n          extraction_mode(ExtractionMode): Type of text extraction format.\n\n        Usage example:\n            ```python\n            from dynamiq.components.converters.pptx import PPTXConverter\n\n            converter = PPTXConverter()\n            documents = converter.run(paths=[\"a/file/path.pptx\", \"a/directory/path\"])[\"documents\"]\n            ```\n    \"\"\"\n\n    document_creation_mode: Literal[DocumentCreationMode.ONE_DOC_PER_FILE, DocumentCreationMode.ONE_DOC_PER_PAGE] = (\n        DocumentCreationMode.ONE_DOC_PER_FILE\n    )\n\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a single presentation and create documents.\n\n        Args:\n            file (Union[Path, BytesIO]): The file to process.\n            metadata (Dict[str, Any]): Metadata to attach to the documents.\n\n        Returns:\n            List[Any]: A list of created documents.\n\n        Raises:\n            ValueError: If the file object doesn't have a name and its extension can't be guessed.\n            TypeError: If the file argument is neither a Path nor a BytesIO object.\n        \"\"\"\n        if isinstance(file, Path):\n            with open(file, \"rb\") as upload_file:\n                file_content = BytesIO(upload_file.read())\n                file_path = upload_file.name\n        elif isinstance(file, BytesIO):\n            file_path = get_filename_for_bytesio(file)\n            file_content = file\n        else:\n            raise TypeError(\"Expected a Path object or a BytesIO object.\")\n        elements = Presentation(file_content)\n        return self._create_documents(\n            filepath=file_path,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: Any,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the elements of the presentation.\n        \"\"\"\n        docs = []\n        if document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            text_all_slides = \"\\n\".join(\n                \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\") and shape.text)\n                for slide in elements.slides\n            )\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n            docs = [Document(content=text_all_slides, metadata=metadata)]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_PAGE:\n            texts_per_page = [\n                \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\") and shape.text)\n                for slide in elements.slides\n            ]\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n\n            docs = [Document(content=text, metadata=metadata) for text in texts_per_page]\n\n        return docs\n</code></pre>"},{"location":"dynamiq/components/converters/pypdf/","title":"Pypdf","text":"<p>pdf</p>"},{"location":"dynamiq/components/converters/text/","title":"Text","text":""},{"location":"dynamiq/components/converters/text/#dynamiq.components.converters.text.TextFileConverter","title":"<code>TextFileConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting text files to Documents using the text file converter.</p> <p>Initializes the object with the configuration for converting documents using the text file converter.</p> <p>Parameters:</p> Name Type Description Default <code>document_creation_mode</code> <code>Literal['one-doc-per-file']</code> <p>Determines how to create Documents from the text file content. Currently only supports: - <code>\"one-doc-per-file\"</code>: Creates one Document per file.     All content is converted to markdown format. Defaults to <code>\"one-doc-per-file\"</code>.</p> required Usage example <pre><code>from dynamiq.components.converters.txt import TextFileConverter\n\nconverter = TextFileConverter()\ndocuments = converter.run(paths=[\"a/file/path.txt\", \"a/directory/path\"])[\"documents\"]\n</code></pre> Source code in <code>dynamiq/components/converters/text.py</code> <pre><code>class TextFileConverter(BaseConverter):\n    \"\"\"\n    A component for converting text files to Documents using the text file converter.\n\n    Initializes the object with the configuration for converting documents using\n    the text file converter.\n\n    Args:\n        document_creation_mode (Literal[\"one-doc-per-file\"], optional):\n            Determines how to create Documents from the text file content. Currently only supports:\n            - `\"one-doc-per-file\"`: Creates one Document per file.\n                All content is converted to markdown format.\n            Defaults to `\"one-doc-per-file\"`.\n\n    Usage example:\n        ```python\n        from dynamiq.components.converters.txt import TextFileConverter\n\n        converter = TextFileConverter()\n        documents = converter.run(paths=[\"a/file/path.txt\", \"a/directory/path\"])[\"documents\"]\n        ```\n    \"\"\"\n\n    document_creation_mode: Literal[DocumentCreationMode.ONE_DOC_PER_FILE] = DocumentCreationMode.ONE_DOC_PER_FILE\n\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a file and return a list of Documents.\n\n        Args:\n            file: Path to a file or BytesIO object\n            metadata: Metadata to attach to the documents\n\n        Returns:\n            List of Documents\n        \"\"\"\n\n        if isinstance(file, BytesIO):\n            filepath = get_filename_for_bytesio(file)\n            file.seek(0)\n            data = file.read()\n        else:\n            filepath = str(file)\n            with open(file, \"rb\") as f:\n                data = f.read()\n\n        encoding = self._detect_encoding(data)\n        content = data.decode(encoding, errors=\"replace\")\n\n        # Create documents from the text file content\n        return self._create_documents(\n            filepath=filepath,\n            content=content,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _create_documents(\n        self,\n        filepath: str,\n        content: str,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the text content.\n        \"\"\"\n        if document_creation_mode != DocumentCreationMode.ONE_DOC_PER_FILE:\n            raise ValueError(\"TextFileConverter only supports one-doc-per-file mode\")\n\n        content = content.strip()\n\n        metadata = copy.deepcopy(metadata)\n        metadata[\"file_path\"] = filepath\n\n        docs = [Document(content=content, metadata=metadata)]\n        return docs\n\n    def _detect_encoding(self, data: bytes) -&gt; str:\n        \"\"\"\n        Detect the encoding of the data using charset_normalizer.\n        If detection fails, fallback to \"utf-8\".\n        \"\"\"\n        try:\n            result = from_bytes(data)\n            best = result.best()\n\n            if best and best.encoding:\n                encoding = best.encoding\n\n                try:\n                    data.decode(encoding)\n                    return encoding\n                except UnicodeDecodeError:\n                    logger.debug(f\"Detected encoding '{encoding}' failed to decode. Falling back...\")\n\n            else:\n                logger.debug(\"Encoding detection returned None. Falling back...\")\n\n        except Exception as e:\n            logger.debug(f\"Encoding detection error: {e}. Falling back...\")\n\n        return \"utf-8\"\n</code></pre>"},{"location":"dynamiq/components/converters/unstructured/","title":"Unstructured","text":""},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.UnstructuredFileConverter","title":"<code>UnstructuredFileConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting files to Documents using the Unstructured API (hosted or running locally).</p> <p>For the supported file types and the specific API parameters, see Unstructured docs.</p> <p>Usage example: <pre><code>from dynamiq.components.converters.unstructured.file_converter import UnstructuredFileConverter\n\n# make sure to either set the environment variable UNSTRUCTURED_API_KEY\n# or run the Unstructured API locally:\n# docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest\n# --port 8000 --host 0.0.0.0\n\nconverter = UnstructuredFileConverter()\ndocuments = converter.run(paths=[\"a/file/path.pdf\", \"a/directory/path\"])[\"documents\"]\n</code></pre></p> Source code in <code>dynamiq/components/converters/unstructured.py</code> <pre><code>class UnstructuredFileConverter(BaseConverter):\n    \"\"\"\n    A component for converting files to Documents using the Unstructured API (hosted or running locally).\n\n    For the supported file types and the specific API parameters, see\n    [Unstructured docs](https://unstructured-io.github.io/unstructured/api.html).\n\n    Usage example:\n    ```python\n    from dynamiq.components.converters.unstructured.file_converter import UnstructuredFileConverter\n\n    # make sure to either set the environment variable UNSTRUCTURED_API_KEY\n    # or run the Unstructured API locally:\n    # docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest\n    # --port 8000 --host 0.0.0.0\n\n    converter = UnstructuredFileConverter()\n    documents = converter.run(paths=[\"a/file/path.pdf\", \"a/directory/path\"])[\"documents\"]\n    ```\n    \"\"\"\n\n    connection: UnstructuredConnection = None\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    separator: str = \"\\n\\n\"\n    strategy: ConvertStrategy = ConvertStrategy.AUTO\n    unstructured_kwargs: dict[str, Any] | None = None\n    extract_image_block_types_enabled: bool = False\n    extract_image_block_types: list[UnstructuredElementTypes] | None = None\n\n    def __init__(self, *args, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = UnstructuredConnection()\n        super().__init__(**kwargs)\n        \"\"\"\n        Initializes the object with the configuration for converting documents using\n        the Unstructured API.\n\n        Args:\n        connection (UnstructuredConnection, optional): The connection to use for the Unstructured API.\n            Defaults to None, which will initialize a new UnstructuredConnection.\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n            Determines how to create Documents from the elements returned by Unstructured. Options are:\n            - `\"one-doc-per-file\"`: Creates one Document per file.\n                All elements are concatenated into one text field.\n            - `\"one-doc-per-page\"`: Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            - `\"one-doc-per-element\"`: Creates one Document per element.\n                Each element is converted to a separate Document.\n            Defaults to `\"one-doc-per-file\"`.\n        separator (str, optional): The separator to use between elements when concatenating them into one text field.\n            Defaults to \"\\n\\n\".\n        strategy (Literal[\"auto\", \"fast\", \"hi_res\", \"ocr_only\"], optional): The strategy to use for document processing.\n            Defaults to \"auto\".\n        unstructured_kwargs (Optional[dict[str, Any]], optional): Additional parameters to pass to the Unstructured API.\n            See [Unstructured API docs](https://unstructured-io.github.io/unstructured/apis/api_parameters.html)\n                for available parameters.\n            Defaults to None.\n        extract_image_block_types_enabled (bool, optional): Whether to extract and embed images/tables in the result.\n            When enabled, Base64-encoded images and tables will be decoded and included in the document content.\n            Defaults to False.\n        extract_image_block_types (Optional[list[UnstructuredElementTypes]], optional): List of element types to extract\n            when extract_image_block_types_enabled is True.\n            If None and extract_image_block_types_enabled is True,\n            defaults to [UnstructuredElementTypes.IMAGE, UnstructuredElementTypes.TABLE].\n            Defaults to None.\n        progress_bar (bool, optional): Whether to show a progress bar during the conversion process.\n            Defaults to True.\n\n        Returns:\n        None\n        \"\"\"\n\n    def _process_file(self, file: Path | str | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a single file and create documents.\n\n        Args:\n            file (Union[Path, str, BytesIO]): The file to process.\n            metadata (Dict[str, Any]): Metadata to attach to the documents.\n\n        Returns:\n            List[Any]: A list of created documents.\n\n        Raises:\n            ValueError: If the file object doesn't have a name and its extension can't be guessed.\n            TypeError: If the file argument is neither a Path, string, nor a BytesIO object.\n        \"\"\"\n        if isinstance(file, (Path, str)):\n            file_name = str(file)\n            elements = self._partition_file_into_elements_by_filepath(file_name)\n        elif isinstance(file, BytesIO):\n            file_name = get_filename_for_bytesio(file)\n            elements = self._partition_file_into_elements_by_file(file, file_name)\n        else:\n            raise TypeError(\"Expected a Path object, a string path, or a BytesIO object.\")\n        return self._create_documents(\n            filepath=file_name,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _partition_file_into_elements_by_filepath(self, filepath: Path | str) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Partition a file into elements using the Unstructured API.\n\n        Args:\n            filepath (Path | str): The path to the file to partition.\n\n        Returns:\n            List[Dict[str, Any]]: A list of elements extracted from the file.\n\n        Raises:\n            FileNotFoundError: If the file doesn't exist\n            ValueError: If the file is empty or cannot be processed\n            Exception: Any other exception that occurs during processing\n        \"\"\"\n        if isinstance(filepath, str):\n            filepath = Path(filepath)\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        if filepath.stat().st_size == 0:\n            raise ValueError(f\"Empty file cannot be processed: {filepath}\")\n\n        extract_types = self.extract_image_block_types if self.extract_image_block_types_enabled else None\n        if not extract_types and self.extract_image_block_types_enabled:\n            extract_types = [UnstructuredElementTypes.IMAGE, UnstructuredElementTypes.TABLE]\n\n        kwargs = copy.deepcopy(self.unstructured_kwargs) if self.unstructured_kwargs else {}\n        if \"extract_image_block_types\" not in kwargs:\n            kwargs[\"extract_image_block_types\"] = extract_types\n\n        return partition_via_api(\n            filename=str(filepath),\n            api_url=self.connection.url,\n            api_key=self.connection.api_key,\n            strategy=self.strategy,\n            **kwargs,\n        )\n\n    def _partition_file_into_elements_by_file(\n        self,\n        file: BytesIO,\n        metadata_filename: str,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Partition a file into elements using the Unstructured API.\n\n        Args:\n            file (BytesIO): The file object to partition.\n            metadata_filename (str): The filename to store in element metadata.\n\n        Returns:\n            List[Dict[str, Any]]: A list of elements extracted from the file.\n\n        Raises:\n            ValueError: If the file is empty or cannot be processed\n            Exception: Any other exception that occurs during processing\n        \"\"\"\n        extract_types = self.extract_image_block_types if self.extract_image_block_types_enabled else None\n        if not extract_types and self.extract_image_block_types_enabled:\n            extract_types = [UnstructuredElementTypes.IMAGE, UnstructuredElementTypes.TABLE]\n\n        kwargs = copy.deepcopy(self.unstructured_kwargs) if self.unstructured_kwargs else {}\n        if \"extract_image_block_types\" not in kwargs:\n            kwargs[\"extract_image_block_types\"] = extract_types\n\n        return partition_via_api(\n            filename=None,\n            file=file,\n            metadata_filename=metadata_filename,\n            api_url=self.connection.url,\n            api_key=self.connection.api_key,\n            strategy=self.strategy,\n            **kwargs,\n        )\n\n    def _collect_images_and_tables(self, elements: list[dict]) -&gt; tuple[list[dict], list[dict]]:\n        \"\"\"\n        Collect images and tables from elements for separate processing.\n\n        Args:\n            elements (list[dict]): List of element dictionaries from Unstructured API.\n\n        Returns:\n            tuple: (images, tables) where each is a list of dicts with element data and metadata.\n        \"\"\"\n        images = []\n        tables = []\n\n        for idx, element in enumerate(elements):\n            metadata = element.get(\"metadata\", {})\n\n            if \"image_base64\" in metadata:\n                base64_data = metadata[\"image_base64\"]\n                try:\n                    decode_chars = max(16, min(50, len(base64_data)))\n                    decoded_sample = base64.b64decode(base64_data[:decode_chars])\n                    if decoded_sample.startswith(b\"\\xff\\xd8\\xff\"):\n                        image_format = \"jpeg\"\n                    elif decoded_sample.startswith(b\"\\x89PNG\"):\n                        image_format = \"png\"\n                    elif decoded_sample.startswith(b\"GIF8\"):\n                        image_format = \"gif\"\n                    elif (\n                        len(decoded_sample) &gt;= 12\n                        and decoded_sample.startswith(b\"RIFF\")\n                        and decoded_sample[8:12] == b\"WEBP\"\n                    ):\n                        image_format = \"webp\"\n                    else:\n                        image_format = \"png\"\n                except Exception:\n                    image_format = \"png\"\n\n                images.append(\n                    {\n                        \"id\": generate_uuid(),\n                        \"index\": idx,\n                        \"format\": image_format,\n                        \"base64_data\": base64_data,\n                        \"text\": str(element.get(\"text\", \"\")),\n                        \"element_metadata\": metadata,\n                    }\n                )\n\n            elif \"text_as_html\" in metadata:\n                tables.append(\n                    {\n                        \"index\": idx,\n                        \"html\": metadata[\"text_as_html\"],\n                        \"text\": str(element.get(\"text\", \"\")),\n                        \"element_metadata\": metadata,\n                    }\n                )\n\n        return images, tables\n\n    def _process_element_with_placeholder(self, element: dict, element_index: int, images: list[dict]) -&gt; str:\n        \"\"\"\n        Process an element with placeholders for images/tables instead of embedded content.\n\n        Args:\n            element (dict): The element dictionary from Unstructured API.\n            element_index (int): Index of this element in the original elements list.\n            images (list[dict]): List of collected images.\n            tables (list[dict]): List of collected tables.\n\n        Returns:\n            str: The processed text content with placeholders for images/tables.\n        \"\"\"\n        text = str(element.get(\"text\", \"\"))\n\n        if \"text_as_html\" in element.get(\"metadata\", {}):\n            text = element.get(\"metadata\", {}).get(\"text_as_html\", \"\")\n        elif \"image_base64\" in element.get(\"metadata\", {}):\n            # Find corresponding image entry\n            image_entry = next((i for i in images if i[\"index\"] == element_index), None)\n            if image_entry:\n                image_id = image_entry[\"id\"]\n                placeholder_text = image_entry[\"text\"].strip()\n                if placeholder_text:\n                    text = f\"[IMAGE:{image_id}:{placeholder_text}]\"\n                else:\n                    text = f\"[IMAGE:{image_id}]\"\n\n        return text\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: list[dict],\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the elements returned by Unstructured.\n        \"\"\"\n        separator = self.separator\n        docs = []\n\n        images, tables = self._collect_images_and_tables(elements)\n\n        if document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            element_texts = []\n            for idx, el in enumerate(elements):\n                if self.extract_image_block_types_enabled:\n                    text = self._process_element_with_placeholder(el, idx, images)\n                else:\n                    text = str(el.get(\"text\", \"\"))\n\n                if el.get(\"category\") == \"Title\":\n                    element_texts.append(\"# \" + text)\n                else:\n                    element_texts.append(text)\n\n            text = separator.join(element_texts)\n            doc_metadata = copy.deepcopy(metadata)\n            doc_metadata[\"file_path\"] = str(filepath)\n\n            if self.extract_image_block_types_enabled:\n                if images:\n                    for image in images:\n                        image.pop(\"element_metadata\", None)\n                    doc_metadata[\"images\"] = images\n                if tables:\n                    for table in tables:\n                        table.pop(\"element_metadata\", None)\n                    doc_metadata[\"tables\"] = tables\n\n            docs = [Document(content=text, metadata=doc_metadata)]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_PAGE:\n            texts_per_page: defaultdict[int, str] = defaultdict(str)\n            meta_per_page: defaultdict[int, dict] = defaultdict(dict)\n            images_per_page: defaultdict[int, list] = defaultdict(list)\n            tables_per_page: defaultdict[int, list] = defaultdict(list)\n\n            for idx, el in enumerate(elements):\n                if self.extract_image_block_types_enabled:\n                    text = self._process_element_with_placeholder(el, idx, images)\n                else:\n                    text = str(el.get(\"text\", \"\"))\n\n                doc_metadata = copy.deepcopy(metadata)\n                doc_metadata[\"file_path\"] = str(filepath)\n                element_metadata = el.get(\"metadata\")\n                if element_metadata:\n                    doc_metadata.update(element_metadata)\n                page_number = int(doc_metadata.get(\"page_number\", 1))\n\n                texts_per_page[page_number] += text + separator\n                meta_per_page[page_number].update(doc_metadata)\n\n                if self.extract_image_block_types_enabled:\n                    # Find images/tables for this element\n                    element_images = [img for img in images if img[\"index\"] == idx]\n                    element_tables = [tbl for tbl in tables if tbl[\"index\"] == idx]\n                    images_per_page[page_number].extend(element_images)\n                    tables_per_page[page_number].extend(element_tables)\n\n            for page in texts_per_page.keys():\n                page_metadata = meta_per_page[page]\n                if images_per_page[page]:\n                    for image in images_per_page[page]:\n                        image.pop(\"element_metadata\", None)\n                    page_metadata[\"images\"] = images_per_page[page]\n                if tables_per_page[page]:\n                    for table in tables_per_page[page]:\n                        table.pop(\"element_metadata\", None)\n                    page_metadata[\"tables\"] = tables_per_page[page]\n\n                docs.append(Document(content=texts_per_page[page], metadata=page_metadata))\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_ELEMENT:\n            for index, el in enumerate(elements):\n                if self.extract_image_block_types_enabled:\n                    text = self._process_element_with_placeholder(el, index, images)\n                else:\n                    text = str(el.get(\"text\", \"\"))\n\n                doc_metadata = copy.deepcopy(metadata)\n                doc_metadata[\"file_path\"] = str(filepath)\n                doc_metadata[\"element_index\"] = index\n                element_metadata = el.get(\"metadata\", {})\n                element_metadata = dict(element_metadata)\n\n                # Add images/tables for this specific element\n                if self.extract_image_block_types_enabled:\n                    element_images = [img for img in images if img[\"index\"] == index]\n                    element_tables = [tbl for tbl in tables if tbl[\"index\"] == index]\n                    if element_images:\n                        image_ids = [img[\"id\"] for img in element_images]\n                        if len(image_ids) == 1:\n                            element_metadata[\"image_id\"] = image_ids[0]\n                        else:\n                            element_metadata[\"image_ids\"] = image_ids\n\n                        for image in element_images:\n                            image.pop(\"element_metadata\", None)\n                        doc_metadata[\"images\"] = element_images\n                    if element_tables:\n                        for table in element_tables:\n                            table.pop(\"element_metadata\", None)\n                        doc_metadata[\"tables\"] = element_tables\n\n                doc_metadata.update(element_metadata)\n                element_category = el.get(\"category\")\n                if element_category:\n                    doc_metadata[\"category\"] = element_category\n\n                doc = Document(content=text, metadata=doc_metadata)\n                docs.append(doc)\n        return docs\n</code></pre>"},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.partition_via_api","title":"<code>partition_via_api(filename=None, file=None, file_filename=None, api_url='https://api.unstructured.io/', api_key='', metadata_filename=None, extract_image_block_types=None, **request_kwargs)</code>","text":"<p>Partitions a document using the Unstructured REST API. This is equivalent to running the document through partition.</p> <p>See https://api.unstructured.io/general/docs for the hosted API documentation or https://github.com/Unstructured-IO/unstructured-api for instructions on how to run the API locally as a container.</p>"},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.partition_via_api--parameters","title":"Parameters","text":"<p>filename     A string defining the target filename path. content_type     A string defining the file content in MIME type file     A file-like object using \"rb\" mode --&gt; open(filename, \"rb\"). metadata_filename     When file is not None, the filename (string) to store in element metadata. E.g. \"foo.txt\" api_url     The URL for the Unstructured API. Defaults to the hosted Unstructured API. api_key     The API key to pass to the Unstructured API. extract_image_block_types     List of element types to extract as Base64-encoded representations.     Common types include \"Image\", \"Table\". Element type names are case-insensitive.     When specified, elements of these types will include an \"image_base64\" field in their metadata. request_kwargs     Additional parameters to pass to the data field of the request to the Unstructured API.</p> Source code in <code>dynamiq/components/converters/unstructured.py</code> <pre><code>def partition_via_api(\n    filename: str | None = None,\n    file: IO[bytes] | None = None,\n    file_filename: str | None = None,\n    api_url: str = \"https://api.unstructured.io/\",\n    api_key: str = \"\",\n    metadata_filename: str | None = None,\n    extract_image_block_types: list[UnstructuredElementTypes] | None = None,\n    **request_kwargs,\n) -&gt; list[dict]:\n    \"\"\"Partitions a document using the Unstructured REST API. This is equivalent to\n    running the document through partition.\n\n    See https://api.unstructured.io/general/docs for the hosted API documentation or\n    https://github.com/Unstructured-IO/unstructured-api for instructions on how to run\n    the API locally as a container.\n\n    Parameters\n    ----------\n    filename\n        A string defining the target filename path.\n    content_type\n        A string defining the file content in MIME type\n    file\n        A file-like object using \"rb\" mode --&gt; open(filename, \"rb\").\n    metadata_filename\n        When file is not None, the filename (string) to store in element metadata. E.g. \"foo.txt\"\n    api_url\n        The URL for the Unstructured API. Defaults to the hosted Unstructured API.\n    api_key\n        The API key to pass to the Unstructured API.\n    extract_image_block_types\n        List of element types to extract as Base64-encoded representations.\n        Common types include \"Image\", \"Table\". Element type names are case-insensitive.\n        When specified, elements of these types will include an \"image_base64\" field in their metadata.\n    request_kwargs\n        Additional parameters to pass to the data field of the request to the Unstructured API.\n    \"\"\"\n\n    if metadata_filename and file_filename:\n        raise ValueError(\n            \"Only one of metadata_filename and file_filename is specified. \"\n            \"metadata_filename is preferred. file_filename is marked for deprecation.\",\n        )\n\n    if file_filename is not None:\n        metadata_filename = file_filename\n        logger.warning(\n            \"The file_filename kwarg will be deprecated in a future version of unstructured. \"\n            \"Please use metadata_filename instead.\",\n        )\n\n    base_url = api_url[:-19] if \"/general/v0/general\" in api_url else api_url\n    client = UnstructuredClient(api_key_auth=api_key, server_url=base_url)\n\n    files = None\n    if filename is not None:\n        with open(filename, \"rb\") as f:\n            files = shared.Files(\n                content=f.read(),\n                file_name=filename,\n            )\n\n    elif file is not None:\n        if metadata_filename is None:\n            raise ValueError(\n                \"If file is specified in partition_via_api, \"\n                \"metadata_filename must be specified as well.\",\n            )\n        file.seek(0)\n        files = shared.Files(\n            content=file.read(),\n            file_name=metadata_filename,\n        )\n\n    req_kwargs = request_kwargs.copy()\n    if extract_image_block_types is not None:\n        req_kwargs[\"extract_image_block_types\"] = [element.value for element in extract_image_block_types]\n\n    req = operations.PartitionRequest(\n        partition_parameters=shared.PartitionParameters(\n            files=files,\n            **req_kwargs,\n        )\n    )\n    response = client.general.partition(request=req)\n\n    if response.status_code == 200:\n        return response.elements\n    else:\n        raise ValueError(\n            f\"Receive unexpected status code {response.status_code} from the API.\",\n        )\n</code></pre>"},{"location":"dynamiq/components/converters/utils/","title":"Utils","text":""},{"location":"dynamiq/components/converters/utils/#dynamiq.components.converters.utils.get_filename_for_bytesio","title":"<code>get_filename_for_bytesio(file)</code>","text":"<p>Get a filepath for a BytesIO object.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>BytesIO</code> <p>The BytesIO object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A filename for the BytesIO object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension couldn't be guessed.</p> Source code in <code>dynamiq/components/converters/utils.py</code> <pre><code>def get_filename_for_bytesio(file: BytesIO) -&gt; str:\n    \"\"\"\n    Get a filepath for a BytesIO object.\n\n    Args:\n        file (BytesIO): The BytesIO object.\n\n    Returns:\n        str: A filename for the BytesIO object.\n\n    Raises:\n        ValueError: If the file extension couldn't be guessed.\n    \"\"\"\n    filename = getattr(file, \"name\", None)\n    if filename is None:\n        file_extension = filetype.guess_extension(file)\n        if file_extension:\n            filename = f\"{generate_uuid()}.{file_extension}\"\n        else:\n            raise ValueError(\n                \"Unable to determine file extension. BytesIO object lacks name and \"\n                \"extension couldn't be guessed.\"\n            )\n    return filename\n</code></pre>"},{"location":"dynamiq/components/embedders/base/","title":"Base","text":""},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder","title":"<code>BaseEmbedder</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Initializes the Embedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Optional[BaseConnection]</code> <p>The connection to the  API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>prefix</code> <code>str</code> <p>A prefix string to prepend to each document text before embedding.</p> <code>suffix</code> <code>str</code> <p>A suffix string to append to each document text after embedding.</p> <code>batch_size</code> <code>int</code> <p>The number of documents to encode in a single batch.</p> <code>meta_fields_to_embed</code> <code>Optional[list[str]]</code> <p>A list of document meta fields to embed alongside the document text.</p> <code>embedding_separator</code> <code>str</code> <p>The separator string used to join document text with meta fields for embedding.</p> <code>truncate(str)</code> <code>str</code> <p>truncate embeddings that are too long from start or end, (\"NONE\"|\"START\"|\"END\"). Passing \"START\" will discard the start of the input. \"END\" will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model. If \"NONE\" is selected, when the input exceeds the maximum input token length an error will be returned.</p> <code>input_type(str)</code> <code>str</code> <p>specifies the type of input you're giving to the model. Supported values are \"search_document\", \"search_query\", \"classification\" and \"clustering\".</p> <code>dimensions(int)</code> <code>str</code> <p>he number of dimensions the resulting output embeddings should have. Only supported in OpenAI/Azure text-embedding-3 and later models.</p> <code>truncation_enabled(bool)</code> <code>str</code> <p>Whether to enable automatic text truncation for long inputs that exceed the embedding model's token limits. Defaults to True.</p> <code>max_input_tokens(int)</code> <code>str</code> <p>Maximum number of tokens allowed for input text. If text exceeds this limit and truncation_enabled is True, the text will be truncated. Defaults to 8192.</p> <code>truncation_method(TruncationMethod)</code> <code>str</code> <p>Method to use for truncation when text exceeds max_input_tokens. Options: TruncationMethod.START, TruncationMethod.END, TruncationMethod.MIDDLE. Defaults to TruncationMethod.MIDDLE.</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>class BaseEmbedder(BaseModel):\n    \"\"\"\n    Initializes the Embedder component with given configuration.\n\n    Attributes:\n        connection (Optional[BaseConnection]): The connection to the  API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding.\n        prefix (str): A prefix string to prepend to each document text before embedding.\n        suffix (str): A suffix string to append to each document text after embedding.\n        batch_size (int): The number of documents to encode in a single batch.\n        meta_fields_to_embed (Optional[list[str]]): A list of document meta fields to embed alongside\n            the document text.\n        embedding_separator (str): The separator string used to join document text with meta fields\n            for embedding.\n        truncate(str): truncate embeddings that are too long from start or end, (\"NONE\"|\"START\"|\"END\").\n            Passing \"START\" will discard the start of the input. \"END\" will discard the end of the input. In both\n            cases, input is discarded until the remaining input is exactly the maximum input token length\n            for the model. If \"NONE\" is selected, when the input exceeds the maximum input token length\n            an error will be returned.\n        input_type(str):specifies the type of input you're giving to the model. Supported values are\n            \"search_document\", \"search_query\", \"classification\" and \"clustering\".\n        dimensions(int):he number of dimensions the resulting output embeddings should have.\n            Only supported in OpenAI/Azure text-embedding-3 and later models.\n        truncation_enabled(bool): Whether to enable automatic text truncation for long inputs that exceed\n            the embedding model's token limits. Defaults to True.\n        max_input_tokens(int): Maximum number of tokens allowed for input text. If text exceeds this limit\n            and truncation_enabled is True, the text will be truncated. Defaults to 8192.\n        truncation_method(TruncationMethod): Method to use for truncation when text exceeds max_input_tokens.\n            Options: TruncationMethod.START, TruncationMethod.END, TruncationMethod.MIDDLE.\n            Defaults to TruncationMethod.MIDDLE.\n\n    \"\"\"\n\n    @staticmethod\n    def validate_embedding(embedding: Any) -&gt; None:\n        \"\"\"\n        Validates that an embedding is valid.\n\n        Args:\n            embedding: The embedding vector to validate\n\n        Raises:\n            InvalidEmbeddingError: If the embedding is None, empty, or malformed\n        \"\"\"\n        try:\n            if embedding is None:\n                raise InvalidEmbeddingError(\"Embedding is None\")\n\n            if len(embedding) == 0:\n                raise InvalidEmbeddingError(\"Embedding is empty (zero length)\")\n        except (TypeError, AttributeError):\n            raise InvalidEmbeddingError(\"Embedding has no length attribute or is not iterable\")\n\n    @staticmethod\n    def validate_document_embeddings(documents: Any) -&gt; None:\n        \"\"\"\n        Validates embeddings for a list of documents.\n\n        Args:\n            documents: List of documents with embeddings\n\n        Raises:\n            DocumentEmbeddingValidationError: If any document embedding is invalid\n        \"\"\"\n        if not documents:\n            return\n\n        try:\n            for i, doc in enumerate(documents):\n                if not hasattr(doc, \"embedding\") or doc.embedding is None:\n                    raise DocumentEmbeddingValidationError(f\"Document at index {i} has no embedding\")\n\n                try:\n                    BaseEmbedder.validate_embedding(doc.embedding)\n                except InvalidEmbeddingError as e:\n                    raise DocumentEmbeddingValidationError(f\"Document at index {i}: {str(e)}\")\n        except (TypeError, AttributeError):\n            raise DocumentEmbeddingValidationError(\"Documents is not iterable or has incorrect structure\")\n    model: str\n    connection: BaseConnection\n    prefix: str = \"\"\n    suffix: str = \"\"\n    batch_size: int = 32\n    meta_fields_to_embed: list[str] | None = []\n    embedding_separator: str = \"\\n\"\n    truncate: str | None = None\n    input_type: str | None = None\n    dimensions: int | None = None\n    truncation_enabled: bool = True\n    max_input_tokens: int = 8192\n    truncation_method: TruncationMethod = TruncationMethod.MIDDLE\n    client: Any | None = None\n\n    _embedding: Callable = PrivateAttr()\n\n    def __init__(self, *args, **kwargs):\n        # Import in runtime to save memory\n        super().__init__(**kwargs)\n        from litellm import embedding\n\n        self._embedding = embedding\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = self.connection.conn_params\n        if self.client:\n            params = {\"client\": self.client}\n        return params\n\n    def _apply_text_truncation(self, text: str) -&gt; str:\n        \"\"\"\n        Apply text truncation if enabled and text exceeds max_input_tokens.\n\n        Args:\n            text: The text to potentially truncate\n\n        Returns:\n            Original or truncated text\n        \"\"\"\n        if not self.truncation_enabled or not text:\n            return text\n\n        return truncate_text_for_embedding(\n            text=text, max_tokens=self.max_input_tokens, truncation_method=self.truncation_method\n        )\n\n    def embed_text(self, text: str) -&gt; dict:\n        \"\"\"\n        Embeds a single string using the Embedder model specified during the initialization of the component.\n\n        Args:\n            text (str): The text string to be embedded.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'embedding': A list representing the embedding vector of the input text.\n                - 'meta': A dictionary with metadata information about the model usage.\n\n        Raises:\n            TypeError: If input is not a string\n            ValueError: If the embedding response is invalid\n        \"\"\"\n        if not isinstance(text, str):\n            msg = (\n                \"TextEmbedder expects a string as input.\"\n                \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n            )\n            raise TypeError(msg)\n\n        text_to_embed = self.prefix + text + self.suffix\n        text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n        text_to_embed = self._apply_text_truncation(text_to_embed)\n\n        response = self._embedding(model=self.model, input=[text_to_embed], **self.embed_params)\n\n        meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n        embedding = response.data[0][\"embedding\"]\n\n        try:\n            self.validate_embedding(embedding)\n        except InvalidEmbeddingError as e:\n            logger.error(f\"Invalid embedding returned by model {self.model}: {str(e)}\")\n            raise ValueError(f\"Invalid embedding returned by the model: {str(e)}\")\n\n        return {\"embedding\": embedding, \"meta\": meta}\n\n    def _prepare_documents_to_embed(self, documents: list[Document]) -&gt; list[str]:\n        \"\"\"\n        Prepare the texts to embed by concatenating the Document text with the metadata fields to embed.\n\n        Args:\n            documents (list[Document]): A list of Document objects to prepare for embedding.\n\n        Returns:\n            list[str]: A list of concatenated strings ready for embedding.\n        \"\"\"\n        texts_to_embed: list[str] = []\n        for doc in documents:\n            meta_values_to_embed = [\n                str(doc.meta[key])\n                for key in self.meta_fields_to_embed\n                if doc.meta.get(key) is not None\n            ]\n\n            text_to_embed = self.embedding_separator.join(\n                meta_values_to_embed + [doc.content or \"\"]\n            )\n            text_to_embed = self._apply_text_truncation(text_to_embed)\n            texts_to_embed.append(text_to_embed)\n        return texts_to_embed\n\n    def _embed_texts_batch(\n        self, texts_to_embed: list[str], batch_size: int\n    ) -&gt; tuple[list[list[float]], dict[str, Any]]:\n        \"\"\"\n        Embed a list of texts in batches.\n        \"\"\"\n        all_embeddings = []\n        meta: dict[str, Any] = {}\n        embed_params = self.embed_params\n        for i in range(0, len(texts_to_embed), batch_size):\n            batch = texts_to_embed[i : i + batch_size]\n            response = self._embedding(model=self.model, input=batch, **embed_params)\n            embeddings = [el[\"embedding\"] for el in response.data]\n            all_embeddings.extend(embeddings)\n\n            if \"model\" not in meta:\n                meta[\"model\"] = response.model\n            if \"usage\" not in meta:\n                meta[\"usage\"] = dict(response.usage)\n            else:\n                meta[\"usage\"][\"prompt_tokens\"] += response.usage.prompt_tokens\n                meta[\"usage\"][\"total_tokens\"] += response.usage.total_tokens\n\n        return all_embeddings, meta\n\n    def embed_documents(self, documents: list[Document]) -&gt; dict:\n        \"\"\"\n        Embeds a list of documents and returns the embedded documents along with meta information.\n\n        Args:\n            documents (list[Document]): The documents to be embedded.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'documents' (list[Document]): The input documents with their embeddings populated.\n                - 'meta' (dict): Metadata information about the embedding process.\n\n        Raises:\n            TypeError: If input is not a list of Documents\n            ValueError: If the embedding response is invalid\n        \"\"\"\n        if (\n            not isinstance(documents, list)\n            or documents\n            and not isinstance(documents[0], Document)\n        ):\n            msg = (\n                \"DocumentEmbedder expects a list of Documents as input.\"\n                \"In case you want to embed a string, please use the embed_text.\"\n            )\n            raise TypeError(msg)\n\n        if not documents:\n            # return early if we were passed an empty list\n            return {\"documents\": [], \"meta\": {}}\n\n        texts_to_embed = self._prepare_documents_to_embed(documents=documents)\n\n        embeddings, meta = self._embed_texts_batch(\n            texts_to_embed=texts_to_embed, batch_size=self.batch_size\n        )\n\n        for doc, emb in zip(documents, embeddings):\n            doc.embedding = emb\n\n        try:\n            self.validate_document_embeddings(documents)\n        except DocumentEmbeddingValidationError as e:\n            logger.error(f\"Invalid document embeddings returned by model {self.model}: {str(e)}\")\n            raise ValueError(f\"Invalid document embeddings: {str(e)}\")\n\n        return {\"documents\": documents, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.embed_documents","title":"<code>embed_documents(documents)</code>","text":"<p>Embeds a list of documents and returns the embedded documents along with meta information.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The documents to be embedded.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - 'documents' (list[Document]): The input documents with their embeddings populated. - 'meta' (dict): Metadata information about the embedding process.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a list of Documents</p> <code>ValueError</code> <p>If the embedding response is invalid</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>def embed_documents(self, documents: list[Document]) -&gt; dict:\n    \"\"\"\n    Embeds a list of documents and returns the embedded documents along with meta information.\n\n    Args:\n        documents (list[Document]): The documents to be embedded.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'documents' (list[Document]): The input documents with their embeddings populated.\n            - 'meta' (dict): Metadata information about the embedding process.\n\n    Raises:\n        TypeError: If input is not a list of Documents\n        ValueError: If the embedding response is invalid\n    \"\"\"\n    if (\n        not isinstance(documents, list)\n        or documents\n        and not isinstance(documents[0], Document)\n    ):\n        msg = (\n            \"DocumentEmbedder expects a list of Documents as input.\"\n            \"In case you want to embed a string, please use the embed_text.\"\n        )\n        raise TypeError(msg)\n\n    if not documents:\n        # return early if we were passed an empty list\n        return {\"documents\": [], \"meta\": {}}\n\n    texts_to_embed = self._prepare_documents_to_embed(documents=documents)\n\n    embeddings, meta = self._embed_texts_batch(\n        texts_to_embed=texts_to_embed, batch_size=self.batch_size\n    )\n\n    for doc, emb in zip(documents, embeddings):\n        doc.embedding = emb\n\n    try:\n        self.validate_document_embeddings(documents)\n    except DocumentEmbeddingValidationError as e:\n        logger.error(f\"Invalid document embeddings returned by model {self.model}: {str(e)}\")\n        raise ValueError(f\"Invalid document embeddings: {str(e)}\")\n\n    return {\"documents\": documents, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.embed_text","title":"<code>embed_text(text)</code>","text":"<p>Embeds a single string using the Embedder model specified during the initialization of the component.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text string to be embedded.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - 'embedding': A list representing the embedding vector of the input text. - 'meta': A dictionary with metadata information about the model usage.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a string</p> <code>ValueError</code> <p>If the embedding response is invalid</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>def embed_text(self, text: str) -&gt; dict:\n    \"\"\"\n    Embeds a single string using the Embedder model specified during the initialization of the component.\n\n    Args:\n        text (str): The text string to be embedded.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'embedding': A list representing the embedding vector of the input text.\n            - 'meta': A dictionary with metadata information about the model usage.\n\n    Raises:\n        TypeError: If input is not a string\n        ValueError: If the embedding response is invalid\n    \"\"\"\n    if not isinstance(text, str):\n        msg = (\n            \"TextEmbedder expects a string as input.\"\n            \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n        )\n        raise TypeError(msg)\n\n    text_to_embed = self.prefix + text + self.suffix\n    text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n    text_to_embed = self._apply_text_truncation(text_to_embed)\n\n    response = self._embedding(model=self.model, input=[text_to_embed], **self.embed_params)\n\n    meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n    embedding = response.data[0][\"embedding\"]\n\n    try:\n        self.validate_embedding(embedding)\n    except InvalidEmbeddingError as e:\n        logger.error(f\"Invalid embedding returned by model {self.model}: {str(e)}\")\n        raise ValueError(f\"Invalid embedding returned by the model: {str(e)}\")\n\n    return {\"embedding\": embedding, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.validate_document_embeddings","title":"<code>validate_document_embeddings(documents)</code>  <code>staticmethod</code>","text":"<p>Validates embeddings for a list of documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>Any</code> <p>List of documents with embeddings</p> required <p>Raises:</p> Type Description <code>DocumentEmbeddingValidationError</code> <p>If any document embedding is invalid</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>@staticmethod\ndef validate_document_embeddings(documents: Any) -&gt; None:\n    \"\"\"\n    Validates embeddings for a list of documents.\n\n    Args:\n        documents: List of documents with embeddings\n\n    Raises:\n        DocumentEmbeddingValidationError: If any document embedding is invalid\n    \"\"\"\n    if not documents:\n        return\n\n    try:\n        for i, doc in enumerate(documents):\n            if not hasattr(doc, \"embedding\") or doc.embedding is None:\n                raise DocumentEmbeddingValidationError(f\"Document at index {i} has no embedding\")\n\n            try:\n                BaseEmbedder.validate_embedding(doc.embedding)\n            except InvalidEmbeddingError as e:\n                raise DocumentEmbeddingValidationError(f\"Document at index {i}: {str(e)}\")\n    except (TypeError, AttributeError):\n        raise DocumentEmbeddingValidationError(\"Documents is not iterable or has incorrect structure\")\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.validate_embedding","title":"<code>validate_embedding(embedding)</code>  <code>staticmethod</code>","text":"<p>Validates that an embedding is valid.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>Any</code> <p>The embedding vector to validate</p> required <p>Raises:</p> Type Description <code>InvalidEmbeddingError</code> <p>If the embedding is None, empty, or malformed</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>@staticmethod\ndef validate_embedding(embedding: Any) -&gt; None:\n    \"\"\"\n    Validates that an embedding is valid.\n\n    Args:\n        embedding: The embedding vector to validate\n\n    Raises:\n        InvalidEmbeddingError: If the embedding is None, empty, or malformed\n    \"\"\"\n    try:\n        if embedding is None:\n            raise InvalidEmbeddingError(\"Embedding is None\")\n\n        if len(embedding) == 0:\n            raise InvalidEmbeddingError(\"Embedding is empty (zero length)\")\n    except (TypeError, AttributeError):\n        raise InvalidEmbeddingError(\"Embedding has no length attribute or is not iterable\")\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.DocumentEmbeddingValidationError","title":"<code>DocumentEmbeddingValidationError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when document embeddings validation fails.</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>class DocumentEmbeddingValidationError(ValueError):\n    \"\"\"Error raised when document embeddings validation fails.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.InvalidEmbeddingError","title":"<code>InvalidEmbeddingError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when an embedding is invalid, including empty, null, or malformed embeddings.</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>class InvalidEmbeddingError(ValueError):\n    \"\"\"Error raised when an embedding is invalid, including empty, null, or malformed embeddings.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/components/embedders/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/components/embedders/bedrock/#dynamiq.components.embedders.bedrock.BedrockEmbedder","title":"<code>BedrockEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the BedrockEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AWS</code> <p>The connection to the  Bedrock API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"amazon.titan-embed-text-v1\".</p> Source code in <code>dynamiq/components/embedders/bedrock.py</code> <pre><code>class BedrockEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the BedrockEmbedder component with given configuration.\n\n    Attributes:\n        connection (BedrockConnection): The connection to the  Bedrock API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"amazon.titan-embed-text-v1\".\n    \"\"\"\n    connection: BedrockConnection\n    model: str = \"amazon.titan-embed-text-v1\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        if \"cohere\" in self.model:\n            params[\"input_type\"] = self.input_type\n            if self.truncate:\n                params[\"truncate\"] = self.truncate\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/cohere/","title":"Cohere","text":""},{"location":"dynamiq/components/embedders/cohere/#dynamiq.components.embedders.cohere.CohereEmbedder","title":"<code>CohereEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the CohereEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cohere</code> <p>The connection to the  Cohere API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"cohere/embed-english-v2.0\"</p> <code>input_type</code> <code>str</code> <p>Specifies the type of input you're giving to the model. Defaults to \"search_query\"</p> Source code in <code>dynamiq/components/embedders/cohere.py</code> <pre><code>class CohereEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the CohereEmbedder component with given configuration.\n\n    Attributes:\n        connection (CohereConnection): The connection to the  Cohere API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"cohere/embed-english-v2.0\"\n        input_type (str): Specifies the type of input you're giving to the model. Defaults to \"search_query\"\n    \"\"\"\n    connection: CohereConnection\n    model: str = \"cohere/embed-english-v2.0\"\n    input_type: str = \"search_query\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        params[\"input_type\"] = self.input_type\n        if self.truncate:\n            params[\"truncate\"] = self.truncate\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/gemini/","title":"Gemini","text":""},{"location":"dynamiq/components/embedders/gemini/#dynamiq.components.embedders.gemini.GeminiEmbedder","title":"<code>GeminiEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the GeminiEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Gemini</code> <p>The connection to the Gemini API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"gemini/gemini-embedding-exp-03-07\"</p> <code>input_type</code> <code>str</code> <p>Specifies the type of embedding task. Defaults to \"search_query\"</p> Source code in <code>dynamiq/components/embedders/gemini.py</code> <pre><code>class GeminiEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the GeminiEmbedder component with given configuration.\n\n    Attributes:\n        connection (GeminiConnection): The connection to the Gemini API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"gemini/gemini-embedding-exp-03-07\"\n        input_type (str): Specifies the type of embedding task. Defaults to \"search_query\"\n    \"\"\"\n\n    connection: GeminiConnection\n    model: str = \"gemini/gemini-embedding-exp-03-07\"\n    input_type: str = \"search_query\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GeminiConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        \"\"\"\n        Returns the embedding parameters for the Gemini API.\n\n        Returns:\n            dict: A dictionary containing the parameters for the embedding call.\n        \"\"\"\n        params = super().embed_params\n\n        input_to_task_mapping = {\n            \"search_document\": \"RETRIEVAL_DOCUMENT\",\n            \"search_query\": \"RETRIEVAL_QUERY\",\n            \"classification\": \"CLASSIFICATION\",\n            \"clustering\": \"CLUSTERING\",\n        }\n        params[\"task_type\"] = input_to_task_mapping.get(self.input_type)\n\n        if self.truncate:\n            params[\"truncate\"] = self.truncate\n\n        if self.dimensions:\n            params[\"dimensions\"] = self.dimensions\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/gemini/#dynamiq.components.embedders.gemini.GeminiEmbedder.embed_params","title":"<code>embed_params: dict</code>  <code>property</code>","text":"<p>Returns the embedding parameters for the Gemini API.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the parameters for the embedding call.</p>"},{"location":"dynamiq/components/embedders/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/components/embedders/huggingface/#dynamiq.components.embedders.huggingface.HuggingFaceEmbedder","title":"<code>HuggingFaceEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the HuggingFaceEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>HuggingFace</code> <p>The connection to the  HuggingFace API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"huggingface/BAAI/bge-large-zh\"</p> Source code in <code>dynamiq/components/embedders/huggingface.py</code> <pre><code>class HuggingFaceEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the HuggingFaceEmbedder component with given configuration.\n\n    Attributes:\n        connection (HuggingFaceConnection): The connection to the  HuggingFace API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"huggingface/BAAI/bge-large-zh\"\n    \"\"\"\n\n    API_BASE_URL: ClassVar[str] = \"https://api-inference.huggingface.co/models\"\n    connection: HuggingFaceConnection\n    model: str = \"huggingface/BAAI/bge-large-zh\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n\n        if self.model.startswith(\"huggingface/\"):\n            model_id = self.model[len(\"huggingface/\") :]\n        else:\n            model_id = self.model\n\n        params[\"api_base\"] = f\"{self.API_BASE_URL}/{model_id}\"\n\n        return params\n\n    def embed_text(self, text: str) -&gt; dict:\n        \"\"\"\n        Embeds a single string using the Embedder model specified during the initialization of the component.\n\n        Args:\n            text (str): The text string to be embedded.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'embedding': A list representing the embedding vector of the input text.\n                - 'meta': A dictionary with metadata information about the model usage.\n\n        Raises:\n            TypeError: If input is not a string\n            ValueError: If the embedding response is invalid\n        \"\"\"\n        if not isinstance(text, str):\n            msg = (\n                \"TextEmbedder expects a string as input.\"\n                \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n            )\n            raise TypeError(msg)\n\n        text_to_embed = f\"{self.prefix}{text}{self.suffix}\"\n        text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n\n        response = self._embedding(model=self.model, input=text_to_embed, **self.embed_params)\n\n        meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n        embedding = response.data[0][\"embedding\"]\n\n        try:\n            self.validate_embedding(embedding)\n        except InvalidEmbeddingError as e:\n            logger.error(f\"Invalid embedding returned by model {self.model}: {str(e)}\")\n            raise ValueError(f\"Invalid embedding returned by the model: {str(e)}\")\n\n        return {\"embedding\": embedding, \"meta\": meta}\n\n    def _embed_texts_batch(\n        self, texts_to_embed: list[str], batch_size: int\n    ) -&gt; tuple[list[list[float]], dict[str, Any]]:\n        \"\"\"\n        Embed a list of texts one by one (non-batched API).\n        \"\"\"\n        all_embeddings = []\n        meta: dict[str, Any] = {}\n        embed_params = self.embed_params\n\n        for i in range(0, len(texts_to_embed), batch_size):\n            batch = texts_to_embed[i : i + batch_size]\n\n            for text in batch:\n                response = self._embedding(model=self.model, input=text, **embed_params)\n\n                embedding = response.data[0][\"embedding\"]\n                all_embeddings.append(embedding)\n\n                if \"model\" not in meta:\n                    meta[\"model\"] = response.model\n\n                if \"usage\" not in meta:\n                    meta[\"usage\"] = dict(response.usage)\n                else:\n                    meta[\"usage\"][\"prompt_tokens\"] += response.usage.prompt_tokens\n                    meta[\"usage\"][\"total_tokens\"] += response.usage.total_tokens\n\n        return all_embeddings, meta\n</code></pre>"},{"location":"dynamiq/components/embedders/huggingface/#dynamiq.components.embedders.huggingface.HuggingFaceEmbedder.embed_text","title":"<code>embed_text(text)</code>","text":"<p>Embeds a single string using the Embedder model specified during the initialization of the component.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text string to be embedded.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - 'embedding': A list representing the embedding vector of the input text. - 'meta': A dictionary with metadata information about the model usage.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a string</p> <code>ValueError</code> <p>If the embedding response is invalid</p> Source code in <code>dynamiq/components/embedders/huggingface.py</code> <pre><code>def embed_text(self, text: str) -&gt; dict:\n    \"\"\"\n    Embeds a single string using the Embedder model specified during the initialization of the component.\n\n    Args:\n        text (str): The text string to be embedded.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'embedding': A list representing the embedding vector of the input text.\n            - 'meta': A dictionary with metadata information about the model usage.\n\n    Raises:\n        TypeError: If input is not a string\n        ValueError: If the embedding response is invalid\n    \"\"\"\n    if not isinstance(text, str):\n        msg = (\n            \"TextEmbedder expects a string as input.\"\n            \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n        )\n        raise TypeError(msg)\n\n    text_to_embed = f\"{self.prefix}{text}{self.suffix}\"\n    text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n\n    response = self._embedding(model=self.model, input=text_to_embed, **self.embed_params)\n\n    meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n    embedding = response.data[0][\"embedding\"]\n\n    try:\n        self.validate_embedding(embedding)\n    except InvalidEmbeddingError as e:\n        logger.error(f\"Invalid embedding returned by model {self.model}: {str(e)}\")\n        raise ValueError(f\"Invalid embedding returned by the model: {str(e)}\")\n\n    return {\"embedding\": embedding, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/mistral/","title":"Mistral","text":""},{"location":"dynamiq/components/embedders/mistral/#dynamiq.components.embedders.mistral.MistralEmbedder","title":"<code>MistralEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the MistralEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Mistral</code> <p>The connection to the  Mistral API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"mistral/mistral-embed\"</p> Source code in <code>dynamiq/components/embedders/mistral.py</code> <pre><code>class MistralEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the MistralEmbedder component with given configuration.\n\n    Attributes:\n        connection (MistralConnection): The connection to the  Mistral API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"mistral/mistral-embed\"\n    \"\"\"\n    connection: MistralConnection\n    model: str = \"mistral/mistral-embed\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/components/embedders/openai/","title":"Openai","text":""},{"location":"dynamiq/components/embedders/openai/#dynamiq.components.embedders.openai.OpenAIEmbedder","title":"<code>OpenAIEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using OpenAI's models.</p> <p>This class leverages the OpenAI API to generate embeddings for given text documents. It's designed to work with instances of the Document class from the dynamiq package. The embeddings generated can be used for tasks such as similarity search, clustering, and more.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>OpenAI</code> <p>The connection to the  OpenAI API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"text-embedding-3-small\"</p> Example <p>from dynamiq.types import Document from dynamiq.components.embedders.openai import OpenAIEmbedder</p> <p>doc = Document(content=\"I love pizza!\")</p> <p>document_embedder = OpenAIEmbedder()</p> <p>result = document_embedder.run([doc]) print(result\"documents\".embedding) [0.017020374536514282, -0.023255806416273117, ...]</p> Note <p>An OpenAI API key must be provided either via environment variables or when creating an instance of OpenAIDocumentEmbedder through the OpenAIConnection.</p> Source code in <code>dynamiq/components/embedders/openai.py</code> <pre><code>class OpenAIEmbedder(BaseEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using OpenAI's models.\n\n    This class leverages the OpenAI API to generate embeddings for given text documents. It's designed to work\n    with instances of the Document class from the dynamiq package. The embeddings generated can be used for tasks\n    such as similarity search, clustering, and more.\n\n    Attributes:\n        connection (OpenAIConnection): The connection to the  OpenAI API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"text-embedding-3-small\"\n\n\n    Example:\n        &gt;&gt;&gt; from dynamiq.types import Document\n        &gt;&gt;&gt; from dynamiq.components.embedders.openai import OpenAIEmbedder\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; doc = Document(content=\"I love pizza!\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; document_embedder = OpenAIEmbedder()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; result = document_embedder.run([doc])\n        &gt;&gt;&gt; print(result[\"documents\"][0].embedding)\n        [0.017020374536514282, -0.023255806416273117, ...]\n\n    Note:\n        An OpenAI API key must be provided either via environment variables or when creating an instance of\n        OpenAIDocumentEmbedder through the OpenAIConnection.\n    \"\"\"\n\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        if self.dimensions:\n            params[\"dimensions\"] = self.dimensions\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/vertexai/","title":"Vertexai","text":""},{"location":"dynamiq/components/embedders/vertexai/#dynamiq.components.embedders.vertexai.VertexAIEmbedder","title":"<code>VertexAIEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>An embedder for generating embeddings using Vertex AI.</p> <p>This component manages a connection to Vertex AI and provides methods to generate embeddings for text or documents.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>VertexAI</code> <p>The Vertex AI connection instance. If not provided, a new connection is created.</p> <code>model</code> <code>str</code> <p>The Vertex AI embedding model name. Defaults to \"vertex_ai/text-embedding-005\".</p> <code>input_type</code> <code>str</code> <p>Specifies the embedding task type. Can be \"search_query\", \"search_document\", \"classification\", or \"clustering\".</p> Source code in <code>dynamiq/components/embedders/vertexai.py</code> <pre><code>class VertexAIEmbedder(BaseEmbedder):\n    \"\"\"\n    An embedder for generating embeddings using Vertex AI.\n\n    This component manages a connection to Vertex AI and provides\n    methods to generate embeddings for text or documents.\n\n    Attributes:\n        connection (VertexAIConnection): The Vertex AI connection instance.\n            If not provided, a new connection is created.\n        model (str): The Vertex AI embedding model name.\n            Defaults to \"vertex_ai/text-embedding-005\".\n        input_type (str): Specifies the embedding task type.\n            Can be \"search_query\", \"search_document\", \"classification\", or \"clustering\".\n    \"\"\"\n\n    connection: VertexAIConnection\n    model: str = \"vertex_ai/text-embedding-005\"\n    input_type: str = \"search_query\"\n\n    def __init__(\n        self,\n        *,\n        connection: VertexAIConnection | None = None,\n        model: str | None = None,\n        input_type: str | None = None,\n        **kwargs: Any\n    ):\n        \"\"\"\n        Initialize the VertexAIEmbedder.\n\n        Args:\n            connection: An existing Vertex AI connection.\n            model: Override the default embedding model.\n            input_type: Override the default embedding task type.\n            **kwargs: Additional BaseEmbedder keyword arguments.\n        \"\"\"\n        if connection is None:\n            connection = VertexAIConnection()\n        super().__init__(connection=connection, **kwargs)\n\n        if model:\n            self.model = model\n        if input_type:\n            self.input_type = input_type\n\n    @property\n    def embed_params(self) -&gt; dict:\n        \"\"\"\n        Build the parameters for the embedding request.\n\n        Returns:\n            A dictionary of parameters including task type,\n            truncate length, and embedding dimensions.\n        \"\"\"\n        # Copy base parameters to avoid side effects\n        params = super().embed_params.copy()\n\n        # Map our input types to Vertex AI task enums\n        task_mapping = {\n            \"search_query\": \"RETRIEVAL_QUERY\",\n            \"search_document\": \"RETRIEVAL_DOCUMENT\",\n            \"classification\": \"CLASSIFICATION\",\n            \"clustering\": \"CLUSTERING\",\n        }\n        params[\"task_type\"] = task_mapping.get(self.input_type, self.input_type)\n\n        if self.truncate is not None:\n            params[\"truncate\"] = self.truncate\n        if self.dimensions is not None:\n            params[\"dimensions\"] = self.dimensions\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/vertexai/#dynamiq.components.embedders.vertexai.VertexAIEmbedder.embed_params","title":"<code>embed_params: dict</code>  <code>property</code>","text":"<p>Build the parameters for the embedding request.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of parameters including task type,</p> <code>dict</code> <p>truncate length, and embedding dimensions.</p>"},{"location":"dynamiq/components/embedders/vertexai/#dynamiq.components.embedders.vertexai.VertexAIEmbedder.__init__","title":"<code>__init__(*, connection=None, model=None, input_type=None, **kwargs)</code>","text":"<p>Initialize the VertexAIEmbedder.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>VertexAI | None</code> <p>An existing Vertex AI connection.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Override the default embedding model.</p> <code>None</code> <code>input_type</code> <code>str | None</code> <p>Override the default embedding task type.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional BaseEmbedder keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/components/embedders/vertexai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    connection: VertexAIConnection | None = None,\n    model: str | None = None,\n    input_type: str | None = None,\n    **kwargs: Any\n):\n    \"\"\"\n    Initialize the VertexAIEmbedder.\n\n    Args:\n        connection: An existing Vertex AI connection.\n        model: Override the default embedding model.\n        input_type: Override the default embedding task type.\n        **kwargs: Additional BaseEmbedder keyword arguments.\n    \"\"\"\n    if connection is None:\n        connection = VertexAIConnection()\n    super().__init__(connection=connection, **kwargs)\n\n    if model:\n        self.model = model\n    if input_type:\n        self.input_type = input_type\n</code></pre>"},{"location":"dynamiq/components/embedders/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/components/embedders/watsonx/#dynamiq.components.embedders.watsonx.WatsonXEmbedder","title":"<code>WatsonXEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Initializes the WatsonXEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>WatsonX</code> <p>The connection to the  WatsonX API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"watsonx/ibm/slate-30m-english-rtrvr\"</p> Source code in <code>dynamiq/components/embedders/watsonx.py</code> <pre><code>class WatsonXEmbedder(BaseEmbedder):\n    \"\"\"\n    Initializes the WatsonXEmbedder component with given configuration.\n\n    Attributes:\n        connection (WatsonXConnection): The connection to the  WatsonX API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to \"watsonx/ibm/slate-30m-english-rtrvr\"\n    \"\"\"\n    connection: WatsonXConnection\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever","title":"<code>ChromaDocumentRetriever</code>","text":"<p>Document Retriever using Chroma.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>class ChromaDocumentRetriever:\n    \"\"\"\n    Document Retriever using Chroma.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: ChromaVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Chroma vector store with optional filtering.\n\n        Args:\n            vector_store (ChromaVectorStore): An instance of ChromaVectorStore to interface with Chroma vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `ChromaVectorStore`.\n\n        This initializer checks if the `vector_store` provided is an instance of the expected `ChromaVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, ChromaVectorStore):\n            msg = \"document_store must be an instance of ChromaVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        query_embeddings = [query_embedding]\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store.search_embeddings(\n            query_embeddings=query_embeddings,\n            filters=filters,\n            top_k=top_k,\n        )[0]\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=False)\n\n        if exclude_document_embeddings:\n            for doc in docs:\n                doc.embedding = None\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a Chroma vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>ChromaVectorStore</code> <p>An instance of ChromaVectorStore to interface with Chroma vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>ChromaVectorStore</code>.</p> <p>This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>ChromaVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: ChromaVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Chroma vector store with optional filtering.\n\n    Args:\n        vector_store (ChromaVectorStore): An instance of ChromaVectorStore to interface with Chroma vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `ChromaVectorStore`.\n\n    This initializer checks if the `vector_store` provided is an instance of the expected `ChromaVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, ChromaVectorStore):\n        msg = \"document_store must be an instance of ChromaVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    query_embeddings = [query_embedding]\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store.search_embeddings(\n        query_embeddings=query_embeddings,\n        filters=filters,\n        top_k=top_k,\n    )[0]\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=False)\n\n    if exclude_document_embeddings:\n        for doc in docs:\n            doc.embedding = None\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/elasticsearch/","title":"Elasticsearch","text":""},{"location":"dynamiq/components/retrievers/elasticsearch/#dynamiq.components.retrievers.elasticsearch.ElasticsearchDocumentRetriever","title":"<code>ElasticsearchDocumentRetriever</code>","text":"<p>Document Retriever using Elasticsearch.</p> Source code in <code>dynamiq/components/retrievers/elasticsearch.py</code> <pre><code>class ElasticsearchDocumentRetriever:\n    \"\"\"Document Retriever using Elasticsearch.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: ElasticsearchVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initialize ElasticsearchDocumentRetriever.\n\n        Args:\n            vector_store (ElasticsearchVectorStore): An instance of ElasticsearchVectorStore.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): Maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If `vector_store` is not an instance of ElasticsearchVectorStore.\n        \"\"\"\n        if not isinstance(vector_store, ElasticsearchVectorStore):\n            raise ValueError(\"vector_store must be an instance of ElasticsearchVectorStore\")\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        return self.vector_store.similarity != ElasticsearchSimilarityMetric.L2\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        scale_scores: bool = False,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieve documents from ElasticsearchVectorStore.\n\n        Args:\n            query_embedding (list[float]): Vector query for similarity search.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            top_k (Optional[int]): Maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): Field used to store content. Defaults to None.\n            embedding_key (Optional[str]): Field used to store vector. Defaults to None.\n            scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n\n        Returns:\n            dict[str, list[Document]]: A dictionary containing a list of retrieved documents.\n\n        Raises:\n            ValueError: If the query format is invalid.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            scale_scores=scale_scores,\n            content_key=content_key,\n            embedding_key=embedding_key,\n        )\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        logger.debug(f\"Retrieved {len(docs)} documents from Elasticsearch Vector Store\")\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/elasticsearch/#dynamiq.components.retrievers.elasticsearch.ElasticsearchDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initialize ElasticsearchDocumentRetriever.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>ElasticsearchVectorStore</code> <p>An instance of ElasticsearchVectorStore.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>vector_store</code> is not an instance of ElasticsearchVectorStore.</p> Source code in <code>dynamiq/components/retrievers/elasticsearch.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: ElasticsearchVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initialize ElasticsearchDocumentRetriever.\n\n    Args:\n        vector_store (ElasticsearchVectorStore): An instance of ElasticsearchVectorStore.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): Maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If `vector_store` is not an instance of ElasticsearchVectorStore.\n    \"\"\"\n    if not isinstance(vector_store, ElasticsearchVectorStore):\n        raise ValueError(\"vector_store must be an instance of ElasticsearchVectorStore\")\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/elasticsearch/#dynamiq.components.retrievers.elasticsearch.ElasticsearchDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, embedding_key=None, scale_scores=False, similarity_threshold=None)</code>","text":"<p>Retrieve documents from ElasticsearchVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>list[float]</code> <p>Vector query for similarity search.</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Whether to exclude embeddings in results. Defaults to True.</p> <code>True</code> <code>top_k</code> <code>Optional[int]</code> <p>Maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Field used to store content. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>Field used to store vector. Defaults to None.</p> <code>None</code> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to the 0-1 range. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>dict[str, list[Document]]: A dictionary containing a list of retrieved documents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the query format is invalid.</p> Source code in <code>dynamiq/components/retrievers/elasticsearch.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    scale_scores: bool = False,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieve documents from ElasticsearchVectorStore.\n\n    Args:\n        query_embedding (list[float]): Vector query for similarity search.\n        exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n        top_k (Optional[int]): Maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): Field used to store content. Defaults to None.\n        embedding_key (Optional[str]): Field used to store vector. Defaults to None.\n        scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n\n    Returns:\n        dict[str, list[Document]]: A dictionary containing a list of retrieved documents.\n\n    Raises:\n        ValueError: If the query format is invalid.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._embedding_retrieval(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        exclude_document_embeddings=exclude_document_embeddings,\n        scale_scores=scale_scores,\n        content_key=content_key,\n        embedding_key=embedding_key,\n    )\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    logger.debug(f\"Retrieved {len(docs)} documents from Elasticsearch Vector Store\")\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever","title":"<code>MilvusDocumentRetriever</code>","text":"<p>Document Retriever using Milvus.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>class MilvusDocumentRetriever:\n    \"\"\"\n    Document Retriever using Milvus.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: MilvusVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Milvus vector store with optional filtering.\n\n        Args:\n            vector_store (MilvusVectorStore): An instance of MilvusVectorStore to interface with Milvus vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `MilvusVectorStore`.\n        \"\"\"\n        if not isinstance(vector_store, MilvusVectorStore):\n            raise ValueError(\"vector_store must be an instance of MilvusVectorStore\")\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        metric = (self.vector_store.metric_type or \"\").upper()\n        return metric not in {\"L2\", \"EUCLIDEAN\"}\n\n    def run(\n        self,\n        query_embedding: list[float],\n        query: str | None = None,\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            query(Optional[str]): The query string to search for (when using hybrid search). Defaults to None.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n\n        Returns:\n            Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance\n            to the query_embedding.\n        \"\"\"\n        query_embeddings = [query_embedding]\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        if query is not None:\n            docs = self.vector_store._hybrid_retrieval(\n                query=query,\n                query_embeddings=query_embeddings,\n                top_k=top_k,\n                content_key=content_key,\n                embedding_key=embedding_key,\n                return_embeddings=not exclude_document_embeddings,\n            )\n        else:\n            docs = self.vector_store._embedding_retrieval(\n                query_embeddings=query_embeddings,\n                filters=filters,\n                top_k=top_k,\n                content_key=content_key,\n                embedding_key=embedding_key,\n                return_embeddings=not exclude_document_embeddings,\n            )\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a Milvus vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>MilvusVectorStore</code> <p>An instance of MilvusVectorStore to interface with Milvus vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>MilvusVectorStore</code>.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: MilvusVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Milvus vector store with optional filtering.\n\n    Args:\n        vector_store (MilvusVectorStore): An instance of MilvusVectorStore to interface with Milvus vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `MilvusVectorStore`.\n    \"\"\"\n    if not isinstance(vector_store, MilvusVectorStore):\n        raise ValueError(\"vector_store must be an instance of MilvusVectorStore\")\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever.run","title":"<code>run(query_embedding, query=None, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, embedding_key=None, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>query(Optional[str])</code> <p>The query string to search for (when using hybrid search). Defaults to None.</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store vector in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance</p> <code>dict[str, list[Document]]</code> <p>to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    query: str | None = None,\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        query(Optional[str]): The query string to search for (when using hybrid search). Defaults to None.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store vector in the storage.\n\n    Returns:\n        Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance\n        to the query_embedding.\n    \"\"\"\n    query_embeddings = [query_embedding]\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    if query is not None:\n        docs = self.vector_store._hybrid_retrieval(\n            query=query,\n            query_embeddings=query_embeddings,\n            top_k=top_k,\n            content_key=content_key,\n            embedding_key=embedding_key,\n            return_embeddings=not exclude_document_embeddings,\n        )\n    else:\n        docs = self.vector_store._embedding_retrieval(\n            query_embeddings=query_embeddings,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            embedding_key=embedding_key,\n            return_embeddings=not exclude_document_embeddings,\n        )\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/opensearch/","title":"Opensearch","text":""},{"location":"dynamiq/components/retrievers/opensearch/#dynamiq.components.retrievers.opensearch.OpenSearchDocumentRetriever","title":"<code>OpenSearchDocumentRetriever</code>","text":"<p>Document Retriever using OpenSearch.</p> Source code in <code>dynamiq/components/retrievers/opensearch.py</code> <pre><code>class OpenSearchDocumentRetriever:\n    \"\"\"Document Retriever using OpenSearch.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: OpenSearchVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initialize OpenSearchDocumentRetriever.\n\n        Args:\n            vector_store (OpenSearchVectorStore): An instance of OpenSearchVectorStore.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): Maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If `vector_store` is not an instance of OpenSearchVectorStore.\n        \"\"\"\n        if not isinstance(vector_store, OpenSearchVectorStore):\n            raise ValueError(\"vector_store must be an instance of OpenSearchVectorStore\")\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        return self.vector_store.similarity in {\n            OpenSearchSimilarityMetric.COSINE,\n            OpenSearchSimilarityMetric.INNER_PRODUCT,\n        }\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        scale_scores: bool = False,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieve documents from OpenSearchVectorStore.\n\n        Args:\n            query_embedding (list[float]): Vector query for similarity search.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            top_k (Optional[int]): Maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): Field used to store content. Defaults to None.\n            embedding_key (Optional[str]): Field used to store vector. Defaults to None.\n            scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n\n        Returns:\n            dict[str, list[Document]]: A dictionary containing a list of retrieved documents.\n\n        Raises:\n            ValueError: If the query format is invalid.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            scale_scores=scale_scores,\n            content_key=content_key,\n            embedding_key=embedding_key,\n        )\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        logger.debug(f\"Retrieved {len(docs)} documents from OpenSearch Vector Store\")\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/opensearch/#dynamiq.components.retrievers.opensearch.OpenSearchDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initialize OpenSearchDocumentRetriever.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>OpenSearchVectorStore</code> <p>An instance of OpenSearchVectorStore.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>vector_store</code> is not an instance of OpenSearchVectorStore.</p> Source code in <code>dynamiq/components/retrievers/opensearch.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: OpenSearchVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initialize OpenSearchDocumentRetriever.\n\n    Args:\n        vector_store (OpenSearchVectorStore): An instance of OpenSearchVectorStore.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): Maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If `vector_store` is not an instance of OpenSearchVectorStore.\n    \"\"\"\n    if not isinstance(vector_store, OpenSearchVectorStore):\n        raise ValueError(\"vector_store must be an instance of OpenSearchVectorStore\")\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/opensearch/#dynamiq.components.retrievers.opensearch.OpenSearchDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, embedding_key=None, scale_scores=False, similarity_threshold=None)</code>","text":"<p>Retrieve documents from OpenSearchVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>list[float]</code> <p>Vector query for similarity search.</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Whether to exclude embeddings in results. Defaults to True.</p> <code>True</code> <code>top_k</code> <code>Optional[int]</code> <p>Maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Field used to store content. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>Field used to store vector. Defaults to None.</p> <code>None</code> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to the 0-1 range. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>dict[str, list[Document]]: A dictionary containing a list of retrieved documents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the query format is invalid.</p> Source code in <code>dynamiq/components/retrievers/opensearch.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    scale_scores: bool = False,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieve documents from OpenSearchVectorStore.\n\n    Args:\n        query_embedding (list[float]): Vector query for similarity search.\n        exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n        top_k (Optional[int]): Maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): Field used to store content. Defaults to None.\n        embedding_key (Optional[str]): Field used to store vector. Defaults to None.\n        scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n\n    Returns:\n        dict[str, list[Document]]: A dictionary containing a list of retrieved documents.\n\n    Raises:\n        ValueError: If the query format is invalid.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._embedding_retrieval(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        exclude_document_embeddings=exclude_document_embeddings,\n        scale_scores=scale_scores,\n        content_key=content_key,\n        embedding_key=embedding_key,\n    )\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    logger.debug(f\"Retrieved {len(docs)} documents from OpenSearch Vector Store\")\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pgvector/","title":"Pgvector","text":""},{"location":"dynamiq/components/retrievers/pgvector/#dynamiq.components.retrievers.pgvector.PGVectorDocumentRetriever","title":"<code>PGVectorDocumentRetriever</code>","text":"<p>Document Retriever using PGVector.</p> Source code in <code>dynamiq/components/retrievers/pgvector.py</code> <pre><code>class PGVectorDocumentRetriever:\n    \"\"\"\n    Document Retriever using PGVector.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: PGVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a PGVector vector store with optional filtering.\n\n        Args:\n            vector_store (PGVectorStore): An instance of PGVectorStore to interface with PGVector vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `PGVectorStore`.\n\n        This initializer checks if the `vector_store` provided is an instance of the expected `PGVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, PGVectorStore):\n            msg = \"document_store must be an instance of PGVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        distance_metrics = {\n            PGVectorVectorFunction.L1_DISTANCE,\n            PGVectorVectorFunction.L2_DISTANCE,\n        }\n        return self.vector_store.vector_function not in distance_metrics\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        query: str | None = None,\n        alpha: float | None = 0.5,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Retrieves documents from the PGVectorStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage. Defaults to None.\n            embedding_key (Optional[str]): The field used to store vector in the storage. Defaults to None.\n            query(Optional[str]): The query string to search for (when using keyword search). Defaults to None.\n            alpha (Optional[float]): The alpha value for hybrid retrieval. Defaults to 0.5.\n\n            When using hybrid retrieval, the alpha value determines the weight of the keyword search score in the\n            final ranking. A value of 0.0 means only keyword search score will be used, and a value of 1.0 means only\n            vector similarity score will be considered.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        if query:\n            docs = self.vector_store._hybrid_retrieval(\n                query_embedding=query_embedding,\n                query=query,\n                filters=filters,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                content_key=content_key,\n                embedding_key=embedding_key,\n                alpha=alpha,\n            )\n        else:\n            docs = self.vector_store._embedding_retrieval(\n                query_embedding=query_embedding,\n                filters=filters,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                content_key=content_key,\n                embedding_key=embedding_key,\n            )\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        logger.debug(f\"Retrieved {len(docs)} documents from pgvector Vector Store.\")\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pgvector/#dynamiq.components.retrievers.pgvector.PGVectorDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a PGVector vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>PGVectorStore</code> <p>An instance of PGVectorStore to interface with PGVector vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>PGVectorStore</code>.</p> <p>This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>PGVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/pgvector.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: PGVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a PGVector vector store with optional filtering.\n\n    Args:\n        vector_store (PGVectorStore): An instance of PGVectorStore to interface with PGVector vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `PGVectorStore`.\n\n    This initializer checks if the `vector_store` provided is an instance of the expected `PGVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, PGVectorStore):\n        msg = \"document_store must be an instance of PGVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/pgvector/#dynamiq.components.retrievers.pgvector.PGVectorDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, embedding_key=None, query=None, alpha=0.5, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the PGVectorStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store vector in the storage. Defaults to None.</p> <code>None</code> <code>query(Optional[str])</code> <p>The query string to search for (when using keyword search). Defaults to None.</p> required <code>alpha</code> <code>Optional[float]</code> <p>The alpha value for hybrid retrieval. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/pgvector.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    query: str | None = None,\n    alpha: float | None = 0.5,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Retrieves documents from the PGVectorStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage. Defaults to None.\n        embedding_key (Optional[str]): The field used to store vector in the storage. Defaults to None.\n        query(Optional[str]): The query string to search for (when using keyword search). Defaults to None.\n        alpha (Optional[float]): The alpha value for hybrid retrieval. Defaults to 0.5.\n\n        When using hybrid retrieval, the alpha value determines the weight of the keyword search score in the\n        final ranking. A value of 0.0 means only keyword search score will be used, and a value of 1.0 means only\n        vector similarity score will be considered.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    if query:\n        docs = self.vector_store._hybrid_retrieval(\n            query_embedding=query_embedding,\n            query=query,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            content_key=content_key,\n            embedding_key=embedding_key,\n            alpha=alpha,\n        )\n    else:\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            content_key=content_key,\n            embedding_key=embedding_key,\n        )\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    logger.debug(f\"Retrieved {len(docs)} documents from pgvector Vector Store.\")\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever","title":"<code>PineconeDocumentRetriever</code>","text":"<p>Document Retriever using Pinecone.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>class PineconeDocumentRetriever:\n    \"\"\"\n    Document Retriever using Pinecone.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: PineconeVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.\n\n        Args:\n            vector_store (PineconeVectorStore): An instance of PineconeVectorStore to interface with Pinecone vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `PineconeVectorStore`.\n\n        This initializer checks if the `vector_store` provided is an instance of the expected `PineconeVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, PineconeVectorStore):\n            msg = \"document_store must be an instance of PineconeVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        metric = self.vector_store.metric\n        return metric != PineconeSimilarityMetric.EUCLIDEAN\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            content_key=content_key,\n        )\n        logger.debug(f\"Retrieved {len(docs)} documents from Pinecone Vector Store.\")\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>PineconeVectorStore</code> <p>An instance of PineconeVectorStore to interface with Pinecone vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>PineconeVectorStore</code>.</p> <p>This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>PineconeVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: PineconeVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.\n\n    Args:\n        vector_store (PineconeVectorStore): An instance of PineconeVectorStore to interface with Pinecone vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `PineconeVectorStore`.\n\n    This initializer checks if the `vector_store` provided is an instance of the expected `PineconeVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, PineconeVectorStore):\n        msg = \"document_store must be an instance of PineconeVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._embedding_retrieval(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        exclude_document_embeddings=exclude_document_embeddings,\n        content_key=content_key,\n    )\n    logger.debug(f\"Retrieved {len(docs)} documents from Pinecone Vector Store.\")\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever","title":"<code>QdrantDocumentRetriever</code>","text":"<p>Document Retriever using Qdrant</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>class QdrantDocumentRetriever:\n    \"\"\"\n    Document Retriever using Qdrant\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: QdrantVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Qdrant vector store with optional filtering.\n        Args:\n            vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `QdrantVectorStore`.\n        This initializer checks if the `vector_store` provided is an instance of the expected `QdrantVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, QdrantVectorStore):\n            msg = \"document_store must be an instance of QdrantVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def _higher_is_better(self) -&gt; bool:\n        return self.vector_store.metric != QdrantSimilarityMetric.L2\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding.\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]], optional): Filters to apply\n                for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n        docs = self.vector_store._query_by_embedding(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            return_embedding=not exclude_document_embeddings,\n            score_threshold=threshold,\n            content_key=content_key,\n        )\n        logger.debug(f\"Retrieved {len(docs)} documents from Qdrant Vector Store.\")\n\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a Qdrant vector store with optional filtering. Args:     vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.     top_k (int): The maximum number of documents to return. Defaults to 10. Raises:     ValueError: If the <code>vector_store</code> is not an instance of <code>QdrantVectorStore</code>. This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>QdrantVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: QdrantVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Qdrant vector store with optional filtering.\n    Args:\n        vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `QdrantVectorStore`.\n    This initializer checks if the `vector_store` provided is an instance of the expected `QdrantVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, QdrantVectorStore):\n        msg = \"document_store must be an instance of QdrantVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding. Args:     query_embedding (List[float]): The embedding vector of the query for which similar documents are to be     retrieved.     exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved     documents from the output.     top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.     filters (Optional[dict[str, Any]], optional): Filters to apply         for retrieving specific documents. Defaults to None.     content_key (Optional[str]): The field used to store content in the storage.</p> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding.\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]], optional): Filters to apply\n            for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n    docs = self.vector_store._query_by_embedding(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        return_embedding=not exclude_document_embeddings,\n        score_threshold=threshold,\n        content_key=content_key,\n    )\n    logger.debug(f\"Retrieved {len(docs)} documents from Qdrant Vector Store.\")\n\n    docs = filter_documents_by_threshold(docs, threshold, higher_is_better=self._higher_is_better())\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/utils/","title":"Utils","text":""},{"location":"dynamiq/components/retrievers/utils/#dynamiq.components.retrievers.utils.filter_documents_by_threshold","title":"<code>filter_documents_by_threshold(documents, threshold, *, higher_is_better)</code>","text":"<p>Filter documents by score threshold while preserving order.</p> Source code in <code>dynamiq/components/retrievers/utils.py</code> <pre><code>def filter_documents_by_threshold(\n    documents: Iterable[Document],\n    threshold: float | None,\n    *,\n    higher_is_better: bool,\n) -&gt; list[Document]:\n    \"\"\"Filter documents by score threshold while preserving order.\"\"\"\n    if threshold is None:\n        return list(documents)\n\n    filtered: list[Document] = []\n    for document in documents:\n        score = document.score\n        if score is None:\n            filtered.append(document)\n            continue\n\n        if higher_is_better:\n            if score &gt;= threshold:\n                filtered.append(document)\n        elif score &lt;= threshold:\n            filtered.append(document)\n\n    return filtered\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever","title":"<code>WeaviateDocumentRetriever</code>","text":"<p>Document Retriever using Weaviate</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>class WeaviateDocumentRetriever:\n    \"\"\"\n    Document Retriever using Weaviate\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: WeaviateVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        similarity_threshold: float | None = None,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Weaviate vector store with optional filtering.\n        Args:\n            vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `WeaviateVectorStore`.\n        This initializer checks if the `vector_store` provided is an instance of the expected `WeaviateVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, WeaviateVectorStore):\n            msg = \"document_store must be an instance of WeaviateVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n        self.similarity_threshold = similarity_threshold\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        query: str | None = None,\n        alpha: float | None = 0.5,\n        similarity_threshold: float | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding.\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage.\n            query(Optional[str]): The query string to search for (when using keyword search). Defaults to None.\n            alpha (Optional[float]): The alpha value for hybrid retrieval. Defaults to 0.5.\n\n            When using hybrid retrieval, the alpha value determines the weight of the keyword search score in the\n            final ranking. A value of 0.0 means only keyword search score will be used, and a value of 1.0 means only\n            vector similarity score will be considered.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n        if query:\n            docs = self.vector_store._hybrid_retrieval(\n                query_embedding=query_embedding,\n                query=query,\n                filters=filters,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                content_key=content_key,\n                alpha=alpha,\n            )\n            docs = filter_documents_by_threshold(docs, threshold, higher_is_better=True)\n\n        else:\n            distance = None\n            certainty = None\n            higher_is_better = True\n            if threshold is not None:\n                if threshold &lt;= 1:\n                    certainty = threshold\n                    higher_is_better = True\n                else:\n                    distance = threshold\n                    higher_is_better = False\n\n            docs = self.vector_store._embedding_retrieval(\n                query_embedding=query_embedding,\n                filters=filters,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                distance=distance,\n                certainty=certainty,\n                content_key=content_key,\n            )\n            docs = filter_documents_by_threshold(docs, threshold, higher_is_better=higher_is_better)\n\n        logger.debug(f\"Retrieved {len(docs)} documents from Weaviate Vector Store.\")\n\n        return {\"documents\": docs}\n\n    def close(self):\n        \"\"\"\n        Closes the WeaviateDocumentRetriever component.\n        \"\"\"\n        self.vector_store.close()\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10, similarity_threshold=None)</code>","text":"<p>Initializes a component for retrieving documents from a Weaviate vector store with optional filtering. Args:     vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.     top_k (int): The maximum number of documents to return. Defaults to 10. Raises:     ValueError: If the <code>vector_store</code> is not an instance of <code>WeaviateVectorStore</code>. This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>WeaviateVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: WeaviateVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n    similarity_threshold: float | None = None,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Weaviate vector store with optional filtering.\n    Args:\n        vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `WeaviateVectorStore`.\n    This initializer checks if the `vector_store` provided is an instance of the expected `WeaviateVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, WeaviateVectorStore):\n        msg = \"document_store must be an instance of WeaviateVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n    self.similarity_threshold = similarity_threshold\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.close","title":"<code>close()</code>","text":"<p>Closes the WeaviateDocumentRetriever component.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes the WeaviateDocumentRetriever component.\n    \"\"\"\n    self.vector_store.close()\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None, content_key=None, query=None, alpha=0.5, similarity_threshold=None)</code>","text":"<p>Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding. Args:     query_embedding (List[float]): The embedding vector of the query for which similar documents are to be     retrieved.     exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved     documents from the output.     top_k (int, optional): The maximum number of documents to return. Defaults to None.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.     content_key (Optional[str]): The field used to store content in the storage.     query(Optional[str]): The query string to search for (when using keyword search). Defaults to None.     alpha (Optional[float]): The alpha value for hybrid retrieval. Defaults to 0.5.</p> <pre><code>When using hybrid retrieval, the alpha value determines the weight of the keyword search score in the\nfinal ranking. A value of 0.0 means only keyword search score will be used, and a value of 1.0 means only\nvector similarity score will be considered.\n</code></pre> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n    content_key: str | None = None,\n    query: str | None = None,\n    alpha: float | None = 0.5,\n    similarity_threshold: float | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding.\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage.\n        query(Optional[str]): The query string to search for (when using keyword search). Defaults to None.\n        alpha (Optional[float]): The alpha value for hybrid retrieval. Defaults to 0.5.\n\n        When using hybrid retrieval, the alpha value determines the weight of the keyword search score in the\n        final ranking. A value of 0.0 means only keyword search score will be used, and a value of 1.0 means only\n        vector similarity score will be considered.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    threshold = similarity_threshold if similarity_threshold is not None else self.similarity_threshold\n\n    if query:\n        docs = self.vector_store._hybrid_retrieval(\n            query_embedding=query_embedding,\n            query=query,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            content_key=content_key,\n            alpha=alpha,\n        )\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=True)\n\n    else:\n        distance = None\n        certainty = None\n        higher_is_better = True\n        if threshold is not None:\n            if threshold &lt;= 1:\n                certainty = threshold\n                higher_is_better = True\n            else:\n                distance = threshold\n                higher_is_better = False\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n            distance=distance,\n            certainty=certainty,\n            content_key=content_key,\n        )\n        docs = filter_documents_by_threshold(docs, threshold, higher_is_better=higher_is_better)\n\n    logger.debug(f\"Retrieved {len(docs)} documents from Weaviate Vector Store.\")\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/splitters/document/","title":"Document","text":""},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitBy","title":"<code>DocumentSplitBy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum class for document splitting methods.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>class DocumentSplitBy(str, enum.Enum):\n    \"\"\"Enum class for document splitting methods.\"\"\"\n\n    WORD = \"word\"\n    SENTENCE = \"sentence\"\n    PAGE = \"page\"\n    PASSAGE = \"passage\"\n    TITLE = \"title\"\n    CHARACTER = \"character\"\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter","title":"<code>DocumentSplitter</code>","text":"<p>Splits a list of text documents into a list of text documents with shorter texts.</p> <p>Splitting documents with long texts is a common preprocessing step during indexing. This allows Embedders to create significant semantic representations and avoids exceeding the maximum context length of language models.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>class DocumentSplitter:\n    \"\"\"\n    Splits a list of text documents into a list of text documents with shorter texts.\n\n    Splitting documents with long texts is a common preprocessing step during indexing.\n    This allows Embedders to create significant semantic representations\n    and avoids exceeding the maximum context length of language models.\n    \"\"\"\n\n    def __init__(\n        self,\n        split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE,\n        split_length: int = 10,\n        split_overlap: int = 0,\n    ):\n        \"\"\"\n        Initializes an object for splitting documents into smaller parts based on specified criteria.\n\n        Args:\n            split_by (DocumentSplitBy): Determines the unit by which the document should be split.\n                Defaults to DocumentSplitBy.PASSAGE.\n            split_length (int): Specifies the maximum number of units to include in each split.\n                Defaults to 10.\n            split_overlap (int): Specifies the number of units that should overlap between consecutive\n                splits. Defaults to 0.\n\n        Raises:\n            ValueError: If split_length is less than or equal to 0.\n            ValueError: If split_overlap is less than 0.\n        \"\"\"\n        self.split_by = split_by\n        if split_length &lt;= 0:\n            raise ValueError(\"split_length must be greater than 0.\")\n        self.split_length = split_length\n        if split_overlap &lt; 0:\n            raise ValueError(\"split_overlap must be greater than or equal to 0.\")\n        self.split_overlap = split_overlap\n\n    def run(self, documents: list[Document]) -&gt; dict:\n        \"\"\"\n        Splits the provided documents into smaller parts based on the specified configuration.\n\n        Args:\n            documents (list[Document]): The list of documents to be split.\n\n        Returns:\n            dict: A dictionary containing one key, 'documents', which is a list of the split Documents.\n\n        Raises:\n            TypeError: If the input is not a list of Document instances.\n            ValueError: If the content of any document is None.\n        \"\"\"\n        if not isinstance(documents, list) or (\n            documents and not isinstance(documents[0], Document)\n        ):\n            raise TypeError(\"DocumentSplitter expects a List of Documents as input.\")\n\n        split_docs = []\n        for doc in documents:\n            if doc.content is None:\n                raise ValueError(\n                    f\"DocumentSplitter only works with text documents but document.content for document \"\n                    f\"ID {doc.id} is None.\"\n                )\n            units = self._split_into_units(doc.content, self.split_by)\n            text_splits = self._concatenate_units(\n                units, self.split_length, self.split_overlap\n            )\n            if doc.metadata is None:\n                doc.metadata = {}\n            metadata = deepcopy(doc.metadata)\n            metadata[\"source_id\"] = doc.id\n            split_docs += [\n                Document(content=txt, metadata=metadata) for txt in text_splits\n            ]\n        return {\"documents\": split_docs}\n\n    def _split_into_units(self, text: str, split_by: DocumentSplitBy) -&gt; list[str]:\n        \"\"\"\n        Splits the input text into units based on the specified split_by method.\n\n        Args:\n            text (str): The input text to be split.\n            split_by (DocumentSplitBy): The method to use for splitting the text.\n\n        Returns:\n            list[str]: A list of text units after splitting.\n        \"\"\"\n        split_at = SPLIT_STR_BY_SPLIT_TYPE[split_by]\n        if split_by == DocumentSplitBy.CHARACTER:\n            return [char for char in text]\n        else:\n            units = text.split(split_at)\n        # Add the delimiter back to all units except the last one\n        for i in range(len(units) - 1):\n            if split_at == \"\\n#\":\n                units[i] = \"\\n# \" + units[i]\n            else:\n                units[i] += split_at\n        return units\n\n    def _concatenate_units(\n        self, elements: list[str], split_length: int, split_overlap: int\n    ) -&gt; list[str]:\n        \"\"\"\n        Concatenates the elements into parts of split_length units.\n\n        Args:\n            elements (list[str]): The list of text units to be concatenated.\n            split_length (int): The maximum number of units in each split.\n            split_overlap (int): The number of overlapping units between splits.\n\n        Returns:\n            list[str]: A list of concatenated text splits.\n        \"\"\"\n        text_splits = []\n        segments = windowed(elements, n=split_length, step=split_length - split_overlap)\n        for seg in segments:\n            current_units = [unit for unit in seg if unit is not None]\n            txt = \"\".join(current_units)\n            if len(txt) &gt; 0:\n                text_splits.append(txt)\n        return text_splits\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter.__init__","title":"<code>__init__(split_by=DocumentSplitBy.PASSAGE, split_length=10, split_overlap=0)</code>","text":"<p>Initializes an object for splitting documents into smaller parts based on specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>split_by</code> <code>DocumentSplitBy</code> <p>Determines the unit by which the document should be split. Defaults to DocumentSplitBy.PASSAGE.</p> <code>PASSAGE</code> <code>split_length</code> <code>int</code> <p>Specifies the maximum number of units to include in each split. Defaults to 10.</p> <code>10</code> <code>split_overlap</code> <code>int</code> <p>Specifies the number of units that should overlap between consecutive splits. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If split_length is less than or equal to 0.</p> <code>ValueError</code> <p>If split_overlap is less than 0.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>def __init__(\n    self,\n    split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE,\n    split_length: int = 10,\n    split_overlap: int = 0,\n):\n    \"\"\"\n    Initializes an object for splitting documents into smaller parts based on specified criteria.\n\n    Args:\n        split_by (DocumentSplitBy): Determines the unit by which the document should be split.\n            Defaults to DocumentSplitBy.PASSAGE.\n        split_length (int): Specifies the maximum number of units to include in each split.\n            Defaults to 10.\n        split_overlap (int): Specifies the number of units that should overlap between consecutive\n            splits. Defaults to 0.\n\n    Raises:\n        ValueError: If split_length is less than or equal to 0.\n        ValueError: If split_overlap is less than 0.\n    \"\"\"\n    self.split_by = split_by\n    if split_length &lt;= 0:\n        raise ValueError(\"split_length must be greater than 0.\")\n    self.split_length = split_length\n    if split_overlap &lt; 0:\n        raise ValueError(\"split_overlap must be greater than or equal to 0.\")\n    self.split_overlap = split_overlap\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter.run","title":"<code>run(documents)</code>","text":"<p>Splits the provided documents into smaller parts based on the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The list of documents to be split.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing one key, 'documents', which is a list of the split Documents.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a list of Document instances.</p> <code>ValueError</code> <p>If the content of any document is None.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>def run(self, documents: list[Document]) -&gt; dict:\n    \"\"\"\n    Splits the provided documents into smaller parts based on the specified configuration.\n\n    Args:\n        documents (list[Document]): The list of documents to be split.\n\n    Returns:\n        dict: A dictionary containing one key, 'documents', which is a list of the split Documents.\n\n    Raises:\n        TypeError: If the input is not a list of Document instances.\n        ValueError: If the content of any document is None.\n    \"\"\"\n    if not isinstance(documents, list) or (\n        documents and not isinstance(documents[0], Document)\n    ):\n        raise TypeError(\"DocumentSplitter expects a List of Documents as input.\")\n\n    split_docs = []\n    for doc in documents:\n        if doc.content is None:\n            raise ValueError(\n                f\"DocumentSplitter only works with text documents but document.content for document \"\n                f\"ID {doc.id} is None.\"\n            )\n        units = self._split_into_units(doc.content, self.split_by)\n        text_splits = self._concatenate_units(\n            units, self.split_length, self.split_overlap\n        )\n        if doc.metadata is None:\n            doc.metadata = {}\n        metadata = deepcopy(doc.metadata)\n        metadata[\"source_id\"] = doc.id\n        split_docs += [\n            Document(content=txt, metadata=metadata) for txt in text_splits\n        ]\n    return {\"documents\": split_docs}\n</code></pre>"},{"location":"dynamiq/connections/connections/","title":"Connections","text":""},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWS","title":"<code>AWS</code>","text":"<p>               Bases: <code>BaseConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class AWS(BaseConnection):\n    access_key_id: str | None = Field(\n        default_factory=partial(get_env_var, \"AWS_ACCESS_KEY_ID\")\n    )\n    secret_access_key: str | None = Field(\n        default_factory=partial(get_env_var, \"AWS_SECRET_ACCESS_KEY\")\n    )\n    region: str = Field(default_factory=partial(get_env_var, \"AWS_DEFAULT_REGION\"))\n    profile: str | None = Field(default_factory=partial(get_env_var, \"AWS_DEFAULT_PROFILE\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"Return parameters with aws_ prefix for compatibility with other systems\"\"\"\n        params = {}\n        if self.profile:\n            params[\"aws_profile_name\"] = self.profile\n            params[\"aws_region_name\"] = self.region\n        else:\n            params[\"aws_access_key_id\"] = self.access_key_id\n            params[\"aws_secret_access_key\"] = self.secret_access_key\n            params[\"aws_region_name\"] = self.region\n        return params\n\n    def get_boto3_session(self):\n        \"\"\"Create and return a boto3.Session with properly formatted parameters\"\"\"\n        import boto3\n        params = {}\n        if self.profile:\n            params[\"profile_name\"] = self.profile\n        elif self.access_key_id and self.secret_access_key:\n            params[\"aws_access_key_id\"] = self.access_key_id\n            params[\"aws_secret_access_key\"] = self.secret_access_key\n        if self.region:\n            params[\"region_name\"] = self.region\n        return boto3.Session(**params)\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWS.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Return parameters with aws_ prefix for compatibility with other systems</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWS.get_boto3_session","title":"<code>get_boto3_session()</code>","text":"<p>Create and return a boto3.Session with properly formatted parameters</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def get_boto3_session(self):\n    \"\"\"Create and return a boto3.Session with properly formatted parameters\"\"\"\n    import boto3\n    params = {}\n    if self.profile:\n        params[\"profile_name\"] = self.profile\n    elif self.access_key_id and self.secret_access_key:\n        params[\"aws_access_key_id\"] = self.access_key_id\n        params[\"aws_secret_access_key\"] = self.secret_access_key\n    if self.region:\n        params[\"region_name\"] = self.region\n    return boto3.Session(**params)\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWSNeptune","title":"<code>AWSNeptune</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to Amazon Neptune openCypher via HTTP.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>Neptune host (e.g. localhost or db-neptune...).</p> <code>port</code> <code>int</code> <p>Neptune port.</p> <code>use_https</code> <code>bool</code> <p>Whether to use https for the endpoint.</p> <code>verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates.</p> <code>timeout</code> <code>int</code> <p>Request timeout in seconds.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class AWSNeptune(BaseConnection):\n    \"\"\"\n    Represents a connection to Amazon Neptune openCypher via HTTP.\n\n    Attributes:\n        host (str): Neptune host (e.g. localhost or db-neptune...).\n        port (int): Neptune port.\n        use_https (bool): Whether to use https for the endpoint.\n        verify_ssl (bool): Whether to verify SSL certificates.\n        timeout (int): Request timeout in seconds.\n    \"\"\"\n\n    host: str = Field(default_factory=partial(get_env_var, \"NEPTUNE_HOST\", \"localhost\"))\n    port: int = Field(default_factory=partial(get_env_var, \"NEPTUNE_PORT\", 8182))\n    use_https: bool = Field(default_factory=partial(get_env_var, \"NEPTUNE_USE_HTTPS\", True))\n    verify_ssl: bool = Field(default_factory=partial(get_env_var, \"NEPTUNE_VERIFY_SSL\", True))\n    timeout: int = Field(default_factory=partial(get_env_var, \"NEPTUNE_TIMEOUT\", 30))\n\n    @property\n    def endpoint(self) -&gt; str:\n        scheme = \"https\" if self.use_https else \"http\"\n        return f\"{scheme}://{self.host}:{self.port}/openCypher\"\n\n    def connect(self):\n        import requests\n\n        session = requests.Session()\n        logger.debug(\"Connected to Neptune via HTTP endpoint=%s\", self.endpoint)\n        return session\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWSOpenSearch","title":"<code>AWSOpenSearch</code>","text":"<p>               Bases: <code>AWS</code></p> <p>Represents a connection to AWS OpenSearch Service.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>The OpenSearch domain endpoint host.</p> <code>port</code> <code>int</code> <p>The port number for the OpenSearch domain. Defaults to 443.</p> <code>region</code> <code>str</code> <p>AWS region where the OpenSearch domain is located.</p> <code>access_key_id</code> <code>str</code> <p>AWS access key ID for authentication.</p> <code>secret_access_key</code> <code>str</code> <p>AWS secret access key for authentication.</p> <code>profile</code> <code>str</code> <p>AWS profile name to use for authentication.</p> <code>service</code> <code>str</code> <p>AWS service name. Defaults to 'es' for Elasticsearch/OpenSearch.</p> <code>use_ssl</code> <code>bool</code> <p>Whether to use SSL for the connection. Defaults to True.</p> <code>verify_certs</code> <code>bool</code> <p>Whether to verify SSL certificates. Defaults to True.</p> <code>ca_certs</code> <code>str</code> <p>Path to CA certificates for SSL verification.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class AWSOpenSearch(AWS):\n    \"\"\"\n    Represents a connection to AWS OpenSearch Service.\n\n    Attributes:\n        host (str): The OpenSearch domain endpoint host.\n        port (int): The port number for the OpenSearch domain. Defaults to 443.\n        region (str): AWS region where the OpenSearch domain is located.\n        access_key_id (str): AWS access key ID for authentication.\n        secret_access_key (str): AWS secret access key for authentication.\n        profile (str): AWS profile name to use for authentication.\n        service (str): AWS service name. Defaults to 'es' for Elasticsearch/OpenSearch.\n        use_ssl (bool): Whether to use SSL for the connection. Defaults to True.\n        verify_certs (bool): Whether to verify SSL certificates. Defaults to True.\n        ca_certs (str): Path to CA certificates for SSL verification.\n    \"\"\"\n\n    host: str = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_HOST\"))\n    port: int = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_PORT\", 443))\n\n    service: str = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_SERVICE\", \"es\"))\n    use_ssl: bool = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_USE_SSL\", True))\n    verify_certs: bool = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_VERIFY_CERTS\", True))\n    ca_certs: str | None = Field(default_factory=partial(get_env_var, \"AWS_OPENSEARCH_CA_CERTS\"))\n\n    def connect(self):\n        \"\"\"\n        Connect to AWS OpenSearch using inherited AWS credentials/session logic.\n        \"\"\"\n        from opensearchpy import OpenSearch, RequestsHttpConnection\n        from requests_aws4auth import AWS4Auth\n\n        session = self.get_boto3_session()\n        credentials = session.get_credentials()\n\n        if not credentials:\n            raise ValueError(\"Missing AWS credentials: provide profile or access_key_id/secret_access_key\")\n\n        awsauth = AWS4Auth(\n            credentials.access_key,\n            credentials.secret_key,\n            self.region,\n            self.service,\n            session_token=credentials.token,\n        )\n\n        conn_params = {\n            \"hosts\": [{\"host\": self.host, \"port\": self.port}],\n            \"http_auth\": awsauth,\n            \"use_ssl\": self.use_ssl,\n            \"verify_certs\": self.verify_certs,\n            \"connection_class\": RequestsHttpConnection,\n        }\n\n        if self.ca_certs:\n            conn_params[\"ca_certs\"] = self.ca_certs\n\n        try:\n            client = OpenSearch(**conn_params)\n        except Exception as e:\n            raise ConnectionError(f\"Failed to connect to AWS OpenSearch: {e}\")\n\n        if not client.ping():\n            raise ConnectionError(f\"Failed to ping AWS OpenSearch cluster at {self.host}:{self.port}\")\n\n        logger.debug(\n            f\"Connected to AWS OpenSearch at {self.host}:{self.port} (region={self.region}, profile={self.profile})\"\n        )\n        return client\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Merge inherited AWS params with OpenSearch-specific settings.\n        \"\"\"\n        params = super().conn_params\n\n        params.update(\n            {\n                \"hosts\": [{\"host\": self.host, \"port\": self.port}],\n                \"service\": self.service,\n                \"use_ssl\": self.use_ssl,\n                \"verify_certs\": self.verify_certs,\n            }\n        )\n\n        if self.ca_certs:\n            params[\"ca_certs\"] = self.ca_certs\n\n        return params\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWSOpenSearch.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Merge inherited AWS params with OpenSearch-specific settings.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AWSOpenSearch.connect","title":"<code>connect()</code>","text":"<p>Connect to AWS OpenSearch using inherited AWS credentials/session logic.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connect to AWS OpenSearch using inherited AWS credentials/session logic.\n    \"\"\"\n    from opensearchpy import OpenSearch, RequestsHttpConnection\n    from requests_aws4auth import AWS4Auth\n\n    session = self.get_boto3_session()\n    credentials = session.get_credentials()\n\n    if not credentials:\n        raise ValueError(\"Missing AWS credentials: provide profile or access_key_id/secret_access_key\")\n\n    awsauth = AWS4Auth(\n        credentials.access_key,\n        credentials.secret_key,\n        self.region,\n        self.service,\n        session_token=credentials.token,\n    )\n\n    conn_params = {\n        \"hosts\": [{\"host\": self.host, \"port\": self.port}],\n        \"http_auth\": awsauth,\n        \"use_ssl\": self.use_ssl,\n        \"verify_certs\": self.verify_certs,\n        \"connection_class\": RequestsHttpConnection,\n    }\n\n    if self.ca_certs:\n        conn_params[\"ca_certs\"] = self.ca_certs\n\n    try:\n        client = OpenSearch(**conn_params)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to AWS OpenSearch: {e}\")\n\n    if not client.ping():\n        raise ConnectionError(f\"Failed to ping AWS OpenSearch cluster at {self.host}:{self.port}\")\n\n    logger.debug(\n        f\"Connected to AWS OpenSearch at {self.host}:{self.port} (region={self.region}, profile={self.profile})\"\n    )\n    return client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ApacheAGE","title":"<code>ApacheAGE</code>","text":"<p>               Bases: <code>PostgreSQL</code></p> <p>Represents a connection to PostgreSQL with Apache AGE enabled.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ApacheAGE(PostgreSQL):\n    \"\"\"\n    Represents a connection to PostgreSQL with Apache AGE enabled.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AzureAI","title":"<code>AzureAI</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class AzureAI(BaseApiKeyConnection):\n    api_key: str = Field(default_factory=partial(get_env_var, \"AZURE_API_KEY\"))\n    url: str = Field(default_factory=partial(get_env_var, \"AZURE_URL\"))\n    api_version: str = Field(default_factory=partial(get_env_var, \"AZURE_API_VERSION\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing\n\n                -the API key with the key 'api_key'.\n\n                -the base url with the key 'api_base'.\n\n                -the API version with the key 'api_version'.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n            \"api_version\": self.api_version,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AzureAI.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing</p> <p>-the API key with the key 'api_key'.</p> <p>-the base url with the key 'api_base'.</p> <p>-the API version with the key 'api_version'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection","title":"<code>BaseApiKeyConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a base connection class that uses an API key for authentication.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class BaseApiKeyConnection(BaseConnection):\n    \"\"\"\n    Represents a base connection class that uses an API key for authentication.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n    \"\"\"\n    api_key: str\n\n    @abstractmethod\n    def connect(self):\n        \"\"\"\n        Connects to the service.\n\n        This method should be implemented by subclasses to establish a connection to the service using\n        the provided API key.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing the API key with the key 'api_key'.\n        \"\"\"\n        return {\"api_key\": self.api_key}\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the API key with the key 'api_key'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection.connect","title":"<code>connect()</code>  <code>abstractmethod</code>","text":"<p>Connects to the service.</p> <p>This method should be implemented by subclasses to establish a connection to the service using the provided API key.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by a subclass.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@abstractmethod\ndef connect(self):\n    \"\"\"\n    Connects to the service.\n\n    This method should be implemented by subclasses to establish a connection to the service using\n    the provided API key.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection","title":"<code>BaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a base connection class.</p> <p>This class should be subclassed to provide specific implementations for different types of connections.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for the connection, generated using <code>generate_uuid</code>.</p> <code>type</code> <code>ConnectionType</code> <p>The type of connection.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class BaseConnection(BaseModel, ABC):\n    \"\"\"Represents a base connection class.\n\n    This class should be subclassed to provide specific implementations for different types of\n    connections.\n\n    Attributes:\n        id (str): A unique identifier for the connection, generated using `generate_uuid`.\n        type (ConnectionType): The type of connection.\n    \"\"\"\n    id: str = Field(default_factory=generate_uuid)\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: An empty dictionary.\n        \"\"\"\n        return {}\n\n    def to_dict(self, for_tracing: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the connection instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the connection instance.\n        \"\"\"\n        if for_tracing:\n            return {\"id\": self.id, \"type\": self.type}\n        return self.model_dump(**kwargs)\n\n    @abstractmethod\n    def connect(self):\n        \"\"\"Connects to the service.\n\n        This method should be implemented by subclasses to establish a connection to the service.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>An empty dictionary.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.connect","title":"<code>connect()</code>  <code>abstractmethod</code>","text":"<p>Connects to the service.</p> <p>This method should be implemented by subclasses to establish a connection to the service.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by a subclass.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@abstractmethod\ndef connect(self):\n    \"\"\"Connects to the service.\n\n    This method should be implemented by subclasses to establish a connection to the service.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.to_dict","title":"<code>to_dict(for_tracing=False, **kwargs)</code>","text":"<p>Converts the connection instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the connection instance.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def to_dict(self, for_tracing: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the connection instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the connection instance.\n    \"\"\"\n    if for_tracing:\n        return {\"id\": self.id, \"type\": self.type}\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma","title":"<code>Chroma</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Chroma service.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>The host address of the Chroma service, fetched from the environment variable 'CHROMA_HOST'.</p> <code>port</code> <code>int</code> <p>The port number of the Chroma service, fetched from the environment variable 'CHROMA_PORT'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Chroma(BaseConnection):\n    \"\"\"\n    Represents a connection to the Chroma service.\n\n    Attributes:\n        host (str): The host address of the Chroma service, fetched from the environment variable 'CHROMA_HOST'.\n        port (int): The port number of the Chroma service, fetched from the environment variable 'CHROMA_PORT'.\n    \"\"\"\n\n    host: str = Field(default_factory=partial(get_env_var, \"CHROMA_HOST\"))\n    port: int = Field(default_factory=partial(get_env_var, \"CHROMA_PORT\"))\n\n    @property\n    def vector_store_cls(self):\n        \"\"\"\n        Returns the ChromaVectorStore class.\n\n        This property dynamically imports and returns the ChromaVectorStore class\n        from the 'dynamiq.storages.vector' module.\n\n        Returns:\n            type: The ChromaVectorStore class.\n        \"\"\"\n        from dynamiq.storages.vector import ChromaVectorStore\n\n        return ChromaVectorStore\n\n    def connect(self) -&gt; \"ChromaClient\":\n        \"\"\"\n        Connects to the Chroma service.\n\n        This method establishes a connection to the Chroma service using the provided host and port.\n\n        Returns:\n            ChromaClient: An instance of the ChromaClient connected to the specified host and port.\n        \"\"\"\n        # Import in runtime to save memory\n        from chromadb import HttpClient\n\n        chroma_client = HttpClient(host=self.host, port=self.port)\n        logger.debug(f\"Connected to Chroma with host={self.host} and port={str(self.port)}\")\n        return chroma_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma.vector_store_cls","title":"<code>vector_store_cls</code>  <code>property</code>","text":"<p>Returns the ChromaVectorStore class.</p> <p>This property dynamically imports and returns the ChromaVectorStore class from the 'dynamiq.storages.vector' module.</p> <p>Returns:</p> Name Type Description <code>type</code> <p>The ChromaVectorStore class.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma.connect","title":"<code>connect()</code>","text":"<p>Connects to the Chroma service.</p> <p>This method establishes a connection to the Chroma service using the provided host and port.</p> <p>Returns:</p> Name Type Description <code>ChromaClient</code> <code>ClientAPI</code> <p>An instance of the ChromaClient connected to the specified host and port.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"ChromaClient\":\n    \"\"\"\n    Connects to the Chroma service.\n\n    This method establishes a connection to the Chroma service using the provided host and port.\n\n    Returns:\n        ChromaClient: An instance of the ChromaClient connected to the specified host and port.\n    \"\"\"\n    # Import in runtime to save memory\n    from chromadb import HttpClient\n\n    chroma_client = HttpClient(host=self.host, port=self.port)\n    logger.debug(f\"Connected to Chroma with host={self.host} and port={str(self.port)}\")\n    return chroma_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Databricks","title":"<code>Databricks</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Databricks(BaseApiKeyConnection):\n    url: str = Field(default_factory=partial(get_env_var, \"DATABRICKS_API_BASE\"))\n    api_key: str = Field(default_factory=partial(get_env_var, \"DATABRICKS_API_KEY\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Databricks.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Dynamiq","title":"<code>Dynamiq</code>","text":"<p>               Bases: <code>HttpApiKey</code></p> <p>Represents a connection to the Dynamiq service.</p> <p>The base URL and API key can be provided explicitly or sourced from the <code>DYNAMIQ_URL</code> and <code>DYNAMIQ_API_KEY</code> environment variables.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Dynamiq(HttpApiKey):\n    \"\"\"\n    Represents a connection to the Dynamiq service.\n\n    The base URL and API key can be provided explicitly or sourced from the\n    ``DYNAMIQ_URL`` and ``DYNAMIQ_API_KEY`` environment variables.\n    \"\"\"\n\n    url: str = Field(\n        default_factory=partial(get_env_var, \"DYNAMIQ_URL\", \"https://api.getdynamiq.ai\")\n    )\n    api_key: str = Field(default_factory=partial(get_env_var, \"DYNAMIQ_API_KEY\"))\n    headers: dict[str, Any] = Field(default_factory=dict)\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Ensure bearer token is included in default headers.\"\"\"\n        if self.api_key:\n            self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return self\n\n    @property\n    def conn_params(self) -&gt; dict:\n        params = super().conn_params.copy()\n        if self.headers:\n            params[\"headers\"] = self.headers.copy()\n        return params\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Dynamiq.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Ensure bearer token is included in default headers.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Ensure bearer token is included in default headers.\"\"\"\n    if self.api_key:\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Elasticsearch","title":"<code>Elasticsearch</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Elasticsearch service.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the Elasticsearch service</p> <code>api_key</code> <code>str</code> <p>API key for authentication</p> <code>username</code> <code>str</code> <p>Username for basic authentication</p> <code>password</code> <code>str</code> <p>Password for basic authentication</p> <code>cloud_id</code> <code>str</code> <p>Cloud ID for Elastic Cloud deployment</p> <code>ca_path</code> <code>str</code> <p>Path to CA certificate for SSL verification</p> <code>verify_certs</code> <code>bool</code> <p>Whether to verify SSL certificates</p> <code>use_ssl</code> <code>bool</code> <p>Whether to use SSL for connection</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Elasticsearch(BaseConnection):\n    \"\"\"\n    Represents a connection to the Elasticsearch service.\n\n    Attributes:\n        url (str): The URL of the Elasticsearch service\n        api_key (str): API key for authentication\n        username (str): Username for basic authentication\n        password (str): Password for basic authentication\n        cloud_id (str): Cloud ID for Elastic Cloud deployment\n        ca_path (str): Path to CA certificate for SSL verification\n        verify_certs (bool): Whether to verify SSL certificates\n        use_ssl (bool): Whether to use SSL for connection\n    \"\"\"\n\n    url: str = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_URL\", None))\n    api_key_id: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_API_KEY_ID\", None))\n    api_key: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_API_KEY\", None))\n    username: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_USERNAME\", None))\n    password: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_PASSWORD\", None))\n    cloud_id: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_CLOUD_ID\", None))\n    ca_path: str | None = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_CA_PATH\", None))\n    verify_certs: bool = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_VERIFY_CERTS\", False))\n    use_ssl: bool = Field(default_factory=partial(get_env_var, \"ELASTICSEARCH_USE_SSL\", True))\n\n    def connect(self):\n        \"\"\"\n        Connects to the Elasticsearch service.\n\n        Returns:\n            elasticsearch.Elasticsearch: An instance of the Elasticsearch client.\n\n        Raises:\n            ConnectionError: If connection fails\n            ValueError: If neither API key nor basic auth credentials are provided\n        \"\"\"\n\n        from elasticsearch import Elasticsearch\n        from elasticsearch.exceptions import AuthenticationException\n\n        # Build connection params\n        conn_params = {}\n\n        # Handle authentication\n        if self.api_key is not None:\n            if self.api_key_id is not None:\n                conn_params[\"api_key\"] = (self.api_key_id, self.api_key)\n            else:\n                conn_params[\"api_key\"] = self.api_key\n        elif self.username is not None and self.password is not None:\n            conn_params[\"basic_auth\"] = (self.username, self.password)\n        elif self.cloud_id is None:  # Only require auth for non-cloud deployments\n            raise ValueError(\"Either API key or username/password must be provided\")\n\n        # Handle SSL/TLS\n        if self.use_ssl:\n            if self.ca_path is not None:\n                conn_params[\"ca_certs\"] = self.ca_path\n            conn_params[\"verify_certs\"] = self.verify_certs\n\n        # Handle cloud deployment\n        if self.cloud_id is not None:\n            conn_params[\"cloud_id\"] = self.cloud_id\n        else:\n            conn_params[\"hosts\"] = [self.url]\n\n        # Create client\n        try:\n            es_client = Elasticsearch(**conn_params)\n        except Exception as e:\n            raise ConnectionError(f\"Failed to connect to Elasticsearch: {str(e)}\")\n\n        if not es_client.ping():\n            try:\n                info = es_client.info()\n            except AuthenticationException as e:\n                info = f\"Authentication error: {e}\"\n            raise ConnectionError(f\"Failed to connect to Elasticsearch. {info}\")\n\n        logger.debug(f\"Connected to Elasticsearch at {self.cloud_id or self.url}\")\n        return es_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Elasticsearch.connect","title":"<code>connect()</code>","text":"<p>Connects to the Elasticsearch service.</p> <p>Returns:</p> Type Description <p>elasticsearch.Elasticsearch: An instance of the Elasticsearch client.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection fails</p> <code>ValueError</code> <p>If neither API key nor basic auth credentials are provided</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the Elasticsearch service.\n\n    Returns:\n        elasticsearch.Elasticsearch: An instance of the Elasticsearch client.\n\n    Raises:\n        ConnectionError: If connection fails\n        ValueError: If neither API key nor basic auth credentials are provided\n    \"\"\"\n\n    from elasticsearch import Elasticsearch\n    from elasticsearch.exceptions import AuthenticationException\n\n    # Build connection params\n    conn_params = {}\n\n    # Handle authentication\n    if self.api_key is not None:\n        if self.api_key_id is not None:\n            conn_params[\"api_key\"] = (self.api_key_id, self.api_key)\n        else:\n            conn_params[\"api_key\"] = self.api_key\n    elif self.username is not None and self.password is not None:\n        conn_params[\"basic_auth\"] = (self.username, self.password)\n    elif self.cloud_id is None:  # Only require auth for non-cloud deployments\n        raise ValueError(\"Either API key or username/password must be provided\")\n\n    # Handle SSL/TLS\n    if self.use_ssl:\n        if self.ca_path is not None:\n            conn_params[\"ca_certs\"] = self.ca_path\n        conn_params[\"verify_certs\"] = self.verify_certs\n\n    # Handle cloud deployment\n    if self.cloud_id is not None:\n        conn_params[\"cloud_id\"] = self.cloud_id\n    else:\n        conn_params[\"hosts\"] = [self.url]\n\n    # Create client\n    try:\n        es_client = Elasticsearch(**conn_params)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to Elasticsearch: {str(e)}\")\n\n    if not es_client.ping():\n        try:\n            info = es_client.info()\n        except AuthenticationException as e:\n            info = f\"Authentication error: {e}\"\n        raise ConnectionError(f\"Failed to connect to Elasticsearch. {info}\")\n\n    logger.debug(f\"Connected to Elasticsearch at {self.cloud_id or self.url}\")\n    return es_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ElevenLabs","title":"<code>ElevenLabs</code>","text":"<p>               Bases: <code>Http</code></p> <p>Represents a connection to the ElevenLabs API using an HTTP request.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>URL of the ElevenLabs API.</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>api_key</code> <code>str</code> <p>API key for authentication, fetched from the environment variable \"ELEVENLABS_API_KEY\".</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ElevenLabs(Http):\n    \"\"\"\n    Represents a connection to the ElevenLabs API using an HTTP request.\n\n    Attributes:\n        url (str): URL of the ElevenLabs API.\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        api_key (str): API key for authentication, fetched from the environment variable \"ELEVENLABS_API_KEY\".\n    \"\"\"\n\n    url: str = Field(\n        default_factory=partial(\n            get_env_var,\n            \"ELEVENLABS_URL\",\n            \"https://api.elevenlabs.io/v1/\",\n        )\n    )\n    method: str = HTTPMethod.POST\n    api_key: str = Field(default_factory=partial(get_env_var, \"ELEVENLABS_API_KEY\"))\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Setup headers after model validation.\"\"\"\n        if self.api_key:\n            self.headers.update({\"xi-api-key\": self.api_key})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ElevenLabs.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Setup headers after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Setup headers after model validation.\"\"\"\n    if self.api_key:\n        self.headers.update({\"xi-api-key\": self.api_key})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Exa","title":"<code>Exa</code>","text":"<p>               Bases: <code>Http</code></p> <p>Represents a connection to the Exa AI Search API.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the Exa API.</p> <code>method</code> <code>Literal[POST]</code> <p>HTTP method used for the request, defaults to POST.</p> <code>api_key</code> <code>str</code> <p>The API key for authentication, fetched from the environment variable 'EXA_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Exa(Http):\n    \"\"\"\n    Represents a connection to the Exa AI Search API.\n\n    Attributes:\n        url (str): The URL of the Exa API.\n        method (Literal[HTTPMethod.POST]): HTTP method used for the request, defaults to POST.\n        api_key (str): The API key for authentication, fetched from the environment variable 'EXA_API_KEY'.\n    \"\"\"\n\n    url: Literal[\"https://api.exa.ai\"] = Field(default=\"https://api.exa.ai\")\n    method: Literal[HTTPMethod.POST] = HTTPMethod.POST\n    api_key: str = Field(default_factory=partial(get_env_var, \"EXA_API_KEY\"))\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Setup headers after model validation.\"\"\"\n        if self.api_key:\n            self.headers.update({\"x-api-key\": self.api_key, \"Content-Type\": \"application/json\"})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Exa.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Setup headers after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Setup headers after model validation.\"\"\"\n    if self.api_key:\n        self.headers.update({\"x-api-key\": self.api_key, \"Content-Type\": \"application/json\"})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Firecrawl","title":"<code>Firecrawl</code>","text":"<p>               Bases: <code>Http</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Firecrawl(Http):\n    url: str = Field(default=\"https://api.firecrawl.dev/v2/\")\n    api_key: str = Field(default_factory=lambda: get_env_var(\"FIRECRAWL_API_KEY\"))\n    method: Literal[HTTPMethod.POST] = HTTPMethod.POST\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Setup authorization headers after model validation.\"\"\"\n        if self.api_key:\n            self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Firecrawl.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Setup authorization headers after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Setup authorization headers after model validation.\"\"\"\n    if self.api_key:\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.GoogleCloud","title":"<code>GoogleCloud</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to Google Cloud Platform (GCP) using service account credentials.</p> <p>Attributes:</p> Name Type Description <code>project_id</code> <code>str</code> <p>The GCP project ID.</p> <code>private_key_id</code> <code>str</code> <p>The private key ID used for authentication.</p> <code>private_key</code> <code>str</code> <p>The private key used for secure access.</p> <code>client_email</code> <code>str</code> <p>The service account email address.</p> <code>client_id</code> <code>str</code> <p>The unique client ID for authentication.</p> <code>auth_uri</code> <code>str</code> <p>The URI for Google's authentication endpoint.</p> <code>token_uri</code> <code>str</code> <p>The URI for obtaining OAuth tokens.</p> <code>auth_provider_x509_cert_url</code> <code>str</code> <p>The URL for Google's authentication provider X.509 certificates.</p> <code>client_x509_cert_url</code> <code>str</code> <p>The URL for the client's X.509 certificate.</p> <code>universe_domain</code> <code>str</code> <p>The domain associated with the Google Cloud environment.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class GoogleCloud(BaseConnection):\n    \"\"\"\n    Represents a connection to Google Cloud Platform (GCP) using service account credentials.\n\n    Attributes:\n        project_id (str): The GCP project ID.\n        private_key_id (str): The private key ID used for authentication.\n        private_key (str): The private key used for secure access.\n        client_email (str): The service account email address.\n        client_id (str): The unique client ID for authentication.\n        auth_uri (str): The URI for Google's authentication endpoint.\n        token_uri (str): The URI for obtaining OAuth tokens.\n        auth_provider_x509_cert_url (str): The URL for Google's authentication provider X.509 certificates.\n        client_x509_cert_url (str): The URL for the client's X.509 certificate.\n        universe_domain (str): The domain associated with the Google Cloud environment.\n    \"\"\"\n\n    project_id: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_PROJECT_ID\"))\n    private_key_id: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_PRIVATE_KEY_ID\"))\n    private_key: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_PRIVATE_KEY\"))\n    client_email: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_CLIENT_EMAIL\"))\n    client_id: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_CLIENT_ID\"))\n    auth_uri: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_AUTH_URI\"))\n    token_uri: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_TOKEN_URI\"))\n    auth_provider_x509_cert_url: str = Field(\n        default_factory=partial(get_env_var, \"GOOGLE_CLOUD_AUTH_PROVIDER_X509_CERT_URL\")\n    )\n    client_x509_cert_url: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_CLIENT_X509_CERT_URL\"))\n    universe_domain: str = Field(default_factory=partial(get_env_var, \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self):\n        \"\"\"\n        Returns the parameters required for the connection.\n\n        This property returns a dictionary containing Google Cloud service account credentials.\n\n        Returns:\n            dict: A dictionary with the keys 'vertex_project' and 'vertex_location'.\n        \"\"\"\n        return {\n            \"project_id\": self.project_id,\n            \"private_key_id\": self.private_key_id,\n            \"private_key\": self.private_key,\n            \"client_email\": self.client_email,\n            \"client_id\": self.client_id,\n            \"client_x509_cert_url\": self.client_x509_cert_url,\n            \"auth_uri\": self.auth_uri,\n            \"token_uri\": self.token_uri,\n            \"auth_provider_x509_cert_url\": self.auth_provider_x509_cert_url,\n            \"universe_domain\": self.universe_domain,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.GoogleCloud.conn_params","title":"<code>conn_params</code>  <code>property</code>","text":"<p>Returns the parameters required for the connection.</p> <p>This property returns a dictionary containing Google Cloud service account credentials.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the keys 'vertex_project' and 'vertex_location'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HTTPMethod","title":"<code>HTTPMethod</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>This enum defines various method types for different HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class HTTPMethod(str, enum.Enum):\n    \"\"\"\n    This enum defines various method types for different HTTP requests.\n    \"\"\"\n\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n    PATCH = \"PATCH\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Http","title":"<code>Http</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to an API.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the API.</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>headers</code> <code>dict[str, Any]</code> <p>Additional headers to include in the request, defaults to an empty dictionary.</p> <code>params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters to include in the request, defaults to an empty dictionary.</p> <code>data</code> <code>Optional[dict[str, Any]]</code> <p>Data to include in the request, defaults to an empty dictionary.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Http(BaseConnection):\n    \"\"\"\n    Represents a connection to an API.\n\n    Attributes:\n        url (str): The URL of the API.\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        headers (dict[str, Any]): Additional headers to include in the request, defaults to an empty dictionary.\n        params (Optional[dict[str, Any]]): Parameters to include in the request, defaults to an empty dictionary.\n        data (Optional[dict[str, Any]]): Data to include in the request, defaults to an empty dictionary.\n    \"\"\"\n\n    url: str = \"\"\n    method: HTTPMethod\n    headers: dict[str, Any] = Field(default_factory=dict)\n    params: dict[str, Any] | None = Field(default_factory=dict)\n    data: dict[str, Any] | None = Field(default_factory=dict)\n\n    def connect(self):\n        \"\"\"\n        Connects to the API.\n\n        This method establishes a connection to the API using the provided URL and returns a requests\n        session.\n\n        Returns:\n            requests: A requests module for making HTTP requests to the API.\n        \"\"\"\n        import requests\n\n        return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Http.connect","title":"<code>connect()</code>","text":"<p>Connects to the API.</p> <p>This method establishes a connection to the API using the provided URL and returns a requests session.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>A requests module for making HTTP requests to the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the API.\n\n    This method establishes a connection to the API using the provided URL and returns a requests\n    session.\n\n    Returns:\n        requests: A requests module for making HTTP requests to the API.\n    \"\"\"\n    import requests\n\n    return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey","title":"<code>HttpApiKey</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to an API that uses an HTTP API key for authentication.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class HttpApiKey(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to an API that uses an HTTP API key for authentication.\n\n    Attributes:\n        url (str): The URL of the API.\n    \"\"\"\n\n    url: str\n\n    def connect(self):\n        \"\"\"\n        Connects to the API.\n\n        This method establishes a connection to the API using the provided URL and returns a requests\n        session.\n\n        Returns:\n            requests: A requests module for making HTTP requests to the API.\n        \"\"\"\n        import requests\n\n        return requests\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing the API key with the key 'api_key' and base url with the key 'api_base'.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the API key with the key 'api_key' and base url with the key 'api_base'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey.connect","title":"<code>connect()</code>","text":"<p>Connects to the API.</p> <p>This method establishes a connection to the API using the provided URL and returns a requests session.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>A requests module for making HTTP requests to the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the API.\n\n    This method establishes a connection to the API using the provided URL and returns a requests\n    session.\n\n    Returns:\n        requests: A requests module for making HTTP requests to the API.\n    \"\"\"\n    import requests\n\n    return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Jina","title":"<code>Jina</code>","text":"<p>               Bases: <code>Http</code></p> <p>Connection class for Jina Scrape API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Jina(Http):\n    \"\"\"\n    Connection class for Jina Scrape API.\n    \"\"\"\n\n    api_key: str = Field(default_factory=partial(get_env_var, \"JINA_API_KEY\"))\n    method: Literal[HTTPMethod.GET] = HTTPMethod.GET\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Setup headers after model validation.\"\"\"\n        if self.api_key:\n            self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Jina.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Setup headers after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Setup headers after model validation.\"\"\"\n    if self.api_key:\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPSse","title":"<code>MCPSse</code>","text":"<p>               Bases: <code>BaseConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class MCPSse(BaseConnection):\n    url: str = Field(..., description=\"The SSE endpoint URL to connect to.\")\n    headers: dict[str, Any] | None = Field(default=None, description=\"Optional headers to include in the SSE request.\")\n    timeout: float = Field(default=5.0, description=\"Timeout in seconds for establishing the initial connection.\")\n    sse_read_timeout: float = Field(default=60 * 5, description=\"Timeout for reading SSE messages (in seconds).\")\n\n    def connect(self):\n        \"\"\"\n        Establishes an SSE connection.\n\n        Returns:\n            Async context manager for the SSE client.\n        \"\"\"\n        from mcp.client.sse import sse_client\n\n        return sse_client(\n            url=self.url,\n            headers=self.headers,\n            timeout=self.timeout,\n            sse_read_timeout=self.sse_read_timeout,\n        )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPSse.connect","title":"<code>connect()</code>","text":"<p>Establishes an SSE connection.</p> <p>Returns:</p> Type Description <p>Async context manager for the SSE client.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establishes an SSE connection.\n\n    Returns:\n        Async context manager for the SSE client.\n    \"\"\"\n    from mcp.client.sse import sse_client\n\n    return sse_client(\n        url=self.url,\n        headers=self.headers,\n        timeout=self.timeout,\n        sse_read_timeout=self.sse_read_timeout,\n    )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPStdio","title":"<code>MCPStdio</code>","text":"<p>               Bases: <code>BaseConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class MCPStdio(BaseConnection):\n    command: str = Field(..., description=\"The executable to run to start the server.\")\n    args: list[str] = Field(default_factory=list, description=\"Command-line arguments to pass to the executable.\")\n    env: dict[str, str] | None = Field(None, description=\"Environment variables for the process.\")\n    cwd: str | Path | None = Field(None, description=\"Working directory for the process.\")\n    encoding: str = Field(default=\"utf-8\", description=\"Text encoding for communication.\")\n    encoding_error_handler: MCPEncodingErrorHandler = Field(\n        default=MCPEncodingErrorHandler.STRICT, description=\"Strategy for handling encoding errors.\"\n    )\n\n    def connect(self):\n        \"\"\"\n        Establishes a STDIO connection using a subprocess.\n\n        Returns:\n            Async context manager for the STDIO client.\n        \"\"\"\n        from mcp import StdioServerParameters\n        from mcp.client.stdio import stdio_client\n\n        return stdio_client(\n            StdioServerParameters(\n                command=self.command,\n                args=self.args,\n                env=self.env,\n                cwd=self.cwd,\n                encoding=self.encoding,\n                encoding_error_handler=self.encoding_error_handler.value,\n            )\n        )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPStdio.connect","title":"<code>connect()</code>","text":"<p>Establishes a STDIO connection using a subprocess.</p> <p>Returns:</p> Type Description <p>Async context manager for the STDIO client.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establishes a STDIO connection using a subprocess.\n\n    Returns:\n        Async context manager for the STDIO client.\n    \"\"\"\n    from mcp import StdioServerParameters\n    from mcp.client.stdio import stdio_client\n\n    return stdio_client(\n        StdioServerParameters(\n            command=self.command,\n            args=self.args,\n            env=self.env,\n            cwd=self.cwd,\n            encoding=self.encoding,\n            encoding_error_handler=self.encoding_error_handler.value,\n        )\n    )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPStreamableHTTP","title":"<code>MCPStreamableHTTP</code>","text":"<p>               Bases: <code>BaseConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class MCPStreamableHTTP(BaseConnection):\n    url: str = Field(..., description=\"The endpoint URL to connect to.\")\n    headers: dict[str, Any] | None = Field(default=None, description=\"Optional headers to include in the request.\")\n    timeout: float = Field(default=30.0, description=\"Timeout in seconds for establishing the initial connection.\")\n    sse_read_timeout: float = Field(default=60 * 5, description=\"Timeout for reading messages (in seconds).\")\n\n    def connect(self):\n        \"\"\"\n        Establishes a streamable HTTP connection.\n\n        Returns:\n            Async context manager for the streamable HTTP client.\n        \"\"\"\n        from mcp.client.streamable_http import streamablehttp_client\n\n        return streamablehttp_client(\n            url=self.url,\n            headers=self.headers,\n            timeout=timedelta(seconds=self.timeout),\n            sse_read_timeout=timedelta(seconds=self.sse_read_timeout),\n        )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MCPStreamableHTTP.connect","title":"<code>connect()</code>","text":"<p>Establishes a streamable HTTP connection.</p> <p>Returns:</p> Type Description <p>Async context manager for the streamable HTTP client.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establishes a streamable HTTP connection.\n\n    Returns:\n        Async context manager for the streamable HTTP client.\n    \"\"\"\n    from mcp.client.streamable_http import streamablehttp_client\n\n    return streamablehttp_client(\n        url=self.url,\n        headers=self.headers,\n        timeout=timedelta(seconds=self.timeout),\n        sse_read_timeout=timedelta(seconds=self.sse_read_timeout),\n    )\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Milvus","title":"<code>Milvus</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Milvus service.</p> <p>Attributes:</p> Name Type Description <code>deployment_type</code> <code>MilvusDeploymentType</code> <p>The deployment type of the Milvus service</p> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Milvus</p> <code>uri</code> <code>str</code> <p>The URI for the Milvus instance (file path or host URL).</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Milvus(BaseConnection):\n    \"\"\"\n    Represents a connection to the Milvus service.\n\n    Attributes:\n        deployment_type (MilvusDeploymentType): The deployment type of the Milvus service\n        api_key (Optional[str]): The API key for Milvus\n        uri (str): The URI for the Milvus instance (file path or host URL).\n    \"\"\"\n\n    deployment_type: MilvusDeploymentType = MilvusDeploymentType.HOST\n    uri: str = Field(default_factory=partial(get_env_var, \"MILVUS_URI\", \"http://localhost:19530\"))\n    api_key: str | None = Field(default_factory=partial(get_env_var, \"MILVUS_API_TOKEN\", None))\n\n    @field_validator(\"uri\")\n    @classmethod\n    def validate_uri(cls, uri: str, values: ValidationInfo) -&gt; str:\n        deployment_type = values.data.get(\"deployment_type\")\n\n        if deployment_type == MilvusDeploymentType.FILE and not uri.endswith(\".db\"):\n            raise ValueError(\"For FILE deployment, URI should point to a file ending with '.db'.\")\n\n        return uri\n\n    def connect(self):\n        from pymilvus import MilvusClient\n\n        if self.deployment_type == MilvusDeploymentType.FILE:\n            milvus_client = MilvusClient(uri=self.uri)\n\n        elif self.deployment_type == MilvusDeploymentType.HOST:\n            if self.api_key:\n                milvus_client = MilvusClient(uri=self.uri, token=self.api_key)\n            else:\n                milvus_client = MilvusClient(uri=self.uri)\n\n        else:\n            raise ValueError(\"Invalid deployment type for Milvus connection.\")\n\n        return milvus_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MilvusDeploymentType","title":"<code>MilvusDeploymentType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Defines general deployment types for Milvus deployments. Attributes:     FILE (str): Represents a file-based deployment, validated with a .db suffix.     HOST (str): Represents a host-based deployment, which could be a cloud, cluster,                 or single machine with or without authentication.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class MilvusDeploymentType(str, enum.Enum):\n    \"\"\"\n    Defines general deployment types for Milvus deployments.\n    Attributes:\n        FILE (str): Represents a file-based deployment, validated with a .db suffix.\n        HOST (str): Represents a host-based deployment, which could be a cloud, cluster,\n                    or single machine with or without authentication.\n    \"\"\"\n\n    FILE = \"file\"\n    HOST = \"host\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Neo4j","title":"<code>Neo4j</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to a Neo4j database.</p> <p>Attributes:</p> Name Type Description <code>uri</code> <code>str</code> <p>The Neo4j connection URI, e.g. neo4j://localhost or neo4j+s://xxx.databases.neo4j.io.</p> <code>username</code> <code>str</code> <p>Username for authentication.</p> <code>password</code> <code>str</code> <p>Password for authentication.</p> <code>database</code> <code>str | None</code> <p>Optional database name; if omitted, Neo4j uses the user's home database.</p> <code>connectivity_verification_enabled</code> <code>bool</code> <p>Whether to call driver.verify_connectivity()</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Neo4j(BaseConnection):\n    \"\"\"\n    Represents a connection to a Neo4j database.\n\n    Attributes:\n        uri (str): The Neo4j connection URI, e.g. neo4j://localhost or neo4j+s://xxx.databases.neo4j.io.\n        username (str): Username for authentication.\n        password (str): Password for authentication.\n        database (str | None): Optional database name; if omitted, Neo4j uses the user's home database.\n        connectivity_verification_enabled (bool): Whether to call driver.verify_connectivity()\n        after creating the driver.\n    \"\"\"\n\n    uri: str = Field(default_factory=partial(get_env_var, \"NEO4J_URI\"))\n    username: str = Field(default_factory=partial(get_env_var, \"NEO4J_USERNAME\"))\n    password: str = Field(default_factory=partial(get_env_var, \"NEO4J_PASSWORD\"))\n    database: str | None = Field(default_factory=partial(get_env_var, \"NEO4J_DATABASE\", None))\n    connectivity_verification_enabled: bool = Field(\n        default=True,\n        validation_alias=\"verify_connectivity\",\n    )\n\n    def connect(self) -&gt; \"Neo4jDriver\":\n        from neo4j import GraphDatabase\n\n        driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n        if self.connectivity_verification_enabled:\n            driver.verify_connectivity()\n        return driver\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.NvidiaNIM","title":"<code>NvidiaNIM</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class NvidiaNIM(BaseApiKeyConnection):\n    url: str = Field(default_factory=partial(get_env_var, \"NVIDIA_NIM_URL\"))\n    api_key: str = Field(default_factory=partial(get_env_var, \"NVIDIA_NIM_API_KEY\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.NvidiaNIM.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to Ollama API.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the Ollama API, defaults to \"http://localhost:11434\".</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Ollama(BaseConnection):\n    \"\"\"Represents a connection to Ollama API.\n\n    Attributes:\n        url (str): The URL of the Ollama API, defaults to \"http://localhost:11434\".\n    \"\"\"\n\n    url: str = Field(default=\"http://localhost:11434\")\n\n    def connect(self):\n        \"\"\"Connects to the Ollama API.\n\n        Returns:\n            requests: A requests module for making HTTP requests to the API.\n        \"\"\"\n        import requests\n\n        return requests\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing the base url with the key 'api_base'.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Ollama.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the base url with the key 'api_base'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Ollama.connect","title":"<code>connect()</code>","text":"<p>Connects to the Ollama API.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>A requests module for making HTTP requests to the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"Connects to the Ollama API.\n\n    Returns:\n        requests: A requests module for making HTTP requests to the API.\n    \"\"\"\n    import requests\n\n    return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the OpenAI service.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for the OpenAI service, fetched from the environment variable 'OPENAI_API_KEY'.</p> <code>url</code> <code>str</code> <p>The endpoint url for the OpenAI service, fetched from the environment variable 'OPENAI_URL'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class OpenAI(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the OpenAI service.\n\n    Attributes:\n        api_key (str): The API key for the OpenAI service, fetched from the environment variable 'OPENAI_API_KEY'.\n        url (str): The endpoint url for the OpenAI service, fetched from the environment variable 'OPENAI_URL'.\n    \"\"\"\n    api_key: str = Field(default_factory=partial(get_env_var, \"OPENAI_API_KEY\"))\n    url: str = Field(default_factory=partial(get_env_var, \"OPENAI_URL\", \"https://api.openai.com/v1\"))\n\n    def connect(self) -&gt; \"OpenAIClient\":\n        \"\"\"\n        Connects to the OpenAI service.\n\n        This method establishes a connection to the OpenAI service using the provided API key.\n\n        Returns:\n            OpenAIClient: An instance of the OpenAIClient connected with the specified API key.\n        \"\"\"\n        # Import in runtime to save memory\n        from openai import OpenAI as OpenAIClient\n\n        openai_client = OpenAIClient(api_key=self.api_key, base_url=self.url)\n        logger.debug(\"Connected to OpenAI\")\n        return openai_client\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.OpenAI.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.OpenAI.connect","title":"<code>connect()</code>","text":"<p>Connects to the OpenAI service.</p> <p>This method establishes a connection to the OpenAI service using the provided API key.</p> <p>Returns:</p> Name Type Description <code>OpenAIClient</code> <code>OpenAI</code> <p>An instance of the OpenAIClient connected with the specified API key.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"OpenAIClient\":\n    \"\"\"\n    Connects to the OpenAI service.\n\n    This method establishes a connection to the OpenAI service using the provided API key.\n\n    Returns:\n        OpenAIClient: An instance of the OpenAIClient connected with the specified API key.\n    \"\"\"\n    # Import in runtime to save memory\n    from openai import OpenAI as OpenAIClient\n\n    openai_client = OpenAIClient(api_key=self.api_key, base_url=self.url)\n    logger.debug(\"Connected to OpenAI\")\n    return openai_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Pinecone","title":"<code>Pinecone</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Pinecone service.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for the service. Defaults to the environment variable 'PINECONE_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Pinecone(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Pinecone service.\n\n    Attributes:\n        api_key (str): The API key for the service.\n            Defaults to the environment variable 'PINECONE_API_KEY'.\n    \"\"\"\n\n    api_key: str = Field(default_factory=partial(get_env_var, \"PINECONE_API_KEY\"))\n\n    def connect(self) -&gt; \"PineconeClient\":\n        \"\"\"\n        Connects to the Pinecone service.\n\n        This method establishes a connection to the Pinecone service using the provided API key.\n\n        Returns:\n            PineconeClient: An instance of the PineconeClient connected to the service.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import Pinecone as PineconeClient\n        pinecone_client = PineconeClient(self.api_key)\n        logger.debug(\"Connected to Pinecone\")\n        return pinecone_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Pinecone.connect","title":"<code>connect()</code>","text":"<p>Connects to the Pinecone service.</p> <p>This method establishes a connection to the Pinecone service using the provided API key.</p> <p>Returns:</p> Name Type Description <code>PineconeClient</code> <code>Pinecone</code> <p>An instance of the PineconeClient connected to the service.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"PineconeClient\":\n    \"\"\"\n    Connects to the Pinecone service.\n\n    This method establishes a connection to the Pinecone service using the provided API key.\n\n    Returns:\n        PineconeClient: An instance of the PineconeClient connected to the service.\n    \"\"\"\n    # Import in runtime to save memory\n    from pinecone import Pinecone as PineconeClient\n    pinecone_client = PineconeClient(self.api_key)\n    logger.debug(\"Connected to Pinecone\")\n    return pinecone_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.PostgreSQL","title":"<code>PostgreSQL</code>","text":"<p>               Bases: <code>BaseConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class PostgreSQL(BaseConnection):\n    host: str = Field(default_factory=partial(get_env_var, \"POSTGRESQL_HOST\", \"localhost\"))\n    port: int = Field(default_factory=partial(get_env_var, \"POSTGRESQL_PORT\", 5432))\n    database: str = Field(default_factory=partial(get_env_var, \"POSTGRESQL_DATABASE\", \"db\"))\n    user: str = Field(default_factory=partial(get_env_var, \"POSTGRESQL_USER\", \"postgres\"))\n    password: str = Field(default_factory=partial(get_env_var, \"POSTGRESQL_PASSWORD\", \"password\"))\n\n    def connect(self):\n        try:\n            import psycopg\n\n            conn = psycopg.connect(\n                host=self.host,\n                port=self.port,\n                dbname=self.database,\n                user=self.user,\n                password=self.password,\n                row_factory=psycopg.rows.dict_row,\n            )\n            conn.autocommit = True\n            logger.debug(\n                f\"Connected to PostgreSQL with host={self.host}, \"\n                f\"port={str(self.port)}, user={self.user}, \"\n                f\"database={self.database}.\"\n            )\n            return conn\n        except Exception as e:\n            raise ConnectionError(f\"Failed to connect to PostgreSQL: {str(e)}\")\n\n    @property\n    def conn_params(self) -&gt; str:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A string containing the host, the port, the database,\n            the user, and the password for the connection.\n        \"\"\"\n        return f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.PostgreSQL.conn_params","title":"<code>conn_params: str</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>str</code> <p>A string containing the host, the port, the database,</p> <code>str</code> <p>the user, and the password for the connection.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Qdrant","title":"<code>Qdrant</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Qdrant service.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the Qdrant service. Defaults to the environment variable 'QDRANT_URL'.</p> <code>api_key</code> <code>str</code> <p>The API key for the Qdrant service. Defaults to the environment variable 'QDRANT_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Qdrant(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Qdrant service.\n\n    Attributes:\n        url (str): The URL of the Qdrant service.\n            Defaults to the environment variable 'QDRANT_URL'.\n        api_key (str): The API key for the Qdrant service.\n            Defaults to the environment variable 'QDRANT_API_KEY'.\n    \"\"\"\n\n    url: str = Field(default_factory=partial(get_env_var, \"QDRANT_URL\"))\n    api_key: str = Field(default_factory=partial(get_env_var, \"QDRANT_API_KEY\"))\n\n    def connect(self) -&gt; \"QdrantClient\":\n        from qdrant_client import QdrantClient\n\n        qdrant_client = QdrantClient(\n            url=self.url,\n            api_key=self.api_key,\n        )\n\n        return qdrant_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ScaleSerp","title":"<code>ScaleSerp</code>","text":"<p>               Bases: <code>Http</code></p> <p>Connection class for Scale SERP Search API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ScaleSerp(Http):\n    \"\"\"\n    Connection class for Scale SERP Search API.\n    \"\"\"\n\n    url: str = \"https://api.scaleserp.com\"\n    api_key: str = Field(default_factory=partial(get_env_var, \"SERP_API_KEY\"))\n    method: str = HTTPMethod.GET\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def setup_params(self):\n        \"\"\"Setup params after model validation.\"\"\"\n        if self.api_key:\n            self.params.update({\"api_key\": self.api_key})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ScaleSerp.setup_params","title":"<code>setup_params()</code>","text":"<p>Setup params after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_params(self):\n    \"\"\"Setup params after model validation.\"\"\"\n    if self.api_key:\n        self.params.update({\"api_key\": self.api_key})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Tavily","title":"<code>Tavily</code>","text":"<p>               Bases: <code>Http</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Tavily(Http):\n    url: str = Field(default=\"https://api.tavily.com\")\n    api_key: str = Field(default_factory=partial(get_env_var, \"TAVILY_API_KEY\"))\n    method: Literal[HTTPMethod.POST] = HTTPMethod.POST\n\n    @model_validator(mode=\"after\")\n    def setup_data(self):\n        \"\"\"Setup data after model validation.\"\"\"\n        if self.api_key:\n            self.data.update({\"api_key\": self.api_key})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Tavily.setup_data","title":"<code>setup_data()</code>","text":"<p>Setup data after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_data(self):\n    \"\"\"Setup data after model validation.\"\"\"\n    if self.api_key:\n        self.data.update({\"api_key\": self.api_key})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Unstructured","title":"<code>Unstructured</code>","text":"<p>               Bases: <code>HttpApiKey</code></p> <p>Represents a connection to the Unstructured API.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_URL'.</p> <code>api_key</code> <code>str</code> <p>The API key for the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Unstructured(HttpApiKey):\n    \"\"\"\n    Represents a connection to the Unstructured API.\n\n    Attributes:\n        url (str): The URL of the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_URL'.\n        api_key (str): The API key for the Unstructured API, fetched from the environment\n            variable 'UNSTRUCTURED_API_KEY'.\n    \"\"\"\n\n    url: str = Field(\n        default_factory=partial(\n            get_env_var,\n            \"UNSTRUCTURED_API_URL\",\n            \"https://api.unstructured.io/\",\n        )\n    )\n    api_key: str = Field(default_factory=partial(get_env_var, \"UNSTRUCTURED_API_KEY\"))\n\n    def connect(self):\n        \"\"\"\n        Connects to the Unstructured API.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Unstructured.connect","title":"<code>connect()</code>","text":"<p>Connects to the Unstructured API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the Unstructured API.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.VertexAI","title":"<code>VertexAI</code>","text":"<p>               Bases: <code>GoogleCloud</code></p> <p>Represents a connection to the Vertex AI service.</p> <p>This connection requires additional GCP application credentials. The credentials should be provided in the connection fields (related to Google Cloud) or set in the environment variables.</p> <p>Attributes:</p> Name Type Description <code>vertex_project_id</code> <code>str</code> <p>The GCP project ID.</p> <code>vertex_project_location</code> <code>str</code> <p>The location of the GCP project.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class VertexAI(GoogleCloud):\n    \"\"\"\n    Represents a connection to the Vertex AI service.\n\n    This connection requires additional GCP application credentials. The credentials should be provided in the\n    connection fields (related to Google Cloud) or set in the environment variables.\n\n    Attributes:\n        vertex_project_id (str): The GCP project ID.\n        vertex_project_location (str): The location of the GCP project.\n    \"\"\"\n\n    vertex_project_id: str = Field(default_factory=partial(get_env_var, \"VERTEXAI_PROJECT_ID\"))\n    vertex_project_location: str = Field(default_factory=partial(get_env_var, \"VERTEXAI_PROJECT_LOCATION\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self):\n        \"\"\"\n        Returns the parameters required for the connection.\n\n        This property returns a dictionary containing the project ID and project location.\n\n        Returns:\n            dict: A dictionary with the keys 'vertex_project' and 'vertex_location'.\n        \"\"\"\n        vertex_credentials = json.dumps(super().conn_params.copy())\n        return {\n            \"vertex_project\": self.vertex_project_id,\n            \"vertex_location\": self.vertex_project_location,\n            \"vertex_credentials\": vertex_credentials,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.VertexAI.conn_params","title":"<code>conn_params</code>  <code>property</code>","text":"<p>Returns the parameters required for the connection.</p> <p>This property returns a dictionary containing the project ID and project location.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the keys 'vertex_project' and 'vertex_location'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WatsonX","title":"<code>WatsonX</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class WatsonX(BaseApiKeyConnection):\n    api_key: str = Field(default_factory=partial(get_env_var, \"WATSONX_API_KEY\"))\n    project_id: str = Field(default_factory=partial(get_env_var, \"WATSONX_PROJECT_ID\"))\n    url: str = Field(default_factory=partial(get_env_var, \"WATSONX_URL\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing\n\n                -the API key with the key 'api_key'.\n\n                -the project ID with the key 'project_id'.\n\n                -the url with the key 'url'.\n        \"\"\"\n        return {\n            \"api_key\": self.api_key,\n            \"project_id\": self.project_id,\n            \"api_base\": self.url,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WatsonX.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing</p> <p>-the API key with the key 'api_key'.</p> <p>-the project ID with the key 'project_id'.</p> <p>-the url with the key 'url'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Weaviate","title":"<code>Weaviate</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Weaviate service.</p> <p>Attributes:</p> Name Type Description <code>deployment_type</code> <code>WeaviateDeploymentType</code> <p>The deployment type of the service.</p> <code>api_key</code> <code>str</code> <p>The API key for the service. Defaults to the environment variable 'WEAVIATE_API_KEY'.</p> <code>url</code> <code>str</code> <p>The URL of the service. Defaults to the environment variable 'WEAVIATE_URL'.</p> <code>http_host</code> <code>str</code> <p>The HTTP host for the service. Defaults to the environment variable 'WEAVIATE_HTTP_HOST'.</p> <code>http_port</code> <code>int</code> <p>The HTTP port for the service. Defaults to the environment variable 'WEAVIATE_HTTP_PORT'.</p> <code>grpc_host</code> <code>str</code> <p>The gRPC host for the service. Defaults to the environment variable 'WEAVIATE_GRPC_HOST'.</p> <code>grpc_port</code> <code>int</code> <p>The gRPC port for the service. Defaults to the environment variable 'WEAVIATE_GRPC_PORT'.</p> <code>timeout_init</code> <code>int</code> <p>Timeout for initialization checks in seconds. Defaults to 30.</p> <code>timeout_query</code> <code>int</code> <p>Timeout for query operations in seconds. Defaults to 180.</p> <code>timeout_insert</code> <code>int</code> <p>Timeout for insert operations in seconds. Defaults to 120.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Weaviate(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Weaviate service.\n\n    Attributes:\n        deployment_type (WeaviateDeploymentType): The deployment type of the service.\n        api_key (str): The API key for the service.\n            Defaults to the environment variable 'WEAVIATE_API_KEY'.\n        url (str): The URL of the service.\n            Defaults to the environment variable 'WEAVIATE_URL'.\n        http_host (str): The HTTP host for the service.\n            Defaults to the environment variable 'WEAVIATE_HTTP_HOST'.\n        http_port (int): The HTTP port for the service.\n            Defaults to the environment variable 'WEAVIATE_HTTP_PORT'.\n        grpc_host (str): The gRPC host for the service.\n            Defaults to the environment variable 'WEAVIATE_GRPC_HOST'.\n        grpc_port (int): The gRPC port for the service.\n            Defaults to the environment variable 'WEAVIATE_GRPC_PORT'.\n        timeout_init (int): Timeout for initialization checks in seconds.\n            Defaults to 30.\n        timeout_query (int): Timeout for query operations in seconds.\n            Defaults to 180.\n        timeout_insert (int): Timeout for insert operations in seconds.\n            Defaults to 120.\n    \"\"\"\n\n    deployment_type: WeaviateDeploymentType = WeaviateDeploymentType.WEAVIATE_CLOUD\n    api_key: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_API_KEY\"))\n    url: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_URL\"))\n    http_host: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_HTTP_HOST\"))\n    http_port: int = Field(default_factory=partial(get_env_var, \"WEAVIATE_HTTP_PORT\", 443))\n    grpc_host: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_GRPC_HOST\"))\n    grpc_port: int = Field(default_factory=partial(get_env_var, \"WEAVIATE_GRPC_PORT\", 50051))\n    timeout_init: int = Field(default=30, description=\"Timeout for initialization checks in seconds\")\n    timeout_query: int = Field(default=180, description=\"Timeout for query operations in seconds\")\n    timeout_insert: int = Field(default=120, description=\"Timeout for insert operations in seconds\")\n\n    def connect(self) -&gt; \"WeaviateClient\":\n        \"\"\"\n        Connects to the Weaviate service.\n\n        This method establishes a connection to the Weaviate service using the provided URL and API key.\n\n        Returns:\n            WeaviateClient: An instance of the WeaviateClient connected to the specified URL.\n        \"\"\"\n        # Import in runtime to save memory\n        from weaviate import connect_to_custom, connect_to_weaviate_cloud\n        from weaviate.classes.init import AdditionalConfig, Auth, Timeout\n\n        if self.deployment_type == WeaviateDeploymentType.WEAVIATE_CLOUD:\n            weaviate_client = connect_to_weaviate_cloud(\n                cluster_url=self.url,\n                auth_credentials=Auth.api_key(self.api_key),\n                additional_config=AdditionalConfig(\n                    timeout=Timeout(init=self.timeout_init, query=self.timeout_query, insert=self.timeout_insert)\n                ),\n            )\n            logger.debug(f\"Connected to Weaviate with url={self.url}\")\n            return weaviate_client\n\n        elif self.deployment_type == WeaviateDeploymentType.CUSTOM:\n            weaviate_client = connect_to_custom(\n                http_host=self.http_host,\n                http_port=self.http_port,\n                http_secure=True,\n                grpc_host=self.grpc_host,\n                grpc_port=self.grpc_port,\n                grpc_secure=True,\n                auth_credentials=Auth.api_key(self.api_key),\n                additional_config=AdditionalConfig(\n                    timeout=Timeout(\n                        init=self.timeout_init, query=self.timeout_query, insert=self.timeout_insert\n                    ),  # Values in seconds\n                ),\n                skip_init_checks=False,\n            )\n            logger.debug(f\"Connected to Weaviate with http_host={self.http_host}\")\n            return weaviate_client\n        else:\n            raise ValueError(\"Invalid deployment type\")\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Weaviate.connect","title":"<code>connect()</code>","text":"<p>Connects to the Weaviate service.</p> <p>This method establishes a connection to the Weaviate service using the provided URL and API key.</p> <p>Returns:</p> Name Type Description <code>WeaviateClient</code> <code>WeaviateClient</code> <p>An instance of the WeaviateClient connected to the specified URL.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"WeaviateClient\":\n    \"\"\"\n    Connects to the Weaviate service.\n\n    This method establishes a connection to the Weaviate service using the provided URL and API key.\n\n    Returns:\n        WeaviateClient: An instance of the WeaviateClient connected to the specified URL.\n    \"\"\"\n    # Import in runtime to save memory\n    from weaviate import connect_to_custom, connect_to_weaviate_cloud\n    from weaviate.classes.init import AdditionalConfig, Auth, Timeout\n\n    if self.deployment_type == WeaviateDeploymentType.WEAVIATE_CLOUD:\n        weaviate_client = connect_to_weaviate_cloud(\n            cluster_url=self.url,\n            auth_credentials=Auth.api_key(self.api_key),\n            additional_config=AdditionalConfig(\n                timeout=Timeout(init=self.timeout_init, query=self.timeout_query, insert=self.timeout_insert)\n            ),\n        )\n        logger.debug(f\"Connected to Weaviate with url={self.url}\")\n        return weaviate_client\n\n    elif self.deployment_type == WeaviateDeploymentType.CUSTOM:\n        weaviate_client = connect_to_custom(\n            http_host=self.http_host,\n            http_port=self.http_port,\n            http_secure=True,\n            grpc_host=self.grpc_host,\n            grpc_port=self.grpc_port,\n            grpc_secure=True,\n            auth_credentials=Auth.api_key(self.api_key),\n            additional_config=AdditionalConfig(\n                timeout=Timeout(\n                    init=self.timeout_init, query=self.timeout_query, insert=self.timeout_insert\n                ),  # Values in seconds\n            ),\n            skip_init_checks=False,\n        )\n        logger.debug(f\"Connected to Weaviate with http_host={self.http_host}\")\n        return weaviate_client\n    else:\n        raise ValueError(\"Invalid deployment type\")\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WeaviateDeploymentType","title":"<code>WeaviateDeploymentType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Defines various deployment types for different Weaviate deployments.</p> <p>Attributes:</p> Name Type Description <code>WEAVIATE_CLOUD</code> <code>str</code> <p>Represents a deployment on Weaviate Cloud. Value is 'weaviate_cloud'.</p> <code>CUSTOM</code> <code>str</code> <p>Represents a custom deployment. Value is 'custom'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class WeaviateDeploymentType(str, enum.Enum):\n    \"\"\"\n    Defines various deployment types for different Weaviate deployments.\n\n    Attributes:\n        WEAVIATE_CLOUD (str): Represents a deployment on Weaviate Cloud.\n            Value is 'weaviate_cloud'.\n        CUSTOM (str): Represents a custom deployment.\n            Value is 'custom'.\n    \"\"\"\n\n    WEAVIATE_CLOUD = \"weaviate_cloud\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Whisper","title":"<code>Whisper</code>","text":"<p>               Bases: <code>Http</code></p> <p>Represents a connection to the Whisper API using an HTTP request.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>URL of the Whisper API, fetched from the environment variable \"WHISPER_URL\".</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>api_key</code> <code>str</code> <p>API key for authentication, fetched from the environment variable \"OPENAI_API_KEY\".</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Whisper(Http):\n    \"\"\"\n    Represents a connection to the Whisper API using an HTTP request.\n\n    Attributes:\n        url (str): URL of the Whisper API, fetched from the environment variable \"WHISPER_URL\".\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        api_key (str): API key for authentication, fetched from the environment variable \"OPENAI_API_KEY\".\n    \"\"\"\n    url: str = Field(\n        default_factory=partial(\n            get_env_var, \"WHISPER_URL\", \"https://api.openai.com/v1/\"\n        )\n    )\n    method: str = HTTPMethod.POST\n    api_key: str = Field(default_factory=partial(get_env_var, \"OPENAI_API_KEY\"))\n\n    @model_validator(mode=\"after\")\n    def setup_headers(self):\n        \"\"\"Setup headers after model validation.\"\"\"\n        if self.api_key:\n            self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Whisper.setup_headers","title":"<code>setup_headers()</code>","text":"<p>Setup headers after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_headers(self):\n    \"\"\"Setup headers after model validation.\"\"\"\n    if self.api_key:\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ZenRows","title":"<code>ZenRows</code>","text":"<p>               Bases: <code>Http</code></p> <p>Connection class for ZenRows Scrape API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ZenRows(Http):\n    \"\"\"\n    Connection class for ZenRows Scrape API.\n    \"\"\"\n\n    url: str = \"https://api.zenrows.com/v1/\"\n    api_key: str = Field(default_factory=partial(get_env_var, \"ZENROWS_API_KEY\"))\n    method: str = HTTPMethod.GET\n\n    @model_validator(mode=\"after\")\n    def setup_params(self):\n        \"\"\"Setup params after model validation.\"\"\"\n        if self.api_key:\n            self.params.update({\"apikey\": self.api_key})\n        return self\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ZenRows.setup_params","title":"<code>setup_params()</code>","text":"<p>Setup params after model validation.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_params(self):\n    \"\"\"Setup params after model validation.\"\"\"\n    if self.api_key:\n        self.params.update({\"apikey\": self.api_key})\n    return self\n</code></pre>"},{"location":"dynamiq/connections/managers/","title":"Managers","text":""},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionClientInitType","title":"<code>ConnectionClientInitType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of connection client initialization types.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionClientInitType(str, enum.Enum):\n    \"\"\"Enumeration of connection client initialization types.\"\"\"\n    DEFAULT = \"DEFAULT\"\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager","title":"<code>ConnectionManager</code>","text":"<p>Manages connections to various services and databases.</p> <p>This class handles the creation, retrieval, and management of connection clients for different types of services and databases.</p> <p>Attributes:</p> Name Type Description <code>serializer</code> <p>An object used for serializing and deserializing data.</p> <code>connection_clients</code> <code>dict[str, Any]</code> <p>A dictionary storing initialized connection clients.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionManager:\n    \"\"\"\n    Manages connections to various services and databases.\n\n    This class handles the creation, retrieval, and management of connection clients\n    for different types of services and databases.\n\n    Attributes:\n        serializer: An object used for serializing and deserializing data.\n        connection_clients: A dictionary storing initialized connection clients.\n    \"\"\"\n\n    def __init__(self, serializer: Any | None = None):\n        \"\"\"\n        Initializes the ConnectionManager.\n\n        Args:\n            serializer: An optional serializer object. If not provided, JsonPickleSerializer is used.\n        \"\"\"\n        self.serializer = serializer or JsonPickleSerializer()\n        self.connection_clients: dict[str, Any] = {}\n        self._connection_locks_guard = threading.Lock()\n        self._connection_locks: dict[str, threading.Lock] = {}\n\n    @staticmethod\n    def get_connection_by_type(conn_type: str) -&gt; type[BaseConnection]:\n        \"\"\"\n        Retrieves the connection class based on the given connection type.\n\n        Args:\n            conn_type: The type of connection to retrieve.\n\n        Returns:\n            The connection class corresponding to the given type.\n\n        Raises:\n            ValueError: If the connection type is not found.\n        \"\"\"\n        try:\n            entity_module, entity_name = conn_type.rsplit(\".\", 1)\n            imported_module = importlib.import_module(entity_module)\n            if entity := getattr(imported_module, entity_name, None):\n                return entity\n        except (ModuleNotFoundError, ImportError):\n            raise ValueError(f\"Connection type {conn_type} not found\")\n\n    def _get_conn_lock(self, conn_id: str) -&gt; threading.Lock:\n        \"\"\"Get or create a per-connection lock for the given conn_id.\"\"\"\n        with self._connection_locks_guard:\n            if conn_id not in self._connection_locks:\n                self._connection_locks[conn_id] = threading.Lock()\n            return self._connection_locks[conn_id]\n\n    def _is_client_alive(self, conn_client: Any) -&gt; bool:\n        \"\"\"Check if an existing connection client is still usable.\"\"\"\n        conn_client_closed = getattr(conn_client, \"closed\", None)\n        if conn_client_closed is None:\n            return True\n        return not conn_client_closed\n\n    def get_connection_client(\n        self,\n        connection: BaseConnection,\n        init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n    ) -&gt; Any | None:\n        \"\"\"\n        Retrieves or initializes a connection client for the given connection.\n\n        Thread-safe: uses per-connection locks so different connections can be\n        created in parallel, while the same connection is never created twice concurrently.\n\n        Args:\n            connection: The connection object.\n            init_type: The initialization type for the connection client.\n\n        Returns:\n            The initialized connection client.\n\n        Raises:\n            ConnectionManagerException: If the connection does not support the specified initialization type.\n        \"\"\"\n        logger.debug(\n            f\"Get connection client for '{connection.id}-{connection.type}' \"\n            f\"with '{init_type.value.lower()}' initialization\"\n        )\n        conn_id = self.get_connection_id(connection, init_type)\n\n        if conn_client := self.connection_clients.get(conn_id):\n            if self._is_client_alive(conn_client):\n                return conn_client\n\n        conn_lock = self._get_conn_lock(conn_id)\n        with conn_lock:\n            logger.info(f\"Acquired lock for '{conn_id}' connection for initialization\")\n            # Double-check after acquiring lock (another thread may have created it)\n            if conn_client := self.connection_clients.get(conn_id):\n                if self._is_client_alive(conn_client):\n                    return conn_client\n\n            logger.debug(\n                f\"Init connection client for '{connection.id}-{connection.type}' \"\n                f\"with '{init_type.value.lower()}' initialization\"\n            )\n            conn_method_name = CONNECTION_METHOD_BY_INIT_TYPE[init_type]\n            if not (conn_method := getattr(connection, conn_method_name, None)) or not callable(conn_method):\n                raise ConnectionManagerException(\n                    f\"Connection '{connection.id}-{connection.type}' not support '{init_type.value}' initialization\"\n                )\n\n            conn_client = conn_method()\n            self.connection_clients[conn_id] = conn_client\n\n            return conn_client\n\n    def get_connection_id(\n        self,\n        connection: BaseConnection,\n        init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n    ) -&gt; str:\n        \"\"\"\n        Generates a unique connection ID based on the connection and initialization type.\n\n        Args:\n            connection: The connection object.\n            init_type: The initialization type for the connection client.\n\n        Returns:\n            A unique string identifier for the connection.\n        \"\"\"\n        conn_hash = self.hash(connection.model_dump_json())\n        return f\"{connection.type.lower()}:{init_type.lower()}:{conn_hash}\"\n\n    def close(self):\n        \"\"\"\n        Closes all open connection clients and clears the connection_clients dictionary.\n        \"\"\"\n        logger.debug(\"Close connection clients\")\n        for conn_client in self.connection_clients.values():\n            if hasattr(conn_client, \"close\"):\n                conn_client.close()\n        self.connection_clients = {}\n\n    @staticmethod\n    def hash(data: str) -&gt; str:\n        \"\"\"\n        Generates a SHA256 hash of the input string.\n\n        Args:\n            data: The input string to hash.\n\n        Returns:\n            The hexadecimal representation of the SHA256 hash.\n        \"\"\"\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def dumps(self, data: Any):\n        \"\"\"\n        Serializes the given data using the serializer.\n\n        Args:\n            data: The data to serialize.\n\n        Returns:\n            The serialized data.\n        \"\"\"\n        return self.serializer.dumps(data)\n\n    def loads(self, value: str):\n        \"\"\"\n        Deserializes the given value using the serializer.\n\n        Args:\n            value: The serialized string to deserialize.\n\n        Returns:\n            The deserialized data.\n        \"\"\"\n        return self.serializer.loads(value)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.__init__","title":"<code>__init__(serializer=None)</code>","text":"<p>Initializes the ConnectionManager.</p> <p>Parameters:</p> Name Type Description Default <code>serializer</code> <code>Any | None</code> <p>An optional serializer object. If not provided, JsonPickleSerializer is used.</p> <code>None</code> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def __init__(self, serializer: Any | None = None):\n    \"\"\"\n    Initializes the ConnectionManager.\n\n    Args:\n        serializer: An optional serializer object. If not provided, JsonPickleSerializer is used.\n    \"\"\"\n    self.serializer = serializer or JsonPickleSerializer()\n    self.connection_clients: dict[str, Any] = {}\n    self._connection_locks_guard = threading.Lock()\n    self._connection_locks: dict[str, threading.Lock] = {}\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.close","title":"<code>close()</code>","text":"<p>Closes all open connection clients and clears the connection_clients dictionary.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes all open connection clients and clears the connection_clients dictionary.\n    \"\"\"\n    logger.debug(\"Close connection clients\")\n    for conn_client in self.connection_clients.values():\n        if hasattr(conn_client, \"close\"):\n            conn_client.close()\n    self.connection_clients = {}\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.dumps","title":"<code>dumps(data)</code>","text":"<p>Serializes the given data using the serializer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to serialize.</p> required <p>Returns:</p> Type Description <p>The serialized data.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def dumps(self, data: Any):\n    \"\"\"\n    Serializes the given data using the serializer.\n\n    Args:\n        data: The data to serialize.\n\n    Returns:\n        The serialized data.\n    \"\"\"\n    return self.serializer.dumps(data)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_by_type","title":"<code>get_connection_by_type(conn_type)</code>  <code>staticmethod</code>","text":"<p>Retrieves the connection class based on the given connection type.</p> <p>Parameters:</p> Name Type Description Default <code>conn_type</code> <code>str</code> <p>The type of connection to retrieve.</p> required <p>Returns:</p> Type Description <code>type[BaseConnection]</code> <p>The connection class corresponding to the given type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the connection type is not found.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@staticmethod\ndef get_connection_by_type(conn_type: str) -&gt; type[BaseConnection]:\n    \"\"\"\n    Retrieves the connection class based on the given connection type.\n\n    Args:\n        conn_type: The type of connection to retrieve.\n\n    Returns:\n        The connection class corresponding to the given type.\n\n    Raises:\n        ValueError: If the connection type is not found.\n    \"\"\"\n    try:\n        entity_module, entity_name = conn_type.rsplit(\".\", 1)\n        imported_module = importlib.import_module(entity_module)\n        if entity := getattr(imported_module, entity_name, None):\n            return entity\n    except (ModuleNotFoundError, ImportError):\n        raise ValueError(f\"Connection type {conn_type} not found\")\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_client","title":"<code>get_connection_client(connection, init_type=ConnectionClientInitType.DEFAULT)</code>","text":"<p>Retrieves or initializes a connection client for the given connection.</p> <p>Thread-safe: uses per-connection locks so different connections can be created in parallel, while the same connection is never created twice concurrently.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>BaseConnection</code> <p>The connection object.</p> required <code>init_type</code> <code>ConnectionClientInitType</code> <p>The initialization type for the connection client.</p> <code>DEFAULT</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>The initialized connection client.</p> <p>Raises:</p> Type Description <code>ConnectionManagerException</code> <p>If the connection does not support the specified initialization type.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def get_connection_client(\n    self,\n    connection: BaseConnection,\n    init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n) -&gt; Any | None:\n    \"\"\"\n    Retrieves or initializes a connection client for the given connection.\n\n    Thread-safe: uses per-connection locks so different connections can be\n    created in parallel, while the same connection is never created twice concurrently.\n\n    Args:\n        connection: The connection object.\n        init_type: The initialization type for the connection client.\n\n    Returns:\n        The initialized connection client.\n\n    Raises:\n        ConnectionManagerException: If the connection does not support the specified initialization type.\n    \"\"\"\n    logger.debug(\n        f\"Get connection client for '{connection.id}-{connection.type}' \"\n        f\"with '{init_type.value.lower()}' initialization\"\n    )\n    conn_id = self.get_connection_id(connection, init_type)\n\n    if conn_client := self.connection_clients.get(conn_id):\n        if self._is_client_alive(conn_client):\n            return conn_client\n\n    conn_lock = self._get_conn_lock(conn_id)\n    with conn_lock:\n        logger.info(f\"Acquired lock for '{conn_id}' connection for initialization\")\n        # Double-check after acquiring lock (another thread may have created it)\n        if conn_client := self.connection_clients.get(conn_id):\n            if self._is_client_alive(conn_client):\n                return conn_client\n\n        logger.debug(\n            f\"Init connection client for '{connection.id}-{connection.type}' \"\n            f\"with '{init_type.value.lower()}' initialization\"\n        )\n        conn_method_name = CONNECTION_METHOD_BY_INIT_TYPE[init_type]\n        if not (conn_method := getattr(connection, conn_method_name, None)) or not callable(conn_method):\n            raise ConnectionManagerException(\n                f\"Connection '{connection.id}-{connection.type}' not support '{init_type.value}' initialization\"\n            )\n\n        conn_client = conn_method()\n        self.connection_clients[conn_id] = conn_client\n\n        return conn_client\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_id","title":"<code>get_connection_id(connection, init_type=ConnectionClientInitType.DEFAULT)</code>","text":"<p>Generates a unique connection ID based on the connection and initialization type.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>BaseConnection</code> <p>The connection object.</p> required <code>init_type</code> <code>ConnectionClientInitType</code> <p>The initialization type for the connection client.</p> <code>DEFAULT</code> <p>Returns:</p> Type Description <code>str</code> <p>A unique string identifier for the connection.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def get_connection_id(\n    self,\n    connection: BaseConnection,\n    init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n) -&gt; str:\n    \"\"\"\n    Generates a unique connection ID based on the connection and initialization type.\n\n    Args:\n        connection: The connection object.\n        init_type: The initialization type for the connection client.\n\n    Returns:\n        A unique string identifier for the connection.\n    \"\"\"\n    conn_hash = self.hash(connection.model_dump_json())\n    return f\"{connection.type.lower()}:{init_type.lower()}:{conn_hash}\"\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.hash","title":"<code>hash(data)</code>  <code>staticmethod</code>","text":"<p>Generates a SHA256 hash of the input string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The input string to hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The hexadecimal representation of the SHA256 hash.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@staticmethod\ndef hash(data: str) -&gt; str:\n    \"\"\"\n    Generates a SHA256 hash of the input string.\n\n    Args:\n        data: The input string to hash.\n\n    Returns:\n        The hexadecimal representation of the SHA256 hash.\n    \"\"\"\n    return hashlib.sha256(data.encode()).hexdigest()\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.loads","title":"<code>loads(value)</code>","text":"<p>Deserializes the given value using the serializer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The serialized string to deserialize.</p> required <p>Returns:</p> Type Description <p>The deserialized data.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def loads(self, value: str):\n    \"\"\"\n    Deserializes the given value using the serializer.\n\n    Args:\n        value: The serialized string to deserialize.\n\n    Returns:\n        The deserialized data.\n    \"\"\"\n    return self.serializer.loads(value)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManagerException","title":"<code>ConnectionManagerException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in the ConnectionManager.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionManagerException(Exception):\n    \"\"\"Exception raised for errors in the ConnectionManager.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.get_connection_manager","title":"<code>get_connection_manager()</code>","text":"<p>A context manager that yields a ConnectionManager instance and ensures it's closed properly.</p> <p>Yields:</p> Type Description <p>A ConnectionManager instance.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@contextmanager\ndef get_connection_manager():\n    \"\"\"\n    A context manager that yields a ConnectionManager instance and ensures it's closed properly.\n\n    Yields:\n        A ConnectionManager instance.\n    \"\"\"\n    cm = ConnectionManager()\n    yield cm\n    cm.close()\n</code></pre>"},{"location":"dynamiq/connections/storages/","title":"Storages","text":""},{"location":"dynamiq/connections/storages/#dynamiq.connections.storages.RedisConnection","title":"<code>RedisConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to a Redis database.</p> <p>This class inherits from BaseConnection and provides specific attributes for connecting to a Redis database.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>The hostname or IP address of the Redis server.</p> <code>port</code> <code>int</code> <p>The port number on which the Redis server is listening.</p> <code>db</code> <code>int</code> <p>The Redis database number to connect to.</p> <code>username</code> <code>str | None</code> <p>The username for authentication (optional).</p> <code>password</code> <code>str | None</code> <p>The password for authentication (optional).</p> Source code in <code>dynamiq/connections/storages.py</code> <pre><code>class RedisConnection(BaseConnection):\n    \"\"\"\n    Represents a connection to a Redis database.\n\n    This class inherits from BaseConnection and provides specific attributes\n    for connecting to a Redis database.\n\n    Attributes:\n        host (str): The hostname or IP address of the Redis server.\n        port (int): The port number on which the Redis server is listening.\n        db (int): The Redis database number to connect to.\n        username (str | None): The username for authentication (optional).\n        password (str | None): The password for authentication (optional).\n    \"\"\"\n\n    host: str\n    port: int\n    db: int\n    username: str | None = None\n    password: str | None = None\n\n    def connect(self):\n        \"\"\"\n        Establishes a connection to the Redis database.\n\n        This method is responsible for creating and initializing the connection\n        to the Redis server using the provided connection details.\n\n        Note:\n            This method is currently a placeholder and does not contain\n            the actual implementation for connecting to Redis.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/connections/storages/#dynamiq.connections.storages.RedisConnection.connect","title":"<code>connect()</code>","text":"<p>Establishes a connection to the Redis database.</p> <p>This method is responsible for creating and initializing the connection to the Redis server using the provided connection details.</p> Note <p>This method is currently a placeholder and does not contain the actual implementation for connecting to Redis.</p> Source code in <code>dynamiq/connections/storages.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establishes a connection to the Redis database.\n\n    This method is responsible for creating and initializing the connection\n    to the Redis server using the provided connection details.\n\n    Note:\n        This method is currently a placeholder and does not contain\n        the actual implementation for connecting to Redis.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/evaluations/base_evaluator/","title":"Base evaluator","text":""},{"location":"dynamiq/evaluations/base_evaluator/#dynamiq.evaluations.base_evaluator.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for evaluators.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the evaluator.</p> Source code in <code>dynamiq/evaluations/base_evaluator.py</code> <pre><code>class BaseEvaluator(BaseModel):\n    \"\"\"\n    Base class for evaluators.\n\n    Attributes:\n        name (str): Name of the evaluator.\n    \"\"\"\n\n    name: str\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        \"\"\"\n        Compute the type identifier for the evaluator.\n\n        Returns:\n            str: A string representing the module and class name.\n        \"\"\"\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    def run(self) -&gt; list[float]:\n        \"\"\"\n        Executes the evaluator.\n        Must be overridden by subclasses.\n\n        Returns:\n            list[float]: Scores for each reference/answer pair.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"dynamiq/evaluations/base_evaluator/#dynamiq.evaluations.base_evaluator.BaseEvaluator.type","title":"<code>type: str</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the type identifier for the evaluator.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the module and class name.</p>"},{"location":"dynamiq/evaluations/base_evaluator/#dynamiq.evaluations.base_evaluator.BaseEvaluator.run","title":"<code>run()</code>","text":"<p>Executes the evaluator. Must be overridden by subclasses.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Scores for each reference/answer pair.</p> Source code in <code>dynamiq/evaluations/base_evaluator.py</code> <pre><code>def run(self) -&gt; list[float]:\n    \"\"\"\n    Executes the evaluator.\n    Must be overridden by subclasses.\n\n    Returns:\n        list[float]: Scores for each reference/answer pair.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method.\")\n</code></pre>"},{"location":"dynamiq/evaluations/llm_evaluator/","title":"Llm evaluator","text":""},{"location":"dynamiq/evaluations/llm_evaluator/#dynamiq.evaluations.llm_evaluator.LLMEvaluator","title":"<code>LLMEvaluator</code>","text":"<p>Uses an LLM to evaluate inputs based on a prompt containing instructions and examples.</p> <p>The inputs and outputs are user-defined depending on the desired metric. The output is a dictionary with a key <code>results</code> containing a list of evaluation results. Each result is a dictionary with user-defined keys and values as specified in the outputs.</p> <p>Usage example:</p> <pre><code>from typing import List\nfrom dynamiq.components.evaluators.llm_evaluator import LLMEvaluator\n\nllm = ...  # Your LLM node instance\n\nevaluator = LLMEvaluator(\n    instructions=\"Is this answer problematic for children?\",\n    inputs=[{\"name\": \"answer\", \"type\": str}],\n    outputs=[{\"name\": \"score\", \"type\": int}],\n    examples=[\n        {\n            \"inputs\": {\"answer\": \"Damn, this is straight outta hell!!!\"},\n            \"outputs\": {\"score\": 1},\n        },\n        {\n            \"inputs\": {answer\": \"Football is the most popular sport.\"},\n            \"outputs\": {\"score\": 0},\n        },\n    ],\n    llm=llm,\n)\n\npredicted_answers = [\n    \"Football is the most popular sport with around 4 billion followers worldwide\",\n    \"Python language was created by Guido van Rossum.\",\n]\nresults = evaluator.run(answer=predicted_answers)\nprint(results)\n# Output: {'results': [{'score': 0}, {'score': 0}]}\n</code></pre> Source code in <code>dynamiq/evaluations/llm_evaluator.py</code> <pre><code>class LLMEvaluator:\n    \"\"\"\n    Uses an LLM to evaluate inputs based on a prompt containing instructions and examples.\n\n    The inputs and outputs are user-defined depending on the desired metric. The output\n    is a dictionary with a key `results` containing a list of evaluation results. Each result\n    is a dictionary with user-defined keys and values as specified in the outputs.\n\n    **Usage example:**\n\n    ```python\n    from typing import List\n    from dynamiq.components.evaluators.llm_evaluator import LLMEvaluator\n\n    llm = ...  # Your LLM node instance\n\n    evaluator = LLMEvaluator(\n        instructions=\"Is this answer problematic for children?\",\n        inputs=[{\"name\": \"answer\", \"type\": str}],\n        outputs=[{\"name\": \"score\", \"type\": int}],\n        examples=[\n            {\n                \"inputs\": {\"answer\": \"Damn, this is straight outta hell!!!\"},\n                \"outputs\": {\"score\": 1},\n            },\n            {\n                \"inputs\": {answer\": \"Football is the most popular sport.\"},\n                \"outputs\": {\"score\": 0},\n            },\n        ],\n        llm=llm,\n    )\n\n    predicted_answers = [\n        \"Football is the most popular sport with around 4 billion followers worldwide\",\n        \"Python language was created by Guido van Rossum.\",\n    ]\n    results = evaluator.run(answer=predicted_answers)\n    print(results)\n    # Output: {'results': [{'score': 0}, {'score': 0}]}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        instructions: str,\n        outputs: list[dict[str, Any]],\n        examples: list[dict[str, Any]] | None = None,\n        inputs: list[dict[str, Any]] | None = None,\n        *,\n        raise_on_failure: bool = True,\n        llm: Node,\n        strings_to_omit_from_llm_output: tuple[str] = STRINGS_TO_OMIT_FROM_LLM_EVALUATOR_OUTPUT,\n    ):\n        \"\"\"\n        Initializes an instance of LLMEvaluator.\n\n        Args:\n            instructions (str): The prompt instructions to use for evaluation.\n            outputs (List[Dict[str, Any]]): A list of dictionaries defining the outputs.\n                Each output dict should have keys \"name\" and \"type\", where \"name\" is the\n                output name and \"type\" is its type.\n            examples (Optional[List[Dict[str, Any]]]): Few-shot examples conforming to the expected input and\n                output format as defined in the `inputs` and `outputs` parameters. Each example is a\n                dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as\n                dictionaries respectively.\n            inputs (Optional[List[Dict[str, Any]]]): A list of dictionaries defining the inputs.\n                Each input dict should have keys \"name\" and \"type\", where \"name\" is the\n                input name and \"type\" is its type. Defaults to None.\n            raise_on_failure (bool): If True, the component will raise an exception on an\n                unsuccessful API call.\n            llm (Node): The LLM node to use for evaluation.\n            strings_to_omit_from_llm_output (Tuple[str]): A tuple of strings to omit from the LLM output.\n        \"\"\"\n        if inputs is None:\n            inputs = []\n        if examples is None:\n            examples = []\n        self._validate_init_parameters(inputs, outputs, examples)\n        self.raise_on_failure = raise_on_failure\n        self.instructions = instructions\n        self.inputs = inputs\n        self.outputs = outputs\n        self.examples = examples\n        self.api_params = {}\n\n        default_generation_kwargs = {\n            \"response_format\": {\"type\": \"json_object\"},\n            \"seed\": 42,\n        }\n        user_generation_kwargs = self.api_params.get(\"generation_kwargs\", {})\n        merged_generation_kwargs = {\n            **default_generation_kwargs,\n            **user_generation_kwargs,\n        }\n        self.api_params[\"generation_kwargs\"] = merged_generation_kwargs\n\n        # Prepare the prompt with placeholders\n        template = self._prepare_prompt_template()\n        message = Message(role=\"user\", content=template)\n        self.prompt = Prompt(messages=[message])\n\n        self.llm = llm\n        self.strings_to_omit_from_llm_output = strings_to_omit_from_llm_output\n\n    @staticmethod\n    def _validate_init_parameters(\n        inputs: list[dict[str, Any]],\n        outputs: list[dict[str, Any]],\n        examples: list[dict[str, Any]],\n    ):\n        \"\"\"\n        Validates the initialization parameters.\n\n        Args:\n            inputs (List[Dict[str, Any]]): The inputs to validate.\n            outputs (List[Dict[str, Any]]): The outputs to validate.\n            examples (List[Dict[str, Any]]): The examples to validate.\n\n        Raises:\n            ValueError: If the inputs or outputs are not correctly formatted.\n        \"\"\"\n        # Validate inputs\n        if not isinstance(inputs, list) or not all(isinstance(inp, dict) for inp in inputs):\n            msg = \"LLM evaluator expects inputs to be a list of dictionaries.\"\n            raise ValueError(msg)\n        for inp in inputs:\n            if \"name\" not in inp or \"type\" not in inp:\n                msg = f\"Each input dict must have 'name' and 'type' keys. Missing in {inp}.\"\n                raise ValueError(msg)\n            if not isinstance(inp[\"name\"], str):\n                msg = f\"Input 'name' must be a string. Got {inp['name']}.\"\n                raise ValueError(msg)\n            # No type check on 'type' to allow types from 'typing' module\n\n        # Validate outputs\n        if not isinstance(outputs, list) or not all(isinstance(outp, dict) for outp in outputs):\n            msg = \"LLM evaluator expects outputs to be a list of dictionaries.\"\n            raise ValueError(msg)\n        for outp in outputs:\n            if \"name\" not in outp or \"type\" not in outp:\n                msg = f\"Each output dict must have 'name' and 'type' keys. Missing in {outp}.\"\n                raise ValueError(msg)\n            if not isinstance(outp[\"name\"], str):\n                msg = f\"Output 'name' must be a string. Got {outp['name']}.\"\n                raise ValueError(msg)\n            # No type check on 'type' to allow types from 'typing' module\n\n        # Validate examples\n        if not isinstance(examples, list) or not all(isinstance(example, dict) for example in examples):\n            msg = f\"LLM evaluator expects examples to be a list of dictionaries but received {examples}.\"\n            raise ValueError(msg)\n\n        for example in examples:\n            if (\n                not all(k in example for k in (\"inputs\", \"outputs\"))\n                or not all(isinstance(example[param], dict) for param in [\"inputs\", \"outputs\"])\n                or not all(isinstance(key, str) for param in [\"inputs\", \"outputs\"] for key in example[param])\n            ):\n                msg = (\n                    f\"Each example must have 'inputs' and 'outputs' as dictionaries with string keys, \"\n                    f\"but received {example}.\"\n                )\n                raise ValueError(msg)\n\n    def run(self, **inputs) -&gt; dict[str, Any]:\n        \"\"\"\n        Runs the LLM evaluator.\n\n        Args:\n            inputs: The input values to evaluate. Keys are input names, values are lists.\n\n        Returns:\n            Dict[str, Any]: A dictionary with a 'results' key containing the evaluation results.\n\n        Raises:\n            ValueError: If input parameters are invalid or LLM execution fails.\n        \"\"\"\n        expected_inputs = {inp[\"name\"]: inp[\"type\"] for inp in self.inputs}\n        self._validate_input_parameters(expected=expected_inputs, received=inputs)\n\n        if self.inputs:\n            input_names = list(inputs.keys())\n            values = list(zip(*inputs.values()))\n            list_of_input_data = [dict(zip(input_names, v)) for v in values]\n        else:\n            # If no inputs are provided, create a list with a single empty dictionary\n            list_of_input_data = [{}]\n\n        results: list[dict[str, Any]] = []\n        errors = 0\n\n        for input_data in list_of_input_data:\n            # Pass the prompt and input_data to LLM\n            try:\n                result = self.llm.execute(input_data=input_data, prompt=self.prompt)\n            except Exception as e:\n                msg = f\"Error while generating response for input {input_data}: {e}\"\n                if self.raise_on_failure:\n                    raise ValueError(msg)\n                warn(msg)\n                results.append(None)\n                errors += 1\n                continue\n\n            expected_output_keys = [outp[\"name\"] for outp in self.outputs]\n            content = self._cleanup_output_content(result[\"content\"])\n\n            parsed_result = self._parse_and_validate_json_output(expected_keys=expected_output_keys, content=content)\n            if parsed_result is not None:\n                results.append(parsed_result)\n            else:\n                results.append(None)\n                errors += 1\n\n        if errors &gt; 0:\n            msg = f\"LLM evaluator failed for {errors} out of {len(list_of_input_data)} inputs.\"\n            warn(msg)\n\n        return {\"results\": results}\n\n    def _prepare_prompt_template(self) -&gt; str:\n        \"\"\"\n        Prepares the prompt template.\n\n        Returns:\n            str: The prompt template.\n        \"\"\"\n        prompt_parts = [\n            \"Instructions:\",\n            self.instructions.strip(),\n        ]\n\n        # Check if outputs are provided\n        if self.outputs:\n            # Prepare expected_output_section\n            expected_output_dict = {outp[\"name\"]: self._get_placeholder_for_type(outp[\"type\"]) for outp in self.outputs}\n            expected_output = json.dumps(expected_output_dict, indent=2)\n            prompt_parts.extend(\n                [\n                    (\n                        \"\\nYour task is to generate a JSON object that contains the following keys \"\n                        \"and their corresponding values.\"\n                    ),\n                    \"The output must be a valid JSON object and should exactly match the specified structure.\",\n                    \"Do not include any additional text, explanations, or markdown.\",\n                    \"Expected JSON format:\",\n                    expected_output,\n                ]\n            )\n\n        if self.examples:\n            # Prepare examples_section with explicit labels\n            examples_parts = []\n            for idx, example in enumerate(self.examples, start=1):\n                example_input = json.dumps(example[\"inputs\"], indent=2)\n                example_output = json.dumps(example[\"outputs\"], indent=2)\n                example_text = f\"Example {idx}:\\n\" f\"Input:\\n{example_input}\\n\" f\"Expected Output:\\n{example_output}\"\n                examples_parts.append(example_text)\n            examples_section = \"\\n\\n\".join(examples_parts)\n            prompt_parts.append(\"\\nHere are some examples:\")\n            prompt_parts.append(examples_section)\n\n        if self.inputs:\n            # Prepare inputs_section with placeholders using {{ variable_name }}\n            inputs_section = (\n                \"{\\n\" + \",\\n\".join([f'  \"{inp[\"name\"]}\": {{{{ {inp[\"name\"]} }}}}' for inp in self.inputs]) + \"\\n}\"\n            )\n            prompt_parts.append(\"\\nNow, process the following input:\")\n            prompt_parts.append(inputs_section)\n\n        prompt_parts.append(\"\\nProvide the output as per the format specified above.\")\n\n        return \"\\n\".join(prompt_parts)\n\n    def _get_placeholder_for_type(self, tp):\n        \"\"\"\n        Generates a placeholder value based on the type.\n\n        Args:\n            tp: The type to generate a placeholder for.\n\n        Returns:\n            An example value corresponding to the type.\n        \"\"\"\n        if tp == str:\n            return \"string_value\"\n        elif tp == int:\n            return 0\n        elif tp == float:\n            return 0.0\n        elif tp == bool:\n            return True\n        elif tp == list:\n            return []\n        elif tp == dict:\n            return {}\n        else:\n            return f\"{tp}\"\n\n    @staticmethod\n    def _get_type_name(tp):\n        \"\"\"\n        Helper function to get the name of a type, including typing types.\n\n        Args:\n            tp: The type to get the name of.\n\n        Returns:\n            str: The name of the type.\n        \"\"\"\n        if hasattr(tp, \"__name__\"):\n            return tp.__name__\n        elif hasattr(tp, \"_name\") and tp._name:\n            args = \", \".join(LLMEvaluator._get_type_name(arg) for arg in tp.__args__)\n            return f\"{tp._name}[{args}]\"\n        else:\n            return str(tp)\n\n    @staticmethod\n    def _validate_input_parameters(expected: dict[str, Any], received: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Validates the input parameters.\n\n        Args:\n            expected (Dict[str, Any]): The expected input parameters with their types.\n            received (Dict[str, Any]): The received input parameters.\n\n        Raises:\n            ValueError: If input parameters are invalid.\n        \"\"\"\n        if expected:\n            for param in expected.keys():\n                if param not in received:\n                    msg = f\"LLM evaluator expected input parameter '{param}' but received {list(received.keys())}.\"\n                    raise ValueError(msg)\n\n            if not all(isinstance(value, list) for value in received.values()):\n                msg = (\n                    \"LLM evaluator expects all input values to be lists but received \"\n                    f\"{[type(value) for value in received.values()]}.\"\n                )\n                raise ValueError(msg)\n\n            inputs = received.values()\n            length = len(next(iter(inputs)))\n            if not all(len(value) == length for value in inputs):\n                msg = (\n                    \"LLM evaluator expects all input lists to have the same length but received input lengths \"\n                    f\"{[len(value) for value in inputs]}.\"\n                )\n                raise ValueError(msg)\n        else:\n            if received:\n                msg = f\"LLM evaluator does not expect any input parameters but received {list(received.keys())}.\"\n                raise ValueError(msg)\n\n    def _parse_and_validate_json_output(self, expected_keys: list[str], content: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Parses the LLM output content as JSON and validates that it contains the expected keys.\n\n        Args:\n            expected_keys (List[str]): Expected output keys.\n            content (str): The received output as a JSON string.\n\n        Returns:\n            Dict[str, Any]: The parsed JSON output if valid, otherwise None.\n\n        Raises:\n            ValueError: If the output is invalid and raise_on_failure is True.\n        \"\"\"\n        try:\n            parsed_output = parse_llm_json_output(content)\n        except json.JSONDecodeError:\n            msg = f\"Response from LLM evaluator is not a valid JSON: {content}.\"\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            return None\n\n        if not all(key in parsed_output for key in expected_keys):\n            msg = (\n                f\"Expected response from LLM evaluator to have keys {expected_keys}, \"\n                f\"but got {list(parsed_output.keys())}.\"\n            )\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            return None\n\n        return parsed_output\n\n    def _cleanup_output_content(self, content: str) -&gt; str:\n        \"\"\"\n        Cleans up the output content by removing unwanted strings.\n\n        Args:\n            content (str): The content to clean up.\n\n        Returns:\n            str: The cleaned content.\n        \"\"\"\n        for omit_string in self.strings_to_omit_from_llm_output:\n            content = content.replace(omit_string, \"\")\n        return content.strip()\n</code></pre>"},{"location":"dynamiq/evaluations/llm_evaluator/#dynamiq.evaluations.llm_evaluator.LLMEvaluator.__init__","title":"<code>__init__(instructions, outputs, examples=None, inputs=None, *, raise_on_failure=True, llm, strings_to_omit_from_llm_output=STRINGS_TO_OMIT_FROM_LLM_EVALUATOR_OUTPUT)</code>","text":"<p>Initializes an instance of LLMEvaluator.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The prompt instructions to use for evaluation.</p> required <code>outputs</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries defining the outputs. Each output dict should have keys \"name\" and \"type\", where \"name\" is the output name and \"type\" is its type.</p> required <code>examples</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Few-shot examples conforming to the expected input and output format as defined in the <code>inputs</code> and <code>outputs</code> parameters. Each example is a dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as dictionaries respectively.</p> <code>None</code> <code>inputs</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A list of dictionaries defining the inputs. Each input dict should have keys \"name\" and \"type\", where \"name\" is the input name and \"type\" is its type. Defaults to None.</p> <code>None</code> <code>raise_on_failure</code> <code>bool</code> <p>If True, the component will raise an exception on an unsuccessful API call.</p> <code>True</code> <code>llm</code> <code>Node</code> <p>The LLM node to use for evaluation.</p> required <code>strings_to_omit_from_llm_output</code> <code>Tuple[str]</code> <p>A tuple of strings to omit from the LLM output.</p> <code>STRINGS_TO_OMIT_FROM_LLM_EVALUATOR_OUTPUT</code> Source code in <code>dynamiq/evaluations/llm_evaluator.py</code> <pre><code>def __init__(\n    self,\n    instructions: str,\n    outputs: list[dict[str, Any]],\n    examples: list[dict[str, Any]] | None = None,\n    inputs: list[dict[str, Any]] | None = None,\n    *,\n    raise_on_failure: bool = True,\n    llm: Node,\n    strings_to_omit_from_llm_output: tuple[str] = STRINGS_TO_OMIT_FROM_LLM_EVALUATOR_OUTPUT,\n):\n    \"\"\"\n    Initializes an instance of LLMEvaluator.\n\n    Args:\n        instructions (str): The prompt instructions to use for evaluation.\n        outputs (List[Dict[str, Any]]): A list of dictionaries defining the outputs.\n            Each output dict should have keys \"name\" and \"type\", where \"name\" is the\n            output name and \"type\" is its type.\n        examples (Optional[List[Dict[str, Any]]]): Few-shot examples conforming to the expected input and\n            output format as defined in the `inputs` and `outputs` parameters. Each example is a\n            dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as\n            dictionaries respectively.\n        inputs (Optional[List[Dict[str, Any]]]): A list of dictionaries defining the inputs.\n            Each input dict should have keys \"name\" and \"type\", where \"name\" is the\n            input name and \"type\" is its type. Defaults to None.\n        raise_on_failure (bool): If True, the component will raise an exception on an\n            unsuccessful API call.\n        llm (Node): The LLM node to use for evaluation.\n        strings_to_omit_from_llm_output (Tuple[str]): A tuple of strings to omit from the LLM output.\n    \"\"\"\n    if inputs is None:\n        inputs = []\n    if examples is None:\n        examples = []\n    self._validate_init_parameters(inputs, outputs, examples)\n    self.raise_on_failure = raise_on_failure\n    self.instructions = instructions\n    self.inputs = inputs\n    self.outputs = outputs\n    self.examples = examples\n    self.api_params = {}\n\n    default_generation_kwargs = {\n        \"response_format\": {\"type\": \"json_object\"},\n        \"seed\": 42,\n    }\n    user_generation_kwargs = self.api_params.get(\"generation_kwargs\", {})\n    merged_generation_kwargs = {\n        **default_generation_kwargs,\n        **user_generation_kwargs,\n    }\n    self.api_params[\"generation_kwargs\"] = merged_generation_kwargs\n\n    # Prepare the prompt with placeholders\n    template = self._prepare_prompt_template()\n    message = Message(role=\"user\", content=template)\n    self.prompt = Prompt(messages=[message])\n\n    self.llm = llm\n    self.strings_to_omit_from_llm_output = strings_to_omit_from_llm_output\n</code></pre>"},{"location":"dynamiq/evaluations/llm_evaluator/#dynamiq.evaluations.llm_evaluator.LLMEvaluator.run","title":"<code>run(**inputs)</code>","text":"<p>Runs the LLM evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>The input values to evaluate. Keys are input names, values are lists.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with a 'results' key containing the evaluation results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input parameters are invalid or LLM execution fails.</p> Source code in <code>dynamiq/evaluations/llm_evaluator.py</code> <pre><code>def run(self, **inputs) -&gt; dict[str, Any]:\n    \"\"\"\n    Runs the LLM evaluator.\n\n    Args:\n        inputs: The input values to evaluate. Keys are input names, values are lists.\n\n    Returns:\n        Dict[str, Any]: A dictionary with a 'results' key containing the evaluation results.\n\n    Raises:\n        ValueError: If input parameters are invalid or LLM execution fails.\n    \"\"\"\n    expected_inputs = {inp[\"name\"]: inp[\"type\"] for inp in self.inputs}\n    self._validate_input_parameters(expected=expected_inputs, received=inputs)\n\n    if self.inputs:\n        input_names = list(inputs.keys())\n        values = list(zip(*inputs.values()))\n        list_of_input_data = [dict(zip(input_names, v)) for v in values]\n    else:\n        # If no inputs are provided, create a list with a single empty dictionary\n        list_of_input_data = [{}]\n\n    results: list[dict[str, Any]] = []\n    errors = 0\n\n    for input_data in list_of_input_data:\n        # Pass the prompt and input_data to LLM\n        try:\n            result = self.llm.execute(input_data=input_data, prompt=self.prompt)\n        except Exception as e:\n            msg = f\"Error while generating response for input {input_data}: {e}\"\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            results.append(None)\n            errors += 1\n            continue\n\n        expected_output_keys = [outp[\"name\"] for outp in self.outputs]\n        content = self._cleanup_output_content(result[\"content\"])\n\n        parsed_result = self._parse_and_validate_json_output(expected_keys=expected_output_keys, content=content)\n        if parsed_result is not None:\n            results.append(parsed_result)\n        else:\n            results.append(None)\n            errors += 1\n\n    if errors &gt; 0:\n        msg = f\"LLM evaluator failed for {errors} out of {len(list_of_input_data)} inputs.\"\n        warn(msg)\n\n    return {\"results\": results}\n</code></pre>"},{"location":"dynamiq/evaluations/python_evaluator/","title":"Python evaluator","text":"<p>thon_evaluator</p>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/","title":"Answer correctness","text":""},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessEvaluator","title":"<code>AnswerCorrectnessEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator for computing answer correctness using candidate statements with explanation of match decisions and weighted scoring.</p> <p>Overview: \u2022 The evaluator splits both the answer and the ground truth answer into a set of     core fact \u201ccandidate statements.\u201d \u2022 It then compares each statement from the answer with the statements from the ground     truth answer to decide if the core fact is present. A \"\u2705\" indicates that the statement     is supported by the ground truth, whereas a \"\u274c\" indicates it is not. \u2022 Similarly, ground truth statements are checked against the answer to see if any are missing. \u2022 Based on these comparisons, the metrics are computed:     - TP (True Positive): Number of answer statements that are correctly supported.     - FP (False Positive): Number of answer statements that are not supported.     - FN (False Negative): Number of ground truth statements missing from the answer.     - Precision = TP / (TP + FP)     - Recall    = TP / (TP + FN)     - F1 Score  = 2 * (Precision * Recall) / (Precision + Recall)</p> <p>The evaluator outputs both the final score and detailed reasoning explaining each step.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class AnswerCorrectnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator for computing answer correctness using candidate statements with\n    explanation of match decisions and weighted scoring.\n\n    Overview:\n    \u2022 The evaluator splits both the answer and the ground truth answer into a set of\n        core fact \u201ccandidate statements.\u201d\n    \u2022 It then compares each statement from the answer with the statements from the ground\n        truth answer to decide if the core fact is present. A \"\u2705\" indicates that the statement\n        is supported by the ground truth, whereas a \"\u274c\" indicates it is not.\n    \u2022 Similarly, ground truth statements are checked against the answer to see if any are missing.\n    \u2022 Based on these comparisons, the metrics are computed:\n        - TP (True Positive): Number of answer statements that are correctly supported.\n        - FP (False Positive): Number of answer statements that are not supported.\n        - FN (False Negative): Number of ground truth statements missing from the answer.\n        - Precision = TP / (TP + FP)\n        - Recall    = TP / (TP + FN)\n        - F1 Score  = 2 * (Precision * Recall) / (Precision + Recall)\n\n    The evaluator outputs both the final score and detailed reasoning explaining each step.\n    \"\"\"\n    name: str = \"AnswerCorrectness\"\n    llm: BaseLLM\n\n    _statement_extractor: LLMEvaluator = PrivateAttr()\n    _statement_classifier: LLMEvaluator = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_evaluators()\n\n    def _initialize_evaluators(self):\n        \"\"\"\n        Initialize the LLMEvaluators.\n        \"\"\"\n        extract_instr = (\n            \"Given a question and an answer, analyze each sentence of the answer and \"\n            \"break it down into one or more fully understandable unique statements. \"\n            \"Replace pronouns with explicit references. \"\n            \"Output the candidate statements as a JSON array of strings using double quotes.\"\n        )\n        self._statement_extractor = LLMEvaluator(\n            instructions=extract_instr.strip(),\n            inputs=[{\"name\": \"questions\", \"type\": list[str]}, {\"name\": \"texts\", \"type\": list[str]}],\n            outputs=[{\"name\": \"statements\", \"type\": list[str]}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"questions\": [\"What is the capital of France?\"],\n                        \"texts\": [\n                            (\n                                \"The capital of France is Paris. It is known for its rich history, art, \"\n                                \"culture, and landmarks such as the Eiffel Tower.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"statements\": [\n                            \"The capital of France is Paris.\",\n                            \"Paris is known for its rich history, art, culture, \"\n                            \"and landmarks such as the Eiffel Tower.\",\n                        ]\n                    },\n                },\n                {\n                    \"inputs\": {\n                        \"questions\": [\"Who developed the theory of relativity?\"],\n                        \"texts\": [\n                            (\n                                \"The theory of relativity was developed by Albert Einstein in the early \"\n                                \"20th century, revolutionizing our understanding of space and time.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"statements\": [\n                            \"The theory of relativity was developed by Albert Einstein in the early 20th century.\",\n                            \"The theory of relativity revolutionized our understanding of space and time.\",\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n        classify_instructions = (\n            \"Given a question, an answer statement, and a reference text, determine if the answer statement \"\n            \"is supported by the reference text. Explain briefly why the statement is or is not supported. \"\n            \"Return a JSON object with keys 'reasoning' (a short explanation) and 'match' (true/false)\"\n        )\n        self._statement_classifier = LLMEvaluator(\n            instructions=classify_instructions.strip(),\n            inputs=[\n                {\"name\": \"question\", \"type\": str},\n                {\"name\": \"answer_statement\", \"type\": str},\n                {\"name\": \"reference_text\", \"type\": str},\n            ],\n            outputs=[{\"name\": \"match\", \"type\": bool}, {\"name\": \"reasoning\", \"type\": str}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"question\": \"What is the capital of France?\",\n                        \"answer_statement\": \"The capital of France is Paris.\",\n                        \"reference_text\": \"Paris is the capital of France.\",\n                    },\n                    \"outputs\": {\n                        \"reasoning\": \"The statement exactly matches the core fact in the reference.\",\n                        \"match\": True,\n                    },\n                },\n                {\n                    \"inputs\": {\n                        \"question\": \"What is the capital of France?\",\n                        \"answer_statement\": \"Paris is known for its rich history.\",\n                        \"reference_text\": \"The capital of France is Paris.\",\n                    },\n                    \"outputs\": {\n                        \"reasoning\": \"The statement includes extra details about history \"\n                        \"that are not present in reference.\",\n                        \"match\": False,\n                    },\n                },\n                {\n                    \"inputs\": {\n                        \"question\": \"Who developed the theory of relativity?\",\n                        \"answer_statement\": \"The theory was developed by Albert Einstein.\",\n                        \"reference_text\": \"The theory of relativity was developed by Albert Einstein.\",\n                    },\n                    \"outputs\": {\n                        \"reasoning\": \"The statement conveys the same core fact as the reference \"\n                        \"despite wording differences.\",\n                        \"match\": True,\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n    def _get_unique_candidates(self, candidates: list[str]) -&gt; list[str]:\n        \"\"\"\n        Return unique candidate statements preserving order.\n        Comparison is done on lowercased, stripped strings.\n        \"\"\"\n        seen = set()\n        unique = []\n        for stmt in candidates:\n            norm = stmt.strip().lower()\n            if norm not in seen:\n                seen.add(norm)\n                unique.append(stmt)\n        return unique\n\n    def extract_statements(self, questions: list[str], texts: list[str]) -&gt; list[list[str]]:\n        \"\"\"\n        Run the extraction evaluator to get candidate statements.\n        \"\"\"\n        results = self._statement_extractor.run(questions=questions, texts=texts)\n        all_stmts = []\n        for res in results[\"results\"]:\n            stmts = res.get(\"statements\", [])\n            if not isinstance(stmts, list):\n                stmts = [stmts]\n            all_stmts.append(self._get_unique_candidates(stmts))\n        return all_stmts\n\n    def classify_statement(self, question: str, answer_stmt: str, ref_text: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        Run the classification evaluator.\n        The ref_text is the string of candidate statements from the ground truth answer.\n        Returns a tuple (match, explanation).\n        \"\"\"\n        data = {\"question\": [question], \"answer_statement\": [answer_stmt], \"reference_text\": [ref_text]}\n        result = self._statement_classifier.run(**data)\n        m = bool(result[\"results\"][0].get(\"match\", False))\n        expl = result[\"results\"][0].get(\"reasoning\", \"\")\n        return m, expl\n\n    def _join_candidates(self, candidates: list[str]) -&gt; str:\n        \"\"\"\n        Join candidate statements into a single string. Append punctuation if needed.\n        \"\"\"\n        joined = \". \".join(candidates)\n        if joined and joined[-1] not in \".!?\":\n            joined += \".\"\n        return joined\n\n    def _evaluate_candidates(self, question: str, candidates: list[str], ref_text: str) -&gt; list[tuple[str, bool, str]]:\n        \"\"\"\n        Classify each candidate statement against the ground truth answer.\n        Returns a list of tuples (statement, match, explanation).\n        \"\"\"\n        outs = []\n        for stmt in candidates:\n            m, expl = self.classify_statement(question, stmt, ref_text)\n            outs.append((stmt, m, expl))\n        return outs\n\n    def _build_reasoning(\n        self,\n        ans_class: list[tuple[str, bool, str]],\n        gt_class: list[tuple[str, bool, str]],\n        tp: int,\n        fp: int,\n        fn: int,\n        precision: float,\n        recall: float,\n        f1: float,\n    ) -&gt; str:\n        \"\"\"\n        Build a detailed reasoning string.\n        This section explains:\n        \u2022 How the answer was split into statements and compared to the ground truth answer.\n        \u2022 What each symbol (\u2705/\u274c) means.\n        \u2022 How TP, FP, and FN are computed.\n        \u2022 How Precision, Recall, and F1 Score are calculated.\n        \"\"\"\n        lines = []\n        lines.extend(\n            [\n                \"Reasoning:\",\n                \"\",\n                \"Overview:\",\n                \"  The evaluator splits the answer and the ground truth answer into core fact statements.\",\n                \"  Each statement from the answer is compared to the ground truth answer to determine if\",\n                \"  the core fact is supported. Similarly, ground truth statements are checked for their\",\n                \"  presence in the answer. '\u2705' indicates support/presence, while '\u274c' indicates lack thereof.\",\n                \"\",\n                \"1. Answer Statements Analysis:\",\n                \"   The answer is split into statements and compared to the ground truth answer.\",\n                \"   '\u2705' means the statement's core fact is supported; '\u274c' means it is not.\",\n                \"\",\n                \"Answer Statements Classification:\",\n            ]\n        )\n\n        for stmt, m, expl in ans_class:\n            mark = \"\u2705\" if m else \"\u274c\"\n            lines.extend([f\" {mark} - {stmt}\", f\"     Explanation: {expl}\", \"\"])\n\n        lines.extend(\n            [\n                f\" -&gt; TP (supported) = {tp}  (correctly supported statements)\",\n                f\" -&gt; FP (not supported) = {fp}  (unsupported statements)\",\n            ]\n        )\n\n        if (tp + fp) &gt; 0:\n            lines.append(f\" -&gt; Precision = TP/(TP+FP) = {precision:.2f}\")\n        else:\n            lines.append(\" -&gt; Precision = 0.00\")\n\n        lines.extend(\n            [\n                \"\",\n                \"2. Ground Truth Statements Analysis:\",\n                \"   The ground truth answer is split into statements and compared to the answer.\",\n                \"   '\u2705' means the statement is present in the answer; '\u274c' means it is missing.\",\n                \"\",\n                \"Ground Truth Statements Classification:\",\n            ]\n        )\n\n        for stmt, m, expl in gt_class:\n            mark = \"\u2705\" if m else \"\u274c\"\n            lines.extend([f\" {mark} - {stmt}\", f\"     Explanation: {expl}\", \"\"])\n\n        lines.extend(\n            [\n                f\" -&gt; TP (present) = {tp}  (ground truth statements found in answer)\",\n                f\" -&gt; FN (missing) = {fn}  (ground truth statements not found)\",\n            ]\n        )\n\n        if (tp + fn) &gt; 0:\n            lines.append(f\" -&gt; Recall = TP/(TP+FN) = {recall:.2f}\")\n        else:\n            lines.append(\" -&gt; Recall = 0.00\")\n\n        lines.extend(\n            [\n                \"\",\n                \"3. Final Metrics:\",\n                \"   F1 Score is the harmonic mean of Precision and Recall:\",\n                \"       F1 Score = 2*(Precision*Recall)/(Precision+Recall)\",\n                f\"       F1 Score = {f1:.2f}\",\n                \"\",\n                f\"Final Score = F1 Score = {round(f1, 2)}\",\n            ]\n        )\n\n        return \"\\n\".join(lines)\n\n    def _evaluate_question(self, question: str, answer_stmts: list[str], gt_stmts: list[str]) -&gt; RunResult:\n        \"\"\"\n        Evaluate one question by comparing candidate statements.\n        \"\"\"\n        unique_ans = self._get_unique_candidates(answer_stmts)\n        unique_gt = self._get_unique_candidates(gt_stmts)\n        gt_text = self._join_candidates(unique_gt)\n        ans_class = self._evaluate_candidates(question, unique_ans, gt_text)\n        ans_text = self._join_candidates(unique_ans)\n        gt_class = self._evaluate_candidates(question, unique_gt, ans_text)\n        tp = sum(1 for _, m, _ in ans_class if m)\n        fp = len(ans_class) - tp\n        fn = sum(1 for _, m, _ in gt_class if not m)\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) &gt; 0 else 0.0\n        reasoning = self._build_reasoning(ans_class, gt_class, tp, fp, fn, precision, recall, f1)\n        return RunResult(score=round(f1, 2), reasoning=reasoning)\n\n    def run_single(self, question: str, answer: str, ground_truth_answer: str, verbose: bool = False) -&gt; RunResult:\n        \"\"\"\n        Evaluate answer correctness for a single sample.\n\n        Steps:\n          1) Extract candidate statements from both the answer and the ground truth answer.\n          2) Compare the candidate statements.\n          3) Compute Precision, Recall, and F1 Score.\n          4) Generate detailed reasoning.\n\n        Args:\n          question (str): The question.\n          answer (str): The answer.\n          ground_truth_answer (str): The ground truth answer.\n          verbose (bool): Flag to output verbose logs.\n\n        Returns:\n          RunResult: The evaluation result with score and reasoning.\n        \"\"\"\n        # Extract candidate statements for answer and ground truth\n        ans_candidates = self.extract_statements([question], [answer])[0]\n        gt_candidates = self.extract_statements([question], [ground_truth_answer])[0]\n        result = self._evaluate_question(question, ans_candidates, gt_candidates)\n        if verbose:\n            logger.debug(f\"Question: {question}\")\n            logger.debug(f\"Answer: {self._join_candidates(ans_candidates)}\")\n            logger.debug(f\"Ground Truth Answer: {self._join_candidates(gt_candidates)}\")\n            logger.debug(result.reasoning)\n        return result\n\n    def run(\n        self, questions: list[str], answers: list[str], ground_truth_answers: list[str], verbose: bool = False\n    ) -&gt; RunOutput:\n        \"\"\"\n        Evaluate answer correctness:\n          1) Extract candidate statements from both the answer and ground truth answer.\n          2) For each question, compare the answer statements to the ground truth answer\n             and vice versa.\n          3) Compute Precision, Recall, and F1 Score.\n          4) Generate detailed and easy-to-understand reasoning that explains the metrics.\n\n        Args:\n          questions (list[str]): List of questions.\n          answers (list[str]): List of answers.\n          ground_truth_answers (list[str]): List of ground truth answers.\n          verbose (bool): Flag for verbose logging.\n\n        Returns:\n          RunOutput: The overall evaluation results.\n        \"\"\"\n        run_input = RunInput(\n            questions=questions, answers=answers, ground_truth_answers=ground_truth_answers, verbose=verbose\n        )\n        out_results = []\n        for i in range(len(run_input.questions)):\n            result = self.run_single(\n                question=run_input.questions[i],\n                answer=run_input.answers[i],\n                ground_truth_answer=run_input.ground_truth_answers[i],\n                verbose=run_input.verbose,\n            )\n            out_results.append(result)\n        return RunOutput(results=out_results)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessEvaluator.classify_statement","title":"<code>classify_statement(question, answer_stmt, ref_text)</code>","text":"<p>Run the classification evaluator. The ref_text is the string of candidate statements from the ground truth answer. Returns a tuple (match, explanation).</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>def classify_statement(self, question: str, answer_stmt: str, ref_text: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Run the classification evaluator.\n    The ref_text is the string of candidate statements from the ground truth answer.\n    Returns a tuple (match, explanation).\n    \"\"\"\n    data = {\"question\": [question], \"answer_statement\": [answer_stmt], \"reference_text\": [ref_text]}\n    result = self._statement_classifier.run(**data)\n    m = bool(result[\"results\"][0].get(\"match\", False))\n    expl = result[\"results\"][0].get(\"reasoning\", \"\")\n    return m, expl\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessEvaluator.extract_statements","title":"<code>extract_statements(questions, texts)</code>","text":"<p>Run the extraction evaluator to get candidate statements.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>def extract_statements(self, questions: list[str], texts: list[str]) -&gt; list[list[str]]:\n    \"\"\"\n    Run the extraction evaluator to get candidate statements.\n    \"\"\"\n    results = self._statement_extractor.run(questions=questions, texts=texts)\n    all_stmts = []\n    for res in results[\"results\"]:\n        stmts = res.get(\"statements\", [])\n        if not isinstance(stmts, list):\n            stmts = [stmts]\n        all_stmts.append(self._get_unique_candidates(stmts))\n    return all_stmts\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessEvaluator.run","title":"<code>run(questions, answers, ground_truth_answers, verbose=False)</code>","text":"Evaluate answer correctness <p>1) Extract candidate statements from both the answer and ground truth answer. 2) For each question, compare the answer statements to the ground truth answer    and vice versa. 3) Compute Precision, Recall, and F1 Score. 4) Generate detailed and easy-to-understand reasoning that explains the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>list[str]</code> <p>List of questions.</p> required <code>answers</code> <code>list[str]</code> <p>List of answers.</p> required <code>ground_truth_answers</code> <code>list[str]</code> <p>List of ground truth answers.</p> required <code>verbose</code> <code>bool</code> <p>Flag for verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RunOutput</code> <code>RunOutput</code> <p>The overall evaluation results.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>def run(\n    self, questions: list[str], answers: list[str], ground_truth_answers: list[str], verbose: bool = False\n) -&gt; RunOutput:\n    \"\"\"\n    Evaluate answer correctness:\n      1) Extract candidate statements from both the answer and ground truth answer.\n      2) For each question, compare the answer statements to the ground truth answer\n         and vice versa.\n      3) Compute Precision, Recall, and F1 Score.\n      4) Generate detailed and easy-to-understand reasoning that explains the metrics.\n\n    Args:\n      questions (list[str]): List of questions.\n      answers (list[str]): List of answers.\n      ground_truth_answers (list[str]): List of ground truth answers.\n      verbose (bool): Flag for verbose logging.\n\n    Returns:\n      RunOutput: The overall evaluation results.\n    \"\"\"\n    run_input = RunInput(\n        questions=questions, answers=answers, ground_truth_answers=ground_truth_answers, verbose=verbose\n    )\n    out_results = []\n    for i in range(len(run_input.questions)):\n        result = self.run_single(\n            question=run_input.questions[i],\n            answer=run_input.answers[i],\n            ground_truth_answer=run_input.ground_truth_answers[i],\n            verbose=run_input.verbose,\n        )\n        out_results.append(result)\n    return RunOutput(results=out_results)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessEvaluator.run_single","title":"<code>run_single(question, answer, ground_truth_answer, verbose=False)</code>","text":"<p>Evaluate answer correctness for a single sample.</p> Steps <p>1) Extract candidate statements from both the answer and the ground truth answer. 2) Compare the candidate statements. 3) Compute Precision, Recall, and F1 Score. 4) Generate detailed reasoning.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question.</p> required <code>answer</code> <code>str</code> <p>The answer.</p> required <code>ground_truth_answer</code> <code>str</code> <p>The ground truth answer.</p> required <code>verbose</code> <code>bool</code> <p>Flag to output verbose logs.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RunResult</code> <code>RunResult</code> <p>The evaluation result with score and reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>def run_single(self, question: str, answer: str, ground_truth_answer: str, verbose: bool = False) -&gt; RunResult:\n    \"\"\"\n    Evaluate answer correctness for a single sample.\n\n    Steps:\n      1) Extract candidate statements from both the answer and the ground truth answer.\n      2) Compare the candidate statements.\n      3) Compute Precision, Recall, and F1 Score.\n      4) Generate detailed reasoning.\n\n    Args:\n      question (str): The question.\n      answer (str): The answer.\n      ground_truth_answer (str): The ground truth answer.\n      verbose (bool): Flag to output verbose logs.\n\n    Returns:\n      RunResult: The evaluation result with score and reasoning.\n    \"\"\"\n    # Extract candidate statements for answer and ground truth\n    ans_candidates = self.extract_statements([question], [answer])[0]\n    gt_candidates = self.extract_statements([question], [ground_truth_answer])[0]\n    result = self._evaluate_question(question, ans_candidates, gt_candidates)\n    if verbose:\n        logger.debug(f\"Question: {question}\")\n        logger.debug(f\"Answer: {self._join_candidates(ans_candidates)}\")\n        logger.debug(f\"Ground Truth Answer: {self._join_candidates(gt_candidates)}\")\n        logger.debug(result.reasoning)\n    return result\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.AnswerCorrectnessRunSingleInput","title":"<code>AnswerCorrectnessRunSingleInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run input model for answer correctness evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class AnswerCorrectnessRunSingleInput(BaseModel):\n    \"\"\"\n    Single-run input model for answer correctness evaluation.\n    \"\"\"\n\n    question: str = Field(description=\"The question to answer\")\n    answer: str = Field(description=\"The answer to the question\")\n    ground_truth_answer: str = Field(description=\"The ground truth answer\")\n    verbose: bool = False\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.ClassifyStatementInput","title":"<code>ClassifyStatementInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for classifying a candidate pair.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class ClassifyStatementInput(BaseModel):\n    \"\"\"\n    Input model for classifying a candidate pair.\n    \"\"\"\n    question: str = Field(description=\"The question for context\")\n    answer_statement: str = Field(description=\"A candidate statement from the answer\")\n    ground_truth_statement: str = Field(\n        description=(\"A string of candidate statements extracted from the ground truth answer\")\n    )\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.ClassifyStatementOutput","title":"<code>ClassifyStatementOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for classifying a candidate pair.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class ClassifyStatementOutput(BaseModel):\n    \"\"\"\n    Output model for classifying a candidate pair.\n    \"\"\"\n    match: bool = Field(\n        description=(\"Verdict: true if the core fact of the statement is supported by the ground truth\")\n    )\n    reasoning: str = Field(description=\"Explanation for why the statement is or is not supported\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.ExtractStatementsInput","title":"<code>ExtractStatementsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for extracting candidate statements.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class ExtractStatementsInput(BaseModel):\n    \"\"\"\n    Input model for extracting candidate statements.\n    \"\"\"\n    question: str = Field(description=\"The question to answer\")\n    answer: str = Field(description=\"The answer to the question\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.ExtractStatementsOutput","title":"<code>ExtractStatementsOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for extracted candidate statements.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class ExtractStatementsOutput(BaseModel):\n    \"\"\"\n    Output model for extracted candidate statements.\n    \"\"\"\n    statements: list[str] = Field(description=\"The generated statements\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.F1Result","title":"<code>F1Result</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for F1 score calculation.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class F1Result(BaseModel):\n    \"\"\"\n    Model for F1 score calculation.\n    \"\"\"\n    precision: float\n    recall: float\n    f1: float\n    tp: int\n    fp: int\n    fn: int\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for running the evaluator.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for running the evaluator.\n    \"\"\"\n    questions: list[str]\n    answers: list[str]\n    ground_truth_answers: list[str]\n    verbose: bool = False\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        if len(self.questions) != len(self.answers) or len(self.questions) != len(self.ground_truth_answers):\n            raise ValueError(\"Questions, answers, and ground truth answers must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for final scores and detailed reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for final scores and detailed reasoning.\n    \"\"\"\n    results: list[RunResult]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/answer_correctness/#dynamiq.evaluations.metrics.answer_correctness.RunResult","title":"<code>RunResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result containing final score and detailed, user-friendly reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/answer_correctness.py</code> <pre><code>class RunResult(BaseModel):\n    \"\"\"\n    Result containing final score and detailed, user-friendly reasoning.\n    \"\"\"\n    score: float\n    reasoning: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/","title":"Bleu score","text":""},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.BleuScoreEvaluator","title":"<code>BleuScoreEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluates BLEU scores using the sacrebleu library.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"BleuScore\".</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>class BleuScoreEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluates BLEU scores using the sacrebleu library.\n\n    Attributes:\n        name (str): Name of the metric. Defaults to \"BleuScore\".\n    \"\"\"\n    name: str = \"BleuScore\"\n\n    # Private attribute to store the sacrebleu corpus_bleu function.\n    _corpus_bleu: Callable = PrivateAttr()\n\n    def __init__(self, **data):\n        \"\"\"\n        Initialize the BleuScoreEvaluator and load the sacrebleu corpus_bleu function.\n        \"\"\"\n        super().__init__(**data)\n        self._initialize_bleu()\n\n    def _initialize_bleu(self) -&gt; None:\n        \"\"\"\n        Initialize the corpus_bleu function from the sacrebleu library.\n\n        Raises:\n            ImportError: If sacrebleu is not installed.\n        \"\"\"\n        from sacrebleu import corpus_bleu\n\n        self._corpus_bleu = corpus_bleu\n\n    def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n        \"\"\"\n        Compute the BLEU score for a single pair of ground truth (reference) and answer.\n\n        The input strings are into sentences. The reference is provided\n        in a format expected by sacrebleu (a list of lists) and the candidate is provided\n        as a list of sentences.\n\n        Args:\n            ground_truth_answer (str): The reference answer.\n            answer (str): The candidate answer.\n\n        Returns:\n            float: The computed BLEU score (as a fraction, e.g., 0.75 for 75%).\n        \"\"\"\n        # Validate inputs using the Pydantic model.\n        single_input = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n\n        # Process text into clean sentences\n        ref_sentences = self._process_text_for_bleu(single_input.ground_truth_answer)\n        resp_sentences = self._process_text_for_bleu(single_input.answer)\n\n        # Format the reference as a list of lists (one per sentence)\n        structured_refs = [[sent] for sent in ref_sentences]\n        hypothesis = resp_sentences\n\n        # Compute the BLEU score; sacrebleu returns a percentage, so we scale it by 1/100.\n        bleu_result = self._corpus_bleu(hypothesis, structured_refs).score / 100.0\n        score = round(float(bleu_result), 2)\n\n        output = RunSingleOutput(score=score)\n        return output.score\n\n    def _process_text_for_bleu(self, text: str) -&gt; list[str]:\n        \"\"\"\n        Process text into clean sentences for BLEU score computation.\n\n        Args:\n            text (str): The text to process.\n\n        Returns:\n            list[str]: List of cleaned sentences.\n        \"\"\"\n        # First split the text into sentences\n        raw_sentences = self._split_text_into_sentences(text)\n\n        # Then clean each sentence by removing punctuation\n        cleaned_sentences = [self._clean_sentence(sentence) for sentence in raw_sentences]\n\n        # Filter out empty or very short sentences (likely fragments)\n        return [s for s in cleaned_sentences if s and len(s.split()) &gt; 1]\n\n    def _split_text_into_sentences(self, text: str) -&gt; list[str]:\n        \"\"\"\n        Split text into sentences based on punctuation boundaries.\n\n        Args:\n            text (str): The text to split.\n\n        Returns:\n            list[str]: List of sentences.\n        \"\"\"\n        # Split on ., !, or ? followed by whitespace or end of string\n        sentences = re.split(r\"(?&lt;=[.!?])\\s+|(?&lt;=[.!?])$\", text)\n        return [s.strip() for s in sentences if s.strip()]\n\n    def _clean_sentence(self, sentence: str) -&gt; str:\n        \"\"\"\n        Clean a sentence by removing all punctuation.\n\n        Args:\n            sentence (str): The sentence to clean.\n\n        Returns:\n            str: Cleaned sentence with punctuation removed.\n        \"\"\"\n        # Remove leading/trailing whitespace\n        cleaned = sentence.strip()\n\n        # Remove all punctuation\n        cleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)\n\n        return cleaned.strip()\n\n    def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n        \"\"\"\n        Compute BLEU scores for each ground_truth_answer/answer pair in batch.\n\n        Args:\n            ground_truth_answers (list[str]): List of reference answers.\n            answers (list[str]): List of candidate answers.\n\n        Returns:\n            list[float]: List of computed BLEU scores.\n        \"\"\"\n        # Validate batch input.\n        input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n        scores = []\n\n        for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n            score = self.run_single(ground_truth_answer=gt, answer=ans)\n            scores.append(score)\n\n        output_data = RunOutput(scores=scores)\n        return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.BleuScoreEvaluator.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the BleuScoreEvaluator and load the sacrebleu corpus_bleu function.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"\n    Initialize the BleuScoreEvaluator and load the sacrebleu corpus_bleu function.\n    \"\"\"\n    super().__init__(**data)\n    self._initialize_bleu()\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.BleuScoreEvaluator.run","title":"<code>run(ground_truth_answers, answers)</code>","text":"<p>Compute BLEU scores for each ground_truth_answer/answer pair in batch.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_answers</code> <code>list[str]</code> <p>List of reference answers.</p> required <code>answers</code> <code>list[str]</code> <p>List of candidate answers.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: List of computed BLEU scores.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n    \"\"\"\n    Compute BLEU scores for each ground_truth_answer/answer pair in batch.\n\n    Args:\n        ground_truth_answers (list[str]): List of reference answers.\n        answers (list[str]): List of candidate answers.\n\n    Returns:\n        list[float]: List of computed BLEU scores.\n    \"\"\"\n    # Validate batch input.\n    input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n    scores = []\n\n    for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n        score = self.run_single(ground_truth_answer=gt, answer=ans)\n        scores.append(score)\n\n    output_data = RunOutput(scores=scores)\n    return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.BleuScoreEvaluator.run_single","title":"<code>run_single(ground_truth_answer, answer)</code>","text":"<p>Compute the BLEU score for a single pair of ground truth (reference) and answer.</p> <p>The input strings are into sentences. The reference is provided in a format expected by sacrebleu (a list of lists) and the candidate is provided as a list of sentences.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_answer</code> <code>str</code> <p>The reference answer.</p> required <code>answer</code> <code>str</code> <p>The candidate answer.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed BLEU score (as a fraction, e.g., 0.75 for 75%).</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n    \"\"\"\n    Compute the BLEU score for a single pair of ground truth (reference) and answer.\n\n    The input strings are into sentences. The reference is provided\n    in a format expected by sacrebleu (a list of lists) and the candidate is provided\n    as a list of sentences.\n\n    Args:\n        ground_truth_answer (str): The reference answer.\n        answer (str): The candidate answer.\n\n    Returns:\n        float: The computed BLEU score (as a fraction, e.g., 0.75 for 75%).\n    \"\"\"\n    # Validate inputs using the Pydantic model.\n    single_input = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n\n    # Process text into clean sentences\n    ref_sentences = self._process_text_for_bleu(single_input.ground_truth_answer)\n    resp_sentences = self._process_text_for_bleu(single_input.answer)\n\n    # Format the reference as a list of lists (one per sentence)\n    structured_refs = [[sent] for sent in ref_sentences]\n    hypothesis = resp_sentences\n\n    # Compute the BLEU score; sacrebleu returns a percentage, so we scale it by 1/100.\n    bleu_result = self._corpus_bleu(hypothesis, structured_refs).score / 100.0\n    score = round(float(bleu_result), 2)\n\n    output = RunSingleOutput(score=score)\n    return output.score\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for batch BLEU score evaluation.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answers</code> <code>list[str]</code> <p>List of reference strings.</p> <code>answers</code> <code>list[str]</code> <p>List of candidate response strings.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for batch BLEU score evaluation.\n\n    Attributes:\n        ground_truth_answers (list[str]): List of reference strings.\n        answers (list[str]): List of candidate response strings.\n    \"\"\"\n    ground_truth_answers: list[str]\n    answers: list[str]\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self) -&gt; \"RunInput\":\n        \"\"\"\n        Validate that the number of ground truth answers matches the number of answers.\n\n        Raises:\n            ValueError: If the lengths differ.\n\n        Returns:\n            RunInput: The validated instance.\n        \"\"\"\n        if len(self.ground_truth_answers) != len(self.answers):\n            raise ValueError(\"The number of ground truth answers must equal the number of answers.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.RunInput.check_equal_length","title":"<code>check_equal_length()</code>","text":"<p>Validate that the number of ground truth answers matches the number of answers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths differ.</p> <p>Returns:</p> Name Type Description <code>RunInput</code> <code>RunInput</code> <p>The validated instance.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_equal_length(self) -&gt; \"RunInput\":\n    \"\"\"\n    Validate that the number of ground truth answers matches the number of answers.\n\n    Raises:\n        ValueError: If the lengths differ.\n\n    Returns:\n        RunInput: The validated instance.\n    \"\"\"\n    if len(self.ground_truth_answers) != len(self.answers):\n        raise ValueError(\"The number of ground truth answers must equal the number of answers.\")\n    return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for batch BLEU score evaluation.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>list[float]</code> <p>List of computed BLEU scores.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for batch BLEU score evaluation.\n\n    Attributes:\n        scores (list[float]): List of computed BLEU scores.\n    \"\"\"\n\n    scores: list[float]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.RunSingleInput","title":"<code>RunSingleInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run input model for BLEU score evaluation.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answer</code> <code>str</code> <p>The reference answer.</p> <code>answer</code> <code>str</code> <p>The candidate answer.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>class RunSingleInput(BaseModel):\n    \"\"\"\n    Single-run input model for BLEU score evaluation.\n\n    Attributes:\n        ground_truth_answer (str): The reference answer.\n        answer (str): The candidate answer.\n    \"\"\"\n\n    ground_truth_answer: str\n    answer: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/bleu_score/#dynamiq.evaluations.metrics.bleu_score.RunSingleOutput","title":"<code>RunSingleOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run output model for BLEU score evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed BLEU score.</p> Source code in <code>dynamiq/evaluations/metrics/bleu_score.py</code> <pre><code>class RunSingleOutput(BaseModel):\n    \"\"\"\n    Single-run output model for BLEU score evaluation.\n\n    Attributes:\n        score (float): The computed BLEU score.\n    \"\"\"\n\n    score: float\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/","title":"Context precision","text":""},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionEvaluator","title":"<code>ContextPrecisionEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator class for context precision metric.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLLM</code> <p>The language model to use for evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>class ContextPrecisionEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator class for context precision metric.\n\n    Attributes:\n        llm (BaseLLM): The language model to use for evaluation.\n    \"\"\"\n    name: str = \"ContextPrecision\"\n    llm: BaseLLM\n\n    # Private attribute (not a Pydantic model field)\n    _context_precision_evaluator: LLMEvaluator = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_evaluator()\n\n    def _initialize_evaluator(self):\n        context_precision_instructions = (\n            'Given a \"Question\", \"Answer\", and \"Context\", verify if the Context was '\n            \"useful in arriving at the given Answer.\\n\"\n            '- Provide a \"verdict\": 1 if useful, 0 if not.\\n'\n            '- Provide a brief \"reason\" for the verdict.\\n'\n            '- Output the result as a JSON object with keys \"verdict\" and \"reason\".\\n'\n            \"- Ensure that your response is valid JSON, using double quotes for all \"\n            \"strings.\"\n        )\n\n        self._context_precision_evaluator = LLMEvaluator(\n            instructions=context_precision_instructions.strip(),\n            inputs=[\n                {\"name\": \"question\", \"type\": list[str]},\n                {\"name\": \"answer\", \"type\": list[str]},\n                {\"name\": \"context\", \"type\": list[str]},\n            ],\n            outputs=[\n                {\"name\": \"verdict\", \"type\": int},\n                {\"name\": \"reason\", \"type\": str},\n            ],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"question\": [\"What can you tell me about Albert Einstein?\"],\n                        \"answer\": [\n                            (\n                                \"Albert Einstein, born on 14 March 1879, was a German-born theoretical \"\n                                \"physicist, widely held to be one of the greatest and most influential \"\n                                \"scientists of all time. He received the 1921 Nobel Prize in Physics \"\n                                \"for his services to theoretical physics.\"\n                            )\n                        ],\n                        \"context\": [\n                            (\n                                \"Albert Einstein (14 March 1879 \u2013 18 April 1955) was a German-born \"\n                                \"theoretical physicist, widely held to be one of the greatest and most \"\n                                \"influential scientists of all time. Best known for developing the theory \"\n                                \"of relativity, he also made important contributions to quantum mechanics, \"\n                                \"and was thus a central figure in modern physics. His mass\u2013energy equivalence \"\n                                \"formula E = mc2 has been called 'the world's most famous equation'.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"reason\": (\n                            \"The context provides detailed info about Einstein that is reflected in the \"\n                            \"answer (e.g. his contributions and Nobel Prize).\"\n                        ),\n                        \"verdict\": 1,\n                    },\n                },\n                {\n                    \"inputs\": {\n                        \"question\": [\"Who won the 2020 ICC World Cup?\"],\n                        \"answer\": [\"England\"],\n                        \"context\": [\n                            (\n                                \"The 2022 ICC Men's T20 World Cup was postponed from 2020 due to COVID-19. \"\n                                \"England won the tournament, defeating Pakistan in the final.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"reason\": (\n                            \"The context explains the tournament details and mentions England's victory, \"\n                            \"which is directly relevant.\"\n                        ),\n                        \"verdict\": 1,\n                    },\n                },\n                {\n                    \"inputs\": {\n                        \"question\": [\"What is the tallest mountain in the world?\"],\n                        \"answer\": [\"Mount Everest.\"],\n                        \"context\": [\n                            (\n                                \"The Andes is the longest continental mountain range, but it does not \"\n                                \"contain Mount Everest.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"reason\": (\"The context discusses the Andes, which is unrelated to Mount Everest.\"),\n                        \"verdict\": 0,\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n    @staticmethod\n    def calculate_average_precision(verdicts: list[int]) -&gt; float:\n        \"\"\"\n        Calculate the average precision based on verdicts.\n\n        Args:\n            verdicts (list[int]): List of verdicts (1 for useful, 0 for not useful).\n\n        Returns:\n            float: The average precision score.\n        \"\"\"\n        numerator = 0.0\n        cumulative_hits = 0\n        total_relevant = sum(verdicts)\n        if total_relevant == 0:\n            return 0.0\n        for index, verdict in enumerate(verdicts):\n            if verdict == 1:\n                cumulative_hits += 1\n                precision_at_index = cumulative_hits / (index + 1)\n                numerator += precision_at_index\n        average_precision = numerator / total_relevant\n        return round(float(average_precision), 2)\n\n    def _build_reasoning(\n        self,\n        question: str,\n        answer: str,\n        contexts: list[str],\n        verdicts: list[int],\n        verdict_details: list[str],\n        average_precision: float,\n    ) -&gt; str:\n        \"\"\"\n        Build a detailed reasoning string for context precision evaluation.\n\n        Explains:\n        \u2022 Each context is evaluated with a verdict (emojis used: \u2705 for supported, \u274c for not).\n        \u2022 The corresponding explanation for each verdict.\n        \u2022 How the average precision is calculated.\n\n        Args:\n            question (str): The evaluation question.\n            answer (str): The answer text.\n            contexts (list[str]): List of contexts evaluated.\n            verdicts (list[int]): List of verdicts (1 or 0) for each context.\n            verdict_details (list[str]): List of explanations for each verdict.\n            average_precision (float): The average precision score.\n\n        Returns:\n            str: Detailed reasoning.\n        \"\"\"\n        reasoning_strings = [\"Reasoning:\", \"\", f\"Question: {question}\", f\"Answer: {answer}\", \"\", \"Context Evaluations:\"]\n        for context_text, verdict_value, detail in zip(contexts, verdicts, verdict_details):\n            verdict_mark = \"\u2705\" if verdict_value == 1 else \"\u274c\"\n            reasoning_strings.extend(\n                [\n                    f\" - Context: {context_text}\",\n                    f\"   Verdict: {verdict_mark} (value: {verdict_value})\",\n                    f\"   Explanation: {detail}\",\n                    \"\",\n                ]\n            )\n        reasoning_strings.append(f\"Average Precision Score = {average_precision:.2f}\")\n        return \"\\n\".join(reasoning_strings)\n\n    def run_single(\n        self, question: str, answer: str, contexts: list[str], verbose: bool = False\n    ) -&gt; ContextPrecisionRunResult:\n        \"\"\"\n        Evaluate the context precision for a single sample.\n\n        Args:\n            question (str): The question.\n            answer (str): The corresponding answer.\n            contexts (list[str]): A list of contexts for this question.\n            verbose (bool): Flag to enable verbose logging.\n\n        Returns:\n            ContextPrecisionRunResult: Contains the computed average precision score and detailed reasoning.\n        \"\"\"\n        verdicts = []\n        verdict_details = []\n        for context in contexts:\n            evaluation_result = self._context_precision_evaluator.run(\n                question=[question], answer=[answer], context=[context]\n            )\n            if (\"results\" not in evaluation_result) or (not evaluation_result[\"results\"]):\n                default_verdict = 0\n                verdicts.append(default_verdict)\n                verdict_details.append(\"No results returned from evaluator.\")\n                if verbose:\n                    logger.debug(f\"Missing results for context: {context}. Defaulting verdict to {default_verdict}.\")\n                continue\n\n            result_item = evaluation_result[\"results\"][0]\n            verdict_raw = result_item.get(\"verdict\", \"0\")\n            try:\n                verdict = int(verdict_raw) if not isinstance(verdict_raw, str) else int(verdict_raw.strip())\n            except (ValueError, AttributeError):\n                verdict = 0\n            verdicts.append(verdict)\n            verdict_details.append(result_item.get(\"reason\", \"No reasoning provided\"))\n\n            if verbose:\n                logger.debug(f\"Question: {question}\")\n                logger.debug(f\"Answer: {answer}\")\n                logger.debug(f\"Context: {context}\")\n                logger.debug(f\"Verdict: {verdict}\")\n                logger.debug(f\"Reason: {result_item.get('reason', 'No reasoning provided')}\")\n                logger.debug(\"-\" * 50)\n\n        average_precision = self.calculate_average_precision(verdicts)\n        reasoning_text = self._build_reasoning(question, answer, contexts, verdicts, verdict_details, average_precision)\n        if verbose:\n            logger.debug(f\"Average Precision Score: {average_precision}\")\n            logger.debug(\"=\" * 50)\n        return ContextPrecisionRunResult(score=average_precision, reasoning=reasoning_text)\n\n    def run(\n        self,\n        questions: list[str],\n        answers: list[str],\n        contexts_list: list[list[str]] | list[str],\n        verbose: bool = False,\n    ) -&gt; ContextPrecisionOutput:\n        \"\"\"\n        Evaluate the context precision for each question.\n\n        Args:\n            questions (list[str]): List of questions.\n            answers (list[str]): List of corresponding answers.\n            contexts_list (list[list[str]] | list[str]): Either a list of contexts per question\n                (list[list[str]]) or a single list of context strings (list[str]).\n            verbose (bool): Flag to enable verbose logging (for internal logging only).\n\n        Returns:\n            ContextPrecisionOutput: Contains a list of context precision scores and reasoning.\n        \"\"\"\n        run_input = ContextPrecisionInput(\n            questions=questions,\n            answers=answers,\n            contexts_list=contexts_list,\n            verbose=verbose,\n        )\n        results_output = []\n        for index in range(len(run_input.questions)):\n            question = run_input.questions[index]\n            answer = run_input.answers[index]\n            contexts = run_input.contexts_list[index]\n            result_single = self.run_single(\n                question=question, answer=answer, contexts=contexts, verbose=run_input.verbose\n            )\n            results_output.append(result_single)\n        return ContextPrecisionOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionEvaluator.calculate_average_precision","title":"<code>calculate_average_precision(verdicts)</code>  <code>staticmethod</code>","text":"<p>Calculate the average precision based on verdicts.</p> <p>Parameters:</p> Name Type Description Default <code>verdicts</code> <code>list[int]</code> <p>List of verdicts (1 for useful, 0 for not useful).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average precision score.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>@staticmethod\ndef calculate_average_precision(verdicts: list[int]) -&gt; float:\n    \"\"\"\n    Calculate the average precision based on verdicts.\n\n    Args:\n        verdicts (list[int]): List of verdicts (1 for useful, 0 for not useful).\n\n    Returns:\n        float: The average precision score.\n    \"\"\"\n    numerator = 0.0\n    cumulative_hits = 0\n    total_relevant = sum(verdicts)\n    if total_relevant == 0:\n        return 0.0\n    for index, verdict in enumerate(verdicts):\n        if verdict == 1:\n            cumulative_hits += 1\n            precision_at_index = cumulative_hits / (index + 1)\n            numerator += precision_at_index\n    average_precision = numerator / total_relevant\n    return round(float(average_precision), 2)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionEvaluator.run","title":"<code>run(questions, answers, contexts_list, verbose=False)</code>","text":"<p>Evaluate the context precision for each question.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>list[str]</code> <p>List of questions.</p> required <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> required <code>contexts_list</code> <code>list[list[str]] | list[str]</code> <p>Either a list of contexts per question (list[list[str]]) or a single list of context strings (list[str]).</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging (for internal logging only).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ContextPrecisionOutput</code> <code>ContextPrecisionOutput</code> <p>Contains a list of context precision scores and reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>def run(\n    self,\n    questions: list[str],\n    answers: list[str],\n    contexts_list: list[list[str]] | list[str],\n    verbose: bool = False,\n) -&gt; ContextPrecisionOutput:\n    \"\"\"\n    Evaluate the context precision for each question.\n\n    Args:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n        contexts_list (list[list[str]] | list[str]): Either a list of contexts per question\n            (list[list[str]]) or a single list of context strings (list[str]).\n        verbose (bool): Flag to enable verbose logging (for internal logging only).\n\n    Returns:\n        ContextPrecisionOutput: Contains a list of context precision scores and reasoning.\n    \"\"\"\n    run_input = ContextPrecisionInput(\n        questions=questions,\n        answers=answers,\n        contexts_list=contexts_list,\n        verbose=verbose,\n    )\n    results_output = []\n    for index in range(len(run_input.questions)):\n        question = run_input.questions[index]\n        answer = run_input.answers[index]\n        contexts = run_input.contexts_list[index]\n        result_single = self.run_single(\n            question=question, answer=answer, contexts=contexts, verbose=run_input.verbose\n        )\n        results_output.append(result_single)\n    return ContextPrecisionOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionEvaluator.run_single","title":"<code>run_single(question, answer, contexts, verbose=False)</code>","text":"<p>Evaluate the context precision for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question.</p> required <code>answer</code> <code>str</code> <p>The corresponding answer.</p> required <code>contexts</code> <code>list[str]</code> <p>A list of contexts for this question.</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ContextPrecisionRunResult</code> <code>ContextPrecisionRunResult</code> <p>Contains the computed average precision score and detailed reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>def run_single(\n    self, question: str, answer: str, contexts: list[str], verbose: bool = False\n) -&gt; ContextPrecisionRunResult:\n    \"\"\"\n    Evaluate the context precision for a single sample.\n\n    Args:\n        question (str): The question.\n        answer (str): The corresponding answer.\n        contexts (list[str]): A list of contexts for this question.\n        verbose (bool): Flag to enable verbose logging.\n\n    Returns:\n        ContextPrecisionRunResult: Contains the computed average precision score and detailed reasoning.\n    \"\"\"\n    verdicts = []\n    verdict_details = []\n    for context in contexts:\n        evaluation_result = self._context_precision_evaluator.run(\n            question=[question], answer=[answer], context=[context]\n        )\n        if (\"results\" not in evaluation_result) or (not evaluation_result[\"results\"]):\n            default_verdict = 0\n            verdicts.append(default_verdict)\n            verdict_details.append(\"No results returned from evaluator.\")\n            if verbose:\n                logger.debug(f\"Missing results for context: {context}. Defaulting verdict to {default_verdict}.\")\n            continue\n\n        result_item = evaluation_result[\"results\"][0]\n        verdict_raw = result_item.get(\"verdict\", \"0\")\n        try:\n            verdict = int(verdict_raw) if not isinstance(verdict_raw, str) else int(verdict_raw.strip())\n        except (ValueError, AttributeError):\n            verdict = 0\n        verdicts.append(verdict)\n        verdict_details.append(result_item.get(\"reason\", \"No reasoning provided\"))\n\n        if verbose:\n            logger.debug(f\"Question: {question}\")\n            logger.debug(f\"Answer: {answer}\")\n            logger.debug(f\"Context: {context}\")\n            logger.debug(f\"Verdict: {verdict}\")\n            logger.debug(f\"Reason: {result_item.get('reason', 'No reasoning provided')}\")\n            logger.debug(\"-\" * 50)\n\n    average_precision = self.calculate_average_precision(verdicts)\n    reasoning_text = self._build_reasoning(question, answer, contexts, verdicts, verdict_details, average_precision)\n    if verbose:\n        logger.debug(f\"Average Precision Score: {average_precision}\")\n        logger.debug(\"=\" * 50)\n    return ContextPrecisionRunResult(score=average_precision, reasoning=reasoning_text)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionInput","title":"<code>ContextPrecisionInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for context precision evaluation.</p> <p>Attributes:</p> Name Type Description <code>questions</code> <code>list[str]</code> <p>List of questions.</p> <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> <code>contexts_list</code> <code>list[list[str]] | list[str]</code> <p>Either a list of lists of strings or a list of strings; it will be normalized to a list of lists.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>class ContextPrecisionInput(BaseModel):\n    \"\"\"\n    Input model for context precision evaluation.\n\n    Attributes:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n        contexts_list (list[list[str]] | list[str]): Either a list of lists of\n            strings or a list of strings; it will be normalized to a list of lists.\n        verbose (bool): Flag to enable verbose logging.\n    \"\"\"\n    questions: list[str]\n    answers: list[str]\n    contexts_list: list[list[str]] | list[str]\n    verbose: bool = False\n\n    @field_validator(\"contexts_list\", mode=\"before\")\n    def normalize_contexts_list(cls, value):\n        # If the user provides a list[str], wrap it into [list[str]].\n        # If the user provides a list[list[str]], leave as-is.\n        # Otherwise, raise an error.\n        if isinstance(value, list):\n            if all(isinstance(item, str) for item in value):\n                return [value]  # e.g. [\"foo\", \"bar\"] becomes [[\"foo\", \"bar\"]]\n            if all(isinstance(item, list) and all(isinstance(x, str) for x in item) for item in value):\n                return value\n        raise ValueError(\"contexts_list must be either a list of strings or a list of list of strings.\")\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        # Now self.contexts_list will always be a list of lists of strings.\n        if not (len(self.questions) == len(self.answers) == len(self.contexts_list)):\n            raise ValueError(\"questions, answers, and contexts_list must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionOutput","title":"<code>ContextPrecisionOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for context precision evaluation.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[ContextPrecisionRunResult]</code> <p>List of evaluation results.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>class ContextPrecisionOutput(BaseModel):\n    \"\"\"\n    Output model for context precision evaluation.\n\n    Attributes:\n        results (list[ContextPrecisionRunResult]): List of evaluation results.\n    \"\"\"\n    results: list[ContextPrecisionRunResult]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.ContextPrecisionRunResult","title":"<code>ContextPrecisionRunResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result model for the context precision evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed context precision score.</p> <code>reasoning</code> <code>str</code> <p>Detailed reasoning explaining how the score was derived.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>class ContextPrecisionRunResult(BaseModel):\n    \"\"\"\n    Result model for the context precision evaluation.\n\n    Attributes:\n        score (float): The computed context precision score.\n        reasoning (str): Detailed reasoning explaining how the score was derived.\n    \"\"\"\n    score: float\n    reasoning: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_precision/#dynamiq.evaluations.metrics.context_precision.VerdictResult","title":"<code>VerdictResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for the verdict result from the evaluator.</p> <p>Attributes:</p> Name Type Description <code>verdict</code> <code>int</code> <p>1 if the context was useful, 0 otherwise.</p> <code>reason</code> <code>str</code> <p>Reason for the verdict.</p> Source code in <code>dynamiq/evaluations/metrics/context_precision.py</code> <pre><code>class VerdictResult(BaseModel):\n    \"\"\"\n    Model for the verdict result from the evaluator.\n\n    Attributes:\n        verdict (int): 1 if the context was useful, 0 otherwise.\n        reason (str): Reason for the verdict.\n    \"\"\"\n    verdict: int\n    reason: str\n\n    @field_validator(\"verdict\")\n    @classmethod\n    def validate_verdict(cls, value):\n        if value not in (0, 1):\n            raise ValueError(\"Verdict must be either 0 or 1.\")\n        return value\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/","title":"Context recall","text":""},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ClassificationItem","title":"<code>ClassificationItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for individual classification result.</p> <p>Attributes:</p> Name Type Description <code>statement</code> <code>str</code> <p>The statement being classified.</p> <code>reason</code> <code>str</code> <p>Reason for the classification.</p> <code>attributed</code> <code>int</code> <p>1 if attributed to context, 0 otherwise.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>class ClassificationItem(BaseModel):\n    \"\"\"\n    Model for individual classification result.\n\n    Attributes:\n        statement (str): The statement being classified.\n        reason (str): Reason for the classification.\n        attributed (int): 1 if attributed to context, 0 otherwise.\n    \"\"\"\n    statement: str\n    reason: str\n    attributed: int\n\n    @field_validator(\"attributed\")\n    @classmethod\n    def validate_attributed(cls, value):\n        if value not in (0, 1):\n            raise ValueError(\"Attributed must be either 0 or 1.\")\n        return value\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallEvaluator","title":"<code>ContextRecallEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator class for context recall metric.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLLM</code> <p>The language model to use for evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>class ContextRecallEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator class for context recall metric.\n\n    Attributes:\n        llm (BaseLLM): The language model to use for evaluation.\n    \"\"\"\n    name: str = \"ContextRecall\"\n    llm: BaseLLM\n\n    _classification_evaluator: LLMEvaluator = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_evaluator()\n\n    def _initialize_evaluator(self):\n        class_instructions = (\n            'Given a \"Question\", \"Context\", and \"Answer\", analyze each sentence in the '\n            \"Answer and classify if the sentence can be attributed to the given Context \"\n            \"or not.\\n\"\n            \"- Use '1' (Yes) or '0' (No) for classification.\\n\"\n            '- Provide a brief \"reason\" for each classification.\\n'\n            '- Output as a JSON object with key \"classifications\", where the value is a list '\n            'of dictionaries with keys \"statement\", \"reason\", and \"attributed\".\\n'\n            \"- Ensure your response is valid JSON, using double quotes for all strings.\"\n        )\n\n        self._classification_evaluator = LLMEvaluator(\n            instructions=class_instructions.strip(),\n            inputs=[\n                {\"name\": \"question\", \"type\": list[str]},\n                {\"name\": \"context\", \"type\": list[str]},\n                {\"name\": \"answer\", \"type\": list[str]},\n            ],\n            outputs=[\n                {\"name\": \"classifications\", \"type\": list[dict[str, Any]]},\n            ],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"question\": [\"What can you tell me about Albert Einstein?\"],\n                        \"context\": [\n                            (\n                                \"Albert Einstein (14 March 1879 - 18 April 1955) was a German-born \"\n                                \"theoretical physicist, widely held to be one of the greatest and most \"\n                                \"influential scientists of all time. Best known for developing the theory \"\n                                \"of relativity, he also made important contributions to quantum mechanics, \"\n                                \"and was thus a pivotal figure in modern physics.\"\n                            )\n                        ],\n                        \"answer\": [\n                            (\n                                \"Albert Einstein, born on 14 March 1879, was a German-born theoretical \"\n                                \"physicist, widely held to be one of the greatest and most influential \"\n                                \"scientists of all time. He received the 1921 Nobel Prize in Physics for his \"\n                                \"services to theoretical physics. He published 4 papers in 1905. Einstein \"\n                                \"moved to Switzerland in 1895.\"\n                            )\n                        ],\n                    },\n                    \"outputs\": {\n                        \"classifications\": [\n                            {\n                                \"statement\": (\n                                    \"Albert Einstein, born on 14 March 1879, was a German-born theoretical \"\n                                    \"physicist, widely held to be one of the greatest and most influential \"\n                                    \"scientists of all time.\"\n                                ),\n                                \"reason\": \"The birth date and status as a theoretical physicist are mentioned.\",\n                                \"attributed\": 1,\n                            },\n                            {\n                                \"statement\": (\n                                    \"He received the 1921 Nobel Prize in Physics for his services to theoretical \"\n                                    \"physics.\"\n                                ),\n                                \"reason\": \"The sentence is present in the context.\",\n                                \"attributed\": 1,\n                            },\n                            {\n                                \"statement\": \"He published 4 papers in 1905.\",\n                                \"reason\": \"There is no mention of his papers in the context.\",\n                                \"attributed\": 0,\n                            },\n                            {\n                                \"statement\": \"Einstein moved to Switzerland in 1895.\",\n                                \"reason\": \"There is no supporting evidence for this in the context.\",\n                                \"attributed\": 0,\n                            },\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n    def _build_reasoning(self, classifications: list[ClassificationItem], score: float) -&gt; str:\n        \"\"\"\n        Build a detailed reasoning string for context recall evaluation.\n\n        Explains:\n        \u2022 Each sentence in the answer is classified (using emojis: \u2705 for attributed, \u274c for not).\n        \u2022 A corresponding explanation is provided for each classification.\n        \u2022 The final context recall score is computed as the ratio of attributable sentences.\n\n        Args:\n            classifications (list[ClassificationItem]): List of classification results.\n            score (float): The computed recall score.\n\n        Returns:\n            str: Detailed reasoning.\n        \"\"\"\n        lines = []\n        lines.extend([\"Reasoning:\", \"\", \"Classifications:\"])\n        for item in classifications:\n            mark = \"\u2705\" if item.attributed == 1 else \"\u274c\"\n            lines.extend(\n                [\n                    f\" - Statement: {item.statement}\",\n                    f\"   Verdict: {mark} (value: {item.attributed})\",\n                    f\"   Explanation: {item.reason}\",\n                    \"\",\n                ]\n            )\n        lines.append(f\"Context Recall Score = {score:.2f}\")\n        return \"\\n\".join(lines)\n\n    def run_single(self, question: str, context: str, answer: str, verbose: bool = False) -&gt; ContextRecallRunResult:\n        \"\"\"\n        Evaluate the context recall for a single sample.\n\n        Args:\n            question (str): The question.\n            context (str): The context (already normalized as a single string).\n            answer (str): The answer.\n            verbose (bool): Flag to enable verbose logging.\n\n        Returns:\n            ContextRecallRunResult: The computed context recall score and detailed reasoning.\n        \"\"\"\n        result = self._classification_evaluator.run(\n            question=[question],\n            context=[context],\n            answer=[answer],\n        )\n\n        classifications = []\n        if \"results\" not in result or not result[\"results\"]:\n            if verbose:\n                logger.debug(f\"No results returned for question: {question}, context: {context}.\")\n        else:\n            first_result = result[\"results\"][0]\n            if \"classifications\" not in first_result or not first_result[\"classifications\"]:\n                if verbose:\n                    logger.debug(f\"No classifications returned for question: {question}, context: {context}.\")\n            else:\n                classifications_raw = first_result[\"classifications\"]\n                for item in classifications_raw:\n                    classification_item = ClassificationItem(\n                        statement=item[\"statement\"],\n                        reason=item[\"reason\"],\n                        attributed=int(item[\"attributed\"]),\n                    )\n                    classifications.append(classification_item)\n\n        attributed_list = [item.attributed for item in classifications]\n        num_sentences = len(attributed_list)\n        num_attributed = sum(attributed_list)\n        score = num_attributed / num_sentences if num_sentences &gt; 0 else 0.0\n        score = round(float(score), 2)\n\n        reasoning_str = self._build_reasoning(classifications, score)\n\n        if verbose:\n            logger.debug(f\"Question: {question}\")\n            logger.debug(f\"Answer: {answer}\")\n            logger.debug(f\"Context: {context}\")\n            logger.debug(\"Classifications:\")\n            logger.debug(json.dumps([item.dict() for item in classifications], indent=2))\n            logger.debug(f\"Context Recall Score: {score}\")\n            logger.debug(\"-\" * 50)\n\n        return ContextRecallRunResult(score=score, reasoning=reasoning_str)\n\n    def run(\n        self,\n        questions: list[str],\n        contexts: list[str] | list[list[str]],\n        answers: list[str],\n        verbose: bool = False,\n    ) -&gt; ContextRecallOutput:\n        \"\"\"\n        Evaluate the context recall for each question.\n\n        Args:\n            questions (list[str]): List of questions.\n            contexts (list[str] or list[list[str]]): Either a single list of context strings or a list\n                of context strings (one per question).\n            answers (list[str]): List of answers.\n            verbose (bool): Flag to enable verbose logging (for internal logging only).\n\n        Returns:\n            ContextRecallOutput: Contains a list of context recall run results.\n        \"\"\"\n        run_input = ContextRecallInput(\n            questions=questions,\n            contexts=contexts,\n            answers=answers,\n            verbose=verbose,\n        )\n        results_output = []\n        for i in range(len(run_input.questions)):\n            question = run_input.questions[i]\n            context = run_input.contexts[i]\n            answer = run_input.answers[i]\n            result_single = self.run_single(\n                question=question, context=context, answer=answer, verbose=run_input.verbose\n            )\n            results_output.append(result_single)\n        return ContextRecallOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallEvaluator.run","title":"<code>run(questions, contexts, answers, verbose=False)</code>","text":"<p>Evaluate the context recall for each question.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>list[str]</code> <p>List of questions.</p> required <code>contexts</code> <code>list[str] or list[list[str]]</code> <p>Either a single list of context strings or a list of context strings (one per question).</p> required <code>answers</code> <code>list[str]</code> <p>List of answers.</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging (for internal logging only).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ContextRecallOutput</code> <code>ContextRecallOutput</code> <p>Contains a list of context recall run results.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>def run(\n    self,\n    questions: list[str],\n    contexts: list[str] | list[list[str]],\n    answers: list[str],\n    verbose: bool = False,\n) -&gt; ContextRecallOutput:\n    \"\"\"\n    Evaluate the context recall for each question.\n\n    Args:\n        questions (list[str]): List of questions.\n        contexts (list[str] or list[list[str]]): Either a single list of context strings or a list\n            of context strings (one per question).\n        answers (list[str]): List of answers.\n        verbose (bool): Flag to enable verbose logging (for internal logging only).\n\n    Returns:\n        ContextRecallOutput: Contains a list of context recall run results.\n    \"\"\"\n    run_input = ContextRecallInput(\n        questions=questions,\n        contexts=contexts,\n        answers=answers,\n        verbose=verbose,\n    )\n    results_output = []\n    for i in range(len(run_input.questions)):\n        question = run_input.questions[i]\n        context = run_input.contexts[i]\n        answer = run_input.answers[i]\n        result_single = self.run_single(\n            question=question, context=context, answer=answer, verbose=run_input.verbose\n        )\n        results_output.append(result_single)\n    return ContextRecallOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallEvaluator.run_single","title":"<code>run_single(question, context, answer, verbose=False)</code>","text":"<p>Evaluate the context recall for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question.</p> required <code>context</code> <code>str</code> <p>The context (already normalized as a single string).</p> required <code>answer</code> <code>str</code> <p>The answer.</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ContextRecallRunResult</code> <code>ContextRecallRunResult</code> <p>The computed context recall score and detailed reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>def run_single(self, question: str, context: str, answer: str, verbose: bool = False) -&gt; ContextRecallRunResult:\n    \"\"\"\n    Evaluate the context recall for a single sample.\n\n    Args:\n        question (str): The question.\n        context (str): The context (already normalized as a single string).\n        answer (str): The answer.\n        verbose (bool): Flag to enable verbose logging.\n\n    Returns:\n        ContextRecallRunResult: The computed context recall score and detailed reasoning.\n    \"\"\"\n    result = self._classification_evaluator.run(\n        question=[question],\n        context=[context],\n        answer=[answer],\n    )\n\n    classifications = []\n    if \"results\" not in result or not result[\"results\"]:\n        if verbose:\n            logger.debug(f\"No results returned for question: {question}, context: {context}.\")\n    else:\n        first_result = result[\"results\"][0]\n        if \"classifications\" not in first_result or not first_result[\"classifications\"]:\n            if verbose:\n                logger.debug(f\"No classifications returned for question: {question}, context: {context}.\")\n        else:\n            classifications_raw = first_result[\"classifications\"]\n            for item in classifications_raw:\n                classification_item = ClassificationItem(\n                    statement=item[\"statement\"],\n                    reason=item[\"reason\"],\n                    attributed=int(item[\"attributed\"]),\n                )\n                classifications.append(classification_item)\n\n    attributed_list = [item.attributed for item in classifications]\n    num_sentences = len(attributed_list)\n    num_attributed = sum(attributed_list)\n    score = num_attributed / num_sentences if num_sentences &gt; 0 else 0.0\n    score = round(float(score), 2)\n\n    reasoning_str = self._build_reasoning(classifications, score)\n\n    if verbose:\n        logger.debug(f\"Question: {question}\")\n        logger.debug(f\"Answer: {answer}\")\n        logger.debug(f\"Context: {context}\")\n        logger.debug(\"Classifications:\")\n        logger.debug(json.dumps([item.dict() for item in classifications], indent=2))\n        logger.debug(f\"Context Recall Score: {score}\")\n        logger.debug(\"-\" * 50)\n\n    return ContextRecallRunResult(score=score, reasoning=reasoning_str)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallInput","title":"<code>ContextRecallInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for context recall evaluation.</p> <p>Attributes:</p> Name Type Description <code>questions</code> <code>list[str]</code> <p>List of questions.</p> <code>contexts</code> <code>list[str]</code> <p>List of corresponding contexts (can also accept list[list[str]]).</p> <code>answers</code> <code>list[str]</code> <p>List of answers.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>class ContextRecallInput(BaseModel):\n    \"\"\"\n    Input model for context recall evaluation.\n\n    Attributes:\n        questions (list[str]): List of questions.\n        contexts (list[str]): List of corresponding contexts (can also accept list[list[str]]).\n        answers (list[str]): List of answers.\n        verbose (bool): Flag to enable verbose logging.\n    \"\"\"\n    questions: list[str]\n    contexts: list[str] | list[list[str]]\n    answers: list[str]\n    verbose: bool = False\n\n    @field_validator(\"contexts\", mode=\"before\")\n    def unify_contexts(cls, value):\n        \"\"\"\n        If we receive a list of lists of strings, join each sublist into a single string.\n        Otherwise, if it's already list[str], do nothing.\n        \"\"\"\n        if not isinstance(value, list):\n            raise ValueError(\"contexts must be either a list[str] or a list[list[str]].\")\n        if all(isinstance(item, list) and all(isinstance(x, str) for x in item) for item in value):\n            return [\" \".join(sublist) for sublist in value]\n        if all(isinstance(item, str) for item in value):\n            return value\n        raise ValueError(\"contexts must be either a list[str] or a list[list[str]].\")\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        if not (len(self.questions) == len(self.contexts) == len(self.answers)):\n            raise ValueError(\"Questions, contexts, and answers must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallInput.unify_contexts","title":"<code>unify_contexts(value)</code>","text":"<p>If we receive a list of lists of strings, join each sublist into a single string. Otherwise, if it's already list[str], do nothing.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>@field_validator(\"contexts\", mode=\"before\")\ndef unify_contexts(cls, value):\n    \"\"\"\n    If we receive a list of lists of strings, join each sublist into a single string.\n    Otherwise, if it's already list[str], do nothing.\n    \"\"\"\n    if not isinstance(value, list):\n        raise ValueError(\"contexts must be either a list[str] or a list[list[str]].\")\n    if all(isinstance(item, list) and all(isinstance(x, str) for x in item) for item in value):\n        return [\" \".join(sublist) for sublist in value]\n    if all(isinstance(item, str) for item in value):\n        return value\n    raise ValueError(\"contexts must be either a list[str] or a list[list[str]].\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallOutput","title":"<code>ContextRecallOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for context recall evaluation.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[ContextRecallRunResult]</code> <p>Detailed run results.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>class ContextRecallOutput(BaseModel):\n    \"\"\"\n    Output model for context recall evaluation.\n\n    Attributes:\n        results (list[ContextRecallRunResult]): Detailed run results.\n    \"\"\"\n    results: list[ContextRecallRunResult]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/context_recall/#dynamiq.evaluations.metrics.context_recall.ContextRecallRunResult","title":"<code>ContextRecallRunResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result model for the context recall evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed context recall score.</p> <code>reasoning</code> <code>str</code> <p>Detailed reasoning explaining how the score was derived.</p> Source code in <code>dynamiq/evaluations/metrics/context_recall.py</code> <pre><code>class ContextRecallRunResult(BaseModel):\n    \"\"\"\n    Result model for the context recall evaluation.\n\n    Attributes:\n        score (float): The computed context recall score.\n        reasoning (str): Detailed reasoning explaining how the score was derived.\n    \"\"\"\n    score: float\n    reasoning: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/","title":"Factual correctness","text":""},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.DecomposeClaimsInput","title":"<code>DecomposeClaimsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for decomposing texts into claims.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>list[str]</code> <p>List of texts to decompose.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class DecomposeClaimsInput(BaseModel):\n    \"\"\"\n    Input model for decomposing texts into claims.\n\n    Attributes:\n        texts (list[str]): List of texts to decompose.\n    \"\"\"\n    texts: list[str]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.DecomposeClaimsOutput","title":"<code>DecomposeClaimsOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for claim decomposition.</p> <p>Attributes:</p> Name Type Description <code>claims_list</code> <code>list[list[str]]</code> <p>List of lists of claims.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class DecomposeClaimsOutput(BaseModel):\n    \"\"\"\n    Output model for claim decomposition.\n\n    Attributes:\n        claims_list (list[list[str]]): List of lists of claims.\n    \"\"\"\n    claims_list: list[list[str]]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator","title":"<code>FactualCorrectnessEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator class for factual correctness metric.</p> Pipeline <p>1) Claim Decomposition: The answer and context are decomposed into standalone,    verifiable claims. 2) Claim Verification: The answer claims are verified against the context to compute    precision (TP vs. FP). Optionally, context claims are verified against answer for    recall (FN). 3) Score Computation: Depending on mode, evaluate precision, recall, or F-beta score. 4) Detailed Reasoning: Generates a user-friendly explanation describing each step,    including claim lists, TP, FP, FN, and metric computations with emojis.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLLM</code> <p>The language model to use for evaluation.</p> <code>mode</code> <code>str</code> <p>Evaluation mode ('precision', 'recall', or 'f1').</p> <code>beta</code> <code>float</code> <p>Beta value for F-beta score.</p> <code>atomicity</code> <code>str</code> <p>Level of atomicity ('low' or 'high').</p> <code>coverage</code> <code>str</code> <p>Level of coverage ('low' or 'high').</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class FactualCorrectnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator class for factual correctness metric.\n\n    Pipeline:\n      1) Claim Decomposition: The answer and context are decomposed into standalone,\n         verifiable claims.\n      2) Claim Verification: The answer claims are verified against the context to compute\n         precision (TP vs. FP). Optionally, context claims are verified against answer for\n         recall (FN).\n      3) Score Computation: Depending on mode, evaluate precision, recall, or F-beta score.\n      4) Detailed Reasoning: Generates a user-friendly explanation describing each step,\n         including claim lists, TP, FP, FN, and metric computations with emojis.\n\n    Attributes:\n        llm (BaseLLM): The language model to use for evaluation.\n        mode (str): Evaluation mode ('precision', 'recall', or 'f1').\n        beta (float): Beta value for F-beta score.\n        atomicity (str): Level of atomicity ('low' or 'high').\n        coverage (str): Level of coverage ('low' or 'high').\n    \"\"\"\n    name: str = \"FactualCorrectness\"\n    llm: BaseLLM\n    mode: str = \"f1\"\n    beta: float = 1.0\n    atomicity: str = \"low\"\n    coverage: str = \"low\"\n\n    _claim_decomposer: LLMEvaluator = PrivateAttr()\n    _nli_evaluator: LLMEvaluator = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_evaluators()\n\n    def _initialize_evaluators(self):\n        # Claim Decomposition Evaluator\n        decomposition_instructions = (\n            \"Decompose the 'Input Text' into standalone factual claims.\\n\"\n            \"- Each claim should be a simple, verifiable statement.\\n\"\n            \"- Do not include personal opinions or interpretations.\\n\"\n            \"- Output a JSON object with key 'claims' containing the list of claims.\\n\"\n            \"- Ensure your response is valid JSON, using double quotes for all strings.\"\n        )\n        self._claim_decomposer = LLMEvaluator(\n            instructions=decomposition_instructions.strip(),\n            inputs=[{\"name\": \"input_text\", \"type\": list[str]}],\n            outputs=[{\"name\": \"claims\", \"type\": list[str]}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"input_text\": [\n                            \"Albert Einstein was a German theoretical physicist. \"\n                            \"He developed the theory of relativity and contributed \"\n                            \"to quantum mechanics.\"\n                        ]\n                    },\n                    \"outputs\": {\n                        \"claims\": [\n                            \"Albert Einstein was a German theoretical physicist.\",\n                            \"Albert Einstein developed the theory of relativity.\",\n                            \"Albert Einstein contributed to quantum mechanics.\",\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n        # NLI Evaluator\n        nli_instructions = (\n            \"For each 'Claim', determine if it is supported by the 'Premise'.\\n\"\n            \"- Return 'verdict': 1 for supported, 0 for unsupported claims.\\n\"\n            \"- Provide a brief 'reason' for each verdict.\\n\"\n            \"- Output a JSON object with key 'results' containing a list of verdicts.\\n\"\n            \"- Each item should have keys 'claim', 'verdict', and 'reason'.\\n\"\n            \"- Ensure your response is valid JSON, using double quotes for all strings.\"\n        )\n        self._nli_evaluator = LLMEvaluator(\n            instructions=nli_instructions.strip(),\n            inputs=[\n                {\"name\": \"premise\", \"type\": list[str]},\n                {\"name\": \"claims\", \"type\": list[list[str]]},\n            ],\n            outputs=[{\"name\": \"results\", \"type\": list[dict[str, Any]]}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"premise\": [\n                            \"Albert Einstein was a German-born theoretical physicist. \"\n                            \"He developed the theory of relativity.\"\n                        ],\n                        \"claims\": [\n                            [\n                                \"Albert Einstein was a German theoretical physicist.\",\n                                \"Albert Einstein developed the theory of relativity.\",\n                                \"Albert Einstein contributed to quantum mechanics.\",\n                            ]\n                        ],\n                    },\n                    \"outputs\": {\n                        \"results\": [\n                            {\n                                \"claim\": \"Albert Einstein was a German theoretical physicist.\",\n                                \"verdict\": 1,\n                                \"reason\": \"The premise states he was a German-born theoretical physicist.\",\n                            },\n                            {\n                                \"claim\": \"Albert Einstein developed the theory of relativity.\",\n                                \"verdict\": 1,\n                                \"reason\": \"This is explicitly mentioned in the premise.\",\n                            },\n                            {\n                                \"claim\": \"Albert Einstein contributed to quantum mechanics.\",\n                                \"verdict\": 0,\n                                \"reason\": \"The premise does not mention contributions to quantum mechanics.\",\n                            },\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n    def decompose_claims(self, texts: list[str]) -&gt; list[list[str]]:\n        \"\"\"\n        Decompose each text into claims.\n\n        Args:\n            texts (list[str]): List of texts to decompose.\n\n        Returns:\n            list[list[str]]: List of lists of claims.\n        \"\"\"\n        input_data = DecomposeClaimsInput(texts=texts)\n        results = self._claim_decomposer.run(input_text=input_data.texts)\n        claims_list = []\n        for result in results[\"results\"]:\n            claims = result.get(\"claims\")\n            if isinstance(claims, list):\n                claims_list.append(claims)\n            else:\n                claims_list.append([claims])\n        output_data = DecomposeClaimsOutput(claims_list=claims_list)\n        return output_data.claims_list\n\n    def verify_claims(self, premises: list[str], claims_list: list[list[str]]) -&gt; list[list[int]]:\n        \"\"\"\n        Verify the claims against the premises.\n\n        Args:\n            premises (list[str]): List of premises.\n            claims_list (list[list[str]]): List of lists of claims.\n\n        Returns:\n            list[list[int]]: List of lists of verdicts.\n        \"\"\"\n        input_data = VerifyClaimsInput(premises=premises, claims_list=claims_list)\n        results = self._nli_evaluator.run(\n            premise=input_data.premises,\n            claims=input_data.claims_list,\n        )\n        verdicts_list = []\n        for result in results[\"results\"]:\n            verdicts_raw = result[\"results\"]\n            verdicts = []\n            for item in verdicts_raw:\n                verdict = int(item[\"verdict\"])\n                verdicts.append(verdict)\n            verdicts_list.append(verdicts)\n        output_data = VerifyClaimsOutput(verdicts_list=verdicts_list)\n        return output_data.verdicts_list\n\n    def fbeta_score(self, tp: int, fp: int, fn: int, beta: float) -&gt; float:\n        \"\"\"\n        Calculate the F-beta score.\n\n        Args:\n            tp (int): True positives.\n            fp (int): False positives.\n            fn (int): False negatives.\n            beta (float): Beta value.\n\n        Returns:\n            float: F-beta score.\n        \"\"\"\n        precision = tp / (tp + fp + 1e-8) if (tp + fp) &gt; 0 else 0.0\n        recall = tp / (tp + fn + 1e-8) if (tp + fn) &gt; 0 else 0.0\n        if (precision + recall) == 0:\n            return 0.0\n        score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-8)\n        return score\n\n    def _build_reasoning(\n        self,\n        answer_claims: list[str],\n        context_claims: list[str],\n        answer_verdicts: list[int],\n        context_verdicts: list[int],\n        tp: int,\n        fp: int,\n        fn: int,\n        score: float,\n        mode: str,\n        beta: float,\n    ) -&gt; str:\n        \"\"\"\n        Build a detailed reasoning string for factual correctness evaluation.\n\n        Explains:\n        \u2022 How the answer and context were decomposed into claims.\n        \u2022 How claim verification produced verdicts (TP, FP, FN) with emojis.\n        \u2022 The calculation of the final score depending on the mode.\n\n        Args:\n            answer_claims (list[str]): Claims from the answer.\n            context_claims (list[str]): Claims from the context.\n            answer_verdicts (list[int]): Verdicts from verifying context claims against answer.\n            context_verdicts (list[int]): Verdicts from verifying answer claims against context.\n            tp (int): True positive count.\n            fp (int): False positive count.\n            fn (int): False negative count.\n            score (float): Computed score.\n            mode (str): Evaluation mode.\n            beta (float): Beta value.\n\n        Returns:\n            str: Detailed reasoning.\n        \"\"\"\n        lines = []\n        lines.extend([\"Reasoning:\", \"\", \"1. Claim Decomposition:\", \"   Answer was decomposed into claims:\"])\n        for claim in answer_claims:\n            lines.append(f\"     - {claim}\")\n        lines.extend([\"   Context was decomposed into claims:\"])\n        for claim in context_claims:\n            lines.append(f\"     - {claim}\")\n        lines.extend([\"\", \"2. Claim Verification:\"])\n        # Map verdicts to emojis: 1 -&gt; \u2705, 0 -&gt; \u274c\n        mapped_context = [(\"\u2705\" if v == 1 else \"\u274c\") for v in context_verdicts]\n        lines.extend(\n            [\n                \"   Verification of answer claims against context yields:\",\n                f\"     Verdicts: {mapped_context}   (\u2705 = supported, \u274c = unsupported)\",\n                f\"     TP (supported): {tp}\",\n                f\"     FP (unsupported): {fp}\",\n            ]\n        )\n        if mode != \"precision\":\n            mapped_answer = [(\"\u2705\" if v == 1 else \"\u274c\") for v in answer_verdicts]\n            lines.extend(\n                [\n                    \"\",\n                    \"   Verification of context claims against answer yields:\",\n                    f\"     Verdicts: {mapped_answer}\",\n                    f\"     FN (not supported): {fn}\",\n                ]\n            )\n        lines.append(\"\")\n        if mode == \"precision\":\n            precision = tp / (tp + fp + 1e-8)\n            lines.extend([f\"Precision = TP/(TP+FP) = {precision:.2f}\"])\n        elif mode == \"recall\":\n            recall = tp / (tp + fn + 1e-8)\n            lines.extend([f\"Recall = TP/(TP+FN) = {recall:.2f}\"])\n        else:\n            precision = tp / (tp + fp + 1e-8)\n            recall = tp / (tp + fn + 1e-8) if (tp + fn) &gt; 0 else 0.0\n            lines.extend(\n                [\n                    f\"Precision = TP/(TP+FP) = {precision:.2f}\",\n                    f\"Recall = TP/(TP+FN) = {recall:.2f}\",\n                    f\"F-beta Score (beta={beta:.2f}) = {score:.2f}\",\n                ]\n            )\n        lines.extend([\"\", f\"Final Score = {score:.2f}\"])\n        return \"\\n\".join(lines)\n\n    def run_single(\n        self, answer: str, context: str, mode: str | None = None, beta: float | None = None, verbose: bool = False\n    ) -&gt; FactualCorrectnessRunResult:\n        \"\"\"\n        Evaluate the factual correctness for a single sample.\n\n        Args:\n            answer (str): The response text.\n            context (str): The reference text.\n            mode (str | None): Evaluation mode ('precision', 'recall', or 'f1').\n            beta (float | None): Beta value for F-beta score.\n            verbose (bool): Flag for verbose logging.\n\n        Returns:\n            FactualCorrectnessRunResult: The computed factual correctness score and detailed reasoning.\n        \"\"\"\n        evaluation_mode = mode or self.mode\n        beta_value = beta or self.beta\n\n        answer_claims_list = self.decompose_claims([answer])\n        if not answer_claims_list or answer_claims_list[0] is None:\n            if verbose:\n                logger.debug(f\"No claims decomposed for answer: {answer}. Using empty list.\")\n            answer_claims = []\n        else:\n            answer_claims = answer_claims_list[0]\n\n        context_claims_list = self.decompose_claims([context])\n        if not context_claims_list or context_claims_list[0] is None:\n            if verbose:\n                logger.debug(f\"No claims decomposed for context: {context}. Using empty list.\")\n            context_claims = []\n        else:\n            context_claims = context_claims_list[0]\n\n        # Verify answer claims against context (precision part).\n        context_verdicts_list = self.verify_claims(premises=[context], claims_list=[answer_claims])\n        if not context_verdicts_list or context_verdicts_list[0] is None:\n            if verbose:\n                logger.debug(f\"No verdicts returned when verifying answer claims against context for answer: {answer}\")\n            context_verdicts = []\n        else:\n            context_verdicts = context_verdicts_list[0]\n        tp = sum(context_verdicts)\n        fp = len(context_verdicts) - tp\n\n        # For recall or F1, verify context claims against answer.\n        if evaluation_mode not in (\"precision\", \"PRECISION\"):\n            answer_verdicts_list = self.verify_claims(premises=[answer], claims_list=[context_claims])\n            if not answer_verdicts_list or answer_verdicts_list[0] is None:\n                if verbose:\n                    logger.debug(\n                        f\"No verdicts returned when verifying context claims against answer for answer: {answer}\"\n                    )\n                answer_verdicts = []\n                fn = 0\n            else:\n                answer_verdicts = answer_verdicts_list[0]\n                fn = sum(1 - v for v in answer_verdicts)\n        else:\n            answer_verdicts = []\n            fn = 0\n\n        if evaluation_mode == \"precision\":\n            computed_score = tp / (tp + fp + 1e-8)\n        elif evaluation_mode == \"recall\":\n            computed_score = tp / (tp + fn + 1e-8)\n        else:\n            computed_score = self.fbeta_score(tp, fp, fn, beta_value)\n\n        reasoning_text = self._build_reasoning(\n            answer_claims=answer_claims,\n            context_claims=context_claims,\n            answer_verdicts=answer_verdicts,\n            context_verdicts=context_verdicts,\n            tp=tp,\n            fp=fp,\n            fn=fn,\n            score=computed_score,\n            mode=evaluation_mode,\n            beta=beta_value,\n        )\n\n        if verbose:\n            logger.debug(f\"Answer: {answer}\")\n            logger.debug(f\"Context: {context}\")\n            logger.debug(f\"Answer Claims: {answer_claims}\")\n            logger.debug(f\"Context Claims: {context_claims}\")\n            logger.debug(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n            logger.debug(f\"Score: {computed_score}\")\n            logger.debug(reasoning_text)\n            logger.debug(\"-\" * 50)\n\n        return FactualCorrectnessRunResult(score=round(computed_score, 2), reasoning=reasoning_text)\n\n    def run(\n        self,\n        answers: list[str],\n        contexts: list[str] | list[list[str]],\n        mode: str | None = None,\n        beta: float | None = None,\n        verbose: bool = False,\n    ) -&gt; RunOutput:\n        \"\"\"\n        Evaluate the factual correctness of answers against contexts.\n\n        Pipeline:\n        1) Decompose both answer and context into claims.\n        2) Verify answer claims against context to compute precision.\n        3) If mode is recall or F1, verify context claims against answer\n           to compute false negatives.\n        4) Compute the final score based on the selected mode.\n        5) Generate detailed reasoning regarding the claim decomposition,\n           verification, and final metric calculations with emojis.\n\n        Args:\n            answers (list[str]): List of response texts.\n            contexts (list[str] | list[list[str]]): List of context texts.\n            mode (str | None): Evaluation mode ('precision', 'recall', or 'f1').\n            beta (float | None): Beta value for F-beta score.\n            verbose (bool): Flag for verbose logging.\n\n        Returns:\n            RunOutput: Contains a list of FactualCorrectnessRunResult.\n        \"\"\"\n        run_input = RunInput(answers=answers, contexts=contexts, mode=mode, beta=beta, verbose=verbose)\n        evaluation_mode = run_input.mode or self.mode\n        beta_value = run_input.beta or self.beta\n\n        results_output = []\n        for index in range(len(run_input.answers)):\n            answer_sample = run_input.answers[index]\n            context_sample = run_input.contexts[index]\n            result_single = self.run_single(\n                answer=answer_sample,\n                context=context_sample,\n                mode=evaluation_mode,\n                beta=beta_value,\n                verbose=run_input.verbose,\n            )\n            results_output.append(result_single)\n        return RunOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator.decompose_claims","title":"<code>decompose_claims(texts)</code>","text":"<p>Decompose each text into claims.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of texts to decompose.</p> required <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: List of lists of claims.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>def decompose_claims(self, texts: list[str]) -&gt; list[list[str]]:\n    \"\"\"\n    Decompose each text into claims.\n\n    Args:\n        texts (list[str]): List of texts to decompose.\n\n    Returns:\n        list[list[str]]: List of lists of claims.\n    \"\"\"\n    input_data = DecomposeClaimsInput(texts=texts)\n    results = self._claim_decomposer.run(input_text=input_data.texts)\n    claims_list = []\n    for result in results[\"results\"]:\n        claims = result.get(\"claims\")\n        if isinstance(claims, list):\n            claims_list.append(claims)\n        else:\n            claims_list.append([claims])\n    output_data = DecomposeClaimsOutput(claims_list=claims_list)\n    return output_data.claims_list\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator.fbeta_score","title":"<code>fbeta_score(tp, fp, fn, beta)</code>","text":"<p>Calculate the F-beta score.</p> <p>Parameters:</p> Name Type Description Default <code>tp</code> <code>int</code> <p>True positives.</p> required <code>fp</code> <code>int</code> <p>False positives.</p> required <code>fn</code> <code>int</code> <p>False negatives.</p> required <code>beta</code> <code>float</code> <p>Beta value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>F-beta score.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>def fbeta_score(self, tp: int, fp: int, fn: int, beta: float) -&gt; float:\n    \"\"\"\n    Calculate the F-beta score.\n\n    Args:\n        tp (int): True positives.\n        fp (int): False positives.\n        fn (int): False negatives.\n        beta (float): Beta value.\n\n    Returns:\n        float: F-beta score.\n    \"\"\"\n    precision = tp / (tp + fp + 1e-8) if (tp + fp) &gt; 0 else 0.0\n    recall = tp / (tp + fn + 1e-8) if (tp + fn) &gt; 0 else 0.0\n    if (precision + recall) == 0:\n        return 0.0\n    score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-8)\n    return score\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator.run","title":"<code>run(answers, contexts, mode=None, beta=None, verbose=False)</code>","text":"<p>Evaluate the factual correctness of answers against contexts.</p> <p>Pipeline: 1) Decompose both answer and context into claims. 2) Verify answer claims against context to compute precision. 3) If mode is recall or F1, verify context claims against answer    to compute false negatives. 4) Compute the final score based on the selected mode. 5) Generate detailed reasoning regarding the claim decomposition,    verification, and final metric calculations with emojis.</p> <p>Parameters:</p> Name Type Description Default <code>answers</code> <code>list[str]</code> <p>List of response texts.</p> required <code>contexts</code> <code>list[str] | list[list[str]]</code> <p>List of context texts.</p> required <code>mode</code> <code>str | None</code> <p>Evaluation mode ('precision', 'recall', or 'f1').</p> <code>None</code> <code>beta</code> <code>float | None</code> <p>Beta value for F-beta score.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Flag for verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RunOutput</code> <code>RunOutput</code> <p>Contains a list of FactualCorrectnessRunResult.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>def run(\n    self,\n    answers: list[str],\n    contexts: list[str] | list[list[str]],\n    mode: str | None = None,\n    beta: float | None = None,\n    verbose: bool = False,\n) -&gt; RunOutput:\n    \"\"\"\n    Evaluate the factual correctness of answers against contexts.\n\n    Pipeline:\n    1) Decompose both answer and context into claims.\n    2) Verify answer claims against context to compute precision.\n    3) If mode is recall or F1, verify context claims against answer\n       to compute false negatives.\n    4) Compute the final score based on the selected mode.\n    5) Generate detailed reasoning regarding the claim decomposition,\n       verification, and final metric calculations with emojis.\n\n    Args:\n        answers (list[str]): List of response texts.\n        contexts (list[str] | list[list[str]]): List of context texts.\n        mode (str | None): Evaluation mode ('precision', 'recall', or 'f1').\n        beta (float | None): Beta value for F-beta score.\n        verbose (bool): Flag for verbose logging.\n\n    Returns:\n        RunOutput: Contains a list of FactualCorrectnessRunResult.\n    \"\"\"\n    run_input = RunInput(answers=answers, contexts=contexts, mode=mode, beta=beta, verbose=verbose)\n    evaluation_mode = run_input.mode or self.mode\n    beta_value = run_input.beta or self.beta\n\n    results_output = []\n    for index in range(len(run_input.answers)):\n        answer_sample = run_input.answers[index]\n        context_sample = run_input.contexts[index]\n        result_single = self.run_single(\n            answer=answer_sample,\n            context=context_sample,\n            mode=evaluation_mode,\n            beta=beta_value,\n            verbose=run_input.verbose,\n        )\n        results_output.append(result_single)\n    return RunOutput(results=results_output)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator.run_single","title":"<code>run_single(answer, context, mode=None, beta=None, verbose=False)</code>","text":"<p>Evaluate the factual correctness for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>str</code> <p>The response text.</p> required <code>context</code> <code>str</code> <p>The reference text.</p> required <code>mode</code> <code>str | None</code> <p>Evaluation mode ('precision', 'recall', or 'f1').</p> <code>None</code> <code>beta</code> <code>float | None</code> <p>Beta value for F-beta score.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Flag for verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FactualCorrectnessRunResult</code> <code>FactualCorrectnessRunResult</code> <p>The computed factual correctness score and detailed reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>def run_single(\n    self, answer: str, context: str, mode: str | None = None, beta: float | None = None, verbose: bool = False\n) -&gt; FactualCorrectnessRunResult:\n    \"\"\"\n    Evaluate the factual correctness for a single sample.\n\n    Args:\n        answer (str): The response text.\n        context (str): The reference text.\n        mode (str | None): Evaluation mode ('precision', 'recall', or 'f1').\n        beta (float | None): Beta value for F-beta score.\n        verbose (bool): Flag for verbose logging.\n\n    Returns:\n        FactualCorrectnessRunResult: The computed factual correctness score and detailed reasoning.\n    \"\"\"\n    evaluation_mode = mode or self.mode\n    beta_value = beta or self.beta\n\n    answer_claims_list = self.decompose_claims([answer])\n    if not answer_claims_list or answer_claims_list[0] is None:\n        if verbose:\n            logger.debug(f\"No claims decomposed for answer: {answer}. Using empty list.\")\n        answer_claims = []\n    else:\n        answer_claims = answer_claims_list[0]\n\n    context_claims_list = self.decompose_claims([context])\n    if not context_claims_list or context_claims_list[0] is None:\n        if verbose:\n            logger.debug(f\"No claims decomposed for context: {context}. Using empty list.\")\n        context_claims = []\n    else:\n        context_claims = context_claims_list[0]\n\n    # Verify answer claims against context (precision part).\n    context_verdicts_list = self.verify_claims(premises=[context], claims_list=[answer_claims])\n    if not context_verdicts_list or context_verdicts_list[0] is None:\n        if verbose:\n            logger.debug(f\"No verdicts returned when verifying answer claims against context for answer: {answer}\")\n        context_verdicts = []\n    else:\n        context_verdicts = context_verdicts_list[0]\n    tp = sum(context_verdicts)\n    fp = len(context_verdicts) - tp\n\n    # For recall or F1, verify context claims against answer.\n    if evaluation_mode not in (\"precision\", \"PRECISION\"):\n        answer_verdicts_list = self.verify_claims(premises=[answer], claims_list=[context_claims])\n        if not answer_verdicts_list or answer_verdicts_list[0] is None:\n            if verbose:\n                logger.debug(\n                    f\"No verdicts returned when verifying context claims against answer for answer: {answer}\"\n                )\n            answer_verdicts = []\n            fn = 0\n        else:\n            answer_verdicts = answer_verdicts_list[0]\n            fn = sum(1 - v for v in answer_verdicts)\n    else:\n        answer_verdicts = []\n        fn = 0\n\n    if evaluation_mode == \"precision\":\n        computed_score = tp / (tp + fp + 1e-8)\n    elif evaluation_mode == \"recall\":\n        computed_score = tp / (tp + fn + 1e-8)\n    else:\n        computed_score = self.fbeta_score(tp, fp, fn, beta_value)\n\n    reasoning_text = self._build_reasoning(\n        answer_claims=answer_claims,\n        context_claims=context_claims,\n        answer_verdicts=answer_verdicts,\n        context_verdicts=context_verdicts,\n        tp=tp,\n        fp=fp,\n        fn=fn,\n        score=computed_score,\n        mode=evaluation_mode,\n        beta=beta_value,\n    )\n\n    if verbose:\n        logger.debug(f\"Answer: {answer}\")\n        logger.debug(f\"Context: {context}\")\n        logger.debug(f\"Answer Claims: {answer_claims}\")\n        logger.debug(f\"Context Claims: {context_claims}\")\n        logger.debug(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n        logger.debug(f\"Score: {computed_score}\")\n        logger.debug(reasoning_text)\n        logger.debug(\"-\" * 50)\n\n    return FactualCorrectnessRunResult(score=round(computed_score, 2), reasoning=reasoning_text)\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessEvaluator.verify_claims","title":"<code>verify_claims(premises, claims_list)</code>","text":"<p>Verify the claims against the premises.</p> <p>Parameters:</p> Name Type Description Default <code>premises</code> <code>list[str]</code> <p>List of premises.</p> required <code>claims_list</code> <code>list[list[str]]</code> <p>List of lists of claims.</p> required <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>list[list[int]]: List of lists of verdicts.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>def verify_claims(self, premises: list[str], claims_list: list[list[str]]) -&gt; list[list[int]]:\n    \"\"\"\n    Verify the claims against the premises.\n\n    Args:\n        premises (list[str]): List of premises.\n        claims_list (list[list[str]]): List of lists of claims.\n\n    Returns:\n        list[list[int]]: List of lists of verdicts.\n    \"\"\"\n    input_data = VerifyClaimsInput(premises=premises, claims_list=claims_list)\n    results = self._nli_evaluator.run(\n        premise=input_data.premises,\n        claims=input_data.claims_list,\n    )\n    verdicts_list = []\n    for result in results[\"results\"]:\n        verdicts_raw = result[\"results\"]\n        verdicts = []\n        for item in verdicts_raw:\n            verdict = int(item[\"verdict\"])\n            verdicts.append(verdict)\n        verdicts_list.append(verdicts)\n    output_data = VerifyClaimsOutput(verdicts_list=verdicts_list)\n    return output_data.verdicts_list\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.FactualCorrectnessRunResult","title":"<code>FactualCorrectnessRunResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result model for factual correctness evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed factual correctness score.</p> <code>reasoning</code> <code>str</code> <p>Detailed reasoning explaining the evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class FactualCorrectnessRunResult(BaseModel):\n    \"\"\"\n    Result model for factual correctness evaluation.\n\n    Attributes:\n        score (float): The computed factual correctness score.\n        reasoning (str): Detailed reasoning explaining the evaluation.\n    \"\"\"\n    score: float\n    reasoning: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for running factual correctness evaluation.</p> <p>Attributes:</p> Name Type Description <code>answers</code> <code>list[str]</code> <p>List of response texts.</p> <code>contexts</code> <code>list[str] | list[list[str]]</code> <p>List of reference texts, or list of lists of reference texts.</p> <code>mode</code> <code>str | None</code> <p>Evaluation mode ('precision', 'recall', or 'f1').</p> <code>beta</code> <code>float | None</code> <p>Beta value for F-beta score.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for running factual correctness evaluation.\n\n    Attributes:\n        answers (list[str]): List of response texts.\n        contexts (list[str] | list[list[str]]): List of reference texts, or list of lists of\n            reference texts.\n        mode (str | None): Evaluation mode ('precision', 'recall', or 'f1').\n        beta (float | None): Beta value for F-beta score.\n        verbose (bool): Flag to enable verbose logging.\n    \"\"\"\n    answers: list[str]\n    contexts: list[str] | list[list[str]]\n    mode: str | None = None\n    beta: float | None = None\n    verbose: bool = False\n\n    @field_validator(\"contexts\", mode=\"before\")\n    def unify_contexts(cls, value):\n        \"\"\"\n        Allow contexts to be either list[str] or list[list[str]]. If list[list[str]],\n        each sub-list is joined into one string. Otherwise, leave as-is.\n        \"\"\"\n        if not isinstance(value, list):\n            raise ValueError(\"contexts must be a list of strings or a list of lists of strings.\")\n        if all(isinstance(item, list) and all(isinstance(element, str) for element in item) for item in value):\n            return [\" \".join(sublist) for sublist in value]\n        if all(isinstance(item, str) for item in value):\n            return value\n        raise ValueError(\"contexts must be either a list of strings or a list of lists of strings.\")\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        \"\"\"\n        Confirm that answers and contexts have the same length.\n        \"\"\"\n        if len(self.answers) != len(self.contexts):\n            raise ValueError(\"answers and contexts must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.RunInput.check_equal_length","title":"<code>check_equal_length()</code>","text":"<p>Confirm that answers and contexts have the same length.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_equal_length(self):\n    \"\"\"\n    Confirm that answers and contexts have the same length.\n    \"\"\"\n    if len(self.answers) != len(self.contexts):\n        raise ValueError(\"answers and contexts must have the same length.\")\n    return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.RunInput.unify_contexts","title":"<code>unify_contexts(value)</code>","text":"<p>Allow contexts to be either list[str] or list[list[str]]. If list[list[str]], each sub-list is joined into one string. Otherwise, leave as-is.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>@field_validator(\"contexts\", mode=\"before\")\ndef unify_contexts(cls, value):\n    \"\"\"\n    Allow contexts to be either list[str] or list[list[str]]. If list[list[str]],\n    each sub-list is joined into one string. Otherwise, leave as-is.\n    \"\"\"\n    if not isinstance(value, list):\n        raise ValueError(\"contexts must be a list of strings or a list of lists of strings.\")\n    if all(isinstance(item, list) and all(isinstance(element, str) for element in item) for item in value):\n        return [\" \".join(sublist) for sublist in value]\n    if all(isinstance(item, str) for item in value):\n        return value\n    raise ValueError(\"contexts must be either a list of strings or a list of lists of strings.\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for factual correctness evaluation.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[FactualCorrectnessRunResult]</code> <p>List of results with score and reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for factual correctness evaluation.\n\n    Attributes:\n        results (list[FactualCorrectnessRunResult]): List of results with score and reasoning.\n    \"\"\"\n    results: list[FactualCorrectnessRunResult]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.VerifyClaimsInput","title":"<code>VerifyClaimsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for verifying claims against premises.</p> <p>Attributes:</p> Name Type Description <code>premises</code> <code>list[str]</code> <p>List of premises.</p> <code>claims_list</code> <code>list[list[str]]</code> <p>List of lists of claims.</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class VerifyClaimsInput(BaseModel):\n    \"\"\"\n    Input model for verifying claims against premises.\n\n    Attributes:\n        premises (list[str]): List of premises.\n        claims_list (list[list[str]]): List of lists of claims.\n    \"\"\"\n    premises: list[str]\n    claims_list: list[list[str]]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/factual_correctness/#dynamiq.evaluations.metrics.factual_correctness.VerifyClaimsOutput","title":"<code>VerifyClaimsOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for claim verification.</p> <p>Attributes:</p> Name Type Description <code>verdicts_list</code> <code>list[list[int]]</code> <p>List of lists of verdicts (0 or 1).</p> Source code in <code>dynamiq/evaluations/metrics/factual_correctness.py</code> <pre><code>class VerifyClaimsOutput(BaseModel):\n    \"\"\"\n    Output model for claim verification.\n\n    Attributes:\n        verdicts_list (list[list[int]]): List of lists of verdicts (0 or 1).\n    \"\"\"\n    verdicts_list: list[list[int]]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/","title":"Faithfulness","text":""},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessEvaluator","title":"<code>FaithfulnessEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator class for faithfulness metric.</p> Pipeline <p>1) Statement Simplification: The answer is broken down into unambiguous statements    with no pronouns. 2) NLI Evaluation: Each statement is compared against the context. A verdict of 1 means    the statement is faithful; 0 means it is not. 3) Score Computation: The score is the ratio of faithful statements to total statements. 4) Detailed Reasoning: A user-friendly explanation is output with every step.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLLM</code> <p>The language model used for evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class FaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluator class for faithfulness metric.\n\n    Pipeline:\n      1) Statement Simplification: The answer is broken down into unambiguous statements\n         with no pronouns.\n      2) NLI Evaluation: Each statement is compared against the context. A verdict of 1 means\n         the statement is faithful; 0 means it is not.\n      3) Score Computation: The score is the ratio of faithful statements to total statements.\n      4) Detailed Reasoning: A user-friendly explanation is output with every step.\n\n    Attributes:\n        llm (BaseLLM): The language model used for evaluation.\n    \"\"\"\n    name: str = \"Faithfulness\"\n    llm: BaseLLM\n\n    _statement_simplifier: LLMEvaluator = PrivateAttr()\n    _nli_evaluator: LLMEvaluator = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_evaluators()\n\n    def _initialize_evaluators(self):\n        simplify_instructions = (\n            \"Given a 'Question' and an 'Answer', break down each sentence in the \"\n            \"Answer into one or more fully understandable statements.\\n\"\n            \"- Ensure no pronouns are used in each statement.\\n\"\n            \"- Output as a JSON object with key 'statements', where the value is a \"\n            \"list of statements.\\n\"\n            \"- Ensure your response is valid JSON, using double quotes for all strings.\"\n        )\n        self._statement_simplifier = LLMEvaluator(\n            instructions=simplify_instructions.strip(),\n            inputs=[{\"name\": \"question\", \"type\": list[str]}, {\"name\": \"answer\", \"type\": list[str]}],\n            outputs=[{\"name\": \"statements\", \"type\": list[str]}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"question\": [\"Who was Albert Einstein and what is he best known for?\"],\n                        \"answer\": [\n                            \"He was a German-born theoretical physicist, widely \"\n                            \"acknowledged to be one of the greatest and most influential \"\n                            \"physicists of all time. He was best known for developing \"\n                            \"the theory of relativity, he also made important contributions \"\n                            \"to the development of quantum mechanics.\"\n                        ],\n                    },\n                    \"outputs\": {\n                        \"statements\": [\n                            \"Albert Einstein was a German-born theoretical physicist.\",\n                            \"Albert Einstein is recognized as one of the greatest and most influential \"\n                            \"physicists of all time.\",\n                            \"Albert Einstein was best known for developing the theory of relativity.\",\n                            \"Albert Einstein also made important contributions to the development of \"\n                            \"quantum mechanics.\",\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n        nli_instructions = (\n            \"Your task is to judge the faithfulness of a series of statements based \"\n            \"on a given Context.\\n\"\n            \"- For each statement, return 'verdict': 1 if it can be directly inferred \"\n            \"from the Context, or 0 if not.\\n\"\n            \"- Provide a brief 'reason' for the verdict.\\n\"\n            \"- Output as a JSON object with key 'results', where the value is a list of \"\n            \"dictionaries with keys 'statement', 'verdict', and 'reason'.\\n\"\n            \"- Ensure your response is valid JSON, using double quotes for all strings.\"\n        )\n        self._nli_evaluator = LLMEvaluator(\n            instructions=nli_instructions.strip(),\n            inputs=[{\"name\": \"context\", \"type\": list[str]}, {\"name\": \"statements\", \"type\": list[list[str]]}],\n            outputs=[{\"name\": \"results\", \"type\": list[dict[str, Any]]}],\n            examples=[\n                {\n                    \"inputs\": {\n                        \"context\": [\n                            \"John is a student at XYZ University. He is pursuing a \"\n                            \"degree in Computer Science. He is enrolled in several courses \"\n                            \"this semester, including Data Structures, Algorithms, and \"\n                            \"Database Management. John is a diligent student and spends a \"\n                            \"significant amount of time studying and completing assignments. \"\n                            \"He often stays late in the library to work on his projects.\"\n                        ],\n                        \"statements\": [\n                            [\n                                \"John is majoring in Biology.\",\n                                \"John is taking a course on Artificial Intelligence.\",\n                                \"John is a dedicated student.\",\n                                \"John has a part-time job.\",\n                            ]\n                        ],\n                    },\n                    \"outputs\": {\n                        \"results\": [\n                            {\n                                \"statement\": \"John is majoring in Biology.\",\n                                \"reason\": \"The context states that John is pursuing a degree in \"\n                                \"Computer Science, not Biology.\",\n                                \"verdict\": 0,\n                            },\n                            {\n                                \"statement\": \"John is taking a course on Artificial Intelligence.\",\n                                \"reason\": \"The context lists his courses, and Artificial Intelligence \"\n                                \"is not mentioned.\",\n                                \"verdict\": 0,\n                            },\n                            {\n                                \"statement\": \"John is a dedicated student.\",\n                                \"reason\": \"The context mentions he spends significant time studying and \"\n                                \"stays late to work on projects.\",\n                                \"verdict\": 1,\n                            },\n                            {\n                                \"statement\": \"John has a part-time job.\",\n                                \"reason\": \"There is no information in the context about John having a \"\n                                \"part-time job.\",\n                                \"verdict\": 0,\n                            },\n                        ]\n                    },\n                },\n            ],\n            llm=self.llm,\n        )\n\n    def simplify_statements(self, questions: list[str], answers: list[str]) -&gt; list[list[str]]:\n        \"\"\"\n        Simplify the answers into clear, unambiguous statements.\n\n        Args:\n            questions (list[str]): List of questions.\n            answers (list[str]): List of corresponding answers.\n\n        Returns:\n            list[list[str]]: Simplified statements.\n        \"\"\"\n        input_data = SimplifyStatementsInput(questions=questions, answers=answers)\n        results = self._statement_simplifier.run(question=input_data.questions, answer=input_data.answers)\n        statements_list = []\n        for result in results[\"results\"]:\n            statements = result.get(\"statements\")\n            if isinstance(statements, list):\n                statements_list.append(statements)\n            else:\n                statements_list.append([statements])\n        output_data = SimplifyStatementsOutput(statements_list=statements_list)\n        return output_data.statements_list\n\n    def check_faithfulness(self, contexts: list[str], statements_list: list[list[str]]) -&gt; list[list[NLIResultItem]]:\n        \"\"\"\n        Check the faithfulness of statements against contexts.\n\n        Args:\n            contexts (list[str]): List of contexts.\n            statements_list (list[list[str]]): Simplified statements.\n\n        Returns:\n            list[list[NLIResultItem]]: NLI results.\n        \"\"\"\n        input_data = NLIInput(contexts=contexts, statements_list=statements_list)\n        results = self._nli_evaluator.run(context=input_data.contexts, statements=input_data.statements_list)\n        results_list = []\n        for result in results[\"results\"]:\n            items = []\n            for item in result[\"results\"]:\n                nli_item = NLIResultItem(\n                    statement=item[\"statement\"], verdict=int(item[\"verdict\"]), reason=item[\"reason\"]\n                )\n                items.append(nli_item)\n            results_list.append(items)\n        output_data = NLIOutput(results_list=results_list)\n        return output_data.results_list\n\n    def _build_reasoning(\n        self,\n        statements: list[str],\n        nli_results: list[NLIResultItem],\n        num_statements: int,\n        num_faithful: int,\n        score: float,\n    ) -&gt; str:\n        \"\"\"\n        Build detailed reasoning for the faithfulness evaluation.\n\n        This explanation covers:\n        \u2022 How the answer was simplified into candidate statements.\n        \u2022 The NLI verdict for each statement along with brief reasons.\n        \u2022 The calculation of the final faithfulness score.\n\n        Args:\n            statements (list[str]): Simplified candidate statements.\n            nli_results (list[NLIResultItem]): NLI results.\n            num_statements (int): Total number of statements.\n            num_faithful (int): Number of statements deemed faithful.\n            score (float): The computed faithfulness score.\n\n        Returns:\n            str: Detailed reasoning.\n        \"\"\"\n        lines = []\n        lines.extend(\n            [\n                \"Reasoning:\",\n                \"\",\n                \"Overview:\",\n                \"  The answer is first simplified into clear statements (without pronouns).\",\n                \"  Each statement is then evaluated for faithfulness against the context via NLI.\",\n                \"  A '\u2705' indicates the statement is faithful; '\u274c' indicates it is not.\",\n                \"\",\n                \"1. Simplified Statements:\",\n            ]\n        )\n\n        # Add each simplified statement\n        lines.extend([f\"   - {stmt}\" for stmt in statements])\n\n        lines.extend([\"\", \"2. NLI Evaluation Results:\"])\n\n        # Add each NLI result with its verdict and explanation\n        for res in nli_results:\n            mark = \"\u2705\" if res.verdict == 1 else \"\u274c\"\n            lines.extend([f\" {mark} - {res.statement}\", f\"     Explanation: {res.reason}\", \"\"])\n\n        lines.extend(\n            [\n                f\" -&gt; Faithful Statements = {num_faithful} out of {num_statements}\",\n                f\" -&gt; Faithfulness Score = {score:.2f} (faithful/total)\",\n                \"\",\n                f\"Final Score = {score:.2f}\",\n            ]\n        )\n\n        return \"\\n\".join(lines)\n\n    def run_single(self, question: str, answer: str, context: str, verbose: bool = False) -&gt; FaithfulnessRunResult:\n        \"\"\"\n        Evaluate the faithfulness for a single sample.\n\n        Args:\n            question (str): The question.\n            answer (str): The corresponding answer.\n            context (str): The evaluation context.\n            verbose (bool): Flag to enable verbose logging.\n\n        Returns:\n            FaithfulnessRunResult: The result with faithfulness score and detailed reasoning.\n        \"\"\"\n        # Validate the single input using a pydantic model\n        single_input = FaithfulnessRunSingleInput(question=question, answer=answer, context=context, verbose=verbose)\n\n        # Simplify the answer (using question and answer)\n        statements_list = self.simplify_statements([single_input.question], [single_input.answer])\n        if not statements_list or statements_list[0] is None:\n            if single_input.verbose:\n                logger.debug(f\"No simplified statements for answer: {single_input.answer}. Using empty list.\")\n            statements = []\n        else:\n            statements = statements_list[0]\n\n        # Evaluate faithfulness via NLI\n        nli_results_list = self.check_faithfulness([single_input.context], [statements])\n        if not nli_results_list or nli_results_list[0] is None:\n            if single_input.verbose:\n                logger.debug(\"No NLI results for context or statements. Using empty list for NLI evaluation.\")\n            nli_results = []\n        else:\n            nli_results = nli_results_list[0]\n\n        num_statements = len(nli_results)\n        num_faithful = sum(item.verdict for item in nli_results)\n        score = num_faithful / num_statements if num_statements else 0.0\n        score = round(float(score), 2)\n\n        reasoning = self._build_reasoning(\n            statements=statements,\n            nli_results=nli_results,\n            num_statements=num_statements,\n            num_faithful=num_faithful,\n            score=score,\n        )\n        if single_input.verbose:\n            logger.debug(f\"Question: {single_input.question}\")\n            logger.debug(f\"Answer: {single_input.answer}\")\n            logger.debug(f\"Context: {single_input.context}\")\n            logger.debug(\"Simplified Statements:\")\n            logger.debug(statements)\n            logger.debug(\"NLI Results:\")\n            logger.debug([item.model_dump() for item in nli_results])\n            logger.debug(reasoning)\n            logger.debug(\"-\" * 50)\n        result_item = FaithfulnessRunResult(score=score, reasoning=reasoning)\n        return result_item\n\n    def run(\n        self,\n        questions: list[str],\n        answers: list[str],\n        contexts: list[str] | list[list[str]],\n        verbose: bool = False,\n    ) -&gt; RunOutput:\n        \"\"\"\n        Evaluate the faithfulness of answers given contexts.\n\n        Pipeline:\n        1) Simplify the answer into clear candidate statements.\n        2) Evaluate each statement via NLI against the context.\n        3) Compute the faithfulness score as the ratio of faithful statements.\n        4) Generate detailed reasoning explaining the process and final score.\n\n        Args:\n            questions (list[str]): List of questions.\n            answers (list[str]): List of corresponding answers.\n            contexts (list[str] | list[list[str]]): List of context texts.\n            verbose (bool): Flag to enable verbose logging.\n\n        Returns:\n            RunOutput: Contains a list of FaithfulnessRunResult.\n        \"\"\"\n        input_data = RunInput(questions=questions, answers=answers, contexts=contexts, verbose=verbose)\n        results_out = []\n        for idx in range(len(input_data.questions)):\n            question = input_data.questions[idx]\n            answer = input_data.answers[idx]\n            context = input_data.contexts[idx]\n            result_item = self.run_single(question=question, answer=answer, context=context, verbose=input_data.verbose)\n            results_out.append(result_item)\n        output_data = RunOutput(results=results_out)\n        return output_data\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessEvaluator.check_faithfulness","title":"<code>check_faithfulness(contexts, statements_list)</code>","text":"<p>Check the faithfulness of statements against contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[str]</code> <p>List of contexts.</p> required <code>statements_list</code> <code>list[list[str]]</code> <p>Simplified statements.</p> required <p>Returns:</p> Type Description <code>list[list[NLIResultItem]]</code> <p>list[list[NLIResultItem]]: NLI results.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>def check_faithfulness(self, contexts: list[str], statements_list: list[list[str]]) -&gt; list[list[NLIResultItem]]:\n    \"\"\"\n    Check the faithfulness of statements against contexts.\n\n    Args:\n        contexts (list[str]): List of contexts.\n        statements_list (list[list[str]]): Simplified statements.\n\n    Returns:\n        list[list[NLIResultItem]]: NLI results.\n    \"\"\"\n    input_data = NLIInput(contexts=contexts, statements_list=statements_list)\n    results = self._nli_evaluator.run(context=input_data.contexts, statements=input_data.statements_list)\n    results_list = []\n    for result in results[\"results\"]:\n        items = []\n        for item in result[\"results\"]:\n            nli_item = NLIResultItem(\n                statement=item[\"statement\"], verdict=int(item[\"verdict\"]), reason=item[\"reason\"]\n            )\n            items.append(nli_item)\n        results_list.append(items)\n    output_data = NLIOutput(results_list=results_list)\n    return output_data.results_list\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessEvaluator.run","title":"<code>run(questions, answers, contexts, verbose=False)</code>","text":"<p>Evaluate the faithfulness of answers given contexts.</p> <p>Pipeline: 1) Simplify the answer into clear candidate statements. 2) Evaluate each statement via NLI against the context. 3) Compute the faithfulness score as the ratio of faithful statements. 4) Generate detailed reasoning explaining the process and final score.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>list[str]</code> <p>List of questions.</p> required <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> required <code>contexts</code> <code>list[str] | list[list[str]]</code> <p>List of context texts.</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RunOutput</code> <code>RunOutput</code> <p>Contains a list of FaithfulnessRunResult.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>def run(\n    self,\n    questions: list[str],\n    answers: list[str],\n    contexts: list[str] | list[list[str]],\n    verbose: bool = False,\n) -&gt; RunOutput:\n    \"\"\"\n    Evaluate the faithfulness of answers given contexts.\n\n    Pipeline:\n    1) Simplify the answer into clear candidate statements.\n    2) Evaluate each statement via NLI against the context.\n    3) Compute the faithfulness score as the ratio of faithful statements.\n    4) Generate detailed reasoning explaining the process and final score.\n\n    Args:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n        contexts (list[str] | list[list[str]]): List of context texts.\n        verbose (bool): Flag to enable verbose logging.\n\n    Returns:\n        RunOutput: Contains a list of FaithfulnessRunResult.\n    \"\"\"\n    input_data = RunInput(questions=questions, answers=answers, contexts=contexts, verbose=verbose)\n    results_out = []\n    for idx in range(len(input_data.questions)):\n        question = input_data.questions[idx]\n        answer = input_data.answers[idx]\n        context = input_data.contexts[idx]\n        result_item = self.run_single(question=question, answer=answer, context=context, verbose=input_data.verbose)\n        results_out.append(result_item)\n    output_data = RunOutput(results=results_out)\n    return output_data\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessEvaluator.run_single","title":"<code>run_single(question, answer, context, verbose=False)</code>","text":"<p>Evaluate the faithfulness for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question.</p> required <code>answer</code> <code>str</code> <p>The corresponding answer.</p> required <code>context</code> <code>str</code> <p>The evaluation context.</p> required <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FaithfulnessRunResult</code> <code>FaithfulnessRunResult</code> <p>The result with faithfulness score and detailed reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>def run_single(self, question: str, answer: str, context: str, verbose: bool = False) -&gt; FaithfulnessRunResult:\n    \"\"\"\n    Evaluate the faithfulness for a single sample.\n\n    Args:\n        question (str): The question.\n        answer (str): The corresponding answer.\n        context (str): The evaluation context.\n        verbose (bool): Flag to enable verbose logging.\n\n    Returns:\n        FaithfulnessRunResult: The result with faithfulness score and detailed reasoning.\n    \"\"\"\n    # Validate the single input using a pydantic model\n    single_input = FaithfulnessRunSingleInput(question=question, answer=answer, context=context, verbose=verbose)\n\n    # Simplify the answer (using question and answer)\n    statements_list = self.simplify_statements([single_input.question], [single_input.answer])\n    if not statements_list or statements_list[0] is None:\n        if single_input.verbose:\n            logger.debug(f\"No simplified statements for answer: {single_input.answer}. Using empty list.\")\n        statements = []\n    else:\n        statements = statements_list[0]\n\n    # Evaluate faithfulness via NLI\n    nli_results_list = self.check_faithfulness([single_input.context], [statements])\n    if not nli_results_list or nli_results_list[0] is None:\n        if single_input.verbose:\n            logger.debug(\"No NLI results for context or statements. Using empty list for NLI evaluation.\")\n        nli_results = []\n    else:\n        nli_results = nli_results_list[0]\n\n    num_statements = len(nli_results)\n    num_faithful = sum(item.verdict for item in nli_results)\n    score = num_faithful / num_statements if num_statements else 0.0\n    score = round(float(score), 2)\n\n    reasoning = self._build_reasoning(\n        statements=statements,\n        nli_results=nli_results,\n        num_statements=num_statements,\n        num_faithful=num_faithful,\n        score=score,\n    )\n    if single_input.verbose:\n        logger.debug(f\"Question: {single_input.question}\")\n        logger.debug(f\"Answer: {single_input.answer}\")\n        logger.debug(f\"Context: {single_input.context}\")\n        logger.debug(\"Simplified Statements:\")\n        logger.debug(statements)\n        logger.debug(\"NLI Results:\")\n        logger.debug([item.model_dump() for item in nli_results])\n        logger.debug(reasoning)\n        logger.debug(\"-\" * 50)\n    result_item = FaithfulnessRunResult(score=score, reasoning=reasoning)\n    return result_item\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessEvaluator.simplify_statements","title":"<code>simplify_statements(questions, answers)</code>","text":"<p>Simplify the answers into clear, unambiguous statements.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>list[str]</code> <p>List of questions.</p> required <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> required <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: Simplified statements.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>def simplify_statements(self, questions: list[str], answers: list[str]) -&gt; list[list[str]]:\n    \"\"\"\n    Simplify the answers into clear, unambiguous statements.\n\n    Args:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n\n    Returns:\n        list[list[str]]: Simplified statements.\n    \"\"\"\n    input_data = SimplifyStatementsInput(questions=questions, answers=answers)\n    results = self._statement_simplifier.run(question=input_data.questions, answer=input_data.answers)\n    statements_list = []\n    for result in results[\"results\"]:\n        statements = result.get(\"statements\")\n        if isinstance(statements, list):\n            statements_list.append(statements)\n        else:\n            statements_list.append([statements])\n    output_data = SimplifyStatementsOutput(statements_list=statements_list)\n    return output_data.statements_list\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessRunResult","title":"<code>FaithfulnessRunResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result model for faithfulness evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed faithfulness score.</p> <code>reasoning</code> <code>str</code> <p>Detailed reasoning explaining the evaluation.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class FaithfulnessRunResult(BaseModel):\n    \"\"\"\n    Result model for faithfulness evaluation.\n\n    Attributes:\n        score (float): The computed faithfulness score.\n        reasoning (str): Detailed reasoning explaining the evaluation.\n    \"\"\"\n    score: float\n    reasoning: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.FaithfulnessRunSingleInput","title":"<code>FaithfulnessRunSingleInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run input model for faithfulness evaluation.</p> <p>Attributes:</p> Name Type Description <code>question</code> <code>str</code> <p>The question.</p> <code>answer</code> <code>str</code> <p>The corresponding answer.</p> <code>context</code> <code>str</code> <p>The context for the evaluation.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class FaithfulnessRunSingleInput(BaseModel):\n    \"\"\"\n    Single-run input model for faithfulness evaluation.\n\n    Attributes:\n        question (str): The question.\n        answer (str): The corresponding answer.\n        context (str): The context for the evaluation.\n        verbose (bool): Flag to enable verbose logging.\n    \"\"\"\n\n    question: str\n    answer: str\n    context: str\n    verbose: bool = False\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.NLIInput","title":"<code>NLIInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for NLI evaluation.</p> <p>Attributes:</p> Name Type Description <code>contexts</code> <code>list[str]</code> <p>List of contexts.</p> <code>statements_list</code> <code>list[list[str]]</code> <p>List of lists of statements.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class NLIInput(BaseModel):\n    \"\"\"\n    Input model for NLI evaluation.\n\n    Attributes:\n        contexts (list[str]): List of contexts.\n        statements_list (list[list[str]]): List of lists of statements.\n    \"\"\"\n    contexts: list[str]\n    statements_list: list[list[str]]\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        if len(self.contexts) != len(self.statements_list):\n            raise ValueError(\"Contexts and statements_list must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.NLIOutput","title":"<code>NLIOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for NLI evaluation.</p> <p>Attributes:</p> Name Type Description <code>results_list</code> <code>list[list[NLIResultItem]]</code> <p>List of lists of NLI results.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class NLIOutput(BaseModel):\n    \"\"\"\n    Output model for NLI evaluation.\n\n    Attributes:\n        results_list (list[list[NLIResultItem]]): List of lists of NLI results.\n    \"\"\"\n    results_list: list[list[NLIResultItem]]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.NLIResultItem","title":"<code>NLIResultItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for individual NLI result.</p> <p>Attributes:</p> Name Type Description <code>statement</code> <code>str</code> <p>The statement being evaluated.</p> <code>verdict</code> <code>int</code> <p>1 if faithful, 0 otherwise.</p> <code>reason</code> <code>str</code> <p>Reason for the verdict.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class NLIResultItem(BaseModel):\n    \"\"\"\n    Model for individual NLI result.\n\n    Attributes:\n        statement (str): The statement being evaluated.\n        verdict (int): 1 if faithful, 0 otherwise.\n        reason (str): Reason for the verdict.\n    \"\"\"\n    statement: str\n    verdict: int\n    reason: str\n\n    @field_validator(\"verdict\")\n    @classmethod\n    def validate_verdict(cls, v):\n        if v not in (0, 1):\n            raise ValueError(\"Verdict must be either 0 or 1.\")\n        return v\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for running the faithfulness evaluation.</p> <p>Attributes:</p> Name Type Description <code>questions</code> <code>list[str]</code> <p>List of questions.</p> <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> <code>contexts</code> <code>list[str] | list[list[str]]</code> <p>List of context texts for each question, which can be either one string per question or multiple strings per question.</p> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for running the faithfulness evaluation.\n\n    Attributes:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n        contexts (list[str] | list[list[str]]): List of context texts for each question,\n            which can be either one string per question or multiple strings per question.\n        verbose (bool): Flag to enable verbose logging.\n    \"\"\"\n    questions: list[str]\n    answers: list[str]\n    contexts: list[str] | list[list[str]]\n    verbose: bool = False\n\n    @field_validator(\"contexts\", mode=\"before\")\n    def unify_contexts(cls, value):\n        \"\"\"\n        If contexts is list[list[str]], join each sublist with a space.\n        Otherwise, if list[str], leave as-is.\n        \"\"\"\n        if not isinstance(value, list):\n            raise ValueError(\"contexts must be either a list of strings or a list of list of strings\")\n        if all(isinstance(item, list) and all(isinstance(x, str) for x in item) for item in value):\n            return [\" \".join(sublist) for sublist in value]\n        if all(isinstance(item, str) for item in value):\n            return value\n        raise ValueError(\"contexts must be either a list[str] or a list[list[str]]\")\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        if not (len(self.questions) == len(self.answers) == len(self.contexts)):\n            raise ValueError(\"Questions, answers, and contexts must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.RunInput.unify_contexts","title":"<code>unify_contexts(value)</code>","text":"<p>If contexts is list[list[str]], join each sublist with a space. Otherwise, if list[str], leave as-is.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>@field_validator(\"contexts\", mode=\"before\")\ndef unify_contexts(cls, value):\n    \"\"\"\n    If contexts is list[list[str]], join each sublist with a space.\n    Otherwise, if list[str], leave as-is.\n    \"\"\"\n    if not isinstance(value, list):\n        raise ValueError(\"contexts must be either a list of strings or a list of list of strings\")\n    if all(isinstance(item, list) and all(isinstance(x, str) for x in item) for item in value):\n        return [\" \".join(sublist) for sublist in value]\n    if all(isinstance(item, str) for item in value):\n        return value\n    raise ValueError(\"contexts must be either a list[str] or a list[list[str]]\")\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for faithfulness evaluation.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[FaithfulnessRunResult]</code> <p>List of results with score and reasoning.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for faithfulness evaluation.\n\n    Attributes:\n        results (list[FaithfulnessRunResult]): List of results with score and reasoning.\n    \"\"\"\n\n    results: list[FaithfulnessRunResult]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.SimplifyStatementsInput","title":"<code>SimplifyStatementsInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for simplifying statements.</p> <p>Attributes:</p> Name Type Description <code>questions</code> <code>list[str]</code> <p>List of questions.</p> <code>answers</code> <code>list[str]</code> <p>List of corresponding answers.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class SimplifyStatementsInput(BaseModel):\n    \"\"\"\n    Input model for simplifying statements.\n\n    Attributes:\n        questions (list[str]): List of questions.\n        answers (list[str]): List of corresponding answers.\n    \"\"\"\n    questions: list[str]\n    answers: list[str]\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self):\n        if len(self.questions) != len(self.answers):\n            raise ValueError(\"Questions and answers must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/faithfulness/#dynamiq.evaluations.metrics.faithfulness.SimplifyStatementsOutput","title":"<code>SimplifyStatementsOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for simplified statements.</p> <p>Attributes:</p> Name Type Description <code>statements_list</code> <code>list[list[str]]</code> <p>List of lists of simplified statements.</p> Source code in <code>dynamiq/evaluations/metrics/faithfulness.py</code> <pre><code>class SimplifyStatementsOutput(BaseModel):\n    \"\"\"\n    Output model for simplified statements.\n\n    Attributes:\n        statements_list (list[list[str]]): List of lists of simplified statements.\n    \"\"\"\n    statements_list: list[list[str]]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/","title":"Rouge score","text":""},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.MeasureType","title":"<code>MeasureType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of measurement types for ROUGE scores.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class MeasureType(str, Enum):\n    \"\"\"\n    Enumeration of measurement types for ROUGE scores.\n    \"\"\"\n    fmeasure = \"fmeasure\"\n    precision = \"precision\"\n    recall = \"recall\"\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RougeScoreEvaluator","title":"<code>RougeScoreEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluates ROUGE scores using the rouge_score library.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the evaluator. Defaults to \"RougeScore\".</p> <code>rouge_type</code> <code>RougeType</code> <p>ROUGE variant to compute. Defaults to RougeType.rougeL.</p> <code>measure_type</code> <code>MeasureType</code> <p>The field of the metric to retrieve. Defaults to MeasureType.fmeasure.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RougeScoreEvaluator(BaseEvaluator):\n    \"\"\"\n    Evaluates ROUGE scores using the rouge_score library.\n\n    Attributes:\n        name (str): Name of the evaluator. Defaults to \"RougeScore\".\n        rouge_type (RougeType): ROUGE variant to compute. Defaults to RougeType.rougeL.\n        measure_type (MeasureType): The field of the metric to retrieve. Defaults to MeasureType.fmeasure.\n    \"\"\"\n    name: str = \"RougeScore\"\n    rouge_type: RougeType = RougeType.rougeL\n    measure_type: MeasureType = MeasureType.fmeasure\n\n    _scorer: Callable = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_rouge()\n\n    def _initialize_rouge(self) -&gt; None:\n        from rouge_score import rouge_scorer\n\n        self._scorer = rouge_scorer.RougeScorer([self.rouge_type.value], use_stemmer=True)\n\n    def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n        \"\"\"\n        Compute the ROUGE score for a single pair of ground truth (reference) and answer.\n\n        Args:\n            ground_truth_answer (str): The reference string.\n            answer (str): The candidate string.\n\n        Returns:\n            float: The computed ROUGE score.\n        \"\"\"\n        # Validate input.\n        single_input = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n        rouge_result = self._scorer.score(single_input.ground_truth_answer, single_input.answer)\n        metric_value = getattr(rouge_result[self.rouge_type.value], self.measure_type.value)\n        score = round(float(metric_value), 2)\n\n        output = RunSingleOutput(score=score)\n        return output.score\n\n    def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n        \"\"\"\n        Compute ROUGE scores for each reference-response pair in batch.\n\n        Args:\n            ground_truth_answers (list[str]): List of reference strings.\n            answers (list[str]): List of candidate strings.\n\n        Returns:\n            list[float]: List of computed ROUGE scores.\n        \"\"\"\n        input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n        scores = []\n        for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n            score = self.run_single(ground_truth_answer=gt, answer=ans)\n            scores.append(score)\n        output_data = RunOutput(scores=scores)\n        return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RougeScoreEvaluator.run","title":"<code>run(ground_truth_answers, answers)</code>","text":"<p>Compute ROUGE scores for each reference-response pair in batch.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_answers</code> <code>list[str]</code> <p>List of reference strings.</p> required <code>answers</code> <code>list[str]</code> <p>List of candidate strings.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: List of computed ROUGE scores.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n    \"\"\"\n    Compute ROUGE scores for each reference-response pair in batch.\n\n    Args:\n        ground_truth_answers (list[str]): List of reference strings.\n        answers (list[str]): List of candidate strings.\n\n    Returns:\n        list[float]: List of computed ROUGE scores.\n    \"\"\"\n    input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n    scores = []\n    for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n        score = self.run_single(ground_truth_answer=gt, answer=ans)\n        scores.append(score)\n    output_data = RunOutput(scores=scores)\n    return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RougeScoreEvaluator.run_single","title":"<code>run_single(ground_truth_answer, answer)</code>","text":"<p>Compute the ROUGE score for a single pair of ground truth (reference) and answer.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_answer</code> <code>str</code> <p>The reference string.</p> required <code>answer</code> <code>str</code> <p>The candidate string.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed ROUGE score.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n    \"\"\"\n    Compute the ROUGE score for a single pair of ground truth (reference) and answer.\n\n    Args:\n        ground_truth_answer (str): The reference string.\n        answer (str): The candidate string.\n\n    Returns:\n        float: The computed ROUGE score.\n    \"\"\"\n    # Validate input.\n    single_input = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n    rouge_result = self._scorer.score(single_input.ground_truth_answer, single_input.answer)\n    metric_value = getattr(rouge_result[self.rouge_type.value], self.measure_type.value)\n    score = round(float(metric_value), 2)\n\n    output = RunSingleOutput(score=score)\n    return output.score\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RougeType","title":"<code>RougeType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of supported ROUGE types.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RougeType(str, Enum):\n    \"\"\"\n    Enumeration of supported ROUGE types.\n    \"\"\"\n    rouge1 = \"rouge1\"\n    rouge2 = \"rouge2\"\n    rougeL = \"rougeL\"\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for batch ROUGE score evaluation.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answers</code> <code>list[str]</code> <p>List of reference strings.</p> <code>answers</code> <code>list[str]</code> <p>List of candidate response strings.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for batch ROUGE score evaluation.\n\n    Attributes:\n        ground_truth_answers (list[str]): List of reference strings.\n        answers (list[str]): List of candidate response strings.\n    \"\"\"\n    ground_truth_answers: list[str]\n    answers: list[str]\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self) -&gt; \"RunInput\":\n        \"\"\"\n        Validate that the number of ground truth answers matches the number of answers.\n\n        Raises:\n            ValueError: If the lengths of `ground_truth_answers` and `answers` do not match.\n\n        Returns:\n            RunInput: The validated instance.\n        \"\"\"\n        if len(self.ground_truth_answers) != len(self.answers):\n            raise ValueError(\"ground_truth_answers and answers must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RunInput.check_equal_length","title":"<code>check_equal_length()</code>","text":"<p>Validate that the number of ground truth answers matches the number of answers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of <code>ground_truth_answers</code> and <code>answers</code> do not match.</p> <p>Returns:</p> Name Type Description <code>RunInput</code> <code>RunInput</code> <p>The validated instance.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_equal_length(self) -&gt; \"RunInput\":\n    \"\"\"\n    Validate that the number of ground truth answers matches the number of answers.\n\n    Raises:\n        ValueError: If the lengths of `ground_truth_answers` and `answers` do not match.\n\n    Returns:\n        RunInput: The validated instance.\n    \"\"\"\n    if len(self.ground_truth_answers) != len(self.answers):\n        raise ValueError(\"ground_truth_answers and answers must have the same length.\")\n    return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for ROUGE score evaluation.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>list[float]</code> <p>List of computed ROUGE scores.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for ROUGE score evaluation.\n\n    Attributes:\n        scores (list[float]): List of computed ROUGE scores.\n    \"\"\"\n\n    scores: list[float]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RunSingleInput","title":"<code>RunSingleInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run input model for ROUGE score evaluation.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answer</code> <code>str</code> <p>The reference string.</p> <code>answer</code> <code>str</code> <p>The candidate string.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RunSingleInput(BaseModel):\n    \"\"\"\n    Single-run input model for ROUGE score evaluation.\n\n    Attributes:\n        ground_truth_answer (str): The reference string.\n        answer (str): The candidate string.\n    \"\"\"\n\n    ground_truth_answer: str\n    answer: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/rouge_score/#dynamiq.evaluations.metrics.rouge_score.RunSingleOutput","title":"<code>RunSingleOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run output model for ROUGE score evaluation.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed ROUGE score.</p> Source code in <code>dynamiq/evaluations/metrics/rouge_score.py</code> <pre><code>class RunSingleOutput(BaseModel):\n    \"\"\"\n    Single-run output model for ROUGE score evaluation.\n\n    Attributes:\n        score (float): The computed ROUGE score.\n    \"\"\"\n\n    score: float\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/","title":"String metrics","text":""},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.DistanceMeasure","title":"<code>DistanceMeasure</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of supported distance measures for string similarity.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class DistanceMeasure(str, Enum):\n    \"\"\"\n    Enumeration of supported distance measures for string similarity.\n    \"\"\"\n    LEVENSHTEIN = \"levenshtein\"\n    HAMMING = \"hamming\"\n    JARO = \"jaro\"\n    JARO_WINKLER = \"jaro_winkler\"\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.ExactMatchEvaluator","title":"<code>ExactMatchEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>An evaluator that checks for exact string matches.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the evaluator. Defaults to \"exact_match\".</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class ExactMatchEvaluator(BaseEvaluator):\n    \"\"\"\n    An evaluator that checks for exact string matches.\n\n    Attributes:\n        name (str): Name of the evaluator. Defaults to \"exact_match\".\n    \"\"\"\n    name: str = \"exact_match\"\n\n    def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n        input_data = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n        exact_match = float(input_data.ground_truth_answer == input_data.answer)\n        logger.debug(f\"Single pair: Exact Match = {exact_match}\")\n        return exact_match\n\n    def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n        input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n        scores = []\n        for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n            score = self.run_single(ground_truth_answer=gt, answer=ans)\n            scores.append(score)\n        output_data = RunOutput(scores=scores)\n        return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.RunInput","title":"<code>RunInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input model for batch string evaluations.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answers</code> <code>list[str]</code> <p>List of reference strings, one per example.</p> <code>answers</code> <code>list[str]</code> <p>List of candidate response strings, one per example.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class RunInput(BaseModel):\n    \"\"\"\n    Input model for batch string evaluations.\n\n    Attributes:\n        ground_truth_answers (list[str]): List of reference strings, one per example.\n        answers (list[str]): List of candidate response strings, one per example.\n    \"\"\"\n    ground_truth_answers: list[str]\n    answers: list[str]\n\n    @model_validator(mode=\"after\")\n    def check_equal_length(self) -&gt; \"RunInput\":\n        if len(self.ground_truth_answers) != len(self.answers):\n            raise ValueError(\"ground_truth_answers and answers must have the same length.\")\n        return self\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.RunOutput","title":"<code>RunOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for string evaluations.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>list[float]</code> <p>List of evaluator scores, one per reference/answer pair.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class RunOutput(BaseModel):\n    \"\"\"\n    Output model for string evaluations.\n\n    Attributes:\n        scores (list[float]): List of evaluator scores, one per reference/answer pair.\n    \"\"\"\n\n    scores: list[float]\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.RunSingleInput","title":"<code>RunSingleInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run input model for string evaluations.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_answer</code> <code>str</code> <p>The reference string.</p> <code>answer</code> <code>str</code> <p>The candidate string.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class RunSingleInput(BaseModel):\n    \"\"\"\n    Single-run input model for string evaluations.\n\n    Attributes:\n        ground_truth_answer (str): The reference string.\n        answer (str): The candidate string.\n    \"\"\"\n\n    ground_truth_answer: str\n    answer: str\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.RunSingleOutput","title":"<code>RunSingleOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single-run output model for string evaluations.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The computed evaluator score.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class RunSingleOutput(BaseModel):\n    \"\"\"\n    Single-run output model for string evaluations.\n\n    Attributes:\n        score (float): The computed evaluator score.\n    \"\"\"\n\n    score: float\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.StringPresenceEvaluator","title":"<code>StringPresenceEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>An evaluator that checks if the reference string is present (as a substring) in the answer.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the evaluator. Defaults to \"string_presence\".</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class StringPresenceEvaluator(BaseEvaluator):\n    \"\"\"\n    An evaluator that checks if the reference string is present (as a substring) in the answer.\n\n    Attributes:\n        name (str): Name of the evaluator. Defaults to \"string_presence\".\n    \"\"\"\n    name: str = \"string_presence\"\n\n    def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n        input_data = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n        presence = float(input_data.ground_truth_answer in input_data.answer)\n        output = RunSingleOutput(score=presence)\n        return output.score\n\n    def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n        input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n        scores = []\n        for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n            score = self.run_single(ground_truth_answer=gt, answer=ans)\n            scores.append(score)\n        output_data = RunOutput(scores=scores)\n        return output_data.scores\n</code></pre>"},{"location":"dynamiq/evaluations/metrics/string_metrics/#dynamiq.evaluations.metrics.string_metrics.StringSimilarityEvaluator","title":"<code>StringSimilarityEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>An evaluator that calculates a similarity score (1 - normalized_distance) using RapidFuzz between reference and answer strings.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the evaluator. Defaults to \"non_llm_string_similarity\".</p> <code>distance_measure</code> <code>DistanceMeasure</code> <p>Which distance measure to use. Defaults to DistanceMeasure.LEVENSHTEIN.</p> Source code in <code>dynamiq/evaluations/metrics/string_metrics.py</code> <pre><code>class StringSimilarityEvaluator(BaseEvaluator):\n    \"\"\"\n    An evaluator that calculates a similarity score (1 - normalized_distance) using RapidFuzz\n    between reference and answer strings.\n\n    Attributes:\n        name (str): Name of the evaluator. Defaults to \"non_llm_string_similarity\".\n        distance_measure (DistanceMeasure): Which distance measure to use. Defaults to DistanceMeasure.LEVENSHTEIN.\n    \"\"\"\n    name: str = \"non_llm_string_similarity\"\n    distance_measure: DistanceMeasure = DistanceMeasure.LEVENSHTEIN\n\n    _distance_map: dict[DistanceMeasure, Callable] = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._initialize_distance_map()\n\n    def _initialize_distance_map(self) -&gt; None:\n        from rapidfuzz import distance as rapidfuzz_distance\n        self._distance_map = {\n            DistanceMeasure.LEVENSHTEIN: rapidfuzz_distance.Levenshtein,\n            DistanceMeasure.HAMMING: rapidfuzz_distance.Hamming,\n            DistanceMeasure.JARO: rapidfuzz_distance.Jaro,\n            DistanceMeasure.JARO_WINKLER: rapidfuzz_distance.JaroWinkler,\n        }\n        logger.debug(f\"Initialized distance map: {self._distance_map}\")\n\n    def run_single(self, ground_truth_answer: str, answer: str) -&gt; float:\n        input_data = RunSingleInput(ground_truth_answer=ground_truth_answer, answer=answer)\n        distance_function = self._distance_map[self.distance_measure]\n        normalized_dist = distance_function.normalized_distance(input_data.ground_truth_answer, input_data.answer)\n        similarity = 1 - float(normalized_dist)\n        similarity_rounded = round(similarity, 2)\n        logger.debug(\n            f\"Single pair: Distance Measure = {self.distance_measure}, \"\n            f\"Normalized Distance = {normalized_dist}, \"\n            f\"Similarity = {similarity_rounded}\"\n        )\n        output = RunSingleOutput(score=similarity_rounded)\n        return output.score\n\n    def run(self, ground_truth_answers: list[str], answers: list[str]) -&gt; list[float]:\n        input_data = RunInput(ground_truth_answers=ground_truth_answers, answers=answers)\n        scores = []\n        for gt, ans in zip(input_data.ground_truth_answers, input_data.answers):\n            score = self.run_single(ground_truth_answer=gt, answer=ans)\n            scores.append(score)\n        output_data = RunOutput(scores=scores)\n        return output_data.scores\n</code></pre>"},{"location":"dynamiq/executors/base/","title":"Base","text":""},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor","title":"<code>BaseExecutor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for executors that run nodes in a workflow.</p> <p>Attributes:</p> Name Type Description <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent workers. None means no limit.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>class BaseExecutor(ABC):\n    \"\"\"\n    Abstract base class for executors that run nodes in a workflow.\n\n    Attributes:\n        max_workers (int | None): Maximum number of concurrent workers. None means no limit.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        \"\"\"\n        Initialize the BaseExecutor.\n\n        Args:\n            max_workers (int | None, optional): Maximum number of concurrent workers. Defaults to None.\n        \"\"\"\n        self.max_workers = max_workers\n\n    @abstractmethod\n    def shutdown(self, wait: bool = True):\n        \"\"\"\n        Shut down the executor.\n\n        Args:\n            wait (bool, optional): Whether to wait for pending tasks to complete. Defaults to True.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def execute(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Execute the given nodes that are ready to run.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready for execution.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.__init__","title":"<code>__init__(max_workers=None)</code>","text":"<p>Initialize the BaseExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent workers. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/base.py</code> <pre><code>def __init__(self, max_workers: int | None = None):\n    \"\"\"\n    Initialize the BaseExecutor.\n\n    Args:\n        max_workers (int | None, optional): Maximum number of concurrent workers. Defaults to None.\n    \"\"\"\n    self.max_workers = max_workers\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.execute","title":"<code>execute(ready_nodes, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute the given nodes that are ready to run.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready for execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Execute the given nodes that are ready to run.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready for execution.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.shutdown","title":"<code>shutdown(wait=True)</code>  <code>abstractmethod</code>","text":"<p>Shut down the executor.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>Whether to wait for pending tasks to complete. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>@abstractmethod\ndef shutdown(self, wait: bool = True):\n    \"\"\"\n    Shut down the executor.\n\n    Args:\n        wait (bool, optional): Whether to wait for pending tasks to complete. Defaults to True.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/context/","title":"Context","text":""},{"location":"dynamiq/executors/context/#dynamiq.executors.context.ContextAwareThreadPoolExecutor","title":"<code>ContextAwareThreadPoolExecutor</code>","text":"<p>               Bases: <code>ThreadPoolExecutor</code></p> <p>ThreadPoolExecutor that automatically propagates contextvars to worker threads.</p> <p>Standard ThreadPoolExecutor does not copy the calling thread's contextvars into worker threads, causing context (e.g. request_id, tracing metadata) to be lost. This subclass captures a snapshot via copy_context() before each submit and runs the callable inside that snapshot.</p> <p>Since ThreadPoolExecutor.map() delegates to submit(), both submit() and map() are covered automatically.</p> Source code in <code>dynamiq/executors/context.py</code> <pre><code>class ContextAwareThreadPoolExecutor(ThreadPoolExecutor):\n    \"\"\"ThreadPoolExecutor that automatically propagates contextvars to worker threads.\n\n    Standard ThreadPoolExecutor does not copy the calling thread's contextvars\n    into worker threads, causing context (e.g. request_id, tracing metadata)\n    to be lost. This subclass captures a snapshot via copy_context() before\n    each submit and runs the callable inside that snapshot.\n\n    Since ThreadPoolExecutor.map() delegates to submit(), both submit() and\n    map() are covered automatically.\n    \"\"\"\n\n    def submit(self, fn, /, *args, **kwargs):\n        ctx = contextvars.copy_context()\n        return super().submit(ctx.run, fn, *args, **kwargs)\n</code></pre>"},{"location":"dynamiq/executors/pool/","title":"Pool","text":""},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor","title":"<code>PoolExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>A pool executor that manages concurrent execution of nodes using either ThreadPoolExecutor or ProcessPoolExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>pool_executor</code> <code>type</code> <p>The type of pool executor to use (ThreadPoolExecutor or ProcessPoolExecutor).</p> required <code>max_workers</code> <code>int</code> <p>The maximum number of workers in the pool. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class PoolExecutor(BaseExecutor):\n    \"\"\"\n    A pool executor that manages concurrent execution of nodes using either ThreadPoolExecutor or\n    ProcessPoolExecutor.\n\n    Args:\n        pool_executor (type): The type of pool executor to use (ThreadPoolExecutor or\n            ProcessPoolExecutor).\n        max_workers (int, optional): The maximum number of workers in the pool. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        pool_executor: (\n            type[futures.ThreadPoolExecutor] | type[futures.ProcessPoolExecutor]\n        ),\n        max_workers: int | None = None,\n    ):\n        super().__init__(max_workers=max_workers)\n        self.executor = pool_executor(max_workers=max_workers)\n        self.node_by_future = {}\n\n    def shutdown(self, wait: bool = True):\n        \"\"\"\n        Shuts down the executor.\n\n        Args:\n            wait (bool, optional): Whether to wait for pending futures to complete. Defaults to True.\n        \"\"\"\n        self.executor.shutdown(wait=wait)\n\n    def execute(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Executes the given ready nodes and returns their results.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n        \"\"\"\n        self.run_nodes(ready_nodes=ready_nodes, config=config, **kwargs)\n        if not self.node_by_future:\n            return {}\n\n        completed_node_futures, _ = futures.wait(\n            fs=self.node_by_future.keys(), return_when=futures.FIRST_COMPLETED\n        )\n        results = self.complete_nodes(completed_node_futures=completed_node_futures)\n\n        return results\n\n    def run_nodes(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Submits ready nodes for execution.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for ready_node in ready_nodes:\n            if ready_node.is_ready:\n                future = self.run_node(ready_node=ready_node, config=config, **kwargs)\n                self.node_by_future[future] = ready_node.node\n            else:\n                logger.error(\n                    f\"Node {ready_node.node.name} - {ready_node.node.id}: not ready to run.\"\n                )\n\n    def complete_nodes(\n        self, completed_node_futures: list[futures.Future]\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Processes completed node futures and returns their results.\n\n        Args:\n            completed_node_futures (list[futures.Future]): List of completed node futures.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n        \"\"\"\n        results = {}\n        for f in completed_node_futures:\n            node = self.node_by_future.pop(f)\n            try:\n                node_result: RunnableResult = f.result()\n            except Exception as e:\n                logger.error(f\"Node {node.name} - {node.id}: execution failed due the unexpected error. Error: {e}\")\n                node_result = RunnableResult(status=RunnableStatus.FAILURE, error=RunnableResultError.from_exception(e))\n\n            results[node.id] = node_result\n\n        return results\n\n    def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Submits ready node for execution.\n\n        Args:\n            ready_node (NodeReadyToRun): node ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        return self.executor.submit(\n            ready_node.node.run,\n            input_data=ready_node.input_data,\n            config=config,\n            depends_result=ready_node.depends_result,\n            **kwargs,\n        )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.complete_nodes","title":"<code>complete_nodes(completed_node_futures)</code>","text":"<p>Processes completed node futures and returns their results.</p> <p>Parameters:</p> Name Type Description Default <code>completed_node_futures</code> <code>list[Future]</code> <p>List of completed node futures.</p> required <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary of node IDs and their execution results.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def complete_nodes(\n    self, completed_node_futures: list[futures.Future]\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Processes completed node futures and returns their results.\n\n    Args:\n        completed_node_futures (list[futures.Future]): List of completed node futures.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n    \"\"\"\n    results = {}\n    for f in completed_node_futures:\n        node = self.node_by_future.pop(f)\n        try:\n            node_result: RunnableResult = f.result()\n        except Exception as e:\n            logger.error(f\"Node {node.name} - {node.id}: execution failed due the unexpected error. Error: {e}\")\n            node_result = RunnableResult(status=RunnableStatus.FAILURE, error=RunnableResultError.from_exception(e))\n\n        results[node.id] = node_result\n\n    return results\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.execute","title":"<code>execute(ready_nodes, config=None, **kwargs)</code>","text":"<p>Executes the given ready nodes and returns their results.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary of node IDs and their execution results.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def execute(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Executes the given ready nodes and returns their results.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n    \"\"\"\n    self.run_nodes(ready_nodes=ready_nodes, config=config, **kwargs)\n    if not self.node_by_future:\n        return {}\n\n    completed_node_futures, _ = futures.wait(\n        fs=self.node_by_future.keys(), return_when=futures.FIRST_COMPLETED\n    )\n    results = self.complete_nodes(completed_node_futures=completed_node_futures)\n\n    return results\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.run_node","title":"<code>run_node(ready_node, config=None, **kwargs)</code>","text":"<p>Submits ready node for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_node</code> <code>NodeReadyToRun</code> <p>node ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Submits ready node for execution.\n\n    Args:\n        ready_node (NodeReadyToRun): node ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    return self.executor.submit(\n        ready_node.node.run,\n        input_data=ready_node.input_data,\n        config=config,\n        depends_result=ready_node.depends_result,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.run_nodes","title":"<code>run_nodes(ready_nodes, config=None, **kwargs)</code>","text":"<p>Submits ready nodes for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_nodes(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n):\n    \"\"\"\n    Submits ready nodes for execution.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for ready_node in ready_nodes:\n        if ready_node.is_ready:\n            future = self.run_node(ready_node=ready_node, config=config, **kwargs)\n            self.node_by_future[future] = ready_node.node\n        else:\n            logger.error(\n                f\"Node {ready_node.node.name} - {ready_node.node.id}: not ready to run.\"\n            )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.shutdown","title":"<code>shutdown(wait=True)</code>","text":"<p>Shuts down the executor.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>Whether to wait for pending futures to complete. Defaults to True.</p> <code>True</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def shutdown(self, wait: bool = True):\n    \"\"\"\n    Shuts down the executor.\n\n    Args:\n        wait (bool, optional): Whether to wait for pending futures to complete. Defaults to True.\n    \"\"\"\n    self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor","title":"<code>ProcessExecutor</code>","text":"<p>               Bases: <code>PoolExecutor</code></p> <p>A process-based pool executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker processes. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class ProcessExecutor(PoolExecutor):\n    \"\"\"\n    A process-based pool executor.\n\n    Args:\n        max_workers (int, optional): The maximum number of worker processes. Defaults to None.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        max_workers = max_workers or MAX_WORKERS_PROCESS_POOL_EXECUTOR\n        super().__init__(\n            pool_executor=futures.ProcessPoolExecutor, max_workers=max_workers\n        )\n\n    @staticmethod\n    def serialize_node(node: Node) -&gt; str:\n        \"\"\"\n        Serializes node data.\n\n        Args:\n            node (Node): Node instance.\n\n        Returns:\n            str: Serialized node data\n        \"\"\"\n        return jsonpickle.encode(node)\n\n    @staticmethod\n    def deserialize_node(node_data: str) -&gt; Node:\n        \"\"\"\n        Deserializes node data.\n\n        Args:\n            node_data (str): Serialized node data.\n\n        Returns:\n            Node: Node instance.\n        \"\"\"\n        return jsonpickle.decode(node_data)  # nosec\n\n    @classmethod\n    def _run_node(cls, node_data: str, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Deserializes node data and runs the node.\n\n        Args:\n            node_data (str): Serialized node data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        node_instance = cls.deserialize_node(node_data)\n        return node_instance.run(**kwargs)\n\n    def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Submits ready node for execution.\n\n        Args:\n            ready_node (NodeReadyToRun): node ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        return self.executor.submit(\n            self._run_node,\n            node_data=self.serialize_node(ready_node.node),\n            input_data=ready_node.input_data,\n            config=config,\n            depends_result=ready_node.depends_result,\n            **kwargs,\n        )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.deserialize_node","title":"<code>deserialize_node(node_data)</code>  <code>staticmethod</code>","text":"<p>Deserializes node data.</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>str</code> <p>Serialized node data.</p> required <p>Returns:</p> Name Type Description <code>Node</code> <code>Node</code> <p>Node instance.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>@staticmethod\ndef deserialize_node(node_data: str) -&gt; Node:\n    \"\"\"\n    Deserializes node data.\n\n    Args:\n        node_data (str): Serialized node data.\n\n    Returns:\n        Node: Node instance.\n    \"\"\"\n    return jsonpickle.decode(node_data)  # nosec\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.run_node","title":"<code>run_node(ready_node, config=None, **kwargs)</code>","text":"<p>Submits ready node for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_node</code> <code>NodeReadyToRun</code> <p>node ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Submits ready node for execution.\n\n    Args:\n        ready_node (NodeReadyToRun): node ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    return self.executor.submit(\n        self._run_node,\n        node_data=self.serialize_node(ready_node.node),\n        input_data=ready_node.input_data,\n        config=config,\n        depends_result=ready_node.depends_result,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.serialize_node","title":"<code>serialize_node(node)</code>  <code>staticmethod</code>","text":"<p>Serializes node data.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>Node instance.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Serialized node data</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>@staticmethod\ndef serialize_node(node: Node) -&gt; str:\n    \"\"\"\n    Serializes node data.\n\n    Args:\n        node (Node): Node instance.\n\n    Returns:\n        str: Serialized node data\n    \"\"\"\n    return jsonpickle.encode(node)\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ThreadExecutor","title":"<code>ThreadExecutor</code>","text":"<p>               Bases: <code>PoolExecutor</code></p> <p>A thread-based pool executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker threads. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class ThreadExecutor(PoolExecutor):\n    \"\"\"\n    A thread-based pool executor.\n\n    Args:\n        max_workers (int, optional): The maximum number of worker threads. Defaults to None.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        max_workers = max_workers or MAX_WORKERS_THREAD_POOL_EXECUTOR\n        super().__init__(pool_executor=ContextAwareThreadPoolExecutor, max_workers=max_workers)\n</code></pre>"},{"location":"dynamiq/flows/base/","title":"Base","text":""},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow","title":"<code>BaseFlow</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code>, <code>ABC</code></p> <p>Base class for flow implementations.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the flow, generated using UUID.</p> Source code in <code>dynamiq/flows/base.py</code> <pre><code>class BaseFlow(BaseModel, Runnable, ABC):\n    \"\"\"\n    Base class for flow implementations.\n\n    Attributes:\n        id (str): Unique identifier for the flow, generated using UUID.\n\n    \"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the BaseFlow instance.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._results = {}\n\n    def reset_run_state(self):\n        \"\"\"Reset the internal run state by clearing the results dictionary.\"\"\"\n        self._results = {}\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        return {}\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n        kwargs.pop(\"for_tracing\", False)\n        data = self.model_dump(\n            exclude=exclude,\n            serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n            **kwargs,\n        )\n\n        return data\n\n    def run_on_flow_start(\n        self, input_data: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when the flow starts.\n\n        Args:\n            input_data (Any): The input data for the flow.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_flow_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n\n    def run_on_flow_end(\n        self, output_data: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when the flow ends.\n\n        Args:\n            output_data (Any): The output data from the flow.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_flow_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n\n    def run_on_flow_error(\n        self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when an error occurs in the flow.\n\n        Args:\n            error (BaseException): The error that occurred during the flow execution.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_flow_error(self.to_dict(**dict_kwargs), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BaseFlow instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent constructor.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the BaseFlow instance.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._results = {}\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the internal run state by clearing the results dictionary.</p> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Reset the internal run state by clearing the results dictionary.\"\"\"\n    self._results = {}\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_end","title":"<code>run_on_flow_end(output_data, config=None, **kwargs)</code>","text":"<p>Execute callbacks when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>output_data</code> <code>Any</code> <p>The output data from the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_end(\n    self, output_data: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when the flow ends.\n\n    Args:\n        output_data (Any): The output data from the flow.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_flow_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_error","title":"<code>run_on_flow_error(error, config=None, **kwargs)</code>","text":"<p>Execute callbacks when an error occurs in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred during the flow execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_error(\n    self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when an error occurs in the flow.\n\n    Args:\n        error (BaseException): The error that occurred during the flow execution.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_flow_error(self.to_dict(**dict_kwargs), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_start","title":"<code>run_on_flow_start(input_data, config=None, **kwargs)</code>","text":"<p>Execute callbacks when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data for the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_start(\n    self, input_data: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when the flow starts.\n\n    Args:\n        input_data (Any): The input data for the flow.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_flow_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n    kwargs.pop(\"for_tracing\", False)\n    data = self.model_dump(\n        exclude=exclude,\n        serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n        **kwargs,\n    )\n\n    return data\n</code></pre>"},{"location":"dynamiq/flows/flow/","title":"Flow","text":""},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow","title":"<code>Flow</code>","text":"<p>               Bases: <code>BaseFlow</code></p> <p>A class for managing and executing a graph-like structure of nodes.</p> <p>Attributes:</p> Name Type Description <code>nodes</code> <code>list[Node]</code> <p>List of nodes in the flow.</p> <code>executor</code> <code>type[BaseExecutor]</code> <p>Executor class for running nodes. Defaults to ThreadExecutor.</p> <code>max_node_workers</code> <code>int | None</code> <p>Maximum number of concurrent node workers. Defaults to None.</p> <code>connection_manager</code> <code>ConnectionManager</code> <p>Manager for handling connections. Defaults to ConnectionManager().</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>class Flow(BaseFlow):\n    \"\"\"\n    A class for managing and executing a graph-like structure of nodes.\n\n    Attributes:\n        nodes (list[Node]): List of nodes in the flow.\n        executor (type[BaseExecutor]): Executor class for running nodes. Defaults to ThreadExecutor.\n        max_node_workers (int | None): Maximum number of concurrent node workers. Defaults to None.\n        connection_manager (ConnectionManager): Manager for handling connections. Defaults to ConnectionManager().\n    \"\"\"\n\n    name: str = \"Flow\"\n    nodes: list[Node] = []\n    executor: type[BaseExecutor] = ThreadExecutor\n    max_node_workers: int | None = None\n    connection_manager: ConnectionManager = Field(default_factory=ConnectionManager)\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the Flow instance.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._node_by_id = {node.id: node for node in self.nodes}\n        self._ts = None\n\n        self._init_components()\n        self.reset_run_state()\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @property\n    def to_dict_exclude_params(self):\n        return {\"nodes\": True, \"connection_manager\": True}\n\n    def to_dict(self, include_secure_params: bool = True, for_tracing=False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(include_secure_params=include_secure_params, **kwargs)\n        data[\"nodes\"] = [\n            node.to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n            for node in self.nodes\n        ]\n        return data\n\n    @field_validator(\"nodes\")\n    @classmethod\n    def validate_nodes(cls, nodes: list[Node]) -&gt; list[Node]:\n        \"\"\"\n        Validates the list of nodes in the flow.\n\n        Args:\n            nodes (list[Node]): List of nodes to validate.\n\n        Returns:\n            list[Node]: Validated list of nodes.\n\n        Raises:\n            ValueError: If there are duplicate node IDs or invalid dependencies.\n        \"\"\"\n        nodes_ids_unique = set()\n        nodes_deps_ids_unique = set()\n        for node in nodes:\n            if node.id in nodes_ids_unique:\n                raise ValueError(\n                    f\"Flow has nodes with duplicated ids: '{node.id}'. Node ids must be unique.\"\n                )\n\n            nodes_ids_unique.add(node.id)\n            node_deps_ids = [dep.node.id for dep in node.depends]\n            if len(set(node_deps_ids)) != len(node_deps_ids):\n                raise ValueError(\n                    f\"Flow node '{node.id}' has duplicated dependency ids. Node dependencies ids must be unique.\"\n                )\n\n            nodes_deps_ids_unique.update(node_deps_ids)\n\n        if not nodes_deps_ids_unique.issubset(nodes_ids_unique):\n            raise ValueError(\n                \"Flow nodes have dependencies that are not present in the flow.\"\n            )\n\n        return nodes\n\n    def _init_components(self):\n        \"\"\"Initializes components for nodes with postponed initialization.\"\"\"\n        for node in self.nodes:\n            if node.is_postponed_component_init:\n                node.init_components(self.connection_manager)\n\n    def _get_nodes_ready_to_run(self, input_data: Any) -&gt; list[NodeReadyToRun]:\n        \"\"\"\n        Gets the list of nodes that are ready to run.\n\n        Args:\n            input_data (Any): Input data for the nodes.\n\n        Returns:\n            list[NodeReadyToRun]: List of nodes ready to run.\n        \"\"\"\n        ready_ts_nodes = self._ts.get_ready()\n        ready_nodes = []\n\n        completed_result = {\n            node_id: result for node_id, result in self._results.items() if result.status != RunnableStatus.UNDEFINED\n        }\n\n        for node_id in ready_ts_nodes:\n            node = self._node_by_id[node_id]\n            is_ready = True\n            for dep in node.depends:\n                if dep.node.id not in completed_result:\n                    is_ready = False\n                    break\n\n            ready_node = NodeReadyToRun(\n                node=node,\n                is_ready=is_ready,\n                input_data=input_data,\n                depends_result=completed_result,\n            )\n            ready_nodes.append(ready_node)\n\n        return ready_nodes\n\n    def _get_output(self) -&gt; dict[str, dict]:\n        \"\"\"\n        Gets the output of the flow.\n\n        Returns:\n            dict[str, dict]: Output of the flow.\n        \"\"\"\n        return {\n            node_id: result.to_dict(skip_format_types={BytesIO, bytes}) for node_id, result in self._results.items()\n        }\n\n    def _get_failed_nodes_with_raise_behavior(self) -&gt; list[RunnableFailedNodeInfo]:\n        \"\"\"\n        Gets the list of nodes that failed with RAISE error behavior.\n\n        Returns:\n            list[FailedNodeInfo]: List of failed node information.\n        \"\"\"\n        failed_nodes: list[RunnableFailedNodeInfo] = []\n        for node_id, result in self._results.items():\n            node = self._node_by_id.get(node_id)\n            if node and result.status == RunnableStatus.FAILURE and node.error_handling.behavior == Behavior.RAISE:\n                error_message = result.error.message if result.error else None\n                failed_nodes.append(RunnableFailedNodeInfo(id=node_id, name=node.name, error_message=error_message))\n        return failed_nodes\n\n    @staticmethod\n    def init_node_topological_sorter(nodes: list[Node]):\n        \"\"\"\n        Initializes a topological sorter for the given nodes.\n\n        Args:\n            nodes (list[Node]): List of nodes to sort.\n\n        Returns:\n            TopologicalSorter: Initialized topological sorter.\n\n        Raises:\n            CycleError: If a cycle is detected in node dependencies.\n        \"\"\"\n        topological_sorter = TopologicalSorter()\n        for node in nodes:\n            topological_sorter.add(node.id, *[d.node.id for d in node.depends])\n\n        try:\n            topological_sorter.prepare()\n        except CycleError as e:\n            logger.error(f\"Node dependencies cycle detected. Error: {e}\")\n            raise\n\n        return topological_sorter\n\n    def reset_run_state(self):\n        \"\"\"Resets the run state of the flow.\"\"\"\n        self._results = {\n            node.id: RunnableResult(status=RunnableStatus.UNDEFINED)\n            for node in self.nodes\n        }\n        self._ts = self.init_node_topological_sorter(nodes=self.nodes)\n\n    def _cleanup_dry_run(self, config: RunnableConfig = None):\n        \"\"\"\n        Clean up resources created during dry run.\n\n        Args:\n            config (RunnableConfig, optional): Configuration for the run.\n        \"\"\"\n        if not config or not getattr(config.dry_run, \"enabled\", False):\n            return\n\n        logger.debug(\"Starting dry run cleanup...\")\n\n        # Filter nodes that have dry_run_cleanup implemented\n        nodes_with_cleanup = [\n            node\n            for node in self.nodes\n            if hasattr(node, \"dry_run_cleanup\")\n            and getattr(node, \"dry_run_cleanup\").__qualname__ != \"Node.dry_run_cleanup\"\n        ]\n        logger.debug(f\"Nodes with cleanup: {[node.name for node in nodes_with_cleanup]}\")\n\n        for node in nodes_with_cleanup:\n            try:\n                node.dry_run_cleanup(config.dry_run)\n            except Exception as e:\n                logger.error(f\"Failed to clean up dry run resources for node {node.id}: {str(e)}\")\n\n    async def _cleanup_dry_run_async(self, config: RunnableConfig = None):\n        \"\"\"Async variant of dry-run cleanup. Runs synchronous cleanup functions in a thread.\"\"\"\n        if not config or not getattr(config.dry_run, \"enabled\", False):\n            return\n\n        logger.debug(\"Starting async dry run cleanup...\")\n\n        # Filter nodes that have dry_run_cleanup implemented\n        nodes_with_cleanup = [\n            node\n            for node in self.nodes\n            if hasattr(node, \"dry_run_cleanup\")\n            and getattr(node, \"dry_run_cleanup\").__qualname__ != \"Node.dry_run_cleanup\"\n        ]\n        logger.debug(f\"Nodes with cleanup: {[node.name for node in nodes_with_cleanup]}\")\n\n        tasks = [asyncio.to_thread(getattr(node, \"dry_run_cleanup\"), config.dry_run) for node in nodes_with_cleanup]\n\n        if not tasks:\n            return\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        for node, res in zip(nodes_with_cleanup, results):\n            if isinstance(res, Exception):\n                logger.error(f\"Failed to clean up dry run resources for node {node.id}: {res}\")\n\n    def run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Run the flow synchronously with the given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the flow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the flow execution.\n        \"\"\"\n        self.reset_run_state()\n        run_id = uuid4()\n        merged_kwargs = kwargs | {\n            \"run_id\": run_id,\n            \"parent_run_id\": kwargs.get(\"parent_run_id\", None),\n        }\n\n        logger.info(f\"Flow {self.id}: execution started.\")\n        self.run_on_flow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        try:\n            if self.nodes:\n                max_workers = (\n                    config.max_node_workers if config else self.max_node_workers\n                )\n                run_executor = self.executor(max_workers=max_workers)\n\n                while self._ts.is_active():\n                    ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                    results = run_executor.execute(\n                        ready_nodes=ready_nodes,\n                        config=config,\n                        **(merged_kwargs | {\"parent_run_id\": run_id}),\n                    )\n                    self._results.update(results)\n                    self._ts.done(*results.keys())\n\n                    # Wait for ready nodes to be processed and reduce CPU usage\n                    time.sleep(0.003)\n\n                run_executor.shutdown()\n\n            output = self._get_output()\n            failed_nodes = self._get_failed_nodes_with_raise_behavior()\n\n            if failed_nodes:\n                failed_names = [node.name or node.id for node in failed_nodes]\n                error_msg = f\"Flow execution failed due to node failures: {', '.join(failed_names)}\"\n                error = FlowNodeFailureException(error_msg, failed_nodes)\n                self.run_on_flow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n                logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n                return RunnableResult(\n                    status=RunnableStatus.FAILURE,\n                    input=input_data,\n                    output=output,\n                    error=RunnableResultError.from_exception(error, failed_nodes=failed_nodes),\n                )\n\n            self.run_on_flow_end(output, config, **merged_kwargs)\n            logger.info(\n                f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n            )\n            return RunnableResult(\n                status=RunnableStatus.SUCCESS, input=input_data, output=output\n            )\n        except Exception as e:\n            failed_nodes = self._get_failed_nodes_with_raise_behavior()\n            self.run_on_flow_error(e, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(f\"Flow {self.id}: execution failed in \" f\"{format_duration(time_start, datetime.now())}.\")\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n                error=RunnableResultError.from_exception(e, failed_nodes=failed_nodes),\n            )\n        finally:\n            self._cleanup_dry_run(config)\n\n    async def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Run the flow asynchronously with the given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the flow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the flow execution.\n        \"\"\"\n        self.reset_run_state()\n        run_id = uuid4()\n        merged_kwargs = kwargs | {\n            \"run_id\": run_id,\n            \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id),\n        }\n\n        logger.info(f\"Flow {self.id}: execution started.\")\n        self.run_on_flow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        try:\n            if self.nodes:\n                while self._ts.is_active():\n                    ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                    nodes_to_run = [node for node in ready_nodes if node.is_ready]\n\n                    if nodes_to_run:\n                        tasks = [\n                            node.node.run_async(\n                                input_data=node.input_data,\n                                depends_result=node.depends_result,\n                                config=config,\n                                **(merged_kwargs | {\"parent_run_id\": run_id}),\n                            )\n                            for node in nodes_to_run\n                        ]\n\n                        results_list = await asyncio.gather(*tasks)\n\n                        results = {node.node.id: result for node, result in zip(nodes_to_run, results_list)}\n\n                        self._results.update(results)\n                        self._ts.done(*results.keys())\n\n                    # Wait for ready nodes to be processed and reduce CPU usage by yielding control to the event loop\n                    await asyncio.sleep(0.003)\n\n            output = self._get_output()\n            failed_nodes = self._get_failed_nodes_with_raise_behavior()\n\n            if failed_nodes:\n                failed_names = [node.name or node.id for node in failed_nodes]\n                error_msg = f\"Flow execution failed due to node failures: {', '.join(failed_names)}\"\n                error = FlowNodeFailureException(error_msg, failed_nodes)\n                self.run_on_flow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n                logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n                return RunnableResult(\n                    status=RunnableStatus.FAILURE,\n                    input=input_data,\n                    output=output,\n                    error=RunnableResultError.from_exception(error, failed_nodes=failed_nodes),\n                )\n\n            self.run_on_flow_end(output, config, **merged_kwargs)\n            logger.info(f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\")\n            return RunnableResult(status=RunnableStatus.SUCCESS, input=input_data, output=output)\n        except Exception as e:\n            failed_nodes = self._get_failed_nodes_with_raise_behavior()\n            self.run_on_flow_error(e, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n                error=RunnableResultError.from_exception(e, failed_nodes=failed_nodes),\n            )\n        finally:\n            try:\n                await self._cleanup_dry_run_async(config)\n            except Exception as e:\n                logger.error(f\"Async dry-run cleanup failed: {e}\")\n\n    def get_dependant_nodes(\n        self, nodes_types_to_skip: set[str] | None = None\n    ) -&gt; list[Node]:\n        \"\"\"\n        Gets the list of dependent nodes in the flow.\n\n        Args:\n            nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n        Returns:\n            list[Node]: List of dependent nodes.\n        \"\"\"\n        if not nodes_types_to_skip:\n            nodes_types_to_skip = set()\n\n        return [\n            dep.node\n            for node in self.nodes\n            if node.type not in nodes_types_to_skip\n            for dep in node.depends\n        ]\n\n    def get_non_dependant_nodes(\n        self, nodes_types_to_skip: set[str] | None = None\n    ) -&gt; list[Node]:\n        \"\"\"\n        Gets the list of non-dependent nodes in the flow.\n\n        Args:\n            nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n        Returns:\n            list[Node]: List of non-dependent nodes.\n        \"\"\"\n        if not nodes_types_to_skip:\n            nodes_types_to_skip = set()\n\n        dependant_nodes = self.get_dependant_nodes(\n            nodes_types_to_skip=nodes_types_to_skip\n        )\n        return [\n            node\n            for node in self.nodes\n            if node.type not in nodes_types_to_skip and node not in dependant_nodes\n        ]\n\n    def add_nodes(self, nodes: Node | list[Node]):\n        \"\"\"\n        Add one or more nodes to the flow.\n\n        Args:\n            nodes (Node or list[Node]): Node(s) to add to the flow.\n\n        Raises:\n            TypeError: If 'nodes' is not a Node or a list of Node.\n            ValueError: If 'nodes' is an empty list, if a node with the same id already exists in the flow,\n                        or if there are duplicate node ids in the input list.\n        \"\"\"\n\n        if nodes is None:\n            raise ValueError(\"No node provided. Nodes cannot be None.\")\n\n        # Convert a single Node to a list for consistent handling\n        if isinstance(nodes, Node):\n            nodes = [nodes]\n\n        # Check if it's a valid list of nodes\n        if not isinstance(nodes, list) or not all(isinstance(n, Node) for n in nodes):\n            raise TypeError(\"Nodes must be a Node instance or a list of Node instances.\")\n\n        if not nodes:\n            raise ValueError(\"Cannot add an empty list of nodes to the flow.\")\n\n        # Add nodes to the flow, checking for duplicates in the flow\n        for node in nodes:\n            if node.id in self._node_by_id:\n                raise ValueError(f\"Node with id {node.id} already exists in the flow.\")\n\n            self.nodes.append(node)\n            self._node_by_id[node.id] = node\n            if node.is_postponed_component_init:\n                node.init_components(self.connection_manager)\n        self.reset_run_state()\n\n        return self  # enable chaining\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the Flow instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the Flow instance.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._node_by_id = {node.id: node for node in self.nodes}\n    self._ts = None\n\n    self._init_components()\n    self.reset_run_state()\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Add one or more nodes to the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Node or list[Node]</code> <p>Node(s) to add to the flow.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'nodes' is not a Node or a list of Node.</p> <code>ValueError</code> <p>If 'nodes' is an empty list, if a node with the same id already exists in the flow,         or if there are duplicate node ids in the input list.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def add_nodes(self, nodes: Node | list[Node]):\n    \"\"\"\n    Add one or more nodes to the flow.\n\n    Args:\n        nodes (Node or list[Node]): Node(s) to add to the flow.\n\n    Raises:\n        TypeError: If 'nodes' is not a Node or a list of Node.\n        ValueError: If 'nodes' is an empty list, if a node with the same id already exists in the flow,\n                    or if there are duplicate node ids in the input list.\n    \"\"\"\n\n    if nodes is None:\n        raise ValueError(\"No node provided. Nodes cannot be None.\")\n\n    # Convert a single Node to a list for consistent handling\n    if isinstance(nodes, Node):\n        nodes = [nodes]\n\n    # Check if it's a valid list of nodes\n    if not isinstance(nodes, list) or not all(isinstance(n, Node) for n in nodes):\n        raise TypeError(\"Nodes must be a Node instance or a list of Node instances.\")\n\n    if not nodes:\n        raise ValueError(\"Cannot add an empty list of nodes to the flow.\")\n\n    # Add nodes to the flow, checking for duplicates in the flow\n    for node in nodes:\n        if node.id in self._node_by_id:\n            raise ValueError(f\"Node with id {node.id} already exists in the flow.\")\n\n        self.nodes.append(node)\n        self._node_by_id[node.id] = node\n        if node.is_postponed_component_init:\n            node.init_components(self.connection_manager)\n    self.reset_run_state()\n\n    return self  # enable chaining\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.get_dependant_nodes","title":"<code>get_dependant_nodes(nodes_types_to_skip=None)</code>","text":"<p>Gets the list of dependent nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_types_to_skip</code> <code>set[NodeType] | None</code> <p>Set of node types to skip. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: List of dependent nodes.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def get_dependant_nodes(\n    self, nodes_types_to_skip: set[str] | None = None\n) -&gt; list[Node]:\n    \"\"\"\n    Gets the list of dependent nodes in the flow.\n\n    Args:\n        nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n    Returns:\n        list[Node]: List of dependent nodes.\n    \"\"\"\n    if not nodes_types_to_skip:\n        nodes_types_to_skip = set()\n\n    return [\n        dep.node\n        for node in self.nodes\n        if node.type not in nodes_types_to_skip\n        for dep in node.depends\n    ]\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.get_non_dependant_nodes","title":"<code>get_non_dependant_nodes(nodes_types_to_skip=None)</code>","text":"<p>Gets the list of non-dependent nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_types_to_skip</code> <code>set[NodeType] | None</code> <p>Set of node types to skip. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: List of non-dependent nodes.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def get_non_dependant_nodes(\n    self, nodes_types_to_skip: set[str] | None = None\n) -&gt; list[Node]:\n    \"\"\"\n    Gets the list of non-dependent nodes in the flow.\n\n    Args:\n        nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n    Returns:\n        list[Node]: List of non-dependent nodes.\n    \"\"\"\n    if not nodes_types_to_skip:\n        nodes_types_to_skip = set()\n\n    dependant_nodes = self.get_dependant_nodes(\n        nodes_types_to_skip=nodes_types_to_skip\n    )\n    return [\n        node\n        for node in self.nodes\n        if node.type not in nodes_types_to_skip and node not in dependant_nodes\n    ]\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.init_node_topological_sorter","title":"<code>init_node_topological_sorter(nodes)</code>  <code>staticmethod</code>","text":"<p>Initializes a topological sorter for the given nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Node]</code> <p>List of nodes to sort.</p> required <p>Returns:</p> Name Type Description <code>TopologicalSorter</code> <p>Initialized topological sorter.</p> <p>Raises:</p> Type Description <code>CycleError</code> <p>If a cycle is detected in node dependencies.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>@staticmethod\ndef init_node_topological_sorter(nodes: list[Node]):\n    \"\"\"\n    Initializes a topological sorter for the given nodes.\n\n    Args:\n        nodes (list[Node]): List of nodes to sort.\n\n    Returns:\n        TopologicalSorter: Initialized topological sorter.\n\n    Raises:\n        CycleError: If a cycle is detected in node dependencies.\n    \"\"\"\n    topological_sorter = TopologicalSorter()\n    for node in nodes:\n        topological_sorter.add(node.id, *[d.node.id for d in node.depends])\n\n    try:\n        topological_sorter.prepare()\n    except CycleError as e:\n        logger.error(f\"Node dependencies cycle detected. Error: {e}\")\n        raise\n\n    return topological_sorter\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Resets the run state of the flow.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Resets the run state of the flow.\"\"\"\n    self._results = {\n        node.id: RunnableResult(status=RunnableStatus.UNDEFINED)\n        for node in self.nodes\n    }\n    self._ts = self.init_node_topological_sorter(nodes=self.nodes)\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.run_async","title":"<code>run_async(input_data, config=None, **kwargs)</code>  <code>async</code>","text":"<p>Run the flow asynchronously with the given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the flow execution.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>async def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"\n    Run the flow asynchronously with the given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the flow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the flow execution.\n    \"\"\"\n    self.reset_run_state()\n    run_id = uuid4()\n    merged_kwargs = kwargs | {\n        \"run_id\": run_id,\n        \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id),\n    }\n\n    logger.info(f\"Flow {self.id}: execution started.\")\n    self.run_on_flow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    try:\n        if self.nodes:\n            while self._ts.is_active():\n                ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                nodes_to_run = [node for node in ready_nodes if node.is_ready]\n\n                if nodes_to_run:\n                    tasks = [\n                        node.node.run_async(\n                            input_data=node.input_data,\n                            depends_result=node.depends_result,\n                            config=config,\n                            **(merged_kwargs | {\"parent_run_id\": run_id}),\n                        )\n                        for node in nodes_to_run\n                    ]\n\n                    results_list = await asyncio.gather(*tasks)\n\n                    results = {node.node.id: result for node, result in zip(nodes_to_run, results_list)}\n\n                    self._results.update(results)\n                    self._ts.done(*results.keys())\n\n                # Wait for ready nodes to be processed and reduce CPU usage by yielding control to the event loop\n                await asyncio.sleep(0.003)\n\n        output = self._get_output()\n        failed_nodes = self._get_failed_nodes_with_raise_behavior()\n\n        if failed_nodes:\n            failed_names = [node.name or node.id for node in failed_nodes]\n            error_msg = f\"Flow execution failed due to node failures: {', '.join(failed_names)}\"\n            error = FlowNodeFailureException(error_msg, failed_nodes)\n            self.run_on_flow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n                output=output,\n                error=RunnableResultError.from_exception(error, failed_nodes=failed_nodes),\n            )\n\n        self.run_on_flow_end(output, config, **merged_kwargs)\n        logger.info(f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\")\n        return RunnableResult(status=RunnableStatus.SUCCESS, input=input_data, output=output)\n    except Exception as e:\n        failed_nodes = self._get_failed_nodes_with_raise_behavior()\n        self.run_on_flow_error(e, config, failed_nodes=failed_nodes, **merged_kwargs)\n        logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n        return RunnableResult(\n            status=RunnableStatus.FAILURE,\n            input=input_data,\n            error=RunnableResultError.from_exception(e, failed_nodes=failed_nodes),\n        )\n    finally:\n        try:\n            await self._cleanup_dry_run_async(config)\n        except Exception as e:\n            logger.error(f\"Async dry-run cleanup failed: {e}\")\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.run_sync","title":"<code>run_sync(input_data, config=None, **kwargs)</code>","text":"<p>Run the flow synchronously with the given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the flow execution.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"\n    Run the flow synchronously with the given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the flow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the flow execution.\n    \"\"\"\n    self.reset_run_state()\n    run_id = uuid4()\n    merged_kwargs = kwargs | {\n        \"run_id\": run_id,\n        \"parent_run_id\": kwargs.get(\"parent_run_id\", None),\n    }\n\n    logger.info(f\"Flow {self.id}: execution started.\")\n    self.run_on_flow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    try:\n        if self.nodes:\n            max_workers = (\n                config.max_node_workers if config else self.max_node_workers\n            )\n            run_executor = self.executor(max_workers=max_workers)\n\n            while self._ts.is_active():\n                ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                results = run_executor.execute(\n                    ready_nodes=ready_nodes,\n                    config=config,\n                    **(merged_kwargs | {\"parent_run_id\": run_id}),\n                )\n                self._results.update(results)\n                self._ts.done(*results.keys())\n\n                # Wait for ready nodes to be processed and reduce CPU usage\n                time.sleep(0.003)\n\n            run_executor.shutdown()\n\n        output = self._get_output()\n        failed_nodes = self._get_failed_nodes_with_raise_behavior()\n\n        if failed_nodes:\n            failed_names = [node.name or node.id for node in failed_nodes]\n            error_msg = f\"Flow execution failed due to node failures: {', '.join(failed_names)}\"\n            error = FlowNodeFailureException(error_msg, failed_nodes)\n            self.run_on_flow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(f\"Flow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n                output=output,\n                error=RunnableResultError.from_exception(error, failed_nodes=failed_nodes),\n            )\n\n        self.run_on_flow_end(output, config, **merged_kwargs)\n        logger.info(\n            f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n        )\n        return RunnableResult(\n            status=RunnableStatus.SUCCESS, input=input_data, output=output\n        )\n    except Exception as e:\n        failed_nodes = self._get_failed_nodes_with_raise_behavior()\n        self.run_on_flow_error(e, config, failed_nodes=failed_nodes, **merged_kwargs)\n        logger.error(f\"Flow {self.id}: execution failed in \" f\"{format_duration(time_start, datetime.now())}.\")\n        return RunnableResult(\n            status=RunnableStatus.FAILURE,\n            input=input_data,\n            error=RunnableResultError.from_exception(e, failed_nodes=failed_nodes),\n        )\n    finally:\n        self._cleanup_dry_run(config)\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.to_dict","title":"<code>to_dict(include_secure_params=True, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def to_dict(self, include_secure_params: bool = True, for_tracing=False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(include_secure_params=include_secure_params, **kwargs)\n    data[\"nodes\"] = [\n        node.to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n        for node in self.nodes\n    ]\n    return data\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.validate_nodes","title":"<code>validate_nodes(nodes)</code>  <code>classmethod</code>","text":"<p>Validates the list of nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Node]</code> <p>List of nodes to validate.</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: Validated list of nodes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are duplicate node IDs or invalid dependencies.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>@field_validator(\"nodes\")\n@classmethod\ndef validate_nodes(cls, nodes: list[Node]) -&gt; list[Node]:\n    \"\"\"\n    Validates the list of nodes in the flow.\n\n    Args:\n        nodes (list[Node]): List of nodes to validate.\n\n    Returns:\n        list[Node]: Validated list of nodes.\n\n    Raises:\n        ValueError: If there are duplicate node IDs or invalid dependencies.\n    \"\"\"\n    nodes_ids_unique = set()\n    nodes_deps_ids_unique = set()\n    for node in nodes:\n        if node.id in nodes_ids_unique:\n            raise ValueError(\n                f\"Flow has nodes with duplicated ids: '{node.id}'. Node ids must be unique.\"\n            )\n\n        nodes_ids_unique.add(node.id)\n        node_deps_ids = [dep.node.id for dep in node.depends]\n        if len(set(node_deps_ids)) != len(node_deps_ids):\n            raise ValueError(\n                f\"Flow node '{node.id}' has duplicated dependency ids. Node dependencies ids must be unique.\"\n            )\n\n        nodes_deps_ids_unique.update(node_deps_ids)\n\n    if not nodes_deps_ids_unique.issubset(nodes_ids_unique):\n        raise ValueError(\n            \"Flow nodes have dependencies that are not present in the flow.\"\n        )\n\n    return nodes\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.FlowNodeFailureException","title":"<code>FlowNodeFailureException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when one or more nodes with RAISE behavior failed during flow execution.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>class FlowNodeFailureException(Exception):\n    \"\"\"Exception raised when one or more nodes with RAISE behavior failed during flow execution.\"\"\"\n\n    def __init__(self, message: str, failed_nodes: list[RunnableFailedNodeInfo] | None = None):\n        super().__init__(message)\n        self.failed_nodes = failed_nodes or []\n</code></pre>"},{"location":"dynamiq/memory/memory/","title":"Memory","text":""},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.FormatType","title":"<code>FormatType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for message format types.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>class FormatType(str, Enum):\n    \"\"\"Enum for message format types.\"\"\"\n    PLAIN = \"plain\"\n    MARKDOWN = \"markdown\"\n    XML = \"xml\"\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory","title":"<code>Memory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Manages the storage and retrieval of messages.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"Manages the storage and retrieval of messages.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    DEFAULT_LIMIT: ClassVar[int] = 1000\n\n    message_limit: int = Field(default=DEFAULT_LIMIT, gt=0, description=\"Default limit for message retrieval\")\n    backend: MemoryBackend = Field(default_factory=InMemory, description=\"Backend storage implementation\")\n    filters: dict[str, Any] = Field(default_factory=dict, description=\"Default filters to apply to searches\")\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {\"backend\": True}\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        data = self.model_dump(exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params), **kwargs)\n        data[\"backend\"] = self.backend.to_dict(\n            include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n        )\n        return data\n\n    def add(self, role: MessageRole, content: str, metadata: dict[str, Any] | None = None) -&gt; None:\n        \"\"\"\n        Adds a message to the memory.\n\n        Args:\n            role: The role of the message sender\n            content: The message content\n            metadata: Additional metadata for the message\n\n        Raises:\n            MemoryError: If the message cannot be added\n        \"\"\"\n        try:\n            metadata = metadata or {}\n            if \"timestamp\" not in metadata:\n                metadata[\"timestamp\"] = datetime.now(timezone.utc).timestamp()\n\n            sanitized_metadata = {}\n            for key, value in metadata.items():\n                sanitized_metadata[key] = \"\" if value is None else value\n\n            message = Message(role=role, content=content, metadata=sanitized_metadata)\n            self.backend.add(message)\n\n            logger.debug(\n                f\"Memory {self.backend.name}: \"\n                f\"Added message: {message.role}: {message.content[:min(20, len(message.content))]}...\"\n            )\n        except Exception as e:\n            logger.error(f\"Unexpected error adding message: {e}\")\n            raise MemoryError(f\"Unexpected error adding message: {e}\") from e\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"\n        Retrieves all messages from the memory, optionally limited to most recent.\n\n        Args:\n            limit: Maximum number of messages to return. If provided, returns the most recent messages.\n                  If None, uses the configured message_limit.\n\n        Returns:\n            List of messages sorted by timestamp (oldest first)\n\n        Raises:\n            MemoryError: If messages cannot be retrieved\n        \"\"\"\n        try:\n            effective_limit = limit if limit is not None else self.message_limit\n\n            messages = self.backend.get_all(limit=effective_limit)\n            retrieved_messages = [Message(**msg.model_dump()) for msg in messages]\n            logger.debug(f\"Memory {self.backend.name}: Retrieved {len(retrieved_messages)} messages\")\n            return retrieved_messages\n        except Exception as e:\n            logger.error(f\"Unexpected error retrieving messages: {e}\")\n            raise MemoryError(f\"Unexpected error retrieving messages: {e}\") from e\n\n    def get_all_messages_as_string(self, format_type: FormatType = FormatType.PLAIN) -&gt; str:\n        \"\"\"\n        Retrieves all messages as a formatted string.\n\n        Args:\n            format_type: Format to use for the string output\n\n        Returns:\n            Formatted string representation of all messages\n\n        Raises:\n            MemoryError: If messages cannot be retrieved or formatted\n        \"\"\"\n        messages = self.get_all()\n        return self._format_messages_as_string(messages=messages, format_type=format_type)\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"\n        Searches for messages relevant to the query or filters.\n\n        Args:\n            query: Search query string (optional)\n            filters: Optional metadata filters to apply (overrides default filters)\n            limit: Maximum number of messages to return (defaults to message_limit)\n\n        Returns:\n            List of matching messages sorted by relevance\n\n        Raises:\n            MemoryError: If search operation fails\n        \"\"\"\n        try:\n            effective_filters = self.filters.copy()\n            if filters:\n                effective_filters.update(filters)\n\n            effective_limit = limit if limit is not None else self.message_limit\n\n            results = self.backend.search(query=query, filters=effective_filters, limit=effective_limit)\n\n            retrieved_messages = [Message(**msg.model_dump()) for msg in results]\n            logger.debug(\n                f\"Memory {self.backend.name}: Found {len(retrieved_messages)} search results for query: {query}, \"\n                f\"filters: {effective_filters}\"\n            )\n            return retrieved_messages\n        except Exception as e:\n            logger.error(f\"Unexpected error searching memory: {e}\")\n            raise MemoryError(f\"Unexpected error searching memory: {e}\") from e\n\n    def get_search_results_as_string(\n        self, query: str, filters: dict[str, Any] | None = None, format_type: FormatType = FormatType.PLAIN\n    ) -&gt; str:\n        \"\"\"\n        Searches for messages relevant to the query and returns them as a string.\n\n        Args:\n            query: Search query string\n            filters: Optional metadata filters to apply\n            format_type: Format to use for the string output\n\n        Returns:\n            Formatted string representation of search results\n\n        Raises:\n            MemoryError: If search operation fails or results cannot be formatted\n        \"\"\"\n        messages = self.search(query, filters)\n        return self._format_messages_as_string(messages=messages, format_type=format_type)\n\n    def _format_messages_as_string(self, messages: list[Message], format_type: FormatType = FormatType.PLAIN) -&gt; str:\n        \"\"\"\n        Converts a list of messages to a formatted string.\n\n        Args:\n            messages: List of messages to format\n            format_type: Format to use for the string output\n\n        Returns:\n            Formatted string representation of messages\n\n        Raises:\n            ValueError: If an unsupported format type is provided\n        \"\"\"\n        if format_type == FormatType.PLAIN:\n            return \"\\n\".join([f\"{msg.role.value}: {msg.content}\" for msg in messages])\n\n        elif format_type == FormatType.MARKDOWN:\n            return \"\\n\".join([f\"**{msg.role.value}:** {msg.content}\" for msg in messages])\n\n        elif format_type == FormatType.XML:\n            return \"\\n\".join(\n                [\n                    \"&lt;messages&gt;\",\n                    *[\n                        \"\\n\".join(\n                            [\n                                \"  &lt;message&gt;\",\n                                f\"    &lt;role&gt;{msg.role.value}&lt;/role&gt;\",\n                                f\"    &lt;content&gt;{msg.content}&lt;/content&gt;\",\n                                \"  &lt;/message&gt;\",\n                            ]\n                        )\n                        for msg in messages\n                    ],\n                    \"&lt;/messages&gt;\",\n                ]\n            )\n\n        raise ValueError(f\"Unsupported format: {format_type}\")\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"\n        Checks if the memory is empty.\n\n        Returns:\n            True if the memory is empty, False otherwise\n\n        Raises:\n            MemoryError: If the check fails\n        \"\"\"\n        try:\n            return self.backend.is_empty()\n        except Exception as e:\n            logger.error(f\"Unexpected error checking if memory is empty: {e}\")\n            raise MemoryError(f\"Unexpected error checking if memory is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clears the memory.\n\n        Raises:\n            MemoryError: If the memory cannot be cleared\n        \"\"\"\n        try:\n            self.backend.clear()\n            logger.debug(f\"Memory {self.backend.name}: Cleared memory\")\n        except Exception as e:\n            logger.error(f\"Unexpected error clearing memory: {e}\")\n            raise MemoryError(f\"Unexpected error clearing memory: {e}\") from e\n\n    def get_agent_conversation(\n        self,\n        query: str | None = None,\n        limit: int | None = None,\n        filters: dict[str, Any] | None = None,\n        strategy: MemoryRetrievalStrategy = MemoryRetrievalStrategy.ALL,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Retrieves messages from an agent's conversation history based on various filtering criteria.\n\n        This method supports three retrieval strategies:\n        - ALL: Returns the most recent messages without semantic search\n        - RELEVANT: Returns only messages relevant to the query using semantic search\n        - BOTH: Returns a combination of recent messages and those relevant to the query\n\n        Parameters:\n        -----------\n        query : str | None, optional\n            The search query to filter messages by relevance. Required for RELEVANT and BOTH strategies.\n            Ignored when strategy is ALL. Default is None.\n\n        limit : int | None, optional\n            Maximum number of messages to return. If None, falls back to self.message_limit.\n\n        filters : dict[str, Any] | None, optional\n            Additional metadata filters to apply to the search results.\n\n        strategy : MemoryRetrievalStrategy, optional\n            The strategy to use for retrieving messages. Choices are:\n            - MemoryRetrievalStrategy.ALL: Return most recent messages\n            - MemoryRetrievalStrategy.RELEVANT: Return messages relevant to query\n            - MemoryRetrievalStrategy.BOTH: Return both recent and relevant messages\n            Default is MemoryRetrievalStrategy.ALL.\n\n        Returns:\n        --------\n        list[Message]\n            A list of conversation messages matching the search criteria, ordered chronologically.\n\n        Raises:\n        -------\n        MemoryError\n            If there is an error retrieving the conversation history.\n        \"\"\"\n        logger.debug(\"Retrieving agent conversation...\")\n        try:\n            effective_limit = limit if limit is not None else self.message_limit\n\n            search_limit = effective_limit * 3\n\n            if strategy == MemoryRetrievalStrategy.RELEVANT and query:\n                messages = self.search(query=query, filters=filters, limit=search_limit)\n            elif strategy == MemoryRetrievalStrategy.BOTH and query:\n                recent_messages = self.search(query=None, filters=filters, limit=max(search_limit, self.DEFAULT_LIMIT))\n\n                relevant_messages = self.search(query=query, filters=filters, limit=search_limit)\n\n                message_dict = {msg.metadata.get(\"timestamp\", 0): msg for msg in recent_messages}\n                for msg in relevant_messages:\n                    message_dict[msg.metadata.get(\"timestamp\", 0)] = msg\n\n                messages = [msg for _, msg in sorted(message_dict.items())]\n            else:\n                messages = self.search(query=None, filters=filters, limit=search_limit)\n\n            final_messages = self._extract_valid_conversation(messages, effective_limit)\n            return final_messages\n        except Exception as e:\n            logger.error(f\"Error retrieving agent conversation: {e}\")\n            raise MemoryError(f\"Failed to retrieve agent conversation: {e}\") from e\n\n    def _extract_valid_conversation(self, messages: list[Message], limit: int) -&gt; list[Message]:\n        \"\"\"\n        Extracts a valid conversation from a list of messages, ensuring it starts with a user message.\n\n        Ensures:\n        1. Messages are sorted chronologically (timestamp, with USER priority for ties).\n        2. The final list respects the message limit (most recent messages).\n        3. The returned list *always* starts with a USER message, unless empty.\n\n        Args:\n            messages: List of messages to process.\n            limit: Maximum number of messages to include in the result.\n\n        Returns:\n            List of messages forming a valid conversation, starting with USER, or an empty list.\n        \"\"\"\n        if not messages:\n            return []\n\n        def message_sort_key(msg):\n            timestamp = msg.metadata.get(\"timestamp\", float(\"inf\"))\n            role_priority = 0 if msg.role == MessageRole.USER else 0.1\n            return (timestamp, role_priority)\n\n        sorted_messages = sorted(messages, key=message_sort_key)\n\n        if limit and len(sorted_messages) &gt; limit:\n            limited_messages = sorted_messages[-limit:]\n        else:\n            limited_messages = sorted_messages\n\n        if not limited_messages:\n            return []\n\n        if limited_messages[0].role == MessageRole.USER:\n            return limited_messages\n        else:\n            first_user_idx = next((i for i, msg in enumerate(limited_messages) if msg.role == MessageRole.USER), None)\n\n            if first_user_idx is not None:\n                return limited_messages[first_user_idx:]\n            else:\n                logger.warning(\n                    f\"Memory._extract_valid_conversation: No USER message found within the \"\n                    f\"last {len(limited_messages)} messages. Returning empty history.\"\n                )\n                return []\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.add","title":"<code>add(role, content, metadata=None)</code>","text":"<p>Adds a message to the memory.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>MessageRole</code> <p>The role of the message sender</p> required <code>content</code> <code>str</code> <p>The message content</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Additional metadata for the message</p> <code>None</code> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If the message cannot be added</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def add(self, role: MessageRole, content: str, metadata: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"\n    Adds a message to the memory.\n\n    Args:\n        role: The role of the message sender\n        content: The message content\n        metadata: Additional metadata for the message\n\n    Raises:\n        MemoryError: If the message cannot be added\n    \"\"\"\n    try:\n        metadata = metadata or {}\n        if \"timestamp\" not in metadata:\n            metadata[\"timestamp\"] = datetime.now(timezone.utc).timestamp()\n\n        sanitized_metadata = {}\n        for key, value in metadata.items():\n            sanitized_metadata[key] = \"\" if value is None else value\n\n        message = Message(role=role, content=content, metadata=sanitized_metadata)\n        self.backend.add(message)\n\n        logger.debug(\n            f\"Memory {self.backend.name}: \"\n            f\"Added message: {message.role}: {message.content[:min(20, len(message.content))]}...\"\n        )\n    except Exception as e:\n        logger.error(f\"Unexpected error adding message: {e}\")\n        raise MemoryError(f\"Unexpected error adding message: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.clear","title":"<code>clear()</code>","text":"<p>Clears the memory.</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If the memory cannot be cleared</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears the memory.\n\n    Raises:\n        MemoryError: If the memory cannot be cleared\n    \"\"\"\n    try:\n        self.backend.clear()\n        logger.debug(f\"Memory {self.backend.name}: Cleared memory\")\n    except Exception as e:\n        logger.error(f\"Unexpected error clearing memory: {e}\")\n        raise MemoryError(f\"Unexpected error clearing memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_agent_conversation","title":"<code>get_agent_conversation(query=None, limit=None, filters=None, strategy=MemoryRetrievalStrategy.ALL)</code>","text":"<p>Retrieves messages from an agent's conversation history based on various filtering criteria.</p> <p>This method supports three retrieval strategies: - ALL: Returns the most recent messages without semantic search - RELEVANT: Returns only messages relevant to the query using semantic search - BOTH: Returns a combination of recent messages and those relevant to the query</p>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_agent_conversation--parameters","title":"Parameters:","text":"<p>query : str | None, optional     The search query to filter messages by relevance. Required for RELEVANT and BOTH strategies.     Ignored when strategy is ALL. Default is None.</p> int | None, optional <p>Maximum number of messages to return. If None, falls back to self.message_limit.</p> dict[str, Any] | None, optional <p>Additional metadata filters to apply to the search results.</p> MemoryRetrievalStrategy, optional <p>The strategy to use for retrieving messages. Choices are: - MemoryRetrievalStrategy.ALL: Return most recent messages - MemoryRetrievalStrategy.RELEVANT: Return messages relevant to query - MemoryRetrievalStrategy.BOTH: Return both recent and relevant messages Default is MemoryRetrievalStrategy.ALL.</p>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_agent_conversation--returns","title":"Returns:","text":"<p>list[Message]     A list of conversation messages matching the search criteria, ordered chronologically.</p>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_agent_conversation--raises","title":"Raises:","text":"<p>MemoryError     If there is an error retrieving the conversation history.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_agent_conversation(\n    self,\n    query: str | None = None,\n    limit: int | None = None,\n    filters: dict[str, Any] | None = None,\n    strategy: MemoryRetrievalStrategy = MemoryRetrievalStrategy.ALL,\n) -&gt; list[Message]:\n    \"\"\"\n    Retrieves messages from an agent's conversation history based on various filtering criteria.\n\n    This method supports three retrieval strategies:\n    - ALL: Returns the most recent messages without semantic search\n    - RELEVANT: Returns only messages relevant to the query using semantic search\n    - BOTH: Returns a combination of recent messages and those relevant to the query\n\n    Parameters:\n    -----------\n    query : str | None, optional\n        The search query to filter messages by relevance. Required for RELEVANT and BOTH strategies.\n        Ignored when strategy is ALL. Default is None.\n\n    limit : int | None, optional\n        Maximum number of messages to return. If None, falls back to self.message_limit.\n\n    filters : dict[str, Any] | None, optional\n        Additional metadata filters to apply to the search results.\n\n    strategy : MemoryRetrievalStrategy, optional\n        The strategy to use for retrieving messages. Choices are:\n        - MemoryRetrievalStrategy.ALL: Return most recent messages\n        - MemoryRetrievalStrategy.RELEVANT: Return messages relevant to query\n        - MemoryRetrievalStrategy.BOTH: Return both recent and relevant messages\n        Default is MemoryRetrievalStrategy.ALL.\n\n    Returns:\n    --------\n    list[Message]\n        A list of conversation messages matching the search criteria, ordered chronologically.\n\n    Raises:\n    -------\n    MemoryError\n        If there is an error retrieving the conversation history.\n    \"\"\"\n    logger.debug(\"Retrieving agent conversation...\")\n    try:\n        effective_limit = limit if limit is not None else self.message_limit\n\n        search_limit = effective_limit * 3\n\n        if strategy == MemoryRetrievalStrategy.RELEVANT and query:\n            messages = self.search(query=query, filters=filters, limit=search_limit)\n        elif strategy == MemoryRetrievalStrategy.BOTH and query:\n            recent_messages = self.search(query=None, filters=filters, limit=max(search_limit, self.DEFAULT_LIMIT))\n\n            relevant_messages = self.search(query=query, filters=filters, limit=search_limit)\n\n            message_dict = {msg.metadata.get(\"timestamp\", 0): msg for msg in recent_messages}\n            for msg in relevant_messages:\n                message_dict[msg.metadata.get(\"timestamp\", 0)] = msg\n\n            messages = [msg for _, msg in sorted(message_dict.items())]\n        else:\n            messages = self.search(query=None, filters=filters, limit=search_limit)\n\n        final_messages = self._extract_valid_conversation(messages, effective_limit)\n        return final_messages\n    except Exception as e:\n        logger.error(f\"Error retrieving agent conversation: {e}\")\n        raise MemoryError(f\"Failed to retrieve agent conversation: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves all messages from the memory, optionally limited to most recent.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return. If provided, returns the most recent messages.   If None, uses the configured message_limit.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by timestamp (oldest first)</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If messages cannot be retrieved</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"\n    Retrieves all messages from the memory, optionally limited to most recent.\n\n    Args:\n        limit: Maximum number of messages to return. If provided, returns the most recent messages.\n              If None, uses the configured message_limit.\n\n    Returns:\n        List of messages sorted by timestamp (oldest first)\n\n    Raises:\n        MemoryError: If messages cannot be retrieved\n    \"\"\"\n    try:\n        effective_limit = limit if limit is not None else self.message_limit\n\n        messages = self.backend.get_all(limit=effective_limit)\n        retrieved_messages = [Message(**msg.model_dump()) for msg in messages]\n        logger.debug(f\"Memory {self.backend.name}: Retrieved {len(retrieved_messages)} messages\")\n        return retrieved_messages\n    except Exception as e:\n        logger.error(f\"Unexpected error retrieving messages: {e}\")\n        raise MemoryError(f\"Unexpected error retrieving messages: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_all_messages_as_string","title":"<code>get_all_messages_as_string(format_type=FormatType.PLAIN)</code>","text":"<p>Retrieves all messages as a formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>format_type</code> <code>FormatType</code> <p>Format to use for the string output</p> <code>PLAIN</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation of all messages</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If messages cannot be retrieved or formatted</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_all_messages_as_string(self, format_type: FormatType = FormatType.PLAIN) -&gt; str:\n    \"\"\"\n    Retrieves all messages as a formatted string.\n\n    Args:\n        format_type: Format to use for the string output\n\n    Returns:\n        Formatted string representation of all messages\n\n    Raises:\n        MemoryError: If messages cannot be retrieved or formatted\n    \"\"\"\n    messages = self.get_all()\n    return self._format_messages_as_string(messages=messages, format_type=format_type)\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_search_results_as_string","title":"<code>get_search_results_as_string(query, filters=None, format_type=FormatType.PLAIN)</code>","text":"<p>Searches for messages relevant to the query and returns them as a string.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional metadata filters to apply</p> <code>None</code> <code>format_type</code> <code>FormatType</code> <p>Format to use for the string output</p> <code>PLAIN</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation of search results</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If search operation fails or results cannot be formatted</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_search_results_as_string(\n    self, query: str, filters: dict[str, Any] | None = None, format_type: FormatType = FormatType.PLAIN\n) -&gt; str:\n    \"\"\"\n    Searches for messages relevant to the query and returns them as a string.\n\n    Args:\n        query: Search query string\n        filters: Optional metadata filters to apply\n        format_type: Format to use for the string output\n\n    Returns:\n        Formatted string representation of search results\n\n    Raises:\n        MemoryError: If search operation fails or results cannot be formatted\n    \"\"\"\n    messages = self.search(query, filters)\n    return self._format_messages_as_string(messages=messages, format_type=format_type)\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the memory is empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the memory is empty, False otherwise</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If the check fails</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"\n    Checks if the memory is empty.\n\n    Returns:\n        True if the memory is empty, False otherwise\n\n    Raises:\n        MemoryError: If the check fails\n    \"\"\"\n    try:\n        return self.backend.is_empty()\n    except Exception as e:\n        logger.error(f\"Unexpected error checking if memory is empty: {e}\")\n        raise MemoryError(f\"Unexpected error checking if memory is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Searches for messages relevant to the query or filters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str | None</code> <p>Search query string (optional)</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional metadata filters to apply (overrides default filters)</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return (defaults to message_limit)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of matching messages sorted by relevance</p> <p>Raises:</p> Type Description <code>MemoryError</code> <p>If search operation fails</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"\n    Searches for messages relevant to the query or filters.\n\n    Args:\n        query: Search query string (optional)\n        filters: Optional metadata filters to apply (overrides default filters)\n        limit: Maximum number of messages to return (defaults to message_limit)\n\n    Returns:\n        List of matching messages sorted by relevance\n\n    Raises:\n        MemoryError: If search operation fails\n    \"\"\"\n    try:\n        effective_filters = self.filters.copy()\n        if filters:\n            effective_filters.update(filters)\n\n        effective_limit = limit if limit is not None else self.message_limit\n\n        results = self.backend.search(query=query, filters=effective_filters, limit=effective_limit)\n\n        retrieved_messages = [Message(**msg.model_dump()) for msg in results]\n        logger.debug(\n            f\"Memory {self.backend.name}: Found {len(retrieved_messages)} search results for query: {query}, \"\n            f\"filters: {effective_filters}\"\n        )\n        return retrieved_messages\n    except Exception as e:\n        logger.error(f\"Unexpected error searching memory: {e}\")\n        raise MemoryError(f\"Unexpected error searching memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    for_tracing = kwargs.pop(\"for_tracing\", False)\n    data = self.model_dump(exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params), **kwargs)\n    data[\"backend\"] = self.backend.to_dict(\n        include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n    )\n    return data\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.MemoryError","title":"<code>MemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Memory errors.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>class MemoryError(Exception):\n    \"\"\"Base exception for Memory errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.MemoryRetrievalStrategy","title":"<code>MemoryRetrievalStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for memory retrieval strategies.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>class MemoryRetrievalStrategy(str, Enum):\n    \"\"\"Enum for memory retrieval strategies.\"\"\"\n    ALL = \"all\"\n    RELEVANT = \"relevant\"\n    BOTH = \"both\"\n</code></pre>"},{"location":"dynamiq/memory/backends/base/","title":"Base","text":""},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend","title":"<code>MemoryBackend</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for memory storage backends.</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>class MemoryBackend(ABC, BaseModel):\n    \"\"\"Abstract base class for memory storage backends.\"\"\"\n\n    name: str = \"MemoryBackend\"\n    id: str = Field(default_factory=generate_uuid)\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {}\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        kwargs.pop(\"include_secure_params\", None)\n        kwargs.pop(\"for_tracing\", None)\n        return self.model_dump(exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params), **kwargs)\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        \"\"\"Returns the backend type as a string.\"\"\"\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @abstractmethod\n    def add(self, message: Message) -&gt; None:\n        \"\"\"\n        Adds a message to the memory storage.\n\n        Args:\n            message: Message to add to storage\n\n        Raises:\n            MemoryBackendError: If the message cannot be added\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"\n        Retrieves all messages from the memory storage, optionally limited.\n\n        Args:\n            limit: Maximum number of messages to return. If provided, returns the most recent messages.\n                  If None, uses the backend's default limit (if applicable).\n\n        Returns:\n            List of messages sorted by timestamp (oldest first)\n\n        Raises:\n            MemoryBackendError: If messages cannot be retrieved\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"\n        Searches for messages relevant to the query.\n\n        Args:\n            query: Search query string (optional)\n            filters: Optional metadata filters to apply\n            limit: Maximum number of messages to return. If None, uses the backend's default limit.\n\n        Returns:\n            List of messages sorted by relevance (most relevant first)\n\n        Raises:\n            MemoryBackendError: If search operation fails\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def is_empty(self) -&gt; bool:\n        \"\"\"\n        Checks if the memory storage is empty.\n\n        Returns:\n            True if the memory is empty, False otherwise\n\n        Raises:\n            MemoryBackendError: If the check fails\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clears the memory storage.\n\n        Raises:\n            MemoryBackendError: If the memory cannot be cleared\n        \"\"\"\n        raise NotImplementedError\n\n    def _prepare_filters(self, filters: dict[str, Any] | None = None) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Default implementation for preparing filters. Override in backend-specific implementations.\n\n        Args:\n            filters: Raw filters to prepare\n\n        Returns:\n            Prepared filters in backend-specific format\n        \"\"\"\n        return filters\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.type","title":"<code>type: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the backend type as a string.</p>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.add","title":"<code>add(message)</code>  <code>abstractmethod</code>","text":"<p>Adds a message to the memory storage.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to add to storage</p> required <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the message cannot be added</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>@abstractmethod\ndef add(self, message: Message) -&gt; None:\n    \"\"\"\n    Adds a message to the memory storage.\n\n    Args:\n        message: Message to add to storage\n\n    Raises:\n        MemoryBackendError: If the message cannot be added\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clears the memory storage.</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the memory cannot be cleared</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"\n    Clears the memory storage.\n\n    Raises:\n        MemoryBackendError: If the memory cannot be cleared\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.get_all","title":"<code>get_all(limit=None)</code>  <code>abstractmethod</code>","text":"<p>Retrieves all messages from the memory storage, optionally limited.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return. If provided, returns the most recent messages.   If None, uses the backend's default limit (if applicable).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by timestamp (oldest first)</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If messages cannot be retrieved</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>@abstractmethod\ndef get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"\n    Retrieves all messages from the memory storage, optionally limited.\n\n    Args:\n        limit: Maximum number of messages to return. If provided, returns the most recent messages.\n              If None, uses the backend's default limit (if applicable).\n\n    Returns:\n        List of messages sorted by timestamp (oldest first)\n\n    Raises:\n        MemoryBackendError: If messages cannot be retrieved\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.is_empty","title":"<code>is_empty()</code>  <code>abstractmethod</code>","text":"<p>Checks if the memory storage is empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the memory is empty, False otherwise</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the check fails</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>@abstractmethod\ndef is_empty(self) -&gt; bool:\n    \"\"\"\n    Checks if the memory storage is empty.\n\n    Returns:\n        True if the memory is empty, False otherwise\n\n    Raises:\n        MemoryBackendError: If the check fails\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.search","title":"<code>search(query=None, filters=None, limit=None)</code>  <code>abstractmethod</code>","text":"<p>Searches for messages relevant to the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str | None</code> <p>Search query string (optional)</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional metadata filters to apply</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return. If None, uses the backend's default limit.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by relevance (most relevant first)</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If search operation fails</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"\n    Searches for messages relevant to the query.\n\n    Args:\n        query: Search query string (optional)\n        filters: Optional metadata filters to apply\n        limit: Maximum number of messages to return. If None, uses the backend's default limit.\n\n    Returns:\n        List of messages sorted by relevance (most relevant first)\n\n    Raises:\n        MemoryBackendError: If search operation fails\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backends/base/#dynamiq.memory.backends.base.MemoryBackend.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/base.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    kwargs.pop(\"include_secure_params\", None)\n    kwargs.pop(\"for_tracing\", None)\n    return self.model_dump(exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params), **kwargs)\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/","title":"Dynamiq","text":""},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq","title":"<code>Dynamiq</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>Memory backend backed by the Dynamiq API.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>class Dynamiq(MemoryBackend):\n    \"\"\"Memory backend backed by the Dynamiq API.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"Dynamiq\"\n    connection: DynamiqConnection = Field(default_factory=DynamiqConnection)\n    memory_id: str = Field(min_length=1, description=\"Identifier of the remote memory resource.\")\n    user_id: str | None = Field(default=None, description=\"Optional default user identifier for memory items.\")\n    session_id: str | None = Field(\n        default=None,\n        description=\"Optional default session identifier for memory items.\",\n    )\n    timeout: float = Field(\n        default=10,\n        description=\"Timeout in seconds for API requests.\",\n    )\n    limit: int = Field(\n        default=100,\n        ge=1,\n        description=\"Default limit used when retrieving memory items.\",\n    )\n    _base_path: str = PrivateAttr(default=\"/v1/memories\")\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Exclude connection details from serialization.\"\"\"\n        return super().to_dict_exclude_params | {\"connection\": True}\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Serialize backend configuration.\"\"\"\n        data = super().to_dict(include_secure_params=include_secure_params, **kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        return data\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Create a new memory item via the remote API.\"\"\"\n        metadata = dict(message.metadata or {})\n        effective_user_id = metadata.get(\"user_id\") or self.user_id\n        effective_session_id = metadata.get(\"session_id\") or self.session_id\n\n        if not effective_user_id:\n            raise DynamiqMemoryError(\"User identifier is required to create a memory item.\")\n        if not effective_session_id:\n            raise DynamiqMemoryError(\"Session identifier is required to create a memory item.\")\n\n        data_payload = {\n            \"role\": message.role.value if isinstance(message.role, MessageRole) else message.role,\n            \"content\": self._format_content_payload(message.content),\n        }\n\n        payload: dict[str, Any] = {\n            \"user_id\": effective_user_id,\n            \"session_id\": effective_session_id,\n            \"type\": \"message\",\n            \"data\": data_payload,\n        }\n\n        logger.debug(\"Creating remote memory item for memory_id=%s\", self.memory_id)\n        self._request(\n            HTTPMethod.POST,\n            f\"{self._base_path}/{self.memory_id}/items\",\n            json=payload,\n        )\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"Return all memory items from the remote store.\"\"\"\n        items = self._list_items(limit=limit)\n        messages = self._items_to_messages(items)\n        return messages\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"Retrieve memory items optionally filtering by metadata and simple text query.\"\"\"\n        items = self._list_items(limit=limit, filters=filters)\n        messages = self._items_to_messages(items)\n\n        if query:\n            lowered_query = query.lower()\n            messages = [msg for msg in messages if lowered_query in (msg.content or \"\").lower()]\n\n        if filters:\n            messages = self._filter_messages(messages, filters)\n\n        return messages\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check whether the remote memory is empty.\"\"\"\n        messages = self.search(limit=1)\n        return len(messages) == 0\n\n    def clear(self) -&gt; None:\n        \"\"\"Not supported by the Dynamiq API.\"\"\"\n        raise DynamiqMemoryError(\"Clearing remote memories is not supported by the Dynamiq backend.\")\n\n    def _filter_messages(self, messages: list[Message], filters: dict[str, Any]) -&gt; list[Message]:\n        \"\"\"Filter messages by metadata.\"\"\"\n        if not filters:\n            return messages\n\n        def matches(message: Message) -&gt; bool:\n            metadata = message.metadata or {}\n            for key, value in filters.items():\n                if isinstance(value, list):\n                    if metadata.get(key) not in value:\n                        return False\n                else:\n                    if metadata.get(key) != value:\n                        return False\n            return True\n\n        return [msg for msg in messages if matches(msg)]\n\n    def _list_items(self, limit: int | None = None, filters: dict[str, Any] | None = None) -&gt; list[dict[str, Any]]:\n        \"\"\"Fetch raw memory items from the API.\"\"\"\n        filters = filters.copy() if filters else {}\n        effective_user_id = filters.pop(\"user_id\", None) or self.user_id\n        effective_session_id = filters.pop(\"session_id\", None) or self.session_id\n\n        if not effective_user_id:\n            raise DynamiqMemoryError(\"User identifier is required to list memory items.\")\n        if not effective_session_id:\n            raise DynamiqMemoryError(\"Session identifier is required to list memory items.\")\n\n        params: dict[str, Any] = {\n            \"user_id\": effective_user_id,\n            \"session_id\": effective_session_id,\n            \"page_size\": limit if limit is not None else self.limit,\n            \"sort\": \"-created_at\",\n        }\n        if filters:\n            params.update(filters)\n\n        response = self._request(\n            HTTPMethod.GET,\n            f\"{self._base_path}/{self.memory_id}/items\",\n            params=params or None,\n        )\n        if not response:\n            return []\n\n        if isinstance(response, dict):\n            data = response.get(\"data\")\n            if isinstance(data, list):\n                return data\n\n        logger.warning(\"Unexpected response shape when listing memory items: %s\", response)\n        return []\n\n    def _format_content_payload(self, content: Any) -&gt; list[dict[str, Any]]:\n        \"\"\"Normalize message content into the Dynamiq API payload structure.\"\"\"\n        if isinstance(content, list):\n            return content\n\n        if isinstance(content, str):\n            text = content\n        else:\n            text = str(content) if content is not None else \"\"\n\n        return [{\"type\": \"text\", \"text\": text}]\n\n    def _items_to_messages(self, items: list[dict[str, Any]]) -&gt; list[Message]:\n        \"\"\"Convert raw API records into Message objects.\"\"\"\n        messages: list[Message] = []\n        for item in items:\n            if not isinstance(item, dict):\n                continue\n\n            data = item.get(\"data\") if isinstance(item.get(\"data\"), dict) else {}\n            role_value = data.get(\"role\") or MessageRole.USER.value\n            try:\n                role = MessageRole(role_value)\n            except ValueError:\n                role = MessageRole.USER\n\n            metadata = dict(data.get(\"metadata\") or {})\n            item_metadata = item.get(\"metadata\")\n            if isinstance(item_metadata, dict):\n                metadata.update({k: v for k, v in item_metadata.items() if k not in metadata})\n\n            metadata[\"user_id\"] = item.get(\"user_id\")\n            metadata[\"session_id\"] = item.get(\"session_id\")\n\n            item_type = item.get(\"type\")\n            if item_type and \"type\" not in metadata:\n                metadata[\"type\"] = item_type\n\n            created_at = item.get(\"created_at\") or data.get(\"created_at\")\n            updated_at = item.get(\"updated_at\") or data.get(\"updated_at\")\n            if created_at and \"created_at\" not in metadata:\n                metadata[\"created_at\"] = created_at\n            if updated_at and \"updated_at\" not in metadata:\n                metadata[\"updated_at\"] = updated_at\n\n            if created_at and \"timestamp\" not in metadata:\n                metadata[\"timestamp\"] = self._coerce_timestamp(created_at)\n\n            content = self._extract_content_text(data) or item.get(\"content\", \"\")\n            messages.append(Message(role=role, content=content, metadata=metadata))\n\n        messages.sort(key=lambda msg: (msg.metadata or {}).get(\"timestamp\") or 0)\n        return messages\n\n    def _extract_content_text(self, data: dict[str, Any]) -&gt; str:\n        \"\"\"Extract textual content from Dynamiq API message payloads.\"\"\"\n        content = data.get(\"content\")\n        if isinstance(content, list):\n            text_chunks = [\n                str(chunk.get(\"text\"))\n                for chunk in content\n                if isinstance(chunk, dict) and chunk.get(\"type\") == \"text\" and chunk.get(\"text\")\n            ]\n            return \"\\n\\n\".join(text_chunks).strip()\n\n        if isinstance(content, str):\n            return content\n\n        if content is not None:\n            return str(content)\n\n        return \"\"\n\n    def _coerce_timestamp(self, value: Any) -&gt; float:\n        \"\"\"Convert timestamp representations to float seconds.\"\"\"\n        if isinstance(value, (int, float)):\n            return float(value)\n        if isinstance(value, str):\n            try:\n                dt = datetime.fromisoformat(value.replace(\"Z\", \"+00:00\"))\n                return dt.timestamp()\n            except ValueError:\n                pass\n            try:\n                return float(value)\n            except ValueError:\n                logger.debug(\"Unable to parse timestamp string '%s'\", value)\n        return datetime.utcnow().timestamp()\n\n    def _request(\n        self,\n        method: HTTPMethod,\n        path: str,\n        params: dict[str, Any] | None = None,\n        json: dict[str, Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Execute an HTTP request against the Dynamiq API.\"\"\"\n        conn_params = self.connection.conn_params\n        base_url = (conn_params.get(\"api_base\") or \"\").rstrip(\"/\")\n        if not base_url:\n            raise DynamiqMemoryError(\"Dynamiq API base URL is not configured.\")\n\n        url = f\"{base_url}/{path.lstrip('/')}\"\n        headers = {\"Content-Type\": \"application/json\"}\n        conn_headers = conn_params.get(\"headers\")\n        if isinstance(conn_headers, dict):\n            headers.update(conn_headers)\n\n        client = self.connection.connect()\n        verb = method.value if isinstance(method, HTTPMethod) else method\n        try:\n            response = client.request(\n                verb,\n                url,\n                headers=headers,\n                params=params,\n                json=json,\n                timeout=self.timeout,\n            )\n        except Exception as exc:\n            raise DynamiqMemoryError(f\"Failed to call Dynamiq API: {exc}\") from exc\n\n        if response.status_code &gt;= 400:\n            raise DynamiqMemoryError(f\"Request to Dynamiq API failed: {response.status_code} {response.text}\")\n\n        if response.status_code == 204 or not response.content:\n            return None\n\n        try:\n            return response.json()\n        except ValueError:\n            logger.debug(\"Received non-JSON response from Dynamiq API for %s: %s\", url, response.text)\n            return response.text\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Exclude connection details from serialization.</p>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.add","title":"<code>add(message)</code>","text":"<p>Create a new memory item via the remote API.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Create a new memory item via the remote API.\"\"\"\n    metadata = dict(message.metadata or {})\n    effective_user_id = metadata.get(\"user_id\") or self.user_id\n    effective_session_id = metadata.get(\"session_id\") or self.session_id\n\n    if not effective_user_id:\n        raise DynamiqMemoryError(\"User identifier is required to create a memory item.\")\n    if not effective_session_id:\n        raise DynamiqMemoryError(\"Session identifier is required to create a memory item.\")\n\n    data_payload = {\n        \"role\": message.role.value if isinstance(message.role, MessageRole) else message.role,\n        \"content\": self._format_content_payload(message.content),\n    }\n\n    payload: dict[str, Any] = {\n        \"user_id\": effective_user_id,\n        \"session_id\": effective_session_id,\n        \"type\": \"message\",\n        \"data\": data_payload,\n    }\n\n    logger.debug(\"Creating remote memory item for memory_id=%s\", self.memory_id)\n    self._request(\n        HTTPMethod.POST,\n        f\"{self._base_path}/{self.memory_id}/items\",\n        json=payload,\n    )\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.clear","title":"<code>clear()</code>","text":"<p>Not supported by the Dynamiq API.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Not supported by the Dynamiq API.\"\"\"\n    raise DynamiqMemoryError(\"Clearing remote memories is not supported by the Dynamiq backend.\")\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Return all memory items from the remote store.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"Return all memory items from the remote store.\"\"\"\n    items = self._list_items(limit=limit)\n    messages = self._items_to_messages(items)\n    return messages\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.is_empty","title":"<code>is_empty()</code>","text":"<p>Check whether the remote memory is empty.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check whether the remote memory is empty.\"\"\"\n    messages = self.search(limit=1)\n    return len(messages) == 0\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Retrieve memory items optionally filtering by metadata and simple text query.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"Retrieve memory items optionally filtering by metadata and simple text query.\"\"\"\n    items = self._list_items(limit=limit, filters=filters)\n    messages = self._items_to_messages(items)\n\n    if query:\n        lowered_query = query.lower()\n        messages = [msg for msg in messages if lowered_query in (msg.content or \"\").lower()]\n\n    if filters:\n        messages = self._filter_messages(messages, filters)\n\n    return messages\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.Dynamiq.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Serialize backend configuration.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Serialize backend configuration.\"\"\"\n    data = super().to_dict(include_secure_params=include_secure_params, **kwargs)\n    data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamiq/#dynamiq.memory.backends.dynamiq.DynamiqMemoryError","title":"<code>DynamiqMemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for errors raised by the Dynamiq memory backend.</p> Source code in <code>dynamiq/memory/backends/dynamiq.py</code> <pre><code>class DynamiqMemoryError(Exception):\n    \"\"\"Base exception for errors raised by the Dynamiq memory backend.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamo_db/","title":"Dynamo db","text":""},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDB","title":"<code>DynamoDB</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>AWS DynamoDB implementation of memory storage using ONLY table scans.</p> <p>Relies exclusively on DynamoDB Scan operations. Scans read the entire table and can be slow and costly for large datasets.</p> <p>Assumed Table Schema: - PK: <code>message_id</code> (String) - SK: <code>timestamp</code> (Number - Unix float stored as Decimal) - Attributes: <code>role</code> (String), <code>content</code> (String), <code>metadata</code> (Map)</p> Source code in <code>dynamiq/memory/backends/dynamo_db.py</code> <pre><code>class DynamoDB(MemoryBackend):\n    \"\"\"\n    AWS DynamoDB implementation of memory storage using ONLY table scans.\n\n    Relies exclusively on DynamoDB Scan operations. Scans read the\n    entire table and can be slow and costly for large datasets.\n\n    Assumed Table Schema:\n    - PK: `message_id` (String)\n    - SK: `timestamp` (Number - Unix float stored as Decimal)\n    - Attributes: `role` (String), `content` (String), `metadata` (Map)\n    \"\"\"\n\n    _MAX_SCAN_PAGE_LIMIT: int = 1000\n    _DEFAULT_SCAN_SIZE: int = 5000\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"DynamoDB\"\n    connection: AWS = Field(default_factory=AWS)\n    table_name: str = Field(\"conversations\", description=\"Name of the DynamoDB table.\")\n    create_if_not_exist: bool = Field(default=False)\n    billing_mode: BillingMode = Field(\n        default=BillingMode.PAY_PER_REQUEST,\n        description=\"DynamoDB billing mode\",\n    )\n    read_capacity_units: int = Field(default=1, gt=0)\n    write_capacity_units: int = Field(default=1, gt=0)\n\n    partition_key_name: str = Field(default=\"message_id\")\n    sort_key_name: str = Field(default=\"timestamp\")\n    role_attribute_name: str = Field(default=\"role\")\n    content_attribute_name: str = Field(default=\"content\")\n    metadata_attribute_name: str = Field(default=\"metadata\")\n    scan_fetch_target_multiplier: int = Field(\n        default=10,\n        gt=0,\n        description=(\n            \"When a limit is provided, multiply the limit by this factor \"\n            \"to determine the initial target number of items to fetch via scan, \"\n            \"before client-side filtering/sorting/limiting.\"\n        ),\n    )\n    default_scan_fetch_target: int = Field(\n        default=_DEFAULT_SCAN_SIZE,\n        gt=0,\n        description=(\n            \"Default target number of items to fetch during a scan operation \"\n            \"when no specific limit is provided but client-side filtering might occur. \"\n            \"Helps balance fetching enough data vs. excessive scanning.\"\n        ),\n    )\n\n    _dynamodb_resource: Any = PrivateAttr(default=None)\n    _dynamodb_table: Any = PrivateAttr(default=None)\n    _dynamodb_client: Any = PrivateAttr(default=None)\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"Define parameters to exclude when converting the class instance to a dictionary.\"\"\"\n        return super().to_dict_exclude_params | {\n            \"connection\": True,\n        }\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params.copy())\n        data = self.model_dump(exclude=exclude, **kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        if \"type\" not in data:\n            data[\"type\"] = self.type\n        return data\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        try:\n            session = self.connection.get_boto3_session()\n            self._dynamodb_resource = session.resource(\"dynamodb\")\n            self._dynamodb_client = session.client(\"dynamodb\")\n            self._dynamodb_table = self._get_or_create_table()\n            region = session.region_name or \"default\"\n            logger.debug(f\"DynamoDB backend (Scan Only) connected to table '{self.table_name}' in region '{region}'.\")\n        except ClientError as e:\n            logger.error(f\"Failed to initialize DynamoDB connection or table: {e}\")\n            raise DynamoDBMemoryError(f\"Failed to initialize DynamoDB connection or table: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error initializing DynamoDB backend: {e}\")\n            raise DynamoDBMemoryError(f\"Unexpected error initializing DynamoDB backend: {e}\") from e\n\n    def _get_or_create_table(self):\n        if self._dynamodb_resource is None:\n            raise DynamoDBMemoryError(\"DynamoDB resource not initialized.\")\n        table = self._dynamodb_resource.Table(self.table_name)\n        try:\n            table.load()\n            logger.debug(f\"DynamoDB table '{self.table_name}' found.\")\n            return table\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n                if self.create_if_not_exist:\n                    logger.info(f\"DynamoDB table '{self.table_name}' not found. Attempting creation (Scan Only)...\")\n                    return self._create_table()\n                else:\n                    logger.error(f\"DynamoDB table '{self.table_name}' not found and create_if_not_exist is False.\")\n                    raise DynamoDBMemoryError(f\"DynamoDB table '{self.table_name}' not found.\") from e\n            else:\n                raise\n\n    def _create_table(self):\n        \"\"\"Creates the DynamoDB table WITHOUT any GSIs.\"\"\"\n        if self._dynamodb_resource is None:\n            raise DynamoDBMemoryError(\"DynamoDB resource not initialized.\")\n        attribute_definitions = [\n            {\"AttributeName\": self.partition_key_name, \"AttributeType\": \"S\"},\n            {\"AttributeName\": self.sort_key_name, \"AttributeType\": \"N\"},\n        ]\n        key_schema = [\n            {\"AttributeName\": self.partition_key_name, \"KeyType\": \"HASH\"},\n            {\"AttributeName\": self.sort_key_name, \"KeyType\": \"RANGE\"},\n        ]\n        create_params = {\n            \"TableName\": self.table_name,\n            \"AttributeDefinitions\": attribute_definitions,\n            \"KeySchema\": key_schema,\n            \"BillingMode\": self.billing_mode,\n        }\n        if self.billing_mode == BillingMode.PROVISIONED:\n            create_params[\"ProvisionedThroughput\"] = {\n                \"ReadCapacityUnits\": self.read_capacity_units,\n                \"WriteCapacityUnits\": self.write_capacity_units,\n            }\n        try:\n            table = self._dynamodb_resource.create_table(**create_params)\n            logger.info(f\"Waiting for table '{self.table_name}' to become active...\")\n            table.wait_until_exists()\n            logger.info(f\"DynamoDB table '{self.table_name}' created successfully (Scan Only - No GSIs).\")\n            return table\n        except ClientError as e:\n            logger.error(f\"Failed to create DynamoDB table '{self.table_name}': {e}\")\n            raise DynamoDBMemoryError(f\"Failed to create DynamoDB table '{self.table_name}': {e}\") from e\n\n    def _serialize_timestamp(self, ts: float | int | Decimal) -&gt; Decimal:\n        if isinstance(ts, (float, int)):\n            if ts != ts or ts == float(\"inf\") or ts == float(\"-inf\"):\n                raise ValueError(f\"Cannot serialize non-finite float {ts} as timestamp\")\n            try:\n                return Decimal(str(ts))\n            except InvalidOperation:\n                raise ValueError(f\"Could not convert numeric value {ts} to Decimal\")\n        elif isinstance(ts, Decimal):\n            if ts.is_infinite():\n                raise ValueError(f\"Cannot serialize infinite Decimal {ts} as timestamp\")\n            elif ts.is_nan():\n                raise ValueError(\"Cannot serialize NaN Decimal as timestamp\")\n            return ts\n        else:\n            raise TypeError(f\"Timestamp must be float, int, or Decimal, not {type(ts)}\")\n\n    def _deserialize_item(self, item: dict[str, Any]) -&gt; Message | None:\n        try:\n            metadata_raw = item.get(self.metadata_attribute_name, {})\n            metadata = metadata_raw\n            timestamp_val = item.get(self.sort_key_name)\n            timestamp = float(timestamp_val) if isinstance(timestamp_val, Decimal) else timestamp_val\n            metadata[self.sort_key_name] = timestamp\n            metadata[self.partition_key_name] = item.get(self.partition_key_name)\n            return Message(\n                role=MessageRole(item.get(self.role_attribute_name, MessageRole.USER.value)),\n                content=item.get(self.content_attribute_name, \"\"),\n                metadata=metadata,\n            )\n        except (TypeError, ValueError, KeyError, InvalidOperation) as e:\n            logger.error(f\"Error deserializing DynamoDB item {item.get(self.partition_key_name)}: {e}. Item: {item}\")\n            return None\n\n    def add(self, message: Message) -&gt; None:\n        if self._dynamodb_table is None:\n            raise DynamoDBMemoryError(\"DynamoDB table not initialized.\")\n        try:\n            message_id = message.metadata.get(self.partition_key_name, str(uuid.uuid4()))\n            timestamp_input = message.metadata.get(self.sort_key_name, time.time())\n            timestamp_decimal = self._serialize_timestamp(timestamp_input)\n            metadata_to_store = message.metadata.copy() if message.metadata else {}\n            metadata_to_store[self.sort_key_name] = timestamp_decimal\n            metadata_to_store[self.partition_key_name] = message_id\n            processed_metadata = _convert_floats_to_decimals(metadata_to_store)\n            logger.debug(\n                f\"Saving item with PK: {message_id}, SK: {timestamp_decimal},\"\n                f\" Metadata: {json.dumps(processed_metadata, default=str)}\"\n            )\n            item = {\n                self.partition_key_name: message_id,\n                self.sort_key_name: timestamp_decimal,\n                self.role_attribute_name: message.role.value,\n                self.content_attribute_name: message.content,\n                self.metadata_attribute_name: processed_metadata,\n            }\n            self._dynamodb_table.put_item(Item=item)\n        except (ClientError, ValueError, TypeError) as e:\n            logger.error(f\"Error adding message to DynamoDB: {e}\")\n            raise DynamoDBMemoryError(f\"Error adding message to DynamoDB: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error adding message to DynamoDB: {e}\")\n            try:\n                logger.error(f\"Problematic metadata (attempted): {json.dumps(message.metadata, default=str)}\")\n            except Exception:\n                logger.error(\"Could not serialize problematic metadata for logging.\")\n            raise DynamoDBMemoryError(f\"Unexpected error adding message: {e}\") from e\n\n    def _scan_and_process(\n        self, limit: int | None = None, filters: dict | None = None, query: str | None = None\n    ) -&gt; list[Message]:\n        \"\"\"Scans table, applies filters/query client-side, sorts, and limits.\"\"\"\n        if self._dynamodb_table is None:\n            raise DynamoDBMemoryError(\"DynamoDB table not initialized.\")\n        logger.warning(\n            f\"Performing DynamoDB Scan on table '{self.table_name}' for search/get_all. This can be slow and costly.\"\n        )\n\n        scan_params = {}\n        processed_filters = _convert_floats_to_decimals(filters) if filters else None\n        filter_expression_attr = self._build_filter_expression_attr(processed_filters)\n        if filter_expression_attr:\n            scan_params[\"FilterExpression\"] = filter_expression_attr\n            logger.debug(f\"Using server-side FilterExpression object: {filter_expression_attr}\")\n        elif filters:\n            logger.debug(\"No filters provided or FilterExpression could not be built.\")\n\n        all_items = []\n        last_evaluated_key = None\n        items_processed = 0\n        scan_fetch_target = (limit * self.scan_fetch_target_multiplier) if limit else self.default_scan_fetch_target\n        try:\n            while True:\n                if last_evaluated_key:\n                    scan_params[\"ExclusiveStartKey\"] = last_evaluated_key\n\n                scan_params[\"Limit\"] = (\n                    min(self._MAX_SCAN_PAGE_LIMIT, scan_fetch_target - items_processed)\n                    if limit\n                    else self._MAX_SCAN_PAGE_LIMIT\n                )\n                if scan_params[\"Limit\"] &lt;= 0 and limit:\n                    break\n\n                logger.debug(f\"Scanning page with params: {scan_params}\")\n                response = self._dynamodb_table.scan(**scan_params)\n                page_items = response.get(\"Items\", [])\n                all_items.extend(page_items)\n                items_processed += len(page_items)\n                logger.debug(f\"Scan page returned {len(page_items)} items. Total fetched: {items_processed}.\")\n\n                last_evaluated_key = response.get(\"LastEvaluatedKey\")\n                if not last_evaluated_key or (limit and items_processed &gt;= scan_fetch_target):\n                    if limit and items_processed &gt;= scan_fetch_target:\n                        logger.debug(f\"Reached scan fetch target ({scan_fetch_target}), stopping pagination.\")\n                    break\n\n            logger.info(f\"Scan completed. Fetched {len(all_items)} total items.\")\n\n            messages = [self._deserialize_item(item) for item in all_items]\n            valid_messages = [msg for msg in messages if msg is not None]\n            logger.debug(f\"Deserialized {len(valid_messages)} valid messages.\")\n\n            if filters:\n                original_count = len(valid_messages)\n                valid_messages = self._apply_filters_client_side_messages(valid_messages, filters)\n                logger.debug(\n                    f\"Applied client-side filters ({filters}). \"\n                    f\"Count changed from {original_count} to {len(valid_messages)}.\"\n                )\n\n            if query:\n                original_count = len(valid_messages)\n                query_lower = query.lower()\n                valid_messages = [msg for msg in valid_messages if query_lower in msg.content.lower()]\n                logger.debug(\n                    f\"Applied client-side text query ('{query}').\"\n                    f\" Count changed from {original_count} to {len(valid_messages)}.\"\n                )\n\n            valid_messages.sort(key=lambda m: m.metadata.get(self.sort_key_name, 0))\n            logger.debug(\"Sorted messages by timestamp.\")\n\n            if limit is not None and limit &gt; 0:\n                original_count = len(valid_messages)\n                final_results = valid_messages[-limit:]\n                logger.debug(\n                    f\"Applied final limit ({limit}). Count changed from {original_count} to {len(final_results)}.\"\n                )\n            else:\n                final_results = valid_messages\n                logger.debug(\"No final limit applied.\")\n\n            return final_results\n\n        except ClientError as e:\n            logger.error(f\"Error scanning DynamoDB table '{self.table_name}': {e}\")\n            logger.error(f\"Scan parameters used: {scan_params}\")  # Log params on error\n            raise DynamoDBMemoryError(f\"Error scanning DynamoDB table: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during scan processing: {e}\")\n            raise DynamoDBMemoryError(f\"Unexpected error during scan processing: {e}\") from e\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"Retrieves messages via table scan, sorts chronologically.\"\"\"\n        return self._scan_and_process(limit=limit)\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"Searches messages via table scan, applying filters and query client-side.\"\"\"\n        return self._scan_and_process(limit=limit, filters=filters, query=query)\n\n    def _build_filter_expression_attr(self, filters: dict | None) -&gt; Attr | None:\n        \"\"\"Builds a combined Attr object for FilterExpression.\"\"\"\n        if not filters:\n            return None\n\n        fe: Attr | None = None\n        for key, value in filters.items():\n            try:\n                current_cond = Attr(f\"{self.metadata_attribute_name}.{key}\").eq(value)\n            except Exception as e:\n                logger.error(f\"Could not build Attr condition for filter key '{key}': {e}. Skipping this filter.\")\n                continue\n\n            if fe is None:\n                fe = current_cond\n            else:\n                fe = fe &amp; current_cond\n\n        if fe:\n            logger.debug(f\"Built FilterExpression Attr object: {fe}\")\n        return fe\n\n    def _apply_filters_client_side_messages(self, messages: list[Message], filters: dict) -&gt; list[Message]:\n        \"\"\"Applies filters to a list of Message objects (client-side).\"\"\"\n        if not filters:\n            return messages\n        filtered_messages = []\n        processed_filters = _convert_floats_to_decimals(filters)\n\n        for msg in messages:\n            match = True\n            for key, filter_value in processed_filters.items():\n                metadata_value = msg.metadata.get(key)\n                try:\n                    if isinstance(filter_value, Decimal) and isinstance(metadata_value, (float, int)):\n                        metadata_value_cmp = Decimal(str(metadata_value))\n                    else:\n                        metadata_value_cmp = metadata_value\n                    if isinstance(filter_value, list):\n                        if metadata_value_cmp not in filter_value:\n                            match = False\n                            break\n                    elif metadata_value_cmp != filter_value:\n                        match = False\n                        break\n                except (TypeError, ValueError, InvalidOperation):\n                    match = False\n                    break\n            if match:\n                filtered_messages.append(msg)\n        return filtered_messages\n\n    def is_empty(self) -&gt; bool:\n        if self._dynamodb_table is None:\n            raise DynamoDBMemoryError(\"DynamoDB table not initialized.\")\n        try:\n            response = self._dynamodb_table.scan(Limit=1, Select=\"COUNT\")\n            return response.get(\"Count\", 0) == 0\n        except ClientError as e:\n            logger.error(f\"Error checking emptiness for DynamoDB table '{self.table_name}': {e}\")\n            raise DynamoDBMemoryError(f\"Error checking if DynamoDB memory is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears memory by deleting all items via scan (slow, costly).\"\"\"\n        if self._dynamodb_table is None:\n            raise DynamoDBMemoryError(\"DynamoDB table not initialized.\")\n        logger.warning(\n            f\"Clearing all items from DynamoDB table '{self.table_name}'\"\n            f\" via Scan/Delete. This may take time and consume capacity.\"\n        )\n        try:\n            with self._dynamodb_table.batch_writer() as batch:\n                scan_params = {\"ProjectionExpression\": f\"{self.partition_key_name}, {self.sort_key_name}\"}\n                while True:\n                    response = self._dynamodb_table.scan(**scan_params)\n                    items = response.get(\"Items\", [])\n                    if not items:\n                        break\n                    for item in items:\n                        batch.delete_item(\n                            Key={\n                                self.partition_key_name: item[self.partition_key_name],\n                                self.sort_key_name: item[self.sort_key_name],\n                            }\n                        )\n                    if \"LastEvaluatedKey\" in response:\n                        scan_params[\"ExclusiveStartKey\"] = response[\"LastEvaluatedKey\"]\n                    else:\n                        break\n            logger.info(f\"DynamoDB Memory ({self.table_name}): Finished clearing items.\")\n        except ClientError as e:\n            logger.error(f\"Error clearing DynamoDB table '{self.table_name}': {e}\")\n            raise DynamoDBMemoryError(f\"Error clearing DynamoDB memory: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error clearing DynamoDB memory: {e}\")\n            raise DynamoDBMemoryError(f\"Unexpected error clearing DynamoDB memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDB.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Define parameters to exclude when converting the class instance to a dictionary.</p>"},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDB.clear","title":"<code>clear()</code>","text":"<p>Clears memory by deleting all items via scan (slow, costly).</p> Source code in <code>dynamiq/memory/backends/dynamo_db.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears memory by deleting all items via scan (slow, costly).\"\"\"\n    if self._dynamodb_table is None:\n        raise DynamoDBMemoryError(\"DynamoDB table not initialized.\")\n    logger.warning(\n        f\"Clearing all items from DynamoDB table '{self.table_name}'\"\n        f\" via Scan/Delete. This may take time and consume capacity.\"\n    )\n    try:\n        with self._dynamodb_table.batch_writer() as batch:\n            scan_params = {\"ProjectionExpression\": f\"{self.partition_key_name}, {self.sort_key_name}\"}\n            while True:\n                response = self._dynamodb_table.scan(**scan_params)\n                items = response.get(\"Items\", [])\n                if not items:\n                    break\n                for item in items:\n                    batch.delete_item(\n                        Key={\n                            self.partition_key_name: item[self.partition_key_name],\n                            self.sort_key_name: item[self.sort_key_name],\n                        }\n                    )\n                if \"LastEvaluatedKey\" in response:\n                    scan_params[\"ExclusiveStartKey\"] = response[\"LastEvaluatedKey\"]\n                else:\n                    break\n        logger.info(f\"DynamoDB Memory ({self.table_name}): Finished clearing items.\")\n    except ClientError as e:\n        logger.error(f\"Error clearing DynamoDB table '{self.table_name}': {e}\")\n        raise DynamoDBMemoryError(f\"Error clearing DynamoDB memory: {e}\") from e\n    except Exception as e:\n        logger.error(f\"Unexpected error clearing DynamoDB memory: {e}\")\n        raise DynamoDBMemoryError(f\"Unexpected error clearing DynamoDB memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDB.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves messages via table scan, sorts chronologically.</p> Source code in <code>dynamiq/memory/backends/dynamo_db.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"Retrieves messages via table scan, sorts chronologically.\"\"\"\n    return self._scan_and_process(limit=limit)\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDB.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Searches messages via table scan, applying filters and query client-side.</p> Source code in <code>dynamiq/memory/backends/dynamo_db.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"Searches messages via table scan, applying filters and query client-side.\"\"\"\n    return self._scan_and_process(limit=limit, filters=filters, query=query)\n</code></pre>"},{"location":"dynamiq/memory/backends/dynamo_db/#dynamiq.memory.backends.dynamo_db.DynamoDBMemoryError","title":"<code>DynamoDBMemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for DynamoDB Memory Backend errors.</p> Source code in <code>dynamiq/memory/backends/dynamo_db.py</code> <pre><code>class DynamoDBMemoryError(Exception):\n    \"\"\"Base exception class for DynamoDB Memory Backend errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/","title":"In memory","text":""},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.BM25DocumentRanker","title":"<code>BM25DocumentRanker</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>BM25 implementation for scoring documents.</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>class BM25DocumentRanker(BaseModel):\n    \"\"\"BM25 implementation for scoring documents.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    documents: list[str]\n    k1: float = 1.5\n    b: float = 0.75\n    avg_dl: float = 0.0\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Initialize average document length after model creation.\"\"\"\n        self.avg_dl = self._calculate_avg_dl()\n\n    def _calculate_avg_dl(self) -&gt; float:\n        \"\"\"Calculates the average document length (number of terms per document).\"\"\"\n        if not self.documents:\n            return 0.0\n        total_length = sum(len(doc.lower().split()) for doc in self.documents)\n        return total_length / len(self.documents)\n\n    def _idf(self, term: str, N: int, df: int) -&gt; float:\n        \"\"\"Calculates the IDF (inverse document frequency) of a term.\"\"\"\n        return math.log((N - df + 0.5) / (df + 0.5) + 1)\n\n    def score(self, query_terms: list[str], document: str) -&gt; float:\n        \"\"\"Calculates the BM25 score for a document.\"\"\"\n        doc_terms = document.lower().split()\n        doc_len = len(doc_terms)\n        doc_term_freqs = Counter(doc_terms)\n        N = len(self.documents)\n        score = 0.0\n\n        for term in query_terms:\n            term_freq = doc_term_freqs.get(term, 0)\n            if term_freq == 0:\n                continue\n            df = sum(1 for doc in self.documents if term in doc.lower().split())\n            idf = self._idf(term, N, df)\n            numerator = term_freq * (self.k1 + 1)\n            denominator = term_freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_dl))\n            score += idf * (numerator / denominator)\n\n        return score\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.BM25DocumentRanker.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize average document length after model creation.</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Initialize average document length after model creation.\"\"\"\n    self.avg_dl = self._calculate_avg_dl()\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.BM25DocumentRanker.score","title":"<code>score(query_terms, document)</code>","text":"<p>Calculates the BM25 score for a document.</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def score(self, query_terms: list[str], document: str) -&gt; float:\n    \"\"\"Calculates the BM25 score for a document.\"\"\"\n    doc_terms = document.lower().split()\n    doc_len = len(doc_terms)\n    doc_term_freqs = Counter(doc_terms)\n    N = len(self.documents)\n    score = 0.0\n\n    for term in query_terms:\n        term_freq = doc_term_freqs.get(term, 0)\n        if term_freq == 0:\n            continue\n        df = sum(1 for doc in self.documents if term in doc.lower().split())\n        idf = self._idf(term, N, df)\n        numerator = term_freq * (self.k1 + 1)\n        denominator = term_freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_dl))\n        score += idf * (numerator / denominator)\n\n    return score\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory","title":"<code>InMemory</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>In-memory implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>class InMemory(MemoryBackend):\n    \"\"\"In-memory implementation of the memory storage backend.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"InMemory\"\n    messages: list[Message] = Field(default_factory=list)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {\"messages\": True}\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        return super().to_dict(include_secure_params=include_secure_params, **kwargs)\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"\n        Adds a message to the in-memory list.\n\n        Args:\n            message: Message to add to storage\n\n        Raises:\n            MemoryBackendError: If the message cannot be added\n        \"\"\"\n        self.messages.append(message)\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"\n        Retrieves all messages from the in-memory list.\n\n        Args:\n            limit: Maximum number of messages to return. If provided, returns the most recent messages.\n                  If None, returns all messages.\n\n        Returns:\n            List of messages sorted by timestamp (oldest first)\n        \"\"\"\n        # Sort messages by timestamp\n        sorted_messages = sorted(self.messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n\n        # Apply limit if provided\n        if limit and len(sorted_messages) &gt; limit:\n            return sorted_messages[-limit:]\n\n        return sorted_messages\n\n    def _apply_filters(self, messages: list[Message], filters: dict[str, Any] | None = None) -&gt; list[Message]:\n        \"\"\"\n        Applies metadata filters to the list of messages.\n\n        Args:\n            messages: List of messages to filter\n            filters: Metadata filters to apply\n\n        Returns:\n            Filtered list of messages\n        \"\"\"\n        if not filters:\n            return messages\n\n        filtered_messages = messages\n        for key, value in filters.items():\n            if isinstance(value, list):\n                filtered_messages = [msg for msg in filtered_messages if msg.metadata.get(key) in value]\n            else:\n                filtered_messages = [msg for msg in filtered_messages if msg.metadata.get(key) == value]\n\n        return filtered_messages\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"\n        Searches for messages using BM25 scoring, with optional filters.\n\n        Args:\n            query: Search query string (optional)\n            filters: Optional metadata filters to apply\n            limit: Maximum number of messages to return. If None, returns all matching messages.\n\n        Returns:\n            List of messages sorted by relevance score (highest first)\n\n        Raises:\n            MemoryBackendError: If the search operation fails\n        \"\"\"\n        # Apply filters first to reduce search space\n        filtered_messages = self._apply_filters(self.messages, filters)\n\n        # If no query provided, return filtered messages\n        if not query:\n            sorted_messages = sorted(filtered_messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n            if limit:\n                return sorted_messages[-limit:]\n            return sorted_messages\n\n        # Perform BM25 search with query\n        query_terms = query.lower().split()\n        document_texts = [msg.content for msg in filtered_messages]\n\n        # Handle empty document list\n        if not document_texts:\n            return []\n\n        # Calculate BM25 scores\n        bm25 = BM25DocumentRanker(documents=document_texts)\n        scored_messages = [(msg, bm25.score(query_terms, msg.content)) for msg in filtered_messages]\n\n        # Filter out zero scores\n        scored_messages = [(msg, score) for msg, score in scored_messages if score &gt; 0]\n\n        # Sort by score (descending)\n        scored_messages.sort(key=lambda x: x[1], reverse=True)\n\n        # Apply limit\n        result = [msg for msg, _ in scored_messages]\n        if limit:\n            return result[:limit]\n\n        return result\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"\n        Checks if the in-memory list is empty.\n\n        Returns:\n            True if the memory is empty, False otherwise\n        \"\"\"\n        return len(self.messages) == 0\n\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clears the in-memory list.\n\n        Raises:\n            MemoryBackendError: If the memory cannot be cleared\n        \"\"\"\n        self.messages = []\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.add","title":"<code>add(message)</code>","text":"<p>Adds a message to the in-memory list.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to add to storage</p> required <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the message cannot be added</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"\n    Adds a message to the in-memory list.\n\n    Args:\n        message: Message to add to storage\n\n    Raises:\n        MemoryBackendError: If the message cannot be added\n    \"\"\"\n    self.messages.append(message)\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.clear","title":"<code>clear()</code>","text":"<p>Clears the in-memory list.</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the memory cannot be cleared</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clears the in-memory list.\n\n    Raises:\n        MemoryBackendError: If the memory cannot be cleared\n    \"\"\"\n    self.messages = []\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves all messages from the in-memory list.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return. If provided, returns the most recent messages.   If None, returns all messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by timestamp (oldest first)</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"\n    Retrieves all messages from the in-memory list.\n\n    Args:\n        limit: Maximum number of messages to return. If provided, returns the most recent messages.\n              If None, returns all messages.\n\n    Returns:\n        List of messages sorted by timestamp (oldest first)\n    \"\"\"\n    # Sort messages by timestamp\n    sorted_messages = sorted(self.messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n\n    # Apply limit if provided\n    if limit and len(sorted_messages) &gt; limit:\n        return sorted_messages[-limit:]\n\n    return sorted_messages\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the in-memory list is empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the memory is empty, False otherwise</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"\n    Checks if the in-memory list is empty.\n\n    Returns:\n        True if the memory is empty, False otherwise\n    \"\"\"\n    return len(self.messages) == 0\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Searches for messages using BM25 scoring, with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str | None</code> <p>Search query string (optional)</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional metadata filters to apply</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return. If None, returns all matching messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by relevance score (highest first)</p> <p>Raises:</p> Type Description <code>MemoryBackendError</code> <p>If the search operation fails</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"\n    Searches for messages using BM25 scoring, with optional filters.\n\n    Args:\n        query: Search query string (optional)\n        filters: Optional metadata filters to apply\n        limit: Maximum number of messages to return. If None, returns all matching messages.\n\n    Returns:\n        List of messages sorted by relevance score (highest first)\n\n    Raises:\n        MemoryBackendError: If the search operation fails\n    \"\"\"\n    # Apply filters first to reduce search space\n    filtered_messages = self._apply_filters(self.messages, filters)\n\n    # If no query provided, return filtered messages\n    if not query:\n        sorted_messages = sorted(filtered_messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n        if limit:\n            return sorted_messages[-limit:]\n        return sorted_messages\n\n    # Perform BM25 search with query\n    query_terms = query.lower().split()\n    document_texts = [msg.content for msg in filtered_messages]\n\n    # Handle empty document list\n    if not document_texts:\n        return []\n\n    # Calculate BM25 scores\n    bm25 = BM25DocumentRanker(documents=document_texts)\n    scored_messages = [(msg, bm25.score(query_terms, msg.content)) for msg in filtered_messages]\n\n    # Filter out zero scores\n    scored_messages = [(msg, score) for msg, score in scored_messages if score &gt; 0]\n\n    # Sort by score (descending)\n    scored_messages.sort(key=lambda x: x[1], reverse=True)\n\n    # Apply limit\n    result = [msg for msg, _ in scored_messages]\n    if limit:\n        return result[:limit]\n\n    return result\n</code></pre>"},{"location":"dynamiq/memory/backends/in_memory/#dynamiq.memory.backends.in_memory.InMemory.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/in_memory.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    return super().to_dict(include_secure_params=include_secure_params, **kwargs)\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone","title":"<code>Pinecone</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>Pinecone memory backend implementation.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>class Pinecone(MemoryBackend):\n    \"\"\"Pinecone memory backend implementation.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = \"Pinecone\"\n    connection: PineconeConnection\n    embedder: DocumentEmbedder\n    index_type: PineconeIndexType\n    index_name: str = Field(default=\"conversations\")\n    dimension: int = Field(default=1536)\n    metric: PineconeSimilarityMetric = Field(default=PineconeSimilarityMetric.COSINE)\n    create_if_not_exist: bool = Field(default=True)\n    namespace: str = Field(default=\"default\")\n    cloud: str | None = Field(default=None)\n    region: str | None = Field(default=None)\n    environment: str | None = Field(default=None)\n    pod_type: str | None = Field(default=None)\n    pods: int = Field(default=1)\n    vector_store: PineconeVectorStore | None = None\n    message_truncation_enabled: bool = Field(\n        default=True, description=\"Enable automatic message truncation for embeddings\"\n    )\n    message_max_tokens: int = Field(default=6000, description=\"Maximum tokens for message content before truncation\")\n    message_truncation_method: TruncationMethod = Field(\n        default=TruncationMethod.START, description=\"Method to use for message truncation\"\n    )\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"Define parameters to exclude when converting the class instance to a dictionary.\"\"\"\n        return super().to_dict_exclude_params | {\n            \"embedder\": True,\n            \"vector_store\": True,\n            \"connection\": True,\n        }\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        kwargs.pop(\"include_secure_params\", None)\n        data = super().to_dict(**kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        data[\"embedder\"] = self.embedder.to_dict(\n            include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n        )\n        return data\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Initialize the vector store after model initialization.\"\"\"\n        if not self.vector_store:\n            self.vector_store = PineconeVectorStore(\n                connection=self.connection,\n                index_name=self.index_name,\n                namespace=self.namespace,\n                create_if_not_exist=self.create_if_not_exist,\n                dimension=self.dimension,\n                metric=self.metric,\n                index_type=self.index_type,\n                cloud=self.cloud,\n                region=self.region,\n                environment=self.environment,\n                pod_type=self.pod_type,\n                pods=self.pods,\n            )\n\n        if not self.vector_store._index:\n            raise PineconeError(\"Failed to initialize Pinecone index\")\n\n        # Configure embedder truncation settings\n        self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n        self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n        self.embedder.document_embedder.truncation_method = self.message_truncation_method\n\n    def _message_to_document(self, message: Message) -&gt; Document:\n        \"\"\"Converts a Message object to a Document object.\"\"\"\n        content = message.content\n        metadata = {\"role\": message.role.value, **(message.metadata or {})}\n\n        if self.message_truncation_enabled and content:\n            original_length = len(content)\n            truncated_content = truncate_text_for_embedding(\n                text=content,\n                max_tokens=self.message_max_tokens,\n                truncation_method=self.message_truncation_method\n            )\n\n            if len(truncated_content) &lt; original_length:\n                content = truncated_content\n                metadata[\"truncated\"] = True\n                metadata[\"original_length\"] = original_length\n                metadata[\"truncated_length\"] = len(content)\n                metadata[\"truncation_method\"] = self.message_truncation_method.value\n\n        return Document(\n            id=str(uuid.uuid4()),\n            content=content,\n            metadata=metadata,\n            embedding=None,\n        )\n\n    def _document_to_message(self, document: Document) -&gt; Message:\n        \"\"\"Converts a Document object to a Message object.\"\"\"\n        metadata = dict(document.metadata)\n        role = metadata.pop(\"role\")\n        return Message(content=document.content, role=role, metadata=metadata)\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Stores a message in Pinecone.\"\"\"\n        try:\n            document = self._message_to_document(message)\n            embedding_result = self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))\n            document_embedding = embedding_result.get(\"documents\")[0].embedding\n            document.embedding = document_embedding\n            self.vector_store.write_documents([document])\n\n        except Exception as e:\n            raise PineconeError(f\"Error adding message to Pinecone: {e}\") from e\n\n    def get_all(self, limit: int = 10000) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from Pinecone.\"\"\"\n        try:\n            documents = self.vector_store.list_documents(include_embeddings=False)\n            messages = [self._document_to_message(doc) for doc in documents]\n            return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n        except Exception as e:\n            raise PineconeError(f\"Error retrieving messages from Pinecone: {e}\") from e\n\n    def _prepare_filters(self, filters: dict | None = None) -&gt; dict | None:\n        \"\"\"Convert simple filters to Pinecone filter format.\"\"\"\n        if not filters:\n            return None\n\n        if all(isinstance(v, (str, int, float, bool)) for v in filters.values()):\n            conditions = []\n            for key, value in filters.items():\n                conditions.append({\"field\": key, \"operator\": \"==\", \"value\": value})\n            return {\"operator\": \"AND\", \"conditions\": conditions}\n        return filters\n\n    def search(self, query: str | None = None, filters: dict | None = None, limit: int = 1000) -&gt; list[Message]:\n        \"\"\"Searches for messages in Pinecone based on the query and/or filters.\"\"\"\n        try:\n            normalized_filters = self._prepare_filters(filters)\n\n            if query:\n                embedding_result = (\n                    self.embedder.execute(\n                        input_data=DocumentEmbedderInputSchema(\n                            documents=[Document(id=str(uuid.uuid4()), content=query)]\n                        )\n                    )\n                    .get(\"documents\")[0]\n                    .embedding\n                )\n                documents = self.vector_store._embedding_retrieval(\n                    query_embedding=embedding_result,\n                    namespace=self.namespace,\n                    filters=normalized_filters,\n                    top_k=limit,\n                    exclude_document_embeddings=True,\n                )\n            elif normalized_filters:\n                dummy_vector = [0.0] * self.vector_store.dimension\n                documents = self.vector_store._embedding_retrieval(\n                    query_embedding=dummy_vector,\n                    namespace=self.namespace,\n                    filters=normalized_filters,\n                    top_k=limit,\n                    exclude_document_embeddings=True,\n                )\n            else:\n                return []\n\n            return [self._document_to_message(doc) for doc in documents]\n        except Exception as e:\n            raise PineconeError(f\"Error searching in Pinecone: {e}\") from e\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the Pinecone index is empty.\"\"\"\n        try:\n            return self.vector_store.count_documents() == 0\n        except Exception as e:\n            raise PineconeError(f\"Error checking if Pinecone index is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the Pinecone index.\"\"\"\n        try:\n            self.vector_store.delete_documents(delete_all=True)\n        except Exception as e:\n            raise PineconeError(f\"Error clearing Pinecone index: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Define parameters to exclude when converting the class instance to a dictionary.</p>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.add","title":"<code>add(message)</code>","text":"<p>Stores a message in Pinecone.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Stores a message in Pinecone.\"\"\"\n    try:\n        document = self._message_to_document(message)\n        embedding_result = self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))\n        document_embedding = embedding_result.get(\"documents\")[0].embedding\n        document.embedding = document_embedding\n        self.vector_store.write_documents([document])\n\n    except Exception as e:\n        raise PineconeError(f\"Error adding message to Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.clear","title":"<code>clear()</code>","text":"<p>Clears the Pinecone index.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the Pinecone index.\"\"\"\n    try:\n        self.vector_store.delete_documents(delete_all=True)\n    except Exception as e:\n        raise PineconeError(f\"Error clearing Pinecone index: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.get_all","title":"<code>get_all(limit=10000)</code>","text":"<p>Retrieves all messages from Pinecone.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def get_all(self, limit: int = 10000) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from Pinecone.\"\"\"\n    try:\n        documents = self.vector_store.list_documents(include_embeddings=False)\n        messages = [self._document_to_message(doc) for doc in documents]\n        return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n    except Exception as e:\n        raise PineconeError(f\"Error retrieving messages from Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the Pinecone index is empty.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the Pinecone index is empty.\"\"\"\n    try:\n        return self.vector_store.count_documents() == 0\n    except Exception as e:\n        raise PineconeError(f\"Error checking if Pinecone index is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the vector store after model initialization.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Initialize the vector store after model initialization.\"\"\"\n    if not self.vector_store:\n        self.vector_store = PineconeVectorStore(\n            connection=self.connection,\n            index_name=self.index_name,\n            namespace=self.namespace,\n            create_if_not_exist=self.create_if_not_exist,\n            dimension=self.dimension,\n            metric=self.metric,\n            index_type=self.index_type,\n            cloud=self.cloud,\n            region=self.region,\n            environment=self.environment,\n            pod_type=self.pod_type,\n            pods=self.pods,\n        )\n\n    if not self.vector_store._index:\n        raise PineconeError(\"Failed to initialize Pinecone index\")\n\n    # Configure embedder truncation settings\n    self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n    self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n    self.embedder.document_embedder.truncation_method = self.message_truncation_method\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.search","title":"<code>search(query=None, filters=None, limit=1000)</code>","text":"<p>Searches for messages in Pinecone based on the query and/or filters.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def search(self, query: str | None = None, filters: dict | None = None, limit: int = 1000) -&gt; list[Message]:\n    \"\"\"Searches for messages in Pinecone based on the query and/or filters.\"\"\"\n    try:\n        normalized_filters = self._prepare_filters(filters)\n\n        if query:\n            embedding_result = (\n                self.embedder.execute(\n                    input_data=DocumentEmbedderInputSchema(\n                        documents=[Document(id=str(uuid.uuid4()), content=query)]\n                    )\n                )\n                .get(\"documents\")[0]\n                .embedding\n            )\n            documents = self.vector_store._embedding_retrieval(\n                query_embedding=embedding_result,\n                namespace=self.namespace,\n                filters=normalized_filters,\n                top_k=limit,\n                exclude_document_embeddings=True,\n            )\n        elif normalized_filters:\n            dummy_vector = [0.0] * self.vector_store.dimension\n            documents = self.vector_store._embedding_retrieval(\n                query_embedding=dummy_vector,\n                namespace=self.namespace,\n                filters=normalized_filters,\n                top_k=limit,\n                exclude_document_embeddings=True,\n            )\n        else:\n            return []\n\n        return [self._document_to_message(doc) for doc in documents]\n    except Exception as e:\n        raise PineconeError(f\"Error searching in Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.Pinecone.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    kwargs.pop(\"include_secure_params\", None)\n    data = super().to_dict(**kwargs)\n    data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n    data[\"embedder\"] = self.embedder.to_dict(\n        include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n    )\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/pinecone/#dynamiq.memory.backends.pinecone.PineconeError","title":"<code>PineconeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for Pinecone-related errors.</p> Source code in <code>dynamiq/memory/backends/pinecone.py</code> <pre><code>class PineconeError(Exception):\n    \"\"\"Base exception class for Pinecone-related errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/","title":"Postgresql","text":""},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.FetchMode","title":"<code>FetchMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for SQL fetch modes.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>class FetchMode(str, Enum):\n    \"\"\"Enum for SQL fetch modes.\"\"\"\n\n    ONE = \"one\"\n    ALL = \"all\"\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL","title":"<code>PostgreSQL</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>PostgreSQL implementation of the memory storage backend.</p> <p>Stores messages in a specified PostgreSQL table, using a JSONB column for metadata to allow flexible filtering.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>class PostgreSQL(MemoryBackend):\n    \"\"\"\n    PostgreSQL implementation of the memory storage backend.\n\n    Stores messages in a specified PostgreSQL table, using a JSONB column\n    for metadata to allow flexible filtering.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"PostgreSQL\"\n    connection: PostgreSQLConnection = Field(default_factory=PostgreSQLConnection)\n    table_name: str = Field(default=\"conversations\")\n    create_if_not_exist: bool = Field(default=True)\n\n    message_id_col: str = Field(default=\"message_id\")\n    role_col: str = Field(default=\"role\")\n    content_col: str = Field(default=\"content\")\n    metadata_col: str = Field(default=\"metadata\")\n    timestamp_col: str = Field(default=\"timestamp\")\n\n    _conn: psycopg.Connection | None = PrivateAttr(default=None)\n    _is_closed: bool = PrivateAttr(default=False)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return super().to_dict_exclude_params | {\n            \"_conn\": True,\n            \"_is_closed\": True,\n            \"connection\": True,\n        }\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params.copy())\n        data = self.model_dump(exclude=exclude, **kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        if \"type\" not in data:\n            data[\"type\"] = self.type\n        return data\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Initialize the PostgreSQL connection and ensure table exists.\"\"\"\n        try:\n            self._conn = self.connection.connect()\n            self._is_closed = False\n            if self.create_if_not_exist:\n                self._create_table_and_indices()\n            logger.debug(f\"PostgreSQL backend connected to table '{self.table_name}'.\")\n        except psycopg.Error as e:\n            logger.error(f\"Failed to initialize PostgreSQL connection or table '{self.table_name}': {e}\")\n            raise PostgresMemoryError(f\"Failed to initialize PostgreSQL connection or table: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error initializing PostgreSQL backend: {e}\")\n            raise PostgresMemoryError(f\"Unexpected error initializing PostgreSQL backend: {e}\") from e\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Explicitly close the PostgreSQL connection.\n\n        This is the recommended way to clean up resources when you're done\n        with the memory backend. Safe to call multiple times.\n        \"\"\"\n        if self._conn and not self._conn.closed:\n            try:\n                self._conn.close()\n                logger.debug(\"PostgreSQL connection closed explicitly.\")\n            except Exception as e:\n                logger.error(f\"Error closing PostgreSQL connection: {e}\")\n            finally:\n                self._is_closed = True\n        else:\n            self._is_closed = True\n\n    def __enter__(self):\n        \"\"\"Context manager entry point.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point - automatically close connection.\"\"\"\n        self.close()\n\n    def _check_connection_state(self) -&gt; None:\n        \"\"\"Check if the backend has been explicitly closed.\"\"\"\n        if self._is_closed:\n            raise PostgresMemoryError(\"PostgreSQL backend has been closed. Create a new instance to reconnect.\")\n\n    def _execute_sql(self, sql_query: SQL | str, params: tuple | list | None = None, fetch: FetchMode | None = None):\n        \"\"\"Helper to execute SQL, handling potential connection issues.\"\"\"\n        self._check_connection_state()\n\n        if self._conn is None or self._conn.closed:\n            logger.warning(\"PostgreSQL connection lost or not initialized. Attempting to reconnect.\")\n            try:\n                self._conn = self.connection.connect()\n            except Exception as e:\n                raise PostgresMemoryError(f\"Failed to re-establish PostgreSQL connection: {e}\") from e\n\n        try:\n            with self._conn.cursor() as cur:\n                cur.execute(sql_query, params)\n                if fetch == FetchMode.ONE:\n                    return cur.fetchone()\n                elif fetch == FetchMode.ALL:\n                    return cur.fetchall()\n                return None\n        except psycopg.Error as e:\n            sql_str = sql_query.as_string(cur) if isinstance(sql_query, SQL) else str(sql_query)\n            logger.error(f\"PostgreSQL error executing SQL: {e}\\nSQL: {sql_str}\\nParams: {params}\")\n            raise PostgresMemoryError(f\"PostgreSQL error: {e}\") from e\n\n    def _create_table_and_indices(self) -&gt; None:\n        \"\"\"Creates the table and necessary indices if they don't exist.\"\"\"\n        logger.debug(f\"Ensuring table '{self.table_name}' and indices exist...\")\n\n        table_sql = SQL(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS {table_name} (\n                {message_id_col} UUID PRIMARY KEY,\n                {role_col} TEXT NOT NULL,\n                {content_col} TEXT,\n                {metadata_col} JSONB,\n                {timestamp_col} DOUBLE PRECISION NOT NULL\n            );\n        \"\"\"\n        ).format(\n            table_name=Identifier(self.table_name),\n            message_id_col=Identifier(self.message_id_col),\n            role_col=Identifier(self.role_col),\n            content_col=Identifier(self.content_col),\n            metadata_col=Identifier(self.metadata_col),\n            timestamp_col=Identifier(self.timestamp_col),\n        )\n        self._execute_sql(table_sql)\n\n        table_short_name = \"\".join(filter(str.isalnum, self.table_name))[:10]\n\n        ts_index_sql = SQL(\n            \"\"\"\n            CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} ({timestamp_col});\n        \"\"\"\n        ).format(\n            index_name=Identifier(f\"idx_{table_short_name}_timestamp\"),\n            table_name=Identifier(self.table_name),\n            timestamp_col=Identifier(self.timestamp_col),\n        )\n        self._execute_sql(ts_index_sql)\n\n        meta_index_sql = SQL(\n            \"\"\"\n            CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} USING GIN ({metadata_col});\n        \"\"\"\n        ).format(\n            index_name=Identifier(f\"idx_{table_short_name}_metadata_gin\"),\n            table_name=Identifier(self.table_name),\n            metadata_col=Identifier(self.metadata_col),\n        )\n        self._execute_sql(meta_index_sql)\n        logger.debug(f\"Table '{self.table_name}' and indices checked/created.\")\n\n    def _row_to_message(self, row: dict) -&gt; Message:\n        \"\"\"Converts a database row (dict) to a Message object.\"\"\"\n        metadata = row.get(self.metadata_col) or {}\n        if \"timestamp\" not in metadata:\n            metadata[\"timestamp\"] = row.get(self.timestamp_col)\n        if \"message_id\" not in metadata:\n            metadata[\"message_id\"] = row.get(self.message_id_col)\n\n        return Message(\n            role=MessageRole(row.get(self.role_col, MessageRole.USER.value)),\n            content=row.get(self.content_col, \"\"),\n            metadata=metadata,\n        )\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Adds a message to the PostgreSQL table.\"\"\"\n        try:\n            message_id = message.metadata.get(\"message_id\", uuid.uuid4())\n            timestamp = float(message.metadata.get(\"timestamp\", time.time()))\n            metadata_to_store = message.metadata or {}\n\n            sql = SQL(\n                \"\"\"\n                INSERT INTO {table_name} ({message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col})\n                VALUES (%s, %s, %s, %s, %s);\n            \"\"\"\n            ).format(\n                table_name=Identifier(self.table_name),\n                message_id_col=Identifier(self.message_id_col),\n                role_col=Identifier(self.role_col),\n                content_col=Identifier(self.content_col),\n                metadata_col=Identifier(self.metadata_col),\n                timestamp_col=Identifier(self.timestamp_col),\n            )\n            params = (\n                message_id,\n                message.role.value,\n                message.content,\n                json.dumps(metadata_to_store),\n                timestamp,\n            )\n            self._execute_sql(sql, params)\n            logger.debug(f\"PostgreSQL Memory ({self.table_name}): Added message {message_id}\")\n\n        except (TypeError, ValueError) as e:\n            logger.error(f\"Error preparing message data for PostgreSQL: {e}\")\n            raise PostgresMemoryError(f\"Error preparing message data: {e}\") from e\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"Retrieves messages from PostgreSQL, sorted chronologically.\"\"\"\n        sql = SQL(\n            \"\"\"\n            SELECT {message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col}\n            FROM {table_name}\n            ORDER BY {timestamp_col} ASC\n        \"\"\"\n        ).format(\n            table_name=Identifier(self.table_name),\n            message_id_col=Identifier(self.message_id_col),\n            role_col=Identifier(self.role_col),\n            content_col=Identifier(self.content_col),\n            metadata_col=Identifier(self.metadata_col),\n            timestamp_col=Identifier(self.timestamp_col),\n        )\n\n        params = []\n        if limit is not None and limit &gt; 0:\n            sql = sql + SQL(\" LIMIT %s\")\n            params.append(limit)\n\n        rows = self._execute_sql(sql, params, fetch=FetchMode.ALL)\n        messages = [self._row_to_message(row) for row in rows]\n        logger.debug(f\"PostgreSQL Memory ({self.table_name}): Retrieved {len(messages)} messages.\")\n        return messages\n\n    def _build_where_clause(self, query: str | None, filters: dict | None) -&gt; tuple[SQL, list]:\n        \"\"\"Builds the WHERE clause and parameters for search.\"\"\"\n        where_clauses = []\n        params = []\n\n        if filters:\n            for key, value in filters.items():\n                where_clauses.append(SQL(\"{metadata_col}-&gt;&gt;%s = %s\").format(metadata_col=Identifier(self.metadata_col)))\n                params.extend([key, str(value)])  # Compare as text\n\n        if query:\n            where_clauses.append(SQL(\"{content_col} ILIKE %s\").format(content_col=Identifier(self.content_col)))\n            params.append(f\"%{query}%\")\n\n        if not where_clauses:\n            return SQL(\"\"), []\n\n        where_clause_sql = SQL(\"WHERE \") + SQL(\" AND \").join(where_clauses)\n        return where_clause_sql, params\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"Searches messages using ILIKE for query and JSONB operators for filters.\"\"\"\n\n        where_clause, params = self._build_where_clause(query, filters)\n\n        sql = SQL(\n            \"\"\"\n            SELECT {message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col}\n            FROM {table_name}\n        \"\"\"\n        ).format(\n            table_name=Identifier(self.table_name),\n            message_id_col=Identifier(self.message_id_col),\n            role_col=Identifier(self.role_col),\n            content_col=Identifier(self.content_col),\n            metadata_col=Identifier(self.metadata_col),\n            timestamp_col=Identifier(self.timestamp_col),\n        )\n\n        if where_clause:\n            sql = sql + SQL(\" \") + where_clause\n\n        sql = sql + SQL(\" ORDER BY {timestamp_col} DESC\").format(timestamp_col=Identifier(self.timestamp_col))\n\n        if limit is not None and limit &gt; 0:\n            sql = sql + SQL(\" LIMIT %s\")\n            params.append(limit)\n\n        rows = self._execute_sql(sql, params, fetch=FetchMode.ALL)\n        messages = [self._row_to_message(row) for row in rows]\n\n        logger.debug(\n            f\"PostgreSQL Memory ({self.table_name}): Found {len(messages)} search results \"\n            f\"(Query: {'Yes' if query else 'No'}, Filters: {'Yes' if filters else 'No'}, Limit: {limit})\"\n        )\n        return messages\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the PostgreSQL table is empty.\"\"\"\n        sql = SQL(\"SELECT EXISTS (SELECT 1 FROM {table_name} LIMIT 1);\").format(table_name=Identifier(self.table_name))\n        result = self._execute_sql(sql, fetch=FetchMode.ONE)\n        return not (result and result.get(\"exists\", False))\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the PostgreSQL table using TRUNCATE.\"\"\"\n        logger.warning(f\"Clearing all messages from PostgreSQL table '{self.table_name}' using TRUNCATE.\")\n        sql = SQL(\"TRUNCATE TABLE {table_name};\").format(table_name=Identifier(self.table_name))\n        self._execute_sql(sql)\n        logger.info(f\"PostgreSQL Memory ({self.table_name}): Cleared table.\")\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry point.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry point.\"\"\"\n    return self\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit point - automatically close connection.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit point - automatically close connection.\"\"\"\n    self.close()\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.add","title":"<code>add(message)</code>","text":"<p>Adds a message to the PostgreSQL table.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Adds a message to the PostgreSQL table.\"\"\"\n    try:\n        message_id = message.metadata.get(\"message_id\", uuid.uuid4())\n        timestamp = float(message.metadata.get(\"timestamp\", time.time()))\n        metadata_to_store = message.metadata or {}\n\n        sql = SQL(\n            \"\"\"\n            INSERT INTO {table_name} ({message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col})\n            VALUES (%s, %s, %s, %s, %s);\n        \"\"\"\n        ).format(\n            table_name=Identifier(self.table_name),\n            message_id_col=Identifier(self.message_id_col),\n            role_col=Identifier(self.role_col),\n            content_col=Identifier(self.content_col),\n            metadata_col=Identifier(self.metadata_col),\n            timestamp_col=Identifier(self.timestamp_col),\n        )\n        params = (\n            message_id,\n            message.role.value,\n            message.content,\n            json.dumps(metadata_to_store),\n            timestamp,\n        )\n        self._execute_sql(sql, params)\n        logger.debug(f\"PostgreSQL Memory ({self.table_name}): Added message {message_id}\")\n\n    except (TypeError, ValueError) as e:\n        logger.error(f\"Error preparing message data for PostgreSQL: {e}\")\n        raise PostgresMemoryError(f\"Error preparing message data: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.clear","title":"<code>clear()</code>","text":"<p>Clears the PostgreSQL table using TRUNCATE.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the PostgreSQL table using TRUNCATE.\"\"\"\n    logger.warning(f\"Clearing all messages from PostgreSQL table '{self.table_name}' using TRUNCATE.\")\n    sql = SQL(\"TRUNCATE TABLE {table_name};\").format(table_name=Identifier(self.table_name))\n    self._execute_sql(sql)\n    logger.info(f\"PostgreSQL Memory ({self.table_name}): Cleared table.\")\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.close","title":"<code>close()</code>","text":"<p>Explicitly close the PostgreSQL connection.</p> <p>This is the recommended way to clean up resources when you're done with the memory backend. Safe to call multiple times.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Explicitly close the PostgreSQL connection.\n\n    This is the recommended way to clean up resources when you're done\n    with the memory backend. Safe to call multiple times.\n    \"\"\"\n    if self._conn and not self._conn.closed:\n        try:\n            self._conn.close()\n            logger.debug(\"PostgreSQL connection closed explicitly.\")\n        except Exception as e:\n            logger.error(f\"Error closing PostgreSQL connection: {e}\")\n        finally:\n            self._is_closed = True\n    else:\n        self._is_closed = True\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves messages from PostgreSQL, sorted chronologically.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"Retrieves messages from PostgreSQL, sorted chronologically.\"\"\"\n    sql = SQL(\n        \"\"\"\n        SELECT {message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col}\n        FROM {table_name}\n        ORDER BY {timestamp_col} ASC\n    \"\"\"\n    ).format(\n        table_name=Identifier(self.table_name),\n        message_id_col=Identifier(self.message_id_col),\n        role_col=Identifier(self.role_col),\n        content_col=Identifier(self.content_col),\n        metadata_col=Identifier(self.metadata_col),\n        timestamp_col=Identifier(self.timestamp_col),\n    )\n\n    params = []\n    if limit is not None and limit &gt; 0:\n        sql = sql + SQL(\" LIMIT %s\")\n        params.append(limit)\n\n    rows = self._execute_sql(sql, params, fetch=FetchMode.ALL)\n    messages = [self._row_to_message(row) for row in rows]\n    logger.debug(f\"PostgreSQL Memory ({self.table_name}): Retrieved {len(messages)} messages.\")\n    return messages\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the PostgreSQL table is empty.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the PostgreSQL table is empty.\"\"\"\n    sql = SQL(\"SELECT EXISTS (SELECT 1 FROM {table_name} LIMIT 1);\").format(table_name=Identifier(self.table_name))\n    result = self._execute_sql(sql, fetch=FetchMode.ONE)\n    return not (result and result.get(\"exists\", False))\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the PostgreSQL connection and ensure table exists.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Initialize the PostgreSQL connection and ensure table exists.\"\"\"\n    try:\n        self._conn = self.connection.connect()\n        self._is_closed = False\n        if self.create_if_not_exist:\n            self._create_table_and_indices()\n        logger.debug(f\"PostgreSQL backend connected to table '{self.table_name}'.\")\n    except psycopg.Error as e:\n        logger.error(f\"Failed to initialize PostgreSQL connection or table '{self.table_name}': {e}\")\n        raise PostgresMemoryError(f\"Failed to initialize PostgreSQL connection or table: {e}\") from e\n    except Exception as e:\n        logger.error(f\"Unexpected error initializing PostgreSQL backend: {e}\")\n        raise PostgresMemoryError(f\"Unexpected error initializing PostgreSQL backend: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Searches messages using ILIKE for query and JSONB operators for filters.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"Searches messages using ILIKE for query and JSONB operators for filters.\"\"\"\n\n    where_clause, params = self._build_where_clause(query, filters)\n\n    sql = SQL(\n        \"\"\"\n        SELECT {message_id_col}, {role_col}, {content_col}, {metadata_col}, {timestamp_col}\n        FROM {table_name}\n    \"\"\"\n    ).format(\n        table_name=Identifier(self.table_name),\n        message_id_col=Identifier(self.message_id_col),\n        role_col=Identifier(self.role_col),\n        content_col=Identifier(self.content_col),\n        metadata_col=Identifier(self.metadata_col),\n        timestamp_col=Identifier(self.timestamp_col),\n    )\n\n    if where_clause:\n        sql = sql + SQL(\" \") + where_clause\n\n    sql = sql + SQL(\" ORDER BY {timestamp_col} DESC\").format(timestamp_col=Identifier(self.timestamp_col))\n\n    if limit is not None and limit &gt; 0:\n        sql = sql + SQL(\" LIMIT %s\")\n        params.append(limit)\n\n    rows = self._execute_sql(sql, params, fetch=FetchMode.ALL)\n    messages = [self._row_to_message(row) for row in rows]\n\n    logger.debug(\n        f\"PostgreSQL Memory ({self.table_name}): Found {len(messages)} search results \"\n        f\"(Query: {'Yes' if query else 'No'}, Filters: {'Yes' if filters else 'No'}, Limit: {limit})\"\n    )\n    return messages\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgreSQL.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params.copy())\n    data = self.model_dump(exclude=exclude, **kwargs)\n    data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n    if \"type\" not in data:\n        data[\"type\"] = self.type\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/postgresql/#dynamiq.memory.backends.postgresql.PostgresMemoryError","title":"<code>PostgresMemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for PostgreSQL Memory Backend errors.</p> Source code in <code>dynamiq/memory/backends/postgresql.py</code> <pre><code>class PostgresMemoryError(Exception):\n    \"\"\"Base exception class for PostgreSQL Memory Backend errors.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant","title":"<code>Qdrant</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>Qdrant implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>class Qdrant(MemoryBackend):\n    \"\"\"Qdrant implementation of the memory storage backend.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"Qdrant\"\n    connection: QdrantConnection\n    embedder: DocumentEmbedder\n    index_name: str = Field(default=\"conversations\")\n    dimension: int = Field(default=1536)\n    metric: str = Field(default=\"cosine\")\n    on_disk: bool = Field(default=False)\n    create_if_not_exist: bool = Field(default=True)\n    recreate_index: bool = Field(default=False)\n    vector_store: QdrantVectorStore | None = None\n    message_truncation_enabled: bool = Field(\n        default=True, description=\"Enable automatic message truncation for embeddings\"\n    )\n    message_max_tokens: int = Field(default=6000, description=\"Maximum tokens for message content before truncation\")\n    message_truncation_method: TruncationMethod = Field(\n        default=TruncationMethod.START, description=\"Method to use for message truncation\"\n    )\n    _client: QdrantClient | None = PrivateAttr(default=None)\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"Define parameters to exclude when converting the class instance to a dictionary.\"\"\"\n        return super().to_dict_exclude_params | {\n            \"embedder\": True,\n            \"vector_store\": True,\n            \"connection\": True,\n        }\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        kwargs.pop(\"include_secure_params\", None)\n        data = super().to_dict(**kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        data[\"embedder\"] = self.embedder.to_dict(\n            include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n        )\n        return data\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Initialize the vector store after model initialization.\"\"\"\n        if not self.vector_store:\n            self.vector_store = QdrantVectorStore(\n                connection=self.connection,\n                index_name=self.index_name,\n                dimension=self.dimension,\n                metric=self.metric,\n                on_disk=self.on_disk,\n                create_if_not_exist=self.create_if_not_exist,\n                recreate_index=self.recreate_index,\n            )\n\n        self._client = self.vector_store._client\n        if not self._client:\n            raise QdrantError(\"Failed to initialize Qdrant client\")\n\n        # Configure embedder truncation settings\n        self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n        self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n        self.embedder.document_embedder.truncation_method = self.message_truncation_method\n\n    def _message_to_document(self, message: Message) -&gt; Document:\n        \"\"\"Converts a Message object to a Document object.\"\"\"\n        content = message.content\n        metadata = {\"role\": message.role.value, **(message.metadata or {})}\n\n        if self.message_truncation_enabled and content:\n            original_length = len(content)\n            truncated_content = truncate_text_for_embedding(\n                text=content,\n                max_tokens=self.message_max_tokens,\n                truncation_method=self.message_truncation_method\n            )\n\n            if len(truncated_content) &lt; original_length:\n                content = truncated_content\n                metadata[\"truncated\"] = True\n                metadata[\"original_length\"] = original_length\n                metadata[\"truncated_length\"] = len(content)\n                metadata[\"truncation_method\"] = self.message_truncation_method.value\n\n        return Document(\n            id=str(uuid.uuid4()),\n            content=content,\n            metadata=metadata,\n            embedding=None,\n        )\n\n    def _document_to_message(self, document: Document) -&gt; Message:\n        \"\"\"Converts a Document object to a Message object.\"\"\"\n        metadata = dict(document.metadata)\n        role = metadata.pop(\"role\")\n        return Message(content=document.content, role=role, metadata=metadata)\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Stores a message in Qdrant.\"\"\"\n        try:\n            document = self._message_to_document(message)\n            embedding_result = (\n                self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))\n                .get(\"documents\")[0]\n                .embedding\n            )\n            document.embedding = embedding_result\n\n            self.vector_store.write_documents(documents=[document], policy=DuplicatePolicy.SKIP)\n        except Exception as e:\n            raise QdrantError(f\"Failed to add message to Qdrant: {e}\") from e\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from Qdrant.\"\"\"\n        try:\n            documents = self.vector_store.list_documents(include_embeddings=False)\n            messages = [self._document_to_message(doc) for doc in documents]\n            return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n        except Exception as e:\n            raise QdrantError(f\"Failed to retrieve messages from Qdrant: {e}\") from e\n\n    def search(self, query: str | None = None, limit: int = 1000, filters: dict | None = None) -&gt; list[Message]:\n        \"\"\"Searches for messages in Qdrant.\"\"\"\n        try:\n            try:\n                if not self._collection_exists():\n                    if self.create_if_not_exist:\n                        self._create_collection()\n                    else:\n                        return []\n            except Exception:\n                return []\n\n            qdrant_filters = self._prepare_filters(filters)\n            if query:\n                embedding_result = (\n                    self.embedder.execute(\n                        input_data=DocumentEmbedderInputSchema(documents=[Document(id=\"query\", content=query)])\n                    )\n                    .get(\"documents\")[0]\n                    .embedding\n                )\n                documents = self.vector_store._query_by_embedding(\n                    query_embedding=embedding_result,\n                    filters=qdrant_filters,\n                    top_k=limit,\n                    return_embedding=False,\n                )\n            elif filters:\n                documents = self.vector_store.filter_documents(filters=qdrant_filters)\n                if limit:\n                    documents = documents[:limit]\n            else:\n                return []\n\n            return [self._document_to_message(doc) for doc in documents]\n        except Exception as e:\n            raise QdrantError(f\"Error searching in Qdrant: {e}\") from e\n\n    def _prepare_filters(self, filters: dict | None = None) -&gt; dict | None:\n        \"\"\"Prepares simple filters for Qdrant vector store format.\"\"\"\n        if not filters:\n            return None\n\n        conditions = []\n        for key, value in filters.items():\n            if isinstance(value, (str, int, float, bool)):\n                condition = {\"operator\": \"==\", \"field\": key, \"value\": value}\n            elif isinstance(value, list):\n                condition = {\"operator\": \"in\", \"field\": key, \"value\": value}\n            elif isinstance(value, dict) and any(k in value for k in [\"gte\", \"lte\", \"gt\", \"lt\"]):\n                condition = {\"operator\": \"range\", \"field\": key, **value}\n            else:\n                raise QdrantError(f\"Unsupported filter value type for key '{key}': {type(value)}\")\n\n            conditions.append(condition)\n        return {\"operator\": \"AND\", \"conditions\": conditions} if conditions else None\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the Qdrant collection is empty.\"\"\"\n        try:\n            return self.vector_store.count_documents() == 0\n        except UnexpectedResponse as e:\n            if e.status_code == 404:  # Collection doesn't exist\n                return True\n            raise QdrantError(f\"Failed to check if Qdrant collection is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the Qdrant collection.\"\"\"\n        try:\n            self.vector_store.delete_documents(delete_all=True)\n        except Exception as e:\n            raise QdrantError(f\"Failed to clear Qdrant collection: {e}\") from e\n\n    def _collection_exists(self) -&gt; bool:\n        \"\"\"Check if the collection exists in Qdrant.\"\"\"\n        collections = self._client.get_collections()\n        return any(collection.name == self.index_name for collection in collections.collections)\n\n    def _create_collection(self) -&gt; None:\n        \"\"\"Create the collection in Qdrant.\"\"\"\n        self._client.create_collection(\n            collection_name=self.index_name,\n            vectors_config={\"default\": {\"size\": self.dimension, \"distance\": self.metric}},\n        )\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Define parameters to exclude when converting the class instance to a dictionary.</p>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.add","title":"<code>add(message)</code>","text":"<p>Stores a message in Qdrant.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Stores a message in Qdrant.\"\"\"\n    try:\n        document = self._message_to_document(message)\n        embedding_result = (\n            self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))\n            .get(\"documents\")[0]\n            .embedding\n        )\n        document.embedding = embedding_result\n\n        self.vector_store.write_documents(documents=[document], policy=DuplicatePolicy.SKIP)\n    except Exception as e:\n        raise QdrantError(f\"Failed to add message to Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.clear","title":"<code>clear()</code>","text":"<p>Clears the Qdrant collection.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the Qdrant collection.\"\"\"\n    try:\n        self.vector_store.delete_documents(delete_all=True)\n    except Exception as e:\n        raise QdrantError(f\"Failed to clear Qdrant collection: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves all messages from Qdrant.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from Qdrant.\"\"\"\n    try:\n        documents = self.vector_store.list_documents(include_embeddings=False)\n        messages = [self._document_to_message(doc) for doc in documents]\n        return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n    except Exception as e:\n        raise QdrantError(f\"Failed to retrieve messages from Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the Qdrant collection is empty.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the Qdrant collection is empty.\"\"\"\n    try:\n        return self.vector_store.count_documents() == 0\n    except UnexpectedResponse as e:\n        if e.status_code == 404:  # Collection doesn't exist\n            return True\n        raise QdrantError(f\"Failed to check if Qdrant collection is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the vector store after model initialization.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Initialize the vector store after model initialization.\"\"\"\n    if not self.vector_store:\n        self.vector_store = QdrantVectorStore(\n            connection=self.connection,\n            index_name=self.index_name,\n            dimension=self.dimension,\n            metric=self.metric,\n            on_disk=self.on_disk,\n            create_if_not_exist=self.create_if_not_exist,\n            recreate_index=self.recreate_index,\n        )\n\n    self._client = self.vector_store._client\n    if not self._client:\n        raise QdrantError(\"Failed to initialize Qdrant client\")\n\n    # Configure embedder truncation settings\n    self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n    self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n    self.embedder.document_embedder.truncation_method = self.message_truncation_method\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.search","title":"<code>search(query=None, limit=1000, filters=None)</code>","text":"<p>Searches for messages in Qdrant.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def search(self, query: str | None = None, limit: int = 1000, filters: dict | None = None) -&gt; list[Message]:\n    \"\"\"Searches for messages in Qdrant.\"\"\"\n    try:\n        try:\n            if not self._collection_exists():\n                if self.create_if_not_exist:\n                    self._create_collection()\n                else:\n                    return []\n        except Exception:\n            return []\n\n        qdrant_filters = self._prepare_filters(filters)\n        if query:\n            embedding_result = (\n                self.embedder.execute(\n                    input_data=DocumentEmbedderInputSchema(documents=[Document(id=\"query\", content=query)])\n                )\n                .get(\"documents\")[0]\n                .embedding\n            )\n            documents = self.vector_store._query_by_embedding(\n                query_embedding=embedding_result,\n                filters=qdrant_filters,\n                top_k=limit,\n                return_embedding=False,\n            )\n        elif filters:\n            documents = self.vector_store.filter_documents(filters=qdrant_filters)\n            if limit:\n                documents = documents[:limit]\n        else:\n            return []\n\n        return [self._document_to_message(doc) for doc in documents]\n    except Exception as e:\n        raise QdrantError(f\"Error searching in Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.Qdrant.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    kwargs.pop(\"include_secure_params\", None)\n    data = super().to_dict(**kwargs)\n    data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n    data[\"embedder\"] = self.embedder.to_dict(\n        include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n    )\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/qdrant/#dynamiq.memory.backends.qdrant.QdrantError","title":"<code>QdrantError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Qdrant-related errors.</p> Source code in <code>dynamiq/memory/backends/qdrant.py</code> <pre><code>class QdrantError(Exception):\n    \"\"\"Base exception for Qdrant-related errors.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/","title":"Sqlite","text":""},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite","title":"<code>SQLite</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>SQLite implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>class SQLite(MemoryBackend):\n    \"\"\"SQLite implementation of the memory storage backend.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"SQLite\"\n    db_path: Annotated[str, Field(default=\"conversations.db\")]\n    index_name: Annotated[str, Field(default=\"conversations\")]\n\n    # SQL Query Constants\n    CREATE_TABLE_QUERY: ClassVar[\n        str\n    ] = \"\"\"\n        CREATE TABLE IF NOT EXISTS {index_name} (\n            id TEXT PRIMARY KEY,\n            role TEXT NOT NULL,\n            content TEXT NOT NULL,\n            metadata TEXT,\n            timestamp REAL\n        )\n    \"\"\"\n\n    VALIDATE_TABLE_QUERY: ClassVar[str] = \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\"\n    INSERT_MESSAGE_QUERY: ClassVar[\n        str\n    ] = \"\"\"\n        INSERT INTO {index_name} (id, role, content, metadata, timestamp)\n        VALUES (?, ?, ?, ?, ?)\n    \"\"\"\n    SELECT_ALL_MESSAGES_QUERY: ClassVar[\n        str\n    ] = \"\"\"\n        SELECT id, role, content, metadata, timestamp\n        FROM {index_name}\n        ORDER BY timestamp ASC\n    \"\"\"\n    CHECK_IF_EMPTY_QUERY: ClassVar[str] = \"SELECT COUNT(*) FROM {index_name}\"\n    CLEAR_TABLE_QUERY: ClassVar[str] = \"DELETE FROM {index_name}\"\n    SEARCH_MESSAGES_QUERY: ClassVar[\n        str\n    ] = \"\"\"\n        SELECT id, role, content, metadata\n        FROM {index_name}\n    \"\"\"\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return super().to_dict_exclude_params | {\n            \"CREATE_TABLE_QUERY\": True,\n            \"VALIDATE_TABLE_QUERY\": True,\n            \"INSERT_MESSAGE_QUERY\": True,\n            \"SELECT_ALL_MESSAGES_QUERY\": True,\n            \"CHECK_IF_EMPTY_QUERY\": True,\n            \"CLEAR_TABLE_QUERY\": True,\n            \"SEARCH_MESSAGES_QUERY\": True,\n        }\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Args:\n            include_secure_params (bool): Whether to include secure parameters\n            **kwargs: Additional arguments\n\n        Returns:\n            dict: Dictionary representation of the instance\n        \"\"\"\n        kwargs.pop(\"include_secure_params\", None)\n        kwargs.pop(\"for_tracing\", None)\n        data = super().to_dict(**kwargs)\n\n        if not include_secure_params:\n            data.pop(\"db_path\", None)\n\n        return data\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Initialize the SQLite database after model initialization.\"\"\"\n        try:\n            self._validate_table_name(create_if_not_exists=True)\n        except Exception as e:\n            raise SQLiteError(f\"Error initializing SQLite backend: {e}\") from e\n\n    def _validate_table_name(self, create_if_not_exists: bool = False) -&gt; None:\n        \"\"\"Validates the table name to prevent SQL injection and optionally creates it.\"\"\"\n        if not re.match(r\"^[A-Za-z0-9_]+$\", self.index_name):\n            raise SQLiteError(f\"Invalid table name: '{self.index_name}'\")\n\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(self.VALIDATE_TABLE_QUERY, (self.index_name,))\n                result = cursor.fetchone()\n\n                if result is None:\n                    if create_if_not_exists:\n                        self._create_table()\n                    else:\n                        raise SQLiteError(f\"Table '{self.index_name}' does not exist in the database.\")\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error validating or creating table: {e}\") from e\n\n    def _create_table(self) -&gt; None:\n        \"\"\"Creates the messages table.\"\"\"\n        query = self.CREATE_TABLE_QUERY.format(index_name=self.index_name)\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                conn.commit()\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error creating table: {e}\") from e\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Stores a message in the SQLite database.\"\"\"\n        try:\n            query = self.INSERT_MESSAGE_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                message_id = str(uuid.uuid4())\n                cursor.execute(\n                    query,\n                    (\n                        message_id,\n                        message.role.value,\n                        message.content,\n                        json.dumps(message.metadata),\n                        message.metadata.get(\"timestamp\", 0),\n                    ),\n                )\n                conn.commit()\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error adding message to database: {e}\") from e\n\n    def get_all(self) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from the SQLite database.\"\"\"\n        try:\n            query = self.SELECT_ALL_MESSAGES_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                rows = cursor.fetchall()\n            return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error retrieving messages from database: {e}\") from e\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the SQLite database is empty.\"\"\"\n        try:\n            query = self.CHECK_IF_EMPTY_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                count = cursor.fetchone()[0]\n            return count == 0\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error checking if database is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the SQLite database by deleting all rows in the table.\"\"\"\n        try:\n            query = self.CLEAR_TABLE_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                conn.commit()\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error clearing database: {e}\") from e\n\n    def search(self, query: str | None = None, limit: int = 10, filters: dict | None = None) -&gt; list[Message]:\n        \"\"\"Searches for messages in SQLite based on the query and/or filters.\"\"\"\n        try:\n            where_clauses = []\n            params = []\n\n            if query:\n                where_clauses.append(\"content LIKE ?\")\n                params.append(f\"%{query}%\")\n\n            if filters:\n                for key, value in filters.items():\n                    if isinstance(value, list):\n                        placeholders = \",\".join(\"?\" for _ in value)\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') IN ({placeholders})\")\n                        params.extend(value)\n                    else:\n                        if isinstance(value, str) and \"%\" in value:\n                            where_clauses.append(f\"json_extract(metadata, '$.{key}') LIKE ?\")\n                            params.append(value)\n                        else:\n                            where_clauses.append(f\"json_extract(metadata, '$.{key}') = ?\")\n                            params.append(value)\n\n            query_str = self.SEARCH_MESSAGES_QUERY.format(index_name=self.index_name)\n            if where_clauses:\n                query_str += f\" WHERE {' AND '.join(where_clauses)}\"\n            query_str += \" ORDER BY id DESC LIMIT ?\"\n            params.append(limit)\n\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query_str, params)\n                rows = cursor.fetchall()\n\n            return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error searching in database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.add","title":"<code>add(message)</code>","text":"<p>Stores a message in the SQLite database.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Stores a message in the SQLite database.\"\"\"\n    try:\n        query = self.INSERT_MESSAGE_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            message_id = str(uuid.uuid4())\n            cursor.execute(\n                query,\n                (\n                    message_id,\n                    message.role.value,\n                    message.content,\n                    json.dumps(message.metadata),\n                    message.metadata.get(\"timestamp\", 0),\n                ),\n            )\n            conn.commit()\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error adding message to database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.clear","title":"<code>clear()</code>","text":"<p>Clears the SQLite database by deleting all rows in the table.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the SQLite database by deleting all rows in the table.\"\"\"\n    try:\n        query = self.CLEAR_TABLE_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            conn.commit()\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error clearing database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.get_all","title":"<code>get_all()</code>","text":"<p>Retrieves all messages from the SQLite database.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def get_all(self) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from the SQLite database.\"\"\"\n    try:\n        query = self.SELECT_ALL_MESSAGES_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            rows = cursor.fetchall()\n        return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error retrieving messages from database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the SQLite database is empty.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the SQLite database is empty.\"\"\"\n    try:\n        query = self.CHECK_IF_EMPTY_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            count = cursor.fetchone()[0]\n        return count == 0\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error checking if database is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the SQLite database after model initialization.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Initialize the SQLite database after model initialization.\"\"\"\n    try:\n        self._validate_table_name(create_if_not_exists=True)\n    except Exception as e:\n        raise SQLiteError(f\"Error initializing SQLite backend: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.search","title":"<code>search(query=None, limit=10, filters=None)</code>","text":"<p>Searches for messages in SQLite based on the query and/or filters.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def search(self, query: str | None = None, limit: int = 10, filters: dict | None = None) -&gt; list[Message]:\n    \"\"\"Searches for messages in SQLite based on the query and/or filters.\"\"\"\n    try:\n        where_clauses = []\n        params = []\n\n        if query:\n            where_clauses.append(\"content LIKE ?\")\n            params.append(f\"%{query}%\")\n\n        if filters:\n            for key, value in filters.items():\n                if isinstance(value, list):\n                    placeholders = \",\".join(\"?\" for _ in value)\n                    where_clauses.append(f\"json_extract(metadata, '$.{key}') IN ({placeholders})\")\n                    params.extend(value)\n                else:\n                    if isinstance(value, str) and \"%\" in value:\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') LIKE ?\")\n                        params.append(value)\n                    else:\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') = ?\")\n                        params.append(value)\n\n        query_str = self.SEARCH_MESSAGES_QUERY.format(index_name=self.index_name)\n        if where_clauses:\n            query_str += f\" WHERE {' AND '.join(where_clauses)}\"\n        query_str += \" ORDER BY id DESC LIMIT ?\"\n        params.append(limit)\n\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query_str, params)\n            rows = cursor.fetchall()\n\n        return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error searching in database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLite.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>include_secure_params</code> <code>bool</code> <p>Whether to include secure parameters</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of the instance</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Args:\n        include_secure_params (bool): Whether to include secure parameters\n        **kwargs: Additional arguments\n\n    Returns:\n        dict: Dictionary representation of the instance\n    \"\"\"\n    kwargs.pop(\"include_secure_params\", None)\n    kwargs.pop(\"for_tracing\", None)\n    data = super().to_dict(**kwargs)\n\n    if not include_secure_params:\n        data.pop(\"db_path\", None)\n\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/sqlite/#dynamiq.memory.backends.sqlite.SQLiteError","title":"<code>SQLiteError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for SQLite-related errors in the memory backend.</p> Source code in <code>dynamiq/memory/backends/sqlite.py</code> <pre><code>class SQLiteError(Exception):\n    \"\"\"Base exception class for SQLite-related errors in the memory backend.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate","title":"<code>Weaviate</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>Weaviate implementation of the memory storage backend.</p> <p>Uses WeaviateVectorStore to manage documents (messages) in a Weaviate collection. Leverages vector embeddings for semantic search and metadata for filtering and chronological ordering.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>class Weaviate(MemoryBackend):\n    \"\"\"\n    Weaviate implementation of the memory storage backend.\n\n    Uses WeaviateVectorStore to manage documents (messages) in a Weaviate collection.\n    Leverages vector embeddings for semantic search and metadata for filtering\n    and chronological ordering.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = \"Weaviate\"\n    connection: WeaviateConnection = Field(default_factory=WeaviateConnection)\n    embedder: DocumentEmbedder\n    collection_name: str = Field(default=\"conversations\")\n    tenant_name: str | None = Field(default=None)\n    create_if_not_exist: bool = Field(default=True)\n    content_property_name: str = Field(default=\"message_content\")\n    alpha: float = Field(default=0.5, description=\"Alpha for hybrid search (0=keyword, 1=vector)\")\n    message_truncation_enabled: bool = Field(\n        default=True, description=\"Enable automatic message truncation for embeddings\"\n    )\n    message_max_tokens: int = Field(default=6000, description=\"Maximum tokens for message content before truncation\")\n    message_truncation_method: TruncationMethod = Field(\n        default=TruncationMethod.START, description=\"Method to use for message truncation\"\n    )\n\n    _vector_store: WeaviateVectorStore | None = PrivateAttr(default=None)\n\n    _ROLE_KEY: ClassVar[str] = \"message_role\"\n    _TIMESTAMP_KEY: ClassVar[str] = \"message_timestamp\"\n    _MESSAGE_ID_KEY: ClassVar[str] = \"message_id\"\n\n    _CORE_MEMORY_PROPERTIES: ClassVar[list[str]] = [\n        \"message_role\",\n        \"message_timestamp\",\n        \"message_id\",\n        \"user_id\",\n        \"session_id\",\n        \"_original_id\",\n    ]\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return super().to_dict_exclude_params | {\"embedder\": True, \"_vector_store\": True, \"connection\": True}\n\n    def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params.copy())\n        data = self.model_dump(exclude=exclude, **kwargs)\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n        data[\"embedder\"] = self.embedder.to_dict(\n            include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n        )\n\n        if \"type\" not in data:\n            data[\"type\"] = self.type\n\n        return data\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Initialize the Weaviate vector store and ensure schema properties.\"\"\"\n        try:\n            writer_params = WeaviateWriterVectorStoreParams(\n                collection_name=self.collection_name,\n                create_if_not_exist=self.create_if_not_exist,\n                content_property_name=self.content_property_name,\n                tenant_name=self.tenant_name,\n            )\n\n            properties_to_define = list(self._CORE_MEMORY_PROPERTIES)\n            properties_to_define.append(self.content_property_name)\n\n            self._vector_store = WeaviateVectorStore(\n                connection=self.connection,\n                **writer_params.model_dump(),\n                alpha=self.alpha,\n            )\n\n            logger.debug(\n                f\"Weaviate backend '{self.name}' (ID: {self.id}) initialized \"\n                f\"for collection '{self._vector_store._collection.name}'\"\n                f\"{f' with tenant {self.tenant_name}' if self.tenant_name else ''}.\"\n            )\n\n            if self._vector_store and self.create_if_not_exist:\n                properties_to_ensure = list(self._CORE_MEMORY_PROPERTIES)\n                properties_to_ensure.append(self.content_property_name)\n                self._vector_store.ensure_properties_exist(properties_to_ensure)\n\n            # Configure embedder truncation settings\n            self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n            self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n            self.embedder.document_embedder.truncation_method = self.message_truncation_method\n\n        except Exception as e:\n            logger.error(f\"Weaviate backend '{self.name}' failed to initialize vector store: {e}\")\n            raise WeaviateMemoryError(f\"Failed to initialize Weaviate vector store: {e}\") from e\n\n    def _message_to_document(self, message: Message) -&gt; Document:\n        \"\"\"Converts a Message object to a Document object for Weaviate.\"\"\"\n        if not self._vector_store:\n            raise WeaviateMemoryError(\"Vector store not initialized.\")\n\n        message_id = message.metadata.get(self._MESSAGE_ID_KEY, str(uuid.uuid4()))\n        timestamp = message.metadata.get(\"timestamp\", time.time())\n\n        doc_metadata = {\n            self._ROLE_KEY: message.role.value,\n            self._TIMESTAMP_KEY: timestamp,\n            self._MESSAGE_ID_KEY: message_id,\n            **(message.metadata or {}),\n        }\n\n        content = message.content\n        if self.message_truncation_enabled and content:\n            original_length = len(content)\n            truncated_content = truncate_text_for_embedding(\n                text=content,\n                max_tokens=self.message_max_tokens,\n                truncation_method=self.message_truncation_method\n            )\n\n            if len(truncated_content) &lt; original_length:\n                content = truncated_content\n                doc_metadata[\"truncated\"] = True\n                doc_metadata[\"original_length\"] = original_length\n                doc_metadata[\"truncated_length\"] = len(content)\n                doc_metadata[\"truncation_method\"] = self.message_truncation_method.value\n\n        sanitized_metadata = {}\n        for k, v in doc_metadata.items():\n            if self._vector_store.is_valid_property_name(k):\n                sanitized_metadata[k] = v\n            else:\n                logger.warning(f\"Skipping invalid metadata key for Weaviate: '{k}'\")\n\n        doc_id = message_id\n\n        return Document(\n            id=doc_id,\n            content=content,\n            metadata=sanitized_metadata,\n            embedding=None,\n        )\n\n    def _document_to_message(self, document: Document) -&gt; Message:\n        \"\"\"Converts a Document object from Weaviate back to a Message object.\"\"\"\n        if not document.metadata:\n            logger.warning(f\"Document {document.id} from Weaviate has no metadata. Cannot reconstruct message fully.\")\n            return Message(role=MessageRole.SYSTEM, content=document.content, metadata={\"retrieval_issue\": True})\n\n        metadata = dict(document.metadata)\n\n        role_str = metadata.pop(self._ROLE_KEY, MessageRole.USER.value)\n        try:\n            role = MessageRole(role_str)\n        except ValueError:\n            logger.warning(f\"Invalid role '{role_str}' found in document {document.id}. Defaulting to USER.\")\n            role = MessageRole.USER\n\n        timestamp = metadata.get(self._TIMESTAMP_KEY)\n        message_id = metadata.get(self._MESSAGE_ID_KEY)\n\n        if document.score is not None:\n            metadata[\"score\"] = document.score\n\n        metadata.pop(self._TIMESTAMP_KEY, None)\n        metadata.pop(self._MESSAGE_ID_KEY, None)\n\n        final_metadata = metadata\n        if timestamp is not None:\n            final_metadata[\"timestamp\"] = timestamp\n        if message_id is not None:\n            final_metadata[\"message_id\"] = message_id\n\n        return Message(role=role, content=document.content or \"\", metadata=final_metadata)\n\n    def add(self, message: Message) -&gt; None:\n        \"\"\"Adds a message to the Weaviate memory.\"\"\"\n        if self._vector_store is None:\n            raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n        if self.embedder is None:\n            raise WeaviateMemoryError(\"Embedder is required for Weaviate memory backend.\")\n\n        try:\n            document = self._message_to_document(message)\n\n            embedding_input = DocumentEmbedderInputSchema(\n                documents=[Document(id=document.id, content=document.content)]\n            )\n            embedding_result = self.embedder.execute(input_data=embedding_input)\n\n            if not embedding_result or not embedding_result.get(\"documents\"):\n                raise WeaviateMemoryError(\"Failed to generate embedding for the message.\")\n\n            document.embedding = embedding_result[\"documents\"][0].embedding\n            if not document.embedding:\n                raise WeaviateMemoryError(\"Generated embedding is empty.\")\n\n            self._vector_store.write_documents([document], content_key=self.content_property_name)\n            logger.debug(f\"Weaviate Memory ({self.collection_name}): Added message {document.id}\")\n\n        except Exception as e:\n            logger.error(f\"Error adding message to Weaviate: {e}\")\n            raise WeaviateMemoryError(f\"Error adding message to Weaviate: {e}\") from e\n\n    def get_all(self, limit: int | None = None) -&gt; list[Message]:\n        \"\"\"\n        Retrieves messages from Weaviate, sorted chronologically (oldest first).\n\n        Note: This fetches all documents and sorts client-side by timestamp.\n              May be inefficient for very large collections.\n              Returns the `limit` most recent messages if limit is specified.\n        \"\"\"\n        if self._vector_store is None:\n            raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n\n        try:\n            documents = self._vector_store.list_documents(\n                include_embeddings=False, content_key=self.content_property_name\n            )\n\n            messages = [self._document_to_message(doc) for doc in documents]\n\n            messages.sort(key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n\n            if limit is not None and limit &gt; 0:\n                retrieved_messages = messages[-limit:]\n            else:\n                retrieved_messages = messages\n\n            logger.debug(\n                f\"Weaviate Memory ({self.collection_name}): Retrieved {len(retrieved_messages)} messages\"\n                f\"{f' (limited to {limit})' if limit else ''}.\"\n            )\n            return retrieved_messages\n\n        except Exception as e:\n            logger.error(f\"Error retrieving messages from Weaviate: {e}\")\n            raise WeaviateMemoryError(f\"Error retrieving messages from Weaviate: {e}\") from e\n\n    def _prepare_filters(self, filters: dict | None = None) -&gt; dict | None:\n        \"\"\"\n        Convert simple key-value filters to the Weaviate filter format if necessary.\n        If the input `filters` already seem to be in Weaviate format (contain 'operator'\n        or 'field'), they are passed through directly. Otherwise, assumes a simple\n        dictionary where keys are fields and values are the values to match with '=='\n        operator, combined with 'AND'.\n\n        Args:\n            filters: Raw filters dictionary.\n\n        Returns:\n            Prepared filters in Weaviate-compatible format, or None.\n        \"\"\"\n        if not filters:\n            return None\n\n        if \"operator\" in filters and \"conditions\" in filters:\n            logger.debug(\"Filters appear to be in Weaviate logical format, passing through.\")\n            return filters\n        if \"field\" in filters and \"operator\" in filters and \"value\" in filters:\n            logger.debug(\"Filters appear to be in Weaviate comparison format, passing through.\")\n            return filters\n\n        logger.debug(\"Filters appear to be simple key-value, converting to Weaviate AND format.\")\n        conditions = []\n        for key, value in filters.items():\n            if self._vector_store and self._vector_store.is_valid_property_name(key):\n                conditions.append({\"field\": key, \"operator\": \"==\", \"value\": value})\n            else:\n                logger.warning(f\"Skipping filter key '{key}' as it's not a valid Weaviate property name.\")\n\n        if not conditions:\n            return None\n        return {\"operator\": \"AND\", \"conditions\": conditions}\n\n    def search(\n        self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n    ) -&gt; list[Message]:\n        \"\"\"\n        Searches for messages in Weaviate using vector similarity and/or filters.\n\n        Args:\n            query: Optional search string for semantic search.\n            filters: Optional dictionary for filtering messages by metadata.\n                     This should be in the Weaviate filter format.\n            limit: Maximum number of messages to return.\n\n        Returns:\n            List of matching messages sorted by relevance (if query provided)\n            or potentially unsorted/timestamp-sorted (if only filters provided).\n            Note: Sorting for filter-only results happens in the Memory class if needed.\n        \"\"\"\n        if self._vector_store is None:\n            raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n        if query and self.embedder is None:\n            raise WeaviateMemoryError(\"Embedder is required for search with query.\")\n\n        prepared_filters = self._prepare_filters(filters)\n\n        try:\n            effective_limit = limit if limit is not None else 10\n\n            if query:\n                embedding_input = DocumentEmbedderInputSchema(documents=[Document(id=\"query\", content=query)])\n                embedding_result = self.embedder.execute(input_data=embedding_input)\n                query_embedding = embedding_result[\"documents\"][0].embedding\n\n                if not query_embedding:\n                    raise WeaviateMemoryError(\"Failed to generate embedding for the search query.\")\n\n                documents = self._vector_store._hybrid_retrieval(\n                    query_embedding=query_embedding,\n                    query=query,\n                    filters=prepared_filters,\n                    top_k=effective_limit,\n                    exclude_document_embeddings=True,\n                    alpha=self.alpha,\n                    content_key=self.content_property_name,\n                )\n                retrieved_messages = [self._document_to_message(doc) for doc in documents]\n\n            elif prepared_filters:\n                documents = self._vector_store.filter_documents(\n                    filters=prepared_filters, content_key=self.content_property_name\n                )\n                retrieved_messages = [self._document_to_message(doc) for doc in documents]\n                if effective_limit &gt; 0:\n                    retrieved_messages = retrieved_messages[:effective_limit]\n\n            else:\n                logger.debug(\n                    f\"Weaviate Memory ({self.collection_name}): Search called with no \"\n                    f\"query or filters. Returning empty.\"\n                )\n                retrieved_messages = []\n\n            logger.debug(\n                f\"Weaviate Memory ({self.collection_name}):\"\n                f\" Found {len(retrieved_messages)} search results \"\n                f\"(Query: {'Yes' if query else 'No'}, \"\n                f\"Filters: {'Yes' if prepared_filters else 'No'}, Limit: {effective_limit})\"\n            )\n            return retrieved_messages\n\n        except Exception as e:\n            if isinstance(e, WeaviateMemoryError) and \"key missing\" in str(e):\n                logger.error(f\"Filter format error during Weaviate search. Filters received: {prepared_filters}\")\n            logger.error(f\"Error searching Weaviate memory: {e}\")\n            raise WeaviateMemoryError(f\"Error searching Weaviate memory: {e}\") from e\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the Weaviate collection associated with this memory is empty.\"\"\"\n        if self._vector_store is None:\n            raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n        try:\n            count = self._vector_store.count_documents()\n            return count == 0\n        except Exception as e:\n            logger.error(f\"Error checking if Weaviate memory is empty: {e}\")\n            raise WeaviateMemoryError(f\"Error checking if Weaviate memory is empty: {e}\") from e\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the Weaviate memory by deleting all documents in the collection/tenant.\"\"\"\n        if self._vector_store is None:\n            raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n        try:\n            count = self._vector_store.count_documents()\n            if count &gt; 0:\n                self._vector_store.delete_documents(delete_all=True)\n                logger.info(\n                    f\"Weaviate Memory ({self.collection_name}): Cleared {count} documents \"\n                    f\"{f'from tenant {self.tenant_name}' if self.tenant_name else 'from collection'}.\"\n                )\n            else:\n                logger.info(f\"Weaviate Memory ({self.collection_name}): Clear called, but memory was already empty.\")\n        except Exception as e:\n            logger.error(f\"Error clearing Weaviate memory: {e}\")\n            raise WeaviateMemoryError(f\"Error clearing Weaviate memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.add","title":"<code>add(message)</code>","text":"<p>Adds a message to the Weaviate memory.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def add(self, message: Message) -&gt; None:\n    \"\"\"Adds a message to the Weaviate memory.\"\"\"\n    if self._vector_store is None:\n        raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n    if self.embedder is None:\n        raise WeaviateMemoryError(\"Embedder is required for Weaviate memory backend.\")\n\n    try:\n        document = self._message_to_document(message)\n\n        embedding_input = DocumentEmbedderInputSchema(\n            documents=[Document(id=document.id, content=document.content)]\n        )\n        embedding_result = self.embedder.execute(input_data=embedding_input)\n\n        if not embedding_result or not embedding_result.get(\"documents\"):\n            raise WeaviateMemoryError(\"Failed to generate embedding for the message.\")\n\n        document.embedding = embedding_result[\"documents\"][0].embedding\n        if not document.embedding:\n            raise WeaviateMemoryError(\"Generated embedding is empty.\")\n\n        self._vector_store.write_documents([document], content_key=self.content_property_name)\n        logger.debug(f\"Weaviate Memory ({self.collection_name}): Added message {document.id}\")\n\n    except Exception as e:\n        logger.error(f\"Error adding message to Weaviate: {e}\")\n        raise WeaviateMemoryError(f\"Error adding message to Weaviate: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.clear","title":"<code>clear()</code>","text":"<p>Clears the Weaviate memory by deleting all documents in the collection/tenant.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the Weaviate memory by deleting all documents in the collection/tenant.\"\"\"\n    if self._vector_store is None:\n        raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n    try:\n        count = self._vector_store.count_documents()\n        if count &gt; 0:\n            self._vector_store.delete_documents(delete_all=True)\n            logger.info(\n                f\"Weaviate Memory ({self.collection_name}): Cleared {count} documents \"\n                f\"{f'from tenant {self.tenant_name}' if self.tenant_name else 'from collection'}.\"\n            )\n        else:\n            logger.info(f\"Weaviate Memory ({self.collection_name}): Clear called, but memory was already empty.\")\n    except Exception as e:\n        logger.error(f\"Error clearing Weaviate memory: {e}\")\n        raise WeaviateMemoryError(f\"Error clearing Weaviate memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves messages from Weaviate, sorted chronologically (oldest first).</p> This fetches all documents and sorts client-side by timestamp. <p>May be inefficient for very large collections. Returns the <code>limit</code> most recent messages if limit is specified.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def get_all(self, limit: int | None = None) -&gt; list[Message]:\n    \"\"\"\n    Retrieves messages from Weaviate, sorted chronologically (oldest first).\n\n    Note: This fetches all documents and sorts client-side by timestamp.\n          May be inefficient for very large collections.\n          Returns the `limit` most recent messages if limit is specified.\n    \"\"\"\n    if self._vector_store is None:\n        raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n\n    try:\n        documents = self._vector_store.list_documents(\n            include_embeddings=False, content_key=self.content_property_name\n        )\n\n        messages = [self._document_to_message(doc) for doc in documents]\n\n        messages.sort(key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n\n        if limit is not None and limit &gt; 0:\n            retrieved_messages = messages[-limit:]\n        else:\n            retrieved_messages = messages\n\n        logger.debug(\n            f\"Weaviate Memory ({self.collection_name}): Retrieved {len(retrieved_messages)} messages\"\n            f\"{f' (limited to {limit})' if limit else ''}.\"\n        )\n        return retrieved_messages\n\n    except Exception as e:\n        logger.error(f\"Error retrieving messages from Weaviate: {e}\")\n        raise WeaviateMemoryError(f\"Error retrieving messages from Weaviate: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the Weaviate collection associated with this memory is empty.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the Weaviate collection associated with this memory is empty.\"\"\"\n    if self._vector_store is None:\n        raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n    try:\n        count = self._vector_store.count_documents()\n        return count == 0\n    except Exception as e:\n        logger.error(f\"Error checking if Weaviate memory is empty: {e}\")\n        raise WeaviateMemoryError(f\"Error checking if Weaviate memory is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Initialize the Weaviate vector store and ensure schema properties.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Initialize the Weaviate vector store and ensure schema properties.\"\"\"\n    try:\n        writer_params = WeaviateWriterVectorStoreParams(\n            collection_name=self.collection_name,\n            create_if_not_exist=self.create_if_not_exist,\n            content_property_name=self.content_property_name,\n            tenant_name=self.tenant_name,\n        )\n\n        properties_to_define = list(self._CORE_MEMORY_PROPERTIES)\n        properties_to_define.append(self.content_property_name)\n\n        self._vector_store = WeaviateVectorStore(\n            connection=self.connection,\n            **writer_params.model_dump(),\n            alpha=self.alpha,\n        )\n\n        logger.debug(\n            f\"Weaviate backend '{self.name}' (ID: {self.id}) initialized \"\n            f\"for collection '{self._vector_store._collection.name}'\"\n            f\"{f' with tenant {self.tenant_name}' if self.tenant_name else ''}.\"\n        )\n\n        if self._vector_store and self.create_if_not_exist:\n            properties_to_ensure = list(self._CORE_MEMORY_PROPERTIES)\n            properties_to_ensure.append(self.content_property_name)\n            self._vector_store.ensure_properties_exist(properties_to_ensure)\n\n        # Configure embedder truncation settings\n        self.embedder.document_embedder.truncation_enabled = self.message_truncation_enabled\n        self.embedder.document_embedder.max_input_tokens = self.message_max_tokens\n        self.embedder.document_embedder.truncation_method = self.message_truncation_method\n\n    except Exception as e:\n        logger.error(f\"Weaviate backend '{self.name}' failed to initialize vector store: {e}\")\n        raise WeaviateMemoryError(f\"Failed to initialize Weaviate vector store: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.search","title":"<code>search(query=None, filters=None, limit=None)</code>","text":"<p>Searches for messages in Weaviate using vector similarity and/or filters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str | None</code> <p>Optional search string for semantic search.</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional dictionary for filtering messages by metadata.      This should be in the Weaviate filter format.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of messages to return.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list[Message]</code> <p>List of matching messages sorted by relevance (if query provided)</p> <code>list[Message]</code> <p>or potentially unsorted/timestamp-sorted (if only filters provided).</p> <code>Note</code> <code>list[Message]</code> <p>Sorting for filter-only results happens in the Memory class if needed.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def search(\n    self, query: str | None = None, filters: dict[str, Any] | None = None, limit: int | None = None\n) -&gt; list[Message]:\n    \"\"\"\n    Searches for messages in Weaviate using vector similarity and/or filters.\n\n    Args:\n        query: Optional search string for semantic search.\n        filters: Optional dictionary for filtering messages by metadata.\n                 This should be in the Weaviate filter format.\n        limit: Maximum number of messages to return.\n\n    Returns:\n        List of matching messages sorted by relevance (if query provided)\n        or potentially unsorted/timestamp-sorted (if only filters provided).\n        Note: Sorting for filter-only results happens in the Memory class if needed.\n    \"\"\"\n    if self._vector_store is None:\n        raise WeaviateMemoryError(\"Weaviate vector store not initialized.\")\n    if query and self.embedder is None:\n        raise WeaviateMemoryError(\"Embedder is required for search with query.\")\n\n    prepared_filters = self._prepare_filters(filters)\n\n    try:\n        effective_limit = limit if limit is not None else 10\n\n        if query:\n            embedding_input = DocumentEmbedderInputSchema(documents=[Document(id=\"query\", content=query)])\n            embedding_result = self.embedder.execute(input_data=embedding_input)\n            query_embedding = embedding_result[\"documents\"][0].embedding\n\n            if not query_embedding:\n                raise WeaviateMemoryError(\"Failed to generate embedding for the search query.\")\n\n            documents = self._vector_store._hybrid_retrieval(\n                query_embedding=query_embedding,\n                query=query,\n                filters=prepared_filters,\n                top_k=effective_limit,\n                exclude_document_embeddings=True,\n                alpha=self.alpha,\n                content_key=self.content_property_name,\n            )\n            retrieved_messages = [self._document_to_message(doc) for doc in documents]\n\n        elif prepared_filters:\n            documents = self._vector_store.filter_documents(\n                filters=prepared_filters, content_key=self.content_property_name\n            )\n            retrieved_messages = [self._document_to_message(doc) for doc in documents]\n            if effective_limit &gt; 0:\n                retrieved_messages = retrieved_messages[:effective_limit]\n\n        else:\n            logger.debug(\n                f\"Weaviate Memory ({self.collection_name}): Search called with no \"\n                f\"query or filters. Returning empty.\"\n            )\n            retrieved_messages = []\n\n        logger.debug(\n            f\"Weaviate Memory ({self.collection_name}):\"\n            f\" Found {len(retrieved_messages)} search results \"\n            f\"(Query: {'Yes' if query else 'No'}, \"\n            f\"Filters: {'Yes' if prepared_filters else 'No'}, Limit: {effective_limit})\"\n        )\n        return retrieved_messages\n\n    except Exception as e:\n        if isinstance(e, WeaviateMemoryError) and \"key missing\" in str(e):\n            logger.error(f\"Filter format error during Weaviate search. Filters received: {prepared_filters}\")\n        logger.error(f\"Error searching Weaviate memory: {e}\")\n        raise WeaviateMemoryError(f\"Error searching Weaviate memory: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.Weaviate.to_dict","title":"<code>to_dict(include_secure_params=False, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, for_tracing: bool = False, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params.copy())\n    data = self.model_dump(exclude=exclude, **kwargs)\n    data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing)\n    data[\"embedder\"] = self.embedder.to_dict(\n        include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs\n    )\n\n    if \"type\" not in data:\n        data[\"type\"] = self.type\n\n    return data\n</code></pre>"},{"location":"dynamiq/memory/backends/weaviate/#dynamiq.memory.backends.weaviate.WeaviateMemoryError","title":"<code>WeaviateMemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for Weaviate Memory Backend errors.</p> Source code in <code>dynamiq/memory/backends/weaviate.py</code> <pre><code>class WeaviateMemoryError(Exception):\n    \"\"\"Base exception class for Weaviate Memory Backend errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/dry_run/","title":"Dry run","text":""},{"location":"dynamiq/nodes/dry_run/#dynamiq.nodes.dry_run.DryRunMixin","title":"<code>DryRunMixin</code>","text":"<p>Mixin class to add dry run functionality to vector stores.</p> <p>This mixin provides resource tracking and cleanup capabilities for vector stores operating in dry run mode. It tracks ingested documents and created collections to enable cleanup based on the DryRunConfig settings.</p> Source code in <code>dynamiq/nodes/dry_run.py</code> <pre><code>class DryRunMixin:\n    \"\"\"Mixin class to add dry run functionality to vector stores.\n\n    This mixin provides resource tracking and cleanup capabilities for vector stores\n    operating in dry run mode. It tracks ingested documents and created collections\n    to enable cleanup based on the DryRunConfig settings.\n    \"\"\"\n\n    def __init__(self, dry_run_config: DryRunConfig | None = None):\n        \"\"\"Initialize the DryRunMixin.\n\n        Args:\n            dry_run_config: Configuration for dry run behavior. If None, default config is used.\n        \"\"\"\n        self._dry_run_config = dry_run_config or DryRunConfig()\n        self._tracked_documents: list[str] = []\n        self._tracked_collection: str | None = None\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"Delete documents by their IDs.\n\n        Args:\n            document_ids: List of document IDs to delete.\n            delete_all: Whether to delete all documents.\n        \"\"\"\n        pass\n\n    def delete_collection(self, collection_name: str) -&gt; None:\n        \"\"\"Delete a collection by its name.\n\n        Args:\n            collection_name: Name of the collection to delete.\n        \"\"\"\n        pass\n\n    def _track_documents(self, document_ids: list[str]) -&gt; None:\n        \"\"\"Track multiple documents for potential cleanup.\n\n        Args:\n            document_ids: List of document IDs to track.\n        \"\"\"\n        self._tracked_documents.extend(document_ids)\n        logger.debug(f\"Tracked {len(document_ids)} documents\")\n\n    def _track_collection(self, collection_name: str) -&gt; None:\n        \"\"\"Track a collection for potential cleanup.\n\n        Args:\n            collection_name: Name of the collection to track.\n        \"\"\"\n        self._tracked_collection = collection_name\n        logger.debug(f\"Tracked collection: {collection_name}\")\n\n    def dry_run_cleanup(self, dry_run_config: DryRunConfig) -&gt; None:\n        \"\"\"Clean up tracked resources based on configuration.\n\n        Args:\n            dry_run_config: Configuration for dry run behavior.\n        \"\"\"\n\n        if dry_run_config.delete_documents and self._tracked_documents:\n            try:\n                self.delete_documents(list(self._tracked_documents))\n                logger.info(f\"Cleaned up {len(self._tracked_documents)} tracked documents\")\n                self._tracked_documents = []\n            except Exception as e:\n                logger.error(f\"Failed to clean up tracked documents: {e}\")\n\n        if dry_run_config.delete_collection and self._tracked_collection:\n            try:\n                self.delete_collection(self._tracked_collection)\n                logger.info(f\"Cleaned up collection: {self._tracked_collection}\")\n                self._tracked_collection = None\n            except Exception as e:\n                logger.error(f\"Failed to clean up collection {self._tracked_collection}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/dry_run/#dynamiq.nodes.dry_run.DryRunMixin.__init__","title":"<code>__init__(dry_run_config=None)</code>","text":"<p>Initialize the DryRunMixin.</p> <p>Parameters:</p> Name Type Description Default <code>dry_run_config</code> <code>DryRunConfig | None</code> <p>Configuration for dry run behavior. If None, default config is used.</p> <code>None</code> Source code in <code>dynamiq/nodes/dry_run.py</code> <pre><code>def __init__(self, dry_run_config: DryRunConfig | None = None):\n    \"\"\"Initialize the DryRunMixin.\n\n    Args:\n        dry_run_config: Configuration for dry run behavior. If None, default config is used.\n    \"\"\"\n    self._dry_run_config = dry_run_config or DryRunConfig()\n    self._tracked_documents: list[str] = []\n    self._tracked_collection: str | None = None\n</code></pre>"},{"location":"dynamiq/nodes/dry_run/#dynamiq.nodes.dry_run.DryRunMixin.delete_collection","title":"<code>delete_collection(collection_name)</code>","text":"<p>Delete a collection by its name.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to delete.</p> required Source code in <code>dynamiq/nodes/dry_run.py</code> <pre><code>def delete_collection(self, collection_name: str) -&gt; None:\n    \"\"\"Delete a collection by its name.\n\n    Args:\n        collection_name: Name of the collection to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/dry_run/#dynamiq.nodes.dry_run.DryRunMixin.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str] | None</code> <p>List of document IDs to delete.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>Whether to delete all documents.</p> <code>False</code> Source code in <code>dynamiq/nodes/dry_run.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"Delete documents by their IDs.\n\n    Args:\n        document_ids: List of document IDs to delete.\n        delete_all: Whether to delete all documents.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/dry_run/#dynamiq.nodes.dry_run.DryRunMixin.dry_run_cleanup","title":"<code>dry_run_cleanup(dry_run_config)</code>","text":"<p>Clean up tracked resources based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>dry_run_config</code> <code>DryRunConfig</code> <p>Configuration for dry run behavior.</p> required Source code in <code>dynamiq/nodes/dry_run.py</code> <pre><code>def dry_run_cleanup(self, dry_run_config: DryRunConfig) -&gt; None:\n    \"\"\"Clean up tracked resources based on configuration.\n\n    Args:\n        dry_run_config: Configuration for dry run behavior.\n    \"\"\"\n\n    if dry_run_config.delete_documents and self._tracked_documents:\n        try:\n            self.delete_documents(list(self._tracked_documents))\n            logger.info(f\"Cleaned up {len(self._tracked_documents)} tracked documents\")\n            self._tracked_documents = []\n        except Exception as e:\n            logger.error(f\"Failed to clean up tracked documents: {e}\")\n\n    if dry_run_config.delete_collection and self._tracked_collection:\n        try:\n            self.delete_collection(self._tracked_collection)\n            logger.info(f\"Cleaned up collection: {self._tracked_collection}\")\n            self._tracked_collection = None\n        except Exception as e:\n            logger.error(f\"Failed to clean up collection {self._tracked_collection}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeConditionFailedException","title":"<code>NodeConditionFailedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node's condition fails to be met.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeConditionFailedException(NodeException):\n    \"\"\"\n    Exception raised when a node's condition fails to be met.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeConditionSkippedException","title":"<code>NodeConditionSkippedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node's condition skipped.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeConditionSkippedException(NodeException):\n    \"\"\"\n    Exception raised when a node's condition skipped.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeException","title":"<code>NodeException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for node-related errors.</p> <p>Attributes:</p> Name Type Description <code>failed_depend</code> <code>NodeDependency</code> <p>The dependency that caused the exception. Defaults to None.</p> <code>message</code> <code>str</code> <p>Additional error message. Defaults to None.</p> <code>recoverable</code> <code>bool</code> <p>Whether the exception is recoverable. Defaults to False.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeException(Exception):\n    \"\"\"\n    Base exception class for node-related errors.\n\n    Attributes:\n        failed_depend (NodeDependency, optional): The dependency that caused the exception. Defaults to None.\n        message (str, optional): Additional error message. Defaults to None.\n        recoverable (bool, optional): Whether the exception is recoverable. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self, failed_depend: Optional[\"NodeDependency\"] = None, message: str = None, recoverable: bool = False\n    ):\n        super().__init__(message)\n        self.failed_depend = failed_depend\n        self.recoverable = recoverable\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeFailedException","title":"<code>NodeFailedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node fails to execute.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeFailedException(NodeException):\n    \"\"\"\n    Exception raised when a node fails to execute.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeSkippedException","title":"<code>NodeSkippedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node is skipped during execution.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeSkippedException(NodeException):\n    \"\"\"\n    Exception raised when a node is skipped during execution.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        failed_depend: Optional[\"NodeDependency\"] = None,\n        message: str = None,\n        recoverable: bool = False,\n        human_feedback: str = None,\n    ):\n        super().__init__(\n            failed_depend=failed_depend,\n            message=message,\n            recoverable=recoverable,\n        )\n        self.human_feedback = human_feedback\n</code></pre>"},{"location":"dynamiq/nodes/managers/","title":"Managers","text":""},{"location":"dynamiq/nodes/managers/#dynamiq.nodes.managers.NodeManager","title":"<code>NodeManager</code>","text":"<p>A class for managing and retrieving node types.</p> Source code in <code>dynamiq/nodes/managers.py</code> <pre><code>class NodeManager:\n    \"\"\"A class for managing and retrieving node types.\"\"\"\n\n    @staticmethod\n    def get_node_by_type(node_type: str) -&gt; type[Node]:\n        \"\"\"\n        Retrieves a node class based on the given node type.\n\n        Args:\n            node_type (str): The type of node to retrieve.\n\n        Returns:\n            type[Node]: The node class corresponding to the given type.\n\n        Raises:\n            ValueError: If the node type is not found.\n\n        Example:\n            &gt;&gt;&gt; node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\")\n            &gt;&gt;&gt; isinstance(node_class, type(Node))\n            True\n        \"\"\"\n        try:\n            entity_module, entity_name = node_type.rsplit(\".\", 1)\n            imported_module = importlib.import_module(entity_module)\n            if entity := getattr(imported_module, entity_name, None):\n                return entity\n        except (ModuleNotFoundError, ImportError):\n            raise ValueError(f\"Node type {node_type} not found\")\n</code></pre>"},{"location":"dynamiq/nodes/managers/#dynamiq.nodes.managers.NodeManager.get_node_by_type","title":"<code>get_node_by_type(node_type)</code>  <code>staticmethod</code>","text":"<p>Retrieves a node class based on the given node type.</p> <p>Parameters:</p> Name Type Description Default <code>node_type</code> <code>str</code> <p>The type of node to retrieve.</p> required <p>Returns:</p> Type Description <code>type[Node]</code> <p>type[Node]: The node class corresponding to the given type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the node type is not found.</p> Example <p>node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\") isinstance(node_class, type(Node)) True</p> Source code in <code>dynamiq/nodes/managers.py</code> <pre><code>@staticmethod\ndef get_node_by_type(node_type: str) -&gt; type[Node]:\n    \"\"\"\n    Retrieves a node class based on the given node type.\n\n    Args:\n        node_type (str): The type of node to retrieve.\n\n    Returns:\n        type[Node]: The node class corresponding to the given type.\n\n    Raises:\n        ValueError: If the node type is not found.\n\n    Example:\n        &gt;&gt;&gt; node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\")\n        &gt;&gt;&gt; isinstance(node_class, type(Node))\n        True\n    \"\"\"\n    try:\n        entity_module, entity_name = node_type.rsplit(\".\", 1)\n        imported_module = importlib.import_module(entity_module)\n        if entity := getattr(imported_module, entity_name, None):\n            return entity\n    except (ModuleNotFoundError, ImportError):\n        raise ValueError(f\"Node type {node_type} not found\")\n</code></pre>"},{"location":"dynamiq/nodes/node/","title":"Node","text":""},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.CachingConfig","title":"<code>CachingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for node caching.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether caching is enabled for the node.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class CachingConfig(BaseModel):\n    \"\"\"\n    Configuration for node caching.\n\n    Attributes:\n        enabled (bool): Whether caching is enabled for the node.\n    \"\"\"\n    enabled: bool = False\n\n    def to_dict(self, for_tracing: bool = False, **kwargs) -&gt; dict:\n        if for_tracing and not self.enabled:\n            return {\"enabled\": False}\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode","title":"<code>ConnectionNode</code>","text":"<p>               Bases: <code>Node</code>, <code>ABC</code></p> <p>Abstract base class for nodes that require a connection.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>BaseConnection | None</code> <p>The connection to use.</p> <code>client</code> <code>Any | None</code> <p>The client instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class ConnectionNode(Node, ABC):\n    \"\"\"\n    Abstract base class for nodes that require a connection.\n\n    Attributes:\n        connection (BaseConnection | None): The connection to use.\n        client (Any | None): The client instance.\n    \"\"\"\n\n    connection: BaseConnection | None = None\n    client: Any | None = None\n    _connection_manager: ConnectionManager | None = PrivateAttr(default=None)\n\n    @model_validator(mode=\"after\")\n    def validate_connection_client(self):\n        \"\"\"Validate that either connection or client is specified.\"\"\"\n        if not self.client and not self.connection:\n            raise ValueError(\"'connection' or 'client' should be specified\")\n        return self\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize components for the node.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        self._connection_manager = connection_manager\n        super().init_components(connection_manager)\n        if self.client is None:\n            self.client = connection_manager.get_connection_client(\n                connection=self.connection\n            )\n\n    def is_client_closed(self) -&gt; bool:\n        \"\"\"\n        Check if the client connection is closed.\n\n        Returns:\n            bool: True if client is closed, False otherwise\n        \"\"\"\n        if self.client is None:\n            return False\n\n        if hasattr(self.client, \"closed\"):\n            return self.client.closed\n\n        if hasattr(self.client, \"is_closed\") and callable(self.client.is_closed):\n            return self.client.is_closed()\n\n        if hasattr(self.client, \"_closed\"):\n            return self.client._closed\n\n        return False\n\n    def ensure_client(self) -&gt; None:\n        \"\"\"\n        Ensure the client is alive and reconnect if needed.\n        Automatically detects closed connections and reinitializes them.\n        \"\"\"\n        if self.is_client_closed():\n            if self.connection is None:\n                logger.debug(\n                    f\"Node {self.name} - {self.id}: Client connection is closed but no connection available \"\n                    f\"for reinitialization.\"\n                )\n                return\n\n            logger.warning(f\"Node {self.name} - {self.id}: Client connection is closed. Reinitializing\")\n            connection_manager = self._connection_manager or ConnectionManager()\n\n            try:\n                self.client = connection_manager.get_connection_client(connection=self.connection)\n                logger.info(f\"Node {self.name} - {self.id}: Client reinitialized successfully\")\n            except Exception as e:\n                logger.error(f\"Node {self.name} - {self.id}: Failed to reinitialize client: {e}\")\n                raise ConnectionManagerException(f\"Failed to reinitialize client for node {self.name}: {e}\") from e\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.ensure_client","title":"<code>ensure_client()</code>","text":"<p>Ensure the client is alive and reconnect if needed. Automatically detects closed connections and reinitializes them.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def ensure_client(self) -&gt; None:\n    \"\"\"\n    Ensure the client is alive and reconnect if needed.\n    Automatically detects closed connections and reinitializes them.\n    \"\"\"\n    if self.is_client_closed():\n        if self.connection is None:\n            logger.debug(\n                f\"Node {self.name} - {self.id}: Client connection is closed but no connection available \"\n                f\"for reinitialization.\"\n            )\n            return\n\n        logger.warning(f\"Node {self.name} - {self.id}: Client connection is closed. Reinitializing\")\n        connection_manager = self._connection_manager or ConnectionManager()\n\n        try:\n            self.client = connection_manager.get_connection_client(connection=self.connection)\n            logger.info(f\"Node {self.name} - {self.id}: Client reinitialized successfully\")\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: Failed to reinitialize client: {e}\")\n            raise ConnectionManagerException(f\"Failed to reinitialize client for node {self.name}: {e}\") from e\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components for the node.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize components for the node.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    self._connection_manager = connection_manager\n    super().init_components(connection_manager)\n    if self.client is None:\n        self.client = connection_manager.get_connection_client(\n            connection=self.connection\n        )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.is_client_closed","title":"<code>is_client_closed()</code>","text":"<p>Check if the client connection is closed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if client is closed, False otherwise</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def is_client_closed(self) -&gt; bool:\n    \"\"\"\n    Check if the client connection is closed.\n\n    Returns:\n        bool: True if client is closed, False otherwise\n    \"\"\"\n    if self.client is None:\n        return False\n\n    if hasattr(self.client, \"closed\"):\n        return self.client.closed\n\n    if hasattr(self.client, \"is_closed\") and callable(self.client.is_closed):\n        return self.client.is_closed()\n\n    if hasattr(self.client, \"_closed\"):\n        return self.client._closed\n\n    return False\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.validate_connection_client","title":"<code>validate_connection_client()</code>","text":"<p>Validate that either connection or client is specified.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_connection_client(self):\n    \"\"\"Validate that either connection or client is specified.\"\"\"\n    if not self.client and not self.connection:\n        raise ValueError(\"'connection' or 'client' should be specified\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ErrorHandling","title":"<code>ErrorHandling</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for error handling in nodes.</p> <p>Attributes:</p> Name Type Description <code>timeout_seconds</code> <code>float | None</code> <p>Timeout in seconds for node execution.</p> <code>retry_interval_seconds</code> <code>float</code> <p>Interval between retries in seconds.</p> <code>max_retries</code> <code>int</code> <p>Maximum number of retries.</p> <code>backoff_rate</code> <code>float</code> <p>Rate of increase for retry intervals.</p> <code>behavior</code> <code>Behavior</code> <p>Behavior for error handling.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class ErrorHandling(BaseModel):\n    \"\"\"\n    Configuration for error handling in nodes.\n\n    Attributes:\n        timeout_seconds (float | None): Timeout in seconds for node execution.\n        retry_interval_seconds (float): Interval between retries in seconds.\n        max_retries (int): Maximum number of retries.\n        backoff_rate (float): Rate of increase for retry intervals.\n        behavior (Behavior): Behavior for error handling.\n    \"\"\"\n    timeout_seconds: float | None = None\n    retry_interval_seconds: float = 1\n    max_retries: int = 0\n    backoff_rate: float = 1\n    behavior: Behavior = Behavior.RAISE\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.InputTransformer","title":"<code>InputTransformer</code>","text":"<p>               Bases: <code>Transformer</code></p> <p>Input transformer for nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class InputTransformer(Transformer):\n    \"\"\"Input transformer for nodes.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code>, <code>DryRunMixin</code>, <code>ABC</code></p> <p>Abstract base class for all nodes in the workflow.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the node.</p> <code>name</code> <code>str | None</code> <p>Optional name for the node.</p> <code>group</code> <code>NodeGroup</code> <p>Group the node belongs to.</p> <code>description</code> <code>str | None</code> <p>Optional description for the node.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>input_transformer</code> <code>InputTransformer</code> <p>Input data transformer.</p> <code>output_transformer</code> <code>OutputTransformer</code> <p>Output data transformer.</p> <code>caching</code> <code>CachingConfig</code> <p>Caching configuration.</p> <code>depends</code> <code>list[NodeDependency]</code> <p>List of node dependencies.</p> <code>metadata</code> <code>NodeMetadata | None</code> <p>Optional metadata for the node.</p> <code>is_postponed_component_init</code> <code>bool</code> <p>Whether component initialization is postponed.</p> <code>is_optimized_for_agents</code> <code>bool</code> <p>Whether to optimize output for agents. By default is set to False.</p> <code>is_files_allowed</code> <code>bool</code> <p>Whether the node is permitted to access files. By default is set to False.</p> <code>is_parallel_execution_allowed</code> <code>bool</code> <p>Whether this node can be executed in parallel. Default False.</p> <code>_json_schema_fields</code> <code>list[str]</code> <p>List of parameter names that will be used when generating json schema with _generate_json_schema.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class Node(BaseModel, Runnable, DryRunMixin, ABC):\n    \"\"\"\n    Abstract base class for all nodes in the workflow.\n\n    Attributes:\n        id (str): Unique identifier for the node.\n        name (str | None): Optional name for the node.\n        group (NodeGroup): Group the node belongs to.\n        description (str | None): Optional description for the node.\n        error_handling (ErrorHandling): Error handling configuration.\n        input_transformer (InputTransformer): Input data transformer.\n        output_transformer (OutputTransformer): Output data transformer.\n        caching (CachingConfig): Caching configuration.\n        depends (list[NodeDependency]): List of node dependencies.\n        metadata (NodeMetadata | None): Optional metadata for the node.\n        is_postponed_component_init (bool): Whether component initialization is postponed.\n        is_optimized_for_agents (bool): Whether to optimize output for agents. By default is set to False.\n        is_files_allowed (bool): Whether the node is permitted to access files. By default is set to False.\n        is_parallel_execution_allowed (bool): Whether this node can be executed in parallel. Default False.\n        _json_schema_fields (list[str]): List of parameter names that will be used when generating json schema\n          with _generate_json_schema.\n\n    \"\"\"\n    id: str = Field(default_factory=generate_uuid)\n    name: str | None = None\n    description: str | None = None\n    group: NodeGroup\n    error_handling: ErrorHandling = Field(default_factory=ErrorHandling)\n    input_transformer: InputTransformer = Field(default_factory=InputTransformer)\n    input_mapping: dict[str, Any] = {}\n    output_transformer: OutputTransformer = Field(default_factory=OutputTransformer)\n    caching: CachingConfig = Field(default_factory=CachingConfig)\n    streaming: StreamingConfig = Field(default_factory=StreamingConfig)\n    approval: ApprovalConfig = Field(default_factory=ApprovalConfig)\n    depends: list[NodeDependency] = []\n    metadata: NodeMetadata | None = None\n\n    is_postponed_component_init: bool = False\n    is_optimized_for_agents: bool = False\n    is_files_allowed: bool = Field(default=False, description=\"Whether the node is permitted to access files.\")\n    is_parallel_execution_allowed: bool = Field(\n        default=False,\n        description=\"Whether this node can be executed in parallel with other nodes inside an agent.\",\n    )\n    action_type: ActionType | None = Field(default=None, description=\"Action type classification for streaming.\")\n\n    _output_references: NodeOutputReferences = PrivateAttr()\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: type[BaseModel] | None = None\n    callbacks: list[NodeCallbackHandler] = []\n    _json_schema_fields: ClassVar[list[str]] = []\n    _clone_init_methods_names: ClassVar[list[str]] = [\"reset_run_state\"]\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not self.is_postponed_component_init:\n            self.init_components()\n\n        self._output_references = NodeOutputReferences(node=self)\n\n    @classmethod\n    def _generate_json_schema(cls, fields: list[str] = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Generates base json schema of Node for specified parameters.\n        This schema is designed for compatibility with the WorkflowYamlParser,\n        containing enough partial information to instantiate an Node.\n        Parameters name to be included in the schema are either defined in the _json_schema_fields class variable or\n        passed via the fields parameter.\n\n        Supported Nodes: Simple (non-nested) nodes and agents.\n\n        Args:\n            fields (list[str]): List of parameters to include in schema.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Generated json schema.\n        \"\"\"\n        fields_to_include = {}\n        generated_schemas = {}\n        for name in fields or cls._json_schema_fields:\n            field = cls.model_fields[name]\n            annotation = clear_annotation(field.annotation)\n\n            schema_annotation = annotation\n            if isinstance(annotation, type) and issubclass(annotation, BaseModel) and annotation is not BaseModel:\n                schema_annotation = object\n\n            parameter_name = name\n            if field.alias:\n                parameter_name = field.alias\n            if hasattr(annotation, \"_generate_json_schema\"):\n                generated_schemas[name] = annotation._generate_json_schema()\n            else:\n                description = (\n                    field.description\n                    if field.description\n                    else (\n                        annotation.__doc__\n                        if issubclass(annotation, BaseModel) and annotation.__doc__\n                        else \"No description.\"\n                    )\n                )\n                fields_to_include[parameter_name] = (schema_annotation, Field(..., description=description))\n\n        model = create_model(cls.__name__, **fields_to_include)\n        schema = model.model_json_schema()\n        schema[\"additionalProperties\"] = False\n        for param, param_schema in generated_schemas.items():\n            schema[\"properties\"][param] = param_schema\n\n        class_type = f\"{cls.__module__.rsplit('.', 1)[0]}.{cls.__name__}\"\n\n        schema[\"properties\"][\"type\"] = {\"type\": \"string\", \"enum\": [class_type]}\n\n        if \"required\" not in schema:\n            schema[\"required\"] = []\n        schema[\"required\"].append(\"type\")\n\n        schema[\"type\"] = \"object\"\n        return schema\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @staticmethod\n    def _validate_dependency_status(depend: NodeDependency, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate the status of a dependency.\n\n        Args:\n            depend (NodeDependency): The dependency to validate.\n            depends_result (dict[str, RunnableResult]): Results of dependent nodes.\n\n        Raises:\n            NodeException: If the dependency result is missing.\n            NodeFailedException: If the dependency failed.\n            NodeSkippedException: If the dependency was skipped.\n        \"\"\"\n        if not (dep_result := depends_result.get(depend.node.id)):\n            raise NodeException(\n                failed_depend=depend,\n                message=f\"Dependency {depend.node.id}: result missed\",\n            )\n\n        if dep_result.status == RunnableStatus.FAILURE and depend.node.error_handling.behavior == Behavior.RAISE:\n            raise NodeFailedException(\n                failed_depend=depend, message=f\"Dependency {depend.node.id}: failed\"\n            )\n\n        if dep_result.status == RunnableStatus.SKIP and depend.node.error_handling.behavior == Behavior.RAISE:\n            raise NodeSkippedException(failed_depend=depend, message=f\"Dependency {depend.node.id}: skipped\")\n\n    @staticmethod\n    def _validate_dependency_option(depend: NodeDependency, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate the option of a dependency.\n\n        Args:\n            depend (NodeDependency): The dependency to validate.\n            depends_result (dict[str, RunnableResult]): Results of dependent nodes.\n\n        Raises:\n            NodeConditionFailedException: If the dependency option is not met.\n            NodeConditionSkippedException: If the dependency option is skipped.\n        \"\"\"\n        if (\n            (dep_output_data := depends_result.get(depend.node.id))\n            and (isinstance(dep_output_data.output, dict))\n            and (dep_condition_result := dep_output_data.output.get(depend.option))\n        ):\n            if dep_condition_result.status == RunnableStatus.FAILURE:\n                raise NodeConditionFailedException(\n                    failed_depend=depend,\n                    message=f\"Dependency {depend.node.id} option {depend.option}: result is false\",\n                )\n            if dep_condition_result.status == RunnableStatus.SKIP:\n                raise NodeConditionSkippedException(\n                    failed_depend=depend,\n                    message=f\"Dependency {depend.node.id} option {depend.option}: skipped\",\n                )\n\n    @staticmethod\n    def _validate_dependency_condition(depend: NodeDependency, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate the result condition of a dependency.\n\n        Args:\n            depend (NodeDependency): The dependency to validate.\n            depends_result (dict[str, RunnableResult]): Results of dependent nodes.\n\n        Raises:\n            NodeConditionFailedException: If the dependency result condition is not met.\n        \"\"\"\n        if dep_result := depends_result.get(depend.node.id):\n            from dynamiq.nodes.operators.operators import Choice\n\n            if not Choice.evaluate(depend.condition, dep_result.to_dict()):\n                raise NodeConditionFailedException(\n                    failed_depend=depend,\n                    message=f\"Dependency {depend.node.id} result condition `{depend.condition}`: result is false\",\n                )\n\n    @staticmethod\n    def _validate_input_mapping_value_func(func: Callable):\n        \"\"\"\n        Validate input mapping value function.\n\n        Args:\n            func (Callable): Input mapping value function.\n\n        Raises:\n            ValueError: If the function does not accept 'inputs' and 'outputs' or **kwargs.\n        \"\"\"\n        params = inspect.signature(func).parameters\n\n        # Check if the function accepts the at least 'inputs' and 'outputs' parameters\n        if len(params) &gt;= 2:\n            return\n\n        # Check if the function accepts **kwargs\n        elif params and list(params.values())[0].kind == inspect.Parameter.VAR_KEYWORD:\n            return\n\n        raise ValueError(f\"Input function '{func.__name__}' must accept parameters 'inputs' and 'outputs' or **kwargs.\")\n\n    def validate_depends(self, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate all dependencies of the node.\n\n        Args:\n            depends_result (dict): Results of dependent nodes.\n            input_data (dict): Input data for the node.\n\n        Raises:\n            Various exceptions based on dependency validation results.\n        \"\"\"\n        for dep in self.depends:\n            self._validate_dependency_status(depend=dep, depends_result=depends_result)\n            if dep.condition:\n                self._validate_dependency_condition(depend=dep, depends_result=depends_result)\n            if dep.option:\n                self._validate_dependency_option(depend=dep, depends_result=depends_result)\n\n    def validate_input_schema(self, input_data: dict[str, Any], **kwargs) -&gt; dict[str, Any] | BaseModel:\n        \"\"\"\n        Validate input data against the input schema. Returns instance of input_schema if it is provided.\n\n        Args:\n            input_data (Any): Input data to validate.\n\n        Raises:\n            NodeException: If input data does not match the input schema.\n        \"\"\"\n        from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n        if self.input_schema:\n            try:\n                return self.input_schema.model_validate(\n                    input_data, context=kwargs | self.get_context_for_input_schema()\n                )\n            except Exception as e:\n                if kwargs.get(\"recoverable_error\", False):\n                    raise RecoverableAgentException(f\"Input data validation failed: {e}\")\n                raise e\n\n        return input_data\n\n    def transform_input(\n        self, input_data: dict, depends_result: dict[Any, RunnableResult], use_input_transformer: bool = True, **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Transform input data for the node.\n\n        Args:\n            input_data (dict): Input data for the node.\n            depends_result (dict): Results of dependent nodes.\n            use_input_transformer (bool): Determines if InputTransformer will be applied to the input.\n\n        Raises:\n            NodeException: If a dependency result is missing or input mapping fails.\n\n        Returns:\n            dict: Transformed input data.\n        \"\"\"\n        # Apply input transformer\n        if (self.input_transformer.path or self.input_transformer.selector) and use_input_transformer:\n            depends_result_as_dict = {k: result.to_depend_dict() for k, result in depends_result.items()}\n            inputs = self.transform(input_data | depends_result_as_dict, self.input_transformer)\n        else:\n            inputs = input_data | {k: result.to_tracing_depend_dict() for k, result in depends_result.items()}\n\n        # Apply input bindings\n        for key, value in self.input_mapping.items():\n            if isinstance(value, NodeOutputReference):\n                depend_result = depends_result.get(value.node.id)\n                if not depend_result:\n                    raise NodeException(message=f\"Dependency {value.node.id}: result not found.\")\n                if value.output_key not in depend_result.output:\n                    raise NodeException(message=f\"Dependency {value.node.id} output {value.output_key}: not found.\")\n\n                inputs[key] = depend_result.output[value.output_key]\n\n            elif callable(value):\n                try:\n                    inputs[key] = value(inputs, {d_id: result.output for d_id, result in depends_result.items()})\n                except Exception:\n                    raise NodeException(message=f\"Input mapping {key}: failed.\")\n            else:\n                inputs[key] = value\n\n        return inputs\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize node components.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager.\n        \"\"\"\n        self.is_postponed_component_init = False\n\n    @staticmethod\n    def transform(data: Any, transformer: Transformer) -&gt; Any:\n        \"\"\"\n        Apply transformation to data.\n\n        Args:\n            data (Any): Input data to transform.\n            transformer (Transformer): Transformer to apply.\n\n        Returns:\n            Any: Transformed data.\n        \"\"\"\n        output = jsonpath_filter(data, transformer.path)\n        output = jsonpath_mapper(output, transformer.selector)\n        return output\n\n    def transform_output(self, output_data: Any, **kwargs) -&gt; Any:\n        \"\"\"\n        Transform output data from the node.\n\n        Args:\n            output_data (Any): Output data to transform.\n\n        Returns:\n            Any: Transformed output data.\n        \"\"\"\n        return self.transform(output_data, self.output_transformer)\n\n    def get_clone_init_methods_names(self) -&gt; list[str]:\n        \"\"\"List of method names to call on the clone to reset per-run state.\"\"\"\n        return list(self._clone_init_methods_names)\n\n    def get_clone_attr_initializers(self) -&gt; dict[str, Callable[[\"Node\"], Any]]:\n        \"\"\"Mapping of attribute name -&gt; initializer callable(node) -&gt; value.\n\n        Default: provides streaming isolation so clones do not share runtime state.\n        \"\"\"\n        return {}\n\n    def clone(self) -&gt; \"Node\":\n        \"\"\"Create a safe clone of the node.\"\"\"\n        cloned_node = self.model_copy(deep=False)\n\n        def _clone_nested(value: Any) -&gt; Any:\n            # Do not attempt to copy modules/functions/classes or other callables\n            if isinstance(value, (ModuleType, FunctionType)) or isinstance(value, type) or callable(value):\n                return value\n            if getattr(value, \"_clone_shared\", False):\n                return value\n            if isinstance(value, Node):\n                return value.clone()\n            elif isinstance(value, BaseModel):\n                try:\n                    bm_copy = value.model_copy(deep=False)\n                    for fname in getattr(value, \"model_fields\", {}):\n                        try:\n                            setattr(bm_copy, fname, _clone_nested(getattr(value, fname)))\n                        except Exception as e:\n                            logger.warning(f\"Clone: failed to clone BaseModel field '{fname}': {e}\")\n\n                    return bm_copy\n                except Exception as e:\n                    logger.warning(f\"Clone: BaseModel copy failed, falling back to shallow copy: {e}\")\n                    return copy.copy(value)\n            elif isinstance(value, list):\n                return [_clone_nested(v) for v in value]\n            elif isinstance(value, dict):\n                return {k: _clone_nested(v) for k, v in value.items()}\n            try:\n                return copy.copy(value)\n            except Exception as e:\n                logger.warning(f\"Clone: failed to clone field '{value}': {e}\")\n                return value\n\n        for _field_name in getattr(cloned_node, \"model_fields\", {}):\n            _val = getattr(cloned_node, _field_name)\n            _new_val = _clone_nested(_val)\n            if _new_val is not _val:\n                try:\n                    setattr(cloned_node, _field_name, _new_val)\n                except Exception as e:\n                    logger.warning(f\"Clone: unable to set field '{_field_name}' during nested clone: {e}\")\n\n        init_map = self.get_clone_attr_initializers()\n        for attr_name, init_fn in init_map.items():\n            try:\n                if hasattr(cloned_node, attr_name):\n                    value = init_fn(cloned_node) if callable(init_fn) else None\n                    if value is not None:\n                        setattr(cloned_node, attr_name, value)\n                    else:\n                        try:\n                            setattr(cloned_node, attr_name, None)\n                        except Exception as e:\n                            logger.warning(f\"Clone: failed to set attr '{attr_name}': {e}\")\n            except Exception as e:\n                logger.warning(f\"Clone: initializer for attr '{attr_name}' failed: {e}\")\n\n        for method_name in self.get_clone_init_methods_names():\n            try:\n                method = getattr(cloned_node, method_name, None)\n                if callable(method):\n                    method()\n            except Exception as e:\n                logger.warning(f\"Clone: method '{method_name}' invocation failed: {e}\")\n\n        return cloned_node\n\n    @property\n    def to_dict_exclude_params(self):\n        return {\n            \"client\": True,\n            \"vector_store\": True,\n            \"depends\": True,\n            \"input_mapping\": True,\n            \"input_transformer\": True,\n            \"output_transformer\": True,\n            \"caching\": True,\n            \"streaming\": True,\n            \"approval\": True,\n        }\n\n    @property\n    def to_dict_exclude_secure_params(self):\n        return self.to_dict_exclude_params | {\"connection\": True}\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        for_tracing: bool = kwargs.pop(\"for_tracing\", False)\n        exclude = kwargs.pop(\n            \"exclude\", self.to_dict_exclude_params if include_secure_params else self.to_dict_exclude_secure_params\n        )\n        data = self.model_dump(\n            exclude=exclude,\n            serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n            **kwargs,\n        )\n\n        it = self.input_transformer.to_dict(for_tracing=for_tracing, **kwargs)\n        if it is not None:\n            data[\"input_transformer\"] = it\n        ot = self.output_transformer.to_dict(for_tracing=for_tracing, **kwargs)\n        if ot is not None:\n            data[\"output_transformer\"] = ot\n        data[\"caching\"] = self.caching.to_dict(for_tracing=for_tracing, **kwargs)\n        data[\"streaming\"] = self.streaming.to_dict(for_tracing=for_tracing, **kwargs)\n        data[\"approval\"] = self.approval.to_dict(for_tracing=for_tracing, **kwargs)\n\n        data[\"depends\"] = [depend.to_dict(for_tracing=for_tracing, **kwargs) for depend in self.depends]\n        data[\"input_mapping\"] = format_value(self.input_mapping)\n\n        if getattr(self, \"connection\", None):\n            data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing, **kwargs)\n\n        if for_tracing:\n            data = {k: v for k, v in data.items() if v is not None or k in (\"input\", \"output\")}\n        return data\n\n    def send_streaming_approval_message(\n        self, template: str, input_data: dict, approval_config: ApprovalConfig, config: RunnableConfig = None, **kwargs\n    ) -&gt; ApprovalInputData:\n        \"\"\"\n        Sends approval message and waits for response.\n\n        Args:\n            template (str): Template to send.\n            input_data (dict): Data that will be sent.\n            approval_config (ApprovalConfig): Configuration for approval.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Return:\n            ApprovalInputData: Response to approval message.\n\n        \"\"\"\n        event = ApprovalStreamingOutputEventMessage(\n            wf_run_id=config.run_id,\n            entity_id=self.id,\n            data={\"template\": template, \"data\": input_data, \"mutable_data_params\": approval_config.mutable_data_params},\n            event=approval_config.event,\n            source=StreamingEntitySource(\n                id=self.id,\n                name=self.name,\n                group=self.group,\n                type=self.type,\n            ),\n        )\n\n        logger.info(f\"Node {self.name} - {self.id}: sending approval.\")\n\n        self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n\n        output: ApprovalInputData = self.get_input_streaming_event(\n            event=approval_config.event, event_msg_type=ApprovalStreamingInputEventMessage, config=config\n        ).data\n\n        return output\n\n    def send_console_approval_message(self, template: str) -&gt; ApprovalInputData:\n        \"\"\"\n        Sends approval message in console and waits for response.\n\n        Args:\n            template (dict): Template to send.\n        Returns:\n            ApprovalInputData: Response to approval message.\n        \"\"\"\n        feedback = input(template)\n        return ApprovalInputData(feedback=feedback)\n\n    def send_approval_message(\n        self, approval_config: ApprovalConfig, input_data: dict, config: RunnableConfig = None, **kwargs\n    ) -&gt; ApprovalInputData:\n        \"\"\"\n        Sends approval message and determines if it was approved or disapproved (canceled).\n\n        Args:\n            approval_config (ApprovalConfig): Configuration for the approval.\n            input_data (dict): Data that will be sent.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            ApprovalInputData: Result of approval.\n        \"\"\"\n\n        message = Template(approval_config.msg_template).render(self.to_dict(), input_data=input_data)\n        match approval_config.feedback_method:\n            case FeedbackMethod.STREAM:\n                approval_result = self.send_streaming_approval_message(\n                    message, input_data, approval_config, config=config, **kwargs\n                )\n            case FeedbackMethod.CONSOLE:\n                approval_result = self.send_console_approval_message(message)\n            case _:\n                raise ValueError(f\"Error: Incorrect feedback method is chosen {approval_config.feedback_method}.\")\n\n        update_params = {\n            feature_name: approval_result.data[feature_name]\n            for feature_name in approval_config.mutable_data_params\n            if feature_name in approval_result.data\n        }\n        approval_result.data = {**input_data, **update_params}\n\n        if approval_result.is_approved is None:\n            if approval_result.feedback == approval_config.accept_pattern:\n                logger.info(\n                    f\"Node {self.name} action was approved by human \"\n                    f\"with provided feedback '{approval_result.feedback}'.\"\n                )\n                approval_result.is_approved = True\n\n            else:\n                approval_result.is_approved = False\n                logger.info(\n                    f\"Node {self.name} action was canceled by human\"\n                    f\"with provided feedback '{approval_result.feedback}'.\"\n                )\n\n        return approval_result\n\n    def get_approved_data_or_origin(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Approves or disapproves (cancels) Node execution by requesting feedback.\n        Updates input data according to the feedback or leaves it the same.\n        Raises NodeSkippedException if execution was canceled by feedback.\n\n        Args:\n            input_data(dict[str, Any]): Input data.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Updated input data.\n\n        Raises:\n            NodeSkippedException: If Node execution was canceled by feedback.\n        \"\"\"\n        if self.approval.enabled:\n            approval_result = self.send_approval_message(self.approval, input_data, config=config, **kwargs)\n            if not approval_result.is_approved:\n                raise NodeSkippedException(\n                    message=f\"Execution was canceled by human with feedback {approval_result.feedback}\",\n                    human_feedback=approval_result.feedback,\n                    recoverable=True,\n                    failed_depend=NodeDependency(self, option=\"Execution was canceled.\"),\n                )\n            return approval_result.data\n\n        return input_data\n\n    def run_sync(\n        self,\n        input_data: dict,\n        config: RunnableConfig = None,\n        depends_result: dict = None,\n        **kwargs,\n    ) -&gt; RunnableResult:\n        \"\"\"\n        Run the node synchronously with given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            depends_result (dict, optional): Results of already executed nodes. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the node execution.\n        \"\"\"\n        from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n        logger.info(f\"Node {self.name} - {self.id}: execution started.\")\n        transformed_input = input_data\n        time_start = datetime.now()\n\n        config = ensure_config(config)\n\n        run_id = uuid4()\n        merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"parent_run_id\": kwargs.get(\"parent_run_id\", None)})\n        if depends_result is None:\n            depends_result = {}\n\n        try:\n            try:\n                self.validate_depends(depends_result)\n                input_data = self.get_approved_data_or_origin(input_data, config=config, **merged_kwargs)\n            except NodeException as e:\n                transformed_input = input_data | {\n                    k: result.to_tracing_depend_dict() for k, result in depends_result.items()\n                }\n                skip_data = {\"failed_dependency\": e.failed_depend.to_dict(for_tracing=True)}\n                self.run_on_node_skip(\n                    callbacks=config.callbacks,\n                    skip_data=skip_data,\n                    input_data=transformed_input,\n                    human_feedback=getattr(e, \"human_feedback\", None),\n                    **merged_kwargs,\n                )\n                logger.info(f\"Node {self.name} - {self.id}: execution skipped.\")\n                return RunnableResult(\n                    status=RunnableStatus.SKIP,\n                    input=transformed_input,\n                    output=None,\n                    error=RunnableResultError.from_exception(e, recoverable=e.recoverable),\n                )\n\n            transformed_input = self.validate_input_schema(\n                self.transform_input(input_data=input_data, depends_result=depends_result, config=config, **kwargs),\n                **kwargs,\n            )\n            self.run_on_node_start(config.callbacks, dict(transformed_input), **merged_kwargs)\n            cache = cache_wf_entity(\n                entity_id=self.id,\n                cache_enabled=self.caching.enabled,\n                cache_config=config.cache,\n            )\n\n            output, from_cache = cache(self.execute_with_retry)(transformed_input, config, **merged_kwargs)\n\n            merged_kwargs[\"is_output_from_cache\"] = from_cache\n            transformed_output = self.transform_output(output, config=config, **kwargs)\n\n            self.run_on_node_end(config.callbacks, transformed_output, **merged_kwargs)\n\n            logger.info(\n                f\"Node {self.name} - {self.id}: execution succeeded in \"\n                f\"{format_duration(time_start, datetime.now())}.\"\n            )\n            return RunnableResult(\n                status=RunnableStatus.SUCCESS, input=dict(transformed_input), output=transformed_output\n            )\n        except Exception as e:\n            self.run_on_node_error(callbacks=config.callbacks, error=e, input_data=transformed_input, **merged_kwargs)\n            logger.error(\n                f\"Node {self.name} - {self.id}: execution failed in \"\n                f\"{format_duration(time_start, datetime.now())}. {e}\"\n            )\n\n            recoverable = isinstance(e, RecoverableAgentException)\n            result = RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=transformed_input,\n                output=None,\n                error=RunnableResultError.from_exception(e, recoverable=recoverable),\n            )\n            return result\n\n    async def run_async(\n        self,\n        input_data: dict,\n        config: RunnableConfig = None,\n        depends_result: dict = None,\n        **kwargs,\n    ) -&gt; RunnableResult:\n        \"\"\"\n        Run the node asynchronously with given input data and configuration.\n        This runs the synchronous implementation in a thread pool to avoid blocking the event loop.\n\n        Args:\n            input_data (Any): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            depends_result (dict, optional): Results of dependent nodes. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the node execution.\n        \"\"\"\n        return await asyncio.to_thread(\n            self.run_sync, input_data=input_data, config=config, depends_result=depends_result, **kwargs\n        )\n\n    def ensure_client(self) -&gt; None:\n        \"\"\"\n        Ensure the client connection is alive and reconnect if needed.\n        Override in subclasses that manage connections.\n        \"\"\"\n        pass\n\n    def execute_with_retry(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the node with retry logic and automatic connection management.\n\n        Args:\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the node execution.\n\n        Raises:\n            Exception: If all retry attempts fail.\n        \"\"\"\n        config = ensure_config(config)\n        timeout = self.error_handling.timeout_seconds\n\n        error = None\n        n_attempt = self.error_handling.max_retries + 1\n        executor = None\n        timed_out = False\n\n        try:\n            if timeout is not None:\n                executor = ContextAwareThreadPoolExecutor()\n\n            for attempt in range(n_attempt):\n                merged_kwargs = merge(kwargs, {\"execution_run_id\": uuid4()})\n\n                try:\n                    self.ensure_client()\n                except Exception as conn_error:\n                    logger.error(f\"Node {self.name} - {self.id}: Failed to ensure client connection: {conn_error}\")\n                    error = conn_error\n                    if attempt &lt; n_attempt - 1:\n                        time_to_sleep = self.error_handling.retry_interval_seconds * (\n                            self.error_handling.backoff_rate**attempt\n                        )\n                        logger.info(f\"Node {self.name} - {self.id}: retrying connection in {time_to_sleep} seconds.\")\n                        time.sleep(time_to_sleep)\n                        continue\n                    else:\n                        raise\n\n                self.run_on_node_execute_start(config.callbacks, input_data, **merged_kwargs)\n\n                try:\n                    if executor and timeout is not None:\n                        output = self.execute_with_timeout(\n                            executor=executor,\n                            timeout=timeout,\n                            input_data=input_data,\n                            config=config,\n                            **merged_kwargs,\n                        )\n                    else:\n                        output = self.execute(input_data=input_data, config=config, **merged_kwargs)\n\n                    self.run_on_node_execute_end(config.callbacks, output, **merged_kwargs)\n                    return output\n                except TimeoutError as e:\n                    error = e\n                    timed_out = True\n                    self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                    logger.warning(f\"Node {self.name} - {self.id}: timeout.\")\n                except Exception as e:\n                    error = e\n                    self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                    logger.error(f\"Node {self.name} - {self.id}: execution error: {e}\")\n\n                # do not sleep after the last attempt\n                if attempt &lt; n_attempt - 1:\n                    time_to_sleep = self.error_handling.retry_interval_seconds * (\n                        self.error_handling.backoff_rate**attempt\n                    )\n                    logger.info(f\"Node {self.name} - {self.id}: retrying in {time_to_sleep} seconds.\")\n                    time.sleep(time_to_sleep)\n\n            logger.error(f\"Node {self.name} - {self.id}: execution failed after {n_attempt} attempts.\")\n            raise error\n        finally:\n            if executor is not None:\n                # Use cancel_futures=True and wait=False when timeout occurred to prevent\n                # blocking on threads that may be stuck waiting (e.g., on input_queue.get())\n                executor.shutdown(wait=not timed_out, cancel_futures=timed_out)\n\n    def execute_with_timeout(\n        self,\n        executor: ContextAwareThreadPoolExecutor,\n        timeout: float | None,\n        input_data: dict[str, Any] | BaseModel,\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Execute the node with a timeout.\n\n        Args:\n            executor (ContextAwareThreadPoolExecutor): Thread pool executor to use.\n            timeout (float | None): Timeout duration in seconds.\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the execution.\n\n        Raises:\n            TimeoutError: If execution exceeds the timeout.\n            Exception: If execution fails.\n        \"\"\"\n        future = executor.submit(self.execute, input_data, config=config, **kwargs)\n\n        try:\n            return future.result(timeout=timeout)\n        except TimeoutError:\n            # Cancel the future to prevent further execution if possible.\n            # Note: cancel() only works if the task hasn't started yet.\n            # For running tasks, we rely on executor.shutdown(cancel_futures=True).\n            future.cancel()\n            raise\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n        return {}\n\n    def get_input_streaming_event(\n        self,\n        event_msg_type: \"type[StreamingEventMessage]\" = StreamingEventMessage,\n        event: str | None = None,\n        config: RunnableConfig = None,\n    ) -&gt; StreamingEventMessage:\n        \"\"\"\n        Get the input streaming event from the input streaming.\n\n        Args:\n            event_msg_type (Type[StreamingEventMessage], optional): The event message type to use.\n            event (str, optional): The event to use for the message.\n            config (RunnableConfig, optional): Configuration for the runnable.\n\n        Returns:\n            StreamingEventMessage: The validated streaming event message.\n\n        Raises:\n            ValueError: If input streaming is not enabled, timeout is exceeded, or the done event is set\n                before receiving valid data.\n        \"\"\"\n        # Use runnable streaming configuration. If not found use node streaming configuration\n        streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n        if not streaming.input_streaming_enabled:\n            raise ValueError(\"Input streaming is not enabled.\")\n\n        def is_done() -&gt; bool:\n            return streaming.input_queue_done_event is not None and streaming.input_queue_done_event.is_set()\n\n        poll_interval = streaming.input_queue_poll_interval\n        elapsed = 0.0\n\n        while not is_done():\n            if streaming.timeout is not None and elapsed &gt;= streaming.timeout:\n                raise ValueError(f\"Input streaming timeout: {streaming.timeout} seconds exceeded.\")\n\n            remaining = streaming.timeout - elapsed if streaming.timeout is not None else poll_interval\n            wait_time = min(poll_interval, remaining)\n\n            try:\n                data = streaming.input_queue.get(timeout=wait_time)\n            except Empty:\n                elapsed += wait_time\n                if is_done():\n                    raise ValueError(\"Input streaming completed without receiving valid data.\")\n                continue\n\n            try:\n                event_msg = event_msg_type.model_validate_json(data)\n                if event and event_msg.event != event:\n                    raise ValueError()\n            except ValueError:\n                logger.error(\n                    f\"Invalid streaming event data: {data}. \"\n                    f\"Allowed event: {event}, event_msg_type: {event_msg_type}\"\n                )\n                continue\n\n            return event_msg\n\n        raise ValueError(\"Input streaming completed without receiving valid data.\")\n\n    def run_on_node_start(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        input_data: dict[str, Any],\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node start.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n                callback.on_node_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_end(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        output_data: dict[str, Any],\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node end.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    output_data = (\n                        output_data.to_dict(for_tracing=True) if hasattr(output_data, \"to_dict\") else output_data\n                    )\n                callback.on_node_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_error(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        error: BaseException,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node error.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            error (BaseException): The error that occurred.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_node_error(self.to_dict(**dict_kwargs), error, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_skip(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        skip_data: dict[str, Any],\n        input_data: dict[str, Any],\n        human_feedback: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node skip.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n                callback.on_node_skip(\n                    self.to_dict(**dict_kwargs), skip_data, input_data, human_feedback=human_feedback, **kwargs\n                )\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_execute_start(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        input_data: dict[str, Any] | BaseModel,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node execute start.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if isinstance(input_data, BaseModel):\n            input_data = dict(input_data)\n\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n                callback.on_node_execute_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_execute_end(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        output_data: dict[str, Any],\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node execute end.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    output_data = (\n                        output_data.to_dict(for_tracing=True) if hasattr(output_data, \"to_dict\") else output_data\n                    )\n                callback.on_node_execute_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_execute_error(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        error: BaseException,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node execute error.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            error (BaseException): The error that occurred.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_node_execute_error(self.to_dict(**dict_kwargs), error, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_execute_run(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node execute run.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_node_execute_run(self.to_dict(**dict_kwargs), **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    def run_on_node_execute_stream(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        chunk: dict[str, Any] | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Run callbacks on node execute stream.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            chunk (dict[str, Any]): Chunk of streaming data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks + self.callbacks:\n            try:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                    chunk = chunk.to_dict(for_tracing=True) if hasattr(chunk, \"to_dict\") else chunk\n                callback.on_node_execute_stream(self.to_dict(**dict_kwargs), chunk, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n\n    @abstractmethod\n    def execute(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs) -&gt; Any:\n        \"\"\"\n        Execute the node with the given input.\n        Args:\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the execution.\n        \"\"\"\n        pass\n\n    def depends_on(self, nodes: Union[\"Node\", list[\"Node\"]], condition: ChoiceCondition | None = None) -&gt; \"Node\":\n        \"\"\"\n        Add dependencies for this node. Accepts either a single node or a list of nodes.\n\n        Args:\n            nodes (Node or list[Node]): A single node or list of nodes this node depends on.\n            condition (ChoiceCondition, optional): The condition for the dependency.\n\n        Raises:\n            TypeError: If the input is neither a Node nor a list of Node instances.\n            ValueError: If an empty list is provided.\n\n        Returns:\n            self: Enables method chaining.\n        \"\"\"\n\n        if nodes is None:\n            raise ValueError(\"Nodes cannot be None.\")\n\n        if isinstance(nodes, Node):\n            nodes = [nodes]\n\n        if not isinstance(nodes, list) or not all(isinstance(node, Node) for node in nodes):\n            raise TypeError(f\"Expected a Node or a list of Node instances, but got {type(nodes).__name__}.\")\n\n        if not nodes:\n            raise ValueError(\"Cannot add an empty list of dependencies.\")\n\n        for node in nodes:\n            self.depends.append(NodeDependency(node=node, condition=condition))\n\n        return self\n\n    def enable_streaming(self, event: str = STREAMING_EVENT):\n        \"\"\"\n        Enable streaming for the node and optionally set the event name.\n\n        Args:\n            event (str): The event name for streaming. Defaults to 'streaming'.\n\n        Returns:\n            self: Enables method chaining.\n        \"\"\"\n        self.streaming.enabled = True\n        self.streaming.event = event\n        return self\n\n    @property\n    def outputs(self):\n        \"\"\"\n        Provide the output references for the node.\n        \"\"\"\n        return self._output_references\n\n    def inputs(self, **kwargs):\n        \"\"\"\n        Add input mappings for the node.\n\n        Returns:\n            self: Enables method chaining.\n\n        Examples:\n            from dynamiq.nodes.llms import OpenAI\n\n            openai_1_node = OpenAI(...)\n            openai_2_node = OpenAI(...)\n            openai_3_node = OpenAI(...)\n\n            def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n                return (\n                    f\"- {outputs[openai_1_node.id]['content'][:200]} \\n - {outputs[openai_2_node.id]['content'][:200]}\"\n                )\n\n            openai_4_node = (\n                OpenAI(\n                    ...\n                    prompt=prompts.Prompt(\n                        messages=[\n                            prompts.Message(\n                                role=\"user\",\n                                content=(\n                                    \"Please simplify that information for {{purpose}}:\\n\"\n                                    \"{{extra_instructions}}\\n\"\n                                    \"{{content}}\\n\"\n                                    \"{{extra_content}}\"\n                                ),\n                            )\n                        ],\n                    ),\n                )\n                .inputs(\n                    purpose=\"10 years old kids\",\n                    extra_instructions=\"Please return information in readable format.\",\n                    content=merge_and_short_content,\n                    extra_content=openai_3_node.outputs.content,\n                )\n                .depends_on([openai_1_node, openai_2_node, openai_3_node])\n            )\n        \"\"\"\n        for key, value in kwargs.items():\n            if callable(value):\n                self._validate_input_mapping_value_func(value)\n\n            self.input_mapping[key] = value\n        return self\n\n    def deep_merge(self, source: dict, destination: dict) -&gt; dict:\n        \"\"\"\n        Recursively merge dictionaries with proper override behavior.\n\n        Args:\n            source: Source dictionary with higher priority values\n            destination: Destination dictionary with lower priority values\n\n        Returns:\n            dict: Merged dictionary where source values override destination values,\n                  and lists are concatenated when both source and destination have lists\n        \"\"\"\n        result = destination.copy()\n        for key, value in source.items():\n            if key in result:\n                if isinstance(value, dict) and isinstance(result[key], dict):\n                    result[key] = self.deep_merge(value, result[key])\n                elif isinstance(value, list) and isinstance(result[key], list):\n                    result[key] = result[key] + value\n                else:\n                    result[key] = value\n            else:\n                result[key] = value\n        return result\n\n    def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n        \"\"\"Clean up resources created during dry run.\n        This method provides a default implementation that nodes can override\n        to perform specific cleanup operations. By default, it does nothing\n        but provides the interface for node-level cleanup.\n\n        Args:\n            dry_run_config: Configuration for dry run behavior.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>Provide the output references for the node.</p>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.clone","title":"<code>clone()</code>","text":"<p>Create a safe clone of the node.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def clone(self) -&gt; \"Node\":\n    \"\"\"Create a safe clone of the node.\"\"\"\n    cloned_node = self.model_copy(deep=False)\n\n    def _clone_nested(value: Any) -&gt; Any:\n        # Do not attempt to copy modules/functions/classes or other callables\n        if isinstance(value, (ModuleType, FunctionType)) or isinstance(value, type) or callable(value):\n            return value\n        if getattr(value, \"_clone_shared\", False):\n            return value\n        if isinstance(value, Node):\n            return value.clone()\n        elif isinstance(value, BaseModel):\n            try:\n                bm_copy = value.model_copy(deep=False)\n                for fname in getattr(value, \"model_fields\", {}):\n                    try:\n                        setattr(bm_copy, fname, _clone_nested(getattr(value, fname)))\n                    except Exception as e:\n                        logger.warning(f\"Clone: failed to clone BaseModel field '{fname}': {e}\")\n\n                return bm_copy\n            except Exception as e:\n                logger.warning(f\"Clone: BaseModel copy failed, falling back to shallow copy: {e}\")\n                return copy.copy(value)\n        elif isinstance(value, list):\n            return [_clone_nested(v) for v in value]\n        elif isinstance(value, dict):\n            return {k: _clone_nested(v) for k, v in value.items()}\n        try:\n            return copy.copy(value)\n        except Exception as e:\n            logger.warning(f\"Clone: failed to clone field '{value}': {e}\")\n            return value\n\n    for _field_name in getattr(cloned_node, \"model_fields\", {}):\n        _val = getattr(cloned_node, _field_name)\n        _new_val = _clone_nested(_val)\n        if _new_val is not _val:\n            try:\n                setattr(cloned_node, _field_name, _new_val)\n            except Exception as e:\n                logger.warning(f\"Clone: unable to set field '{_field_name}' during nested clone: {e}\")\n\n    init_map = self.get_clone_attr_initializers()\n    for attr_name, init_fn in init_map.items():\n        try:\n            if hasattr(cloned_node, attr_name):\n                value = init_fn(cloned_node) if callable(init_fn) else None\n                if value is not None:\n                    setattr(cloned_node, attr_name, value)\n                else:\n                    try:\n                        setattr(cloned_node, attr_name, None)\n                    except Exception as e:\n                        logger.warning(f\"Clone: failed to set attr '{attr_name}': {e}\")\n        except Exception as e:\n            logger.warning(f\"Clone: initializer for attr '{attr_name}' failed: {e}\")\n\n    for method_name in self.get_clone_init_methods_names():\n        try:\n            method = getattr(cloned_node, method_name, None)\n            if callable(method):\n                method()\n        except Exception as e:\n            logger.warning(f\"Clone: method '{method_name}' invocation failed: {e}\")\n\n    return cloned_node\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.deep_merge","title":"<code>deep_merge(source, destination)</code>","text":"<p>Recursively merge dictionaries with proper override behavior.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>dict</code> <p>Source dictionary with higher priority values</p> required <code>destination</code> <code>dict</code> <p>Destination dictionary with lower priority values</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Merged dictionary where source values override destination values,   and lists are concatenated when both source and destination have lists</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def deep_merge(self, source: dict, destination: dict) -&gt; dict:\n    \"\"\"\n    Recursively merge dictionaries with proper override behavior.\n\n    Args:\n        source: Source dictionary with higher priority values\n        destination: Destination dictionary with lower priority values\n\n    Returns:\n        dict: Merged dictionary where source values override destination values,\n              and lists are concatenated when both source and destination have lists\n    \"\"\"\n    result = destination.copy()\n    for key, value in source.items():\n        if key in result:\n            if isinstance(value, dict) and isinstance(result[key], dict):\n                result[key] = self.deep_merge(value, result[key])\n            elif isinstance(value, list) and isinstance(result[key], list):\n                result[key] = result[key] + value\n            else:\n                result[key] = value\n        else:\n            result[key] = value\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.depends_on","title":"<code>depends_on(nodes, condition=None)</code>","text":"<p>Add dependencies for this node. Accepts either a single node or a list of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Node or list[Node]</code> <p>A single node or list of nodes this node depends on.</p> required <code>condition</code> <code>ChoiceCondition</code> <p>The condition for the dependency.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is neither a Node nor a list of Node instances.</p> <code>ValueError</code> <p>If an empty list is provided.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>Node</code> <p>Enables method chaining.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def depends_on(self, nodes: Union[\"Node\", list[\"Node\"]], condition: ChoiceCondition | None = None) -&gt; \"Node\":\n    \"\"\"\n    Add dependencies for this node. Accepts either a single node or a list of nodes.\n\n    Args:\n        nodes (Node or list[Node]): A single node or list of nodes this node depends on.\n        condition (ChoiceCondition, optional): The condition for the dependency.\n\n    Raises:\n        TypeError: If the input is neither a Node nor a list of Node instances.\n        ValueError: If an empty list is provided.\n\n    Returns:\n        self: Enables method chaining.\n    \"\"\"\n\n    if nodes is None:\n        raise ValueError(\"Nodes cannot be None.\")\n\n    if isinstance(nodes, Node):\n        nodes = [nodes]\n\n    if not isinstance(nodes, list) or not all(isinstance(node, Node) for node in nodes):\n        raise TypeError(f\"Expected a Node or a list of Node instances, but got {type(nodes).__name__}.\")\n\n    if not nodes:\n        raise ValueError(\"Cannot add an empty list of dependencies.\")\n\n    for node in nodes:\n        self.depends.append(NodeDependency(node=node, condition=condition))\n\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.dry_run_cleanup","title":"<code>dry_run_cleanup(dry_run_config=None)</code>","text":"<p>Clean up resources created during dry run. This method provides a default implementation that nodes can override to perform specific cleanup operations. By default, it does nothing but provides the interface for node-level cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>dry_run_config</code> <code>DryRunConfig | None</code> <p>Configuration for dry run behavior.</p> <code>None</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n    \"\"\"Clean up resources created during dry run.\n    This method provides a default implementation that nodes can override\n    to perform specific cleanup operations. By default, it does nothing\n    but provides the interface for node-level cleanup.\n\n    Args:\n        dry_run_config: Configuration for dry run behavior.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.enable_streaming","title":"<code>enable_streaming(event=STREAMING_EVENT)</code>","text":"<p>Enable streaming for the node and optionally set the event name.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>str</code> <p>The event name for streaming. Defaults to 'streaming'.</p> <code>STREAMING_EVENT</code> <p>Returns:</p> Name Type Description <code>self</code> <p>Enables method chaining.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def enable_streaming(self, event: str = STREAMING_EVENT):\n    \"\"\"\n    Enable streaming for the node and optionally set the event name.\n\n    Args:\n        event (str): The event name for streaming. Defaults to 'streaming'.\n\n    Returns:\n        self: Enables method chaining.\n    \"\"\"\n    self.streaming.enabled = True\n    self.streaming.event = event\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.ensure_client","title":"<code>ensure_client()</code>","text":"<p>Ensure the client connection is alive and reconnect if needed. Override in subclasses that manage connections.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def ensure_client(self) -&gt; None:\n    \"\"\"\n    Ensure the client connection is alive and reconnect if needed.\n    Override in subclasses that manage connections.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute the node with the given input. Args:     input_data (dict[str, Any]): Input data for the node.     config (RunnableConfig, optional): Configuration for the runnable.     **kwargs: Additional keyword arguments.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of the execution.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@abstractmethod\ndef execute(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs) -&gt; Any:\n    \"\"\"\n    Execute the node with the given input.\n    Args:\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute_with_retry","title":"<code>execute_with_retry(input_data, config=None, **kwargs)</code>","text":"<p>Execute the node with retry logic and automatic connection management.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Result of the node execution.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If all retry attempts fail.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def execute_with_retry(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the node with retry logic and automatic connection management.\n\n    Args:\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the node execution.\n\n    Raises:\n        Exception: If all retry attempts fail.\n    \"\"\"\n    config = ensure_config(config)\n    timeout = self.error_handling.timeout_seconds\n\n    error = None\n    n_attempt = self.error_handling.max_retries + 1\n    executor = None\n    timed_out = False\n\n    try:\n        if timeout is not None:\n            executor = ContextAwareThreadPoolExecutor()\n\n        for attempt in range(n_attempt):\n            merged_kwargs = merge(kwargs, {\"execution_run_id\": uuid4()})\n\n            try:\n                self.ensure_client()\n            except Exception as conn_error:\n                logger.error(f\"Node {self.name} - {self.id}: Failed to ensure client connection: {conn_error}\")\n                error = conn_error\n                if attempt &lt; n_attempt - 1:\n                    time_to_sleep = self.error_handling.retry_interval_seconds * (\n                        self.error_handling.backoff_rate**attempt\n                    )\n                    logger.info(f\"Node {self.name} - {self.id}: retrying connection in {time_to_sleep} seconds.\")\n                    time.sleep(time_to_sleep)\n                    continue\n                else:\n                    raise\n\n            self.run_on_node_execute_start(config.callbacks, input_data, **merged_kwargs)\n\n            try:\n                if executor and timeout is not None:\n                    output = self.execute_with_timeout(\n                        executor=executor,\n                        timeout=timeout,\n                        input_data=input_data,\n                        config=config,\n                        **merged_kwargs,\n                    )\n                else:\n                    output = self.execute(input_data=input_data, config=config, **merged_kwargs)\n\n                self.run_on_node_execute_end(config.callbacks, output, **merged_kwargs)\n                return output\n            except TimeoutError as e:\n                error = e\n                timed_out = True\n                self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                logger.warning(f\"Node {self.name} - {self.id}: timeout.\")\n            except Exception as e:\n                error = e\n                self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                logger.error(f\"Node {self.name} - {self.id}: execution error: {e}\")\n\n            # do not sleep after the last attempt\n            if attempt &lt; n_attempt - 1:\n                time_to_sleep = self.error_handling.retry_interval_seconds * (\n                    self.error_handling.backoff_rate**attempt\n                )\n                logger.info(f\"Node {self.name} - {self.id}: retrying in {time_to_sleep} seconds.\")\n                time.sleep(time_to_sleep)\n\n        logger.error(f\"Node {self.name} - {self.id}: execution failed after {n_attempt} attempts.\")\n        raise error\n    finally:\n        if executor is not None:\n            # Use cancel_futures=True and wait=False when timeout occurred to prevent\n            # blocking on threads that may be stuck waiting (e.g., on input_queue.get())\n            executor.shutdown(wait=not timed_out, cancel_futures=timed_out)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute_with_timeout","title":"<code>execute_with_timeout(executor, timeout, input_data, config=None, **kwargs)</code>","text":"<p>Execute the node with a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ContextAwareThreadPoolExecutor</code> <p>Thread pool executor to use.</p> required <code>timeout</code> <code>float | None</code> <p>Timeout duration in seconds.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Result of the execution.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If execution exceeds the timeout.</p> <code>Exception</code> <p>If execution fails.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def execute_with_timeout(\n    self,\n    executor: ContextAwareThreadPoolExecutor,\n    timeout: float | None,\n    input_data: dict[str, Any] | BaseModel,\n    config: RunnableConfig = None,\n    **kwargs,\n):\n    \"\"\"\n    Execute the node with a timeout.\n\n    Args:\n        executor (ContextAwareThreadPoolExecutor): Thread pool executor to use.\n        timeout (float | None): Timeout duration in seconds.\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the execution.\n\n    Raises:\n        TimeoutError: If execution exceeds the timeout.\n        Exception: If execution fails.\n    \"\"\"\n    future = executor.submit(self.execute, input_data, config=config, **kwargs)\n\n    try:\n        return future.result(timeout=timeout)\n    except TimeoutError:\n        # Cancel the future to prevent further execution if possible.\n        # Note: cancel() only works if the task hasn't started yet.\n        # For running tasks, we rely on executor.shutdown(cancel_futures=True).\n        future.cancel()\n        raise\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_approved_data_or_origin","title":"<code>get_approved_data_or_origin(input_data, config=None, **kwargs)</code>","text":"<p>Approves or disapproves (cancels) Node execution by requesting feedback. Updates input data according to the feedback or leaves it the same. Raises NodeSkippedException if execution was canceled by feedback.</p> <p>Parameters:</p> Name Type Description Default <code>input_data(dict[str,</code> <code>Any]</code> <p>Input data.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Updated input data.</p> <p>Raises:</p> Type Description <code>NodeSkippedException</code> <p>If Node execution was canceled by feedback.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_approved_data_or_origin(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Approves or disapproves (cancels) Node execution by requesting feedback.\n    Updates input data according to the feedback or leaves it the same.\n    Raises NodeSkippedException if execution was canceled by feedback.\n\n    Args:\n        input_data(dict[str, Any]): Input data.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Updated input data.\n\n    Raises:\n        NodeSkippedException: If Node execution was canceled by feedback.\n    \"\"\"\n    if self.approval.enabled:\n        approval_result = self.send_approval_message(self.approval, input_data, config=config, **kwargs)\n        if not approval_result.is_approved:\n            raise NodeSkippedException(\n                message=f\"Execution was canceled by human with feedback {approval_result.feedback}\",\n                human_feedback=approval_result.feedback,\n                recoverable=True,\n                failed_depend=NodeDependency(self, option=\"Execution was canceled.\"),\n            )\n        return approval_result.data\n\n    return input_data\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_clone_attr_initializers","title":"<code>get_clone_attr_initializers()</code>","text":"<p>Mapping of attribute name -&gt; initializer callable(node) -&gt; value.</p> <p>Default: provides streaming isolation so clones do not share runtime state.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_clone_attr_initializers(self) -&gt; dict[str, Callable[[\"Node\"], Any]]:\n    \"\"\"Mapping of attribute name -&gt; initializer callable(node) -&gt; value.\n\n    Default: provides streaming isolation so clones do not share runtime state.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_clone_init_methods_names","title":"<code>get_clone_init_methods_names()</code>","text":"<p>List of method names to call on the clone to reset per-run state.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_clone_init_methods_names(self) -&gt; list[str]:\n    \"\"\"List of method names to call on the clone to reset per-run state.\"\"\"\n    return list(self._clone_init_methods_names)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_context_for_input_schema","title":"<code>get_context_for_input_schema()</code>","text":"<p>Provides context for input schema that is required for proper validation.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_context_for_input_schema(self) -&gt; dict:\n    \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n    return {}\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_input_streaming_event","title":"<code>get_input_streaming_event(event_msg_type=StreamingEventMessage, event=None, config=None)</code>","text":"<p>Get the input streaming event from the input streaming.</p> <p>Parameters:</p> Name Type Description Default <code>event_msg_type</code> <code>Type[StreamingEventMessage]</code> <p>The event message type to use.</p> <code>StreamingEventMessage</code> <code>event</code> <code>str</code> <p>The event to use for the message.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>StreamingEventMessage</code> <code>StreamingEventMessage</code> <p>The validated streaming event message.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input streaming is not enabled, timeout is exceeded, or the done event is set before receiving valid data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_input_streaming_event(\n    self,\n    event_msg_type: \"type[StreamingEventMessage]\" = StreamingEventMessage,\n    event: str | None = None,\n    config: RunnableConfig = None,\n) -&gt; StreamingEventMessage:\n    \"\"\"\n    Get the input streaming event from the input streaming.\n\n    Args:\n        event_msg_type (Type[StreamingEventMessage], optional): The event message type to use.\n        event (str, optional): The event to use for the message.\n        config (RunnableConfig, optional): Configuration for the runnable.\n\n    Returns:\n        StreamingEventMessage: The validated streaming event message.\n\n    Raises:\n        ValueError: If input streaming is not enabled, timeout is exceeded, or the done event is set\n            before receiving valid data.\n    \"\"\"\n    # Use runnable streaming configuration. If not found use node streaming configuration\n    streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n    if not streaming.input_streaming_enabled:\n        raise ValueError(\"Input streaming is not enabled.\")\n\n    def is_done() -&gt; bool:\n        return streaming.input_queue_done_event is not None and streaming.input_queue_done_event.is_set()\n\n    poll_interval = streaming.input_queue_poll_interval\n    elapsed = 0.0\n\n    while not is_done():\n        if streaming.timeout is not None and elapsed &gt;= streaming.timeout:\n            raise ValueError(f\"Input streaming timeout: {streaming.timeout} seconds exceeded.\")\n\n        remaining = streaming.timeout - elapsed if streaming.timeout is not None else poll_interval\n        wait_time = min(poll_interval, remaining)\n\n        try:\n            data = streaming.input_queue.get(timeout=wait_time)\n        except Empty:\n            elapsed += wait_time\n            if is_done():\n                raise ValueError(\"Input streaming completed without receiving valid data.\")\n            continue\n\n        try:\n            event_msg = event_msg_type.model_validate_json(data)\n            if event and event_msg.event != event:\n                raise ValueError()\n        except ValueError:\n            logger.error(\n                f\"Invalid streaming event data: {data}. \"\n                f\"Allowed event: {event}, event_msg_type: {event_msg_type}\"\n            )\n            continue\n\n        return event_msg\n\n    raise ValueError(\"Input streaming completed without receiving valid data.\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize node components.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager.</p> <code>None</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize node components.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager.\n    \"\"\"\n    self.is_postponed_component_init = False\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.inputs","title":"<code>inputs(**kwargs)</code>","text":"<pre><code>    Add input mappings for the node.\n\n    Returns:\n        self: Enables method chaining.\n\n    Examples:\n        from dynamiq.nodes.llms import OpenAI\n\n        openai_1_node = OpenAI(...)\n        openai_2_node = OpenAI(...)\n        openai_3_node = OpenAI(...)\n\n        def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n            return (\n                f\"- {outputs[openai_1_node.id]['content'][:200]}\n</code></pre> <ul> <li>{outputsopenai_2_node.id[:200]}\"                 )<pre><code>    openai_4_node = (\n        OpenAI(\n            ...\n            prompt=prompts.Prompt(\n                messages=[\n                    prompts.Message(\n                        role=\"user\",\n                        content=(\n                            \"Please simplify that information for {{purpose}}:\n</code></pre> <p>\"                                 \"{{extra_instructions}} \"                                 \"{{content}} \"                                 \"{{extra_content}}\"                             ),                         )                     ],                 ),             )             .inputs(                 purpose=\"10 years old kids\",                 extra_instructions=\"Please return information in readable format.\",                 content=merge_and_short_content,                 extra_content=openai_3_node.outputs.content,             )             .depends_on([openai_1_node, openai_2_node, openai_3_node])         )</p> </li> </ul> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def inputs(self, **kwargs):\n    \"\"\"\n    Add input mappings for the node.\n\n    Returns:\n        self: Enables method chaining.\n\n    Examples:\n        from dynamiq.nodes.llms import OpenAI\n\n        openai_1_node = OpenAI(...)\n        openai_2_node = OpenAI(...)\n        openai_3_node = OpenAI(...)\n\n        def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n            return (\n                f\"- {outputs[openai_1_node.id]['content'][:200]} \\n - {outputs[openai_2_node.id]['content'][:200]}\"\n            )\n\n        openai_4_node = (\n            OpenAI(\n                ...\n                prompt=prompts.Prompt(\n                    messages=[\n                        prompts.Message(\n                            role=\"user\",\n                            content=(\n                                \"Please simplify that information for {{purpose}}:\\n\"\n                                \"{{extra_instructions}}\\n\"\n                                \"{{content}}\\n\"\n                                \"{{extra_content}}\"\n                            ),\n                        )\n                    ],\n                ),\n            )\n            .inputs(\n                purpose=\"10 years old kids\",\n                extra_instructions=\"Please return information in readable format.\",\n                content=merge_and_short_content,\n                extra_content=openai_3_node.outputs.content,\n            )\n            .depends_on([openai_1_node, openai_2_node, openai_3_node])\n        )\n    \"\"\"\n    for key, value in kwargs.items():\n        if callable(value):\n            self._validate_input_mapping_value_func(value)\n\n        self.input_mapping[key] = value\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_async","title":"<code>run_async(input_data, config=None, depends_result=None, **kwargs)</code>  <code>async</code>","text":"<p>Run the node asynchronously with given input data and configuration. This runs the synchronous implementation in a thread pool to avoid blocking the event loop.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the node execution.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>async def run_async(\n    self,\n    input_data: dict,\n    config: RunnableConfig = None,\n    depends_result: dict = None,\n    **kwargs,\n) -&gt; RunnableResult:\n    \"\"\"\n    Run the node asynchronously with given input data and configuration.\n    This runs the synchronous implementation in a thread pool to avoid blocking the event loop.\n\n    Args:\n        input_data (Any): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        depends_result (dict, optional): Results of dependent nodes. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the node execution.\n    \"\"\"\n    return await asyncio.to_thread(\n        self.run_sync, input_data=input_data, config=config, depends_result=depends_result, **kwargs\n    )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_end","title":"<code>run_on_node_end(callbacks, output_data, **kwargs)</code>","text":"<p>Run callbacks on node end.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_end(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    output_data: dict[str, Any],\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node end.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                output_data = (\n                    output_data.to_dict(for_tracing=True) if hasattr(output_data, \"to_dict\") else output_data\n                )\n            callback.on_node_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_error","title":"<code>run_on_node_error(callbacks, error, **kwargs)</code>","text":"<p>Run callbacks on node error.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_error(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    error: BaseException,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node error.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        error (BaseException): The error that occurred.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_node_error(self.to_dict(**dict_kwargs), error, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_end","title":"<code>run_on_node_execute_end(callbacks, output_data, **kwargs)</code>","text":"<p>Run callbacks on node execute end.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_end(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    output_data: dict[str, Any],\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node execute end.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                output_data = (\n                    output_data.to_dict(for_tracing=True) if hasattr(output_data, \"to_dict\") else output_data\n                )\n            callback.on_node_execute_end(self.to_dict(**dict_kwargs), output_data, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_error","title":"<code>run_on_node_execute_error(callbacks, error, **kwargs)</code>","text":"<p>Run callbacks on node execute error.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_error(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    error: BaseException,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node execute error.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        error (BaseException): The error that occurred.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_node_execute_error(self.to_dict(**dict_kwargs), error, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_run","title":"<code>run_on_node_execute_run(callbacks, **kwargs)</code>","text":"<p>Run callbacks on node execute run.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_run(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node execute run.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_node_execute_run(self.to_dict(**dict_kwargs), **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_start","title":"<code>run_on_node_execute_start(callbacks, input_data, **kwargs)</code>","text":"<p>Run callbacks on node execute start.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_start(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    input_data: dict[str, Any] | BaseModel,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node execute start.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if isinstance(input_data, BaseModel):\n        input_data = dict(input_data)\n\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n            callback.on_node_execute_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_stream","title":"<code>run_on_node_execute_stream(callbacks, chunk=None, **kwargs)</code>","text":"<p>Run callbacks on node execute stream.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>chunk</code> <code>dict[str, Any]</code> <p>Chunk of streaming data.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_stream(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    chunk: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node execute stream.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        chunk (dict[str, Any]): Chunk of streaming data.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                chunk = chunk.to_dict(for_tracing=True) if hasattr(chunk, \"to_dict\") else chunk\n            callback.on_node_execute_stream(self.to_dict(**dict_kwargs), chunk, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_skip","title":"<code>run_on_node_skip(callbacks, skip_data, input_data, human_feedback=None, **kwargs)</code>","text":"<p>Run callbacks on node skip.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_skip(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    skip_data: dict[str, Any],\n    input_data: dict[str, Any],\n    human_feedback: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node skip.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n            callback.on_node_skip(\n                self.to_dict(**dict_kwargs), skip_data, input_data, human_feedback=human_feedback, **kwargs\n            )\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_start","title":"<code>run_on_node_start(callbacks, input_data, **kwargs)</code>","text":"<p>Run callbacks on node start.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_start(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    input_data: dict[str, Any],\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run callbacks on node start.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    for callback in callbacks + self.callbacks:\n        try:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n                input_data = input_data.to_dict(for_tracing=True) if hasattr(input_data, \"to_dict\") else input_data\n            callback.on_node_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error running callback {callback.__class__.__name__}: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_sync","title":"<code>run_sync(input_data, config=None, depends_result=None, **kwargs)</code>","text":"<p>Run the node synchronously with given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>depends_result</code> <code>dict</code> <p>Results of already executed nodes. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the node execution.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_sync(\n    self,\n    input_data: dict,\n    config: RunnableConfig = None,\n    depends_result: dict = None,\n    **kwargs,\n) -&gt; RunnableResult:\n    \"\"\"\n    Run the node synchronously with given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        depends_result (dict, optional): Results of already executed nodes. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the node execution.\n    \"\"\"\n    from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n    logger.info(f\"Node {self.name} - {self.id}: execution started.\")\n    transformed_input = input_data\n    time_start = datetime.now()\n\n    config = ensure_config(config)\n\n    run_id = uuid4()\n    merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"parent_run_id\": kwargs.get(\"parent_run_id\", None)})\n    if depends_result is None:\n        depends_result = {}\n\n    try:\n        try:\n            self.validate_depends(depends_result)\n            input_data = self.get_approved_data_or_origin(input_data, config=config, **merged_kwargs)\n        except NodeException as e:\n            transformed_input = input_data | {\n                k: result.to_tracing_depend_dict() for k, result in depends_result.items()\n            }\n            skip_data = {\"failed_dependency\": e.failed_depend.to_dict(for_tracing=True)}\n            self.run_on_node_skip(\n                callbacks=config.callbacks,\n                skip_data=skip_data,\n                input_data=transformed_input,\n                human_feedback=getattr(e, \"human_feedback\", None),\n                **merged_kwargs,\n            )\n            logger.info(f\"Node {self.name} - {self.id}: execution skipped.\")\n            return RunnableResult(\n                status=RunnableStatus.SKIP,\n                input=transformed_input,\n                output=None,\n                error=RunnableResultError.from_exception(e, recoverable=e.recoverable),\n            )\n\n        transformed_input = self.validate_input_schema(\n            self.transform_input(input_data=input_data, depends_result=depends_result, config=config, **kwargs),\n            **kwargs,\n        )\n        self.run_on_node_start(config.callbacks, dict(transformed_input), **merged_kwargs)\n        cache = cache_wf_entity(\n            entity_id=self.id,\n            cache_enabled=self.caching.enabled,\n            cache_config=config.cache,\n        )\n\n        output, from_cache = cache(self.execute_with_retry)(transformed_input, config, **merged_kwargs)\n\n        merged_kwargs[\"is_output_from_cache\"] = from_cache\n        transformed_output = self.transform_output(output, config=config, **kwargs)\n\n        self.run_on_node_end(config.callbacks, transformed_output, **merged_kwargs)\n\n        logger.info(\n            f\"Node {self.name} - {self.id}: execution succeeded in \"\n            f\"{format_duration(time_start, datetime.now())}.\"\n        )\n        return RunnableResult(\n            status=RunnableStatus.SUCCESS, input=dict(transformed_input), output=transformed_output\n        )\n    except Exception as e:\n        self.run_on_node_error(callbacks=config.callbacks, error=e, input_data=transformed_input, **merged_kwargs)\n        logger.error(\n            f\"Node {self.name} - {self.id}: execution failed in \"\n            f\"{format_duration(time_start, datetime.now())}. {e}\"\n        )\n\n        recoverable = isinstance(e, RecoverableAgentException)\n        result = RunnableResult(\n            status=RunnableStatus.FAILURE,\n            input=transformed_input,\n            output=None,\n            error=RunnableResultError.from_exception(e, recoverable=recoverable),\n        )\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.send_approval_message","title":"<code>send_approval_message(approval_config, input_data, config=None, **kwargs)</code>","text":"<p>Sends approval message and determines if it was approved or disapproved (canceled).</p> <p>Parameters:</p> Name Type Description Default <code>approval_config</code> <code>ApprovalConfig</code> <p>Configuration for the approval.</p> required <code>input_data</code> <code>dict</code> <p>Data that will be sent.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ApprovalInputData</code> <code>ApprovalInputData</code> <p>Result of approval.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def send_approval_message(\n    self, approval_config: ApprovalConfig, input_data: dict, config: RunnableConfig = None, **kwargs\n) -&gt; ApprovalInputData:\n    \"\"\"\n    Sends approval message and determines if it was approved or disapproved (canceled).\n\n    Args:\n        approval_config (ApprovalConfig): Configuration for the approval.\n        input_data (dict): Data that will be sent.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ApprovalInputData: Result of approval.\n    \"\"\"\n\n    message = Template(approval_config.msg_template).render(self.to_dict(), input_data=input_data)\n    match approval_config.feedback_method:\n        case FeedbackMethod.STREAM:\n            approval_result = self.send_streaming_approval_message(\n                message, input_data, approval_config, config=config, **kwargs\n            )\n        case FeedbackMethod.CONSOLE:\n            approval_result = self.send_console_approval_message(message)\n        case _:\n            raise ValueError(f\"Error: Incorrect feedback method is chosen {approval_config.feedback_method}.\")\n\n    update_params = {\n        feature_name: approval_result.data[feature_name]\n        for feature_name in approval_config.mutable_data_params\n        if feature_name in approval_result.data\n    }\n    approval_result.data = {**input_data, **update_params}\n\n    if approval_result.is_approved is None:\n        if approval_result.feedback == approval_config.accept_pattern:\n            logger.info(\n                f\"Node {self.name} action was approved by human \"\n                f\"with provided feedback '{approval_result.feedback}'.\"\n            )\n            approval_result.is_approved = True\n\n        else:\n            approval_result.is_approved = False\n            logger.info(\n                f\"Node {self.name} action was canceled by human\"\n                f\"with provided feedback '{approval_result.feedback}'.\"\n            )\n\n    return approval_result\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.send_console_approval_message","title":"<code>send_console_approval_message(template)</code>","text":"<p>Sends approval message in console and waits for response.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>dict</code> <p>Template to send.</p> required <p>Returns:     ApprovalInputData: Response to approval message.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def send_console_approval_message(self, template: str) -&gt; ApprovalInputData:\n    \"\"\"\n    Sends approval message in console and waits for response.\n\n    Args:\n        template (dict): Template to send.\n    Returns:\n        ApprovalInputData: Response to approval message.\n    \"\"\"\n    feedback = input(template)\n    return ApprovalInputData(feedback=feedback)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.send_streaming_approval_message","title":"<code>send_streaming_approval_message(template, input_data, approval_config, config=None, **kwargs)</code>","text":"<p>Sends approval message and waits for response.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Template to send.</p> required <code>input_data</code> <code>dict</code> <p>Data that will be sent.</p> required <code>approval_config</code> <code>ApprovalConfig</code> <p>Configuration for approval.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Return <p>ApprovalInputData: Response to approval message.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def send_streaming_approval_message(\n    self, template: str, input_data: dict, approval_config: ApprovalConfig, config: RunnableConfig = None, **kwargs\n) -&gt; ApprovalInputData:\n    \"\"\"\n    Sends approval message and waits for response.\n\n    Args:\n        template (str): Template to send.\n        input_data (dict): Data that will be sent.\n        approval_config (ApprovalConfig): Configuration for approval.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Return:\n        ApprovalInputData: Response to approval message.\n\n    \"\"\"\n    event = ApprovalStreamingOutputEventMessage(\n        wf_run_id=config.run_id,\n        entity_id=self.id,\n        data={\"template\": template, \"data\": input_data, \"mutable_data_params\": approval_config.mutable_data_params},\n        event=approval_config.event,\n        source=StreamingEntitySource(\n            id=self.id,\n            name=self.name,\n            group=self.group,\n            type=self.type,\n        ),\n    )\n\n    logger.info(f\"Node {self.name} - {self.id}: sending approval.\")\n\n    self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n\n    output: ApprovalInputData = self.get_input_streaming_event(\n        event=approval_config.event, event_msg_type=ApprovalStreamingInputEventMessage, config=config\n    ).data\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    for_tracing: bool = kwargs.pop(\"for_tracing\", False)\n    exclude = kwargs.pop(\n        \"exclude\", self.to_dict_exclude_params if include_secure_params else self.to_dict_exclude_secure_params\n    )\n    data = self.model_dump(\n        exclude=exclude,\n        serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n        **kwargs,\n    )\n\n    it = self.input_transformer.to_dict(for_tracing=for_tracing, **kwargs)\n    if it is not None:\n        data[\"input_transformer\"] = it\n    ot = self.output_transformer.to_dict(for_tracing=for_tracing, **kwargs)\n    if ot is not None:\n        data[\"output_transformer\"] = ot\n    data[\"caching\"] = self.caching.to_dict(for_tracing=for_tracing, **kwargs)\n    data[\"streaming\"] = self.streaming.to_dict(for_tracing=for_tracing, **kwargs)\n    data[\"approval\"] = self.approval.to_dict(for_tracing=for_tracing, **kwargs)\n\n    data[\"depends\"] = [depend.to_dict(for_tracing=for_tracing, **kwargs) for depend in self.depends]\n    data[\"input_mapping\"] = format_value(self.input_mapping)\n\n    if getattr(self, \"connection\", None):\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing, **kwargs)\n\n    if for_tracing:\n        data = {k: v for k, v in data.items() if v is not None or k in (\"input\", \"output\")}\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform","title":"<code>transform(data, transformer)</code>  <code>staticmethod</code>","text":"<p>Apply transformation to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data to transform.</p> required <code>transformer</code> <code>Transformer</code> <p>Transformer to apply.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Transformed data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@staticmethod\ndef transform(data: Any, transformer: Transformer) -&gt; Any:\n    \"\"\"\n    Apply transformation to data.\n\n    Args:\n        data (Any): Input data to transform.\n        transformer (Transformer): Transformer to apply.\n\n    Returns:\n        Any: Transformed data.\n    \"\"\"\n    output = jsonpath_filter(data, transformer.path)\n    output = jsonpath_mapper(output, transformer.selector)\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform_input","title":"<code>transform_input(input_data, depends_result, use_input_transformer=True, **kwargs)</code>","text":"<p>Transform input data for the node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input data for the node.</p> required <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes.</p> required <code>use_input_transformer</code> <code>bool</code> <p>Determines if InputTransformer will be applied to the input.</p> <code>True</code> <p>Raises:</p> Type Description <code>NodeException</code> <p>If a dependency result is missing or input mapping fails.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Transformed input data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def transform_input(\n    self, input_data: dict, depends_result: dict[Any, RunnableResult], use_input_transformer: bool = True, **kwargs\n) -&gt; dict:\n    \"\"\"\n    Transform input data for the node.\n\n    Args:\n        input_data (dict): Input data for the node.\n        depends_result (dict): Results of dependent nodes.\n        use_input_transformer (bool): Determines if InputTransformer will be applied to the input.\n\n    Raises:\n        NodeException: If a dependency result is missing or input mapping fails.\n\n    Returns:\n        dict: Transformed input data.\n    \"\"\"\n    # Apply input transformer\n    if (self.input_transformer.path or self.input_transformer.selector) and use_input_transformer:\n        depends_result_as_dict = {k: result.to_depend_dict() for k, result in depends_result.items()}\n        inputs = self.transform(input_data | depends_result_as_dict, self.input_transformer)\n    else:\n        inputs = input_data | {k: result.to_tracing_depend_dict() for k, result in depends_result.items()}\n\n    # Apply input bindings\n    for key, value in self.input_mapping.items():\n        if isinstance(value, NodeOutputReference):\n            depend_result = depends_result.get(value.node.id)\n            if not depend_result:\n                raise NodeException(message=f\"Dependency {value.node.id}: result not found.\")\n            if value.output_key not in depend_result.output:\n                raise NodeException(message=f\"Dependency {value.node.id} output {value.output_key}: not found.\")\n\n            inputs[key] = depend_result.output[value.output_key]\n\n        elif callable(value):\n            try:\n                inputs[key] = value(inputs, {d_id: result.output for d_id, result in depends_result.items()})\n            except Exception:\n                raise NodeException(message=f\"Input mapping {key}: failed.\")\n        else:\n            inputs[key] = value\n\n    return inputs\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform_output","title":"<code>transform_output(output_data, **kwargs)</code>","text":"<p>Transform output data from the node.</p> <p>Parameters:</p> Name Type Description Default <code>output_data</code> <code>Any</code> <p>Output data to transform.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Transformed output data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def transform_output(self, output_data: Any, **kwargs) -&gt; Any:\n    \"\"\"\n    Transform output data from the node.\n\n    Args:\n        output_data (Any): Output data to transform.\n\n    Returns:\n        Any: Transformed output data.\n    \"\"\"\n    return self.transform(output_data, self.output_transformer)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.validate_depends","title":"<code>validate_depends(depends_result)</code>","text":"<p>Validate all dependencies of the node.</p> <p>Parameters:</p> Name Type Description Default <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the node.</p> required Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def validate_depends(self, depends_result: dict[str, RunnableResult]):\n    \"\"\"\n    Validate all dependencies of the node.\n\n    Args:\n        depends_result (dict): Results of dependent nodes.\n        input_data (dict): Input data for the node.\n\n    Raises:\n        Various exceptions based on dependency validation results.\n    \"\"\"\n    for dep in self.depends:\n        self._validate_dependency_status(depend=dep, depends_result=depends_result)\n        if dep.condition:\n            self._validate_dependency_condition(depend=dep, depends_result=depends_result)\n        if dep.option:\n            self._validate_dependency_option(depend=dep, depends_result=depends_result)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.validate_input_schema","title":"<code>validate_input_schema(input_data, **kwargs)</code>","text":"<p>Validate input data against the input schema. Returns instance of input_schema if it is provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data to validate.</p> required <p>Raises:</p> Type Description <code>NodeException</code> <p>If input data does not match the input schema.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def validate_input_schema(self, input_data: dict[str, Any], **kwargs) -&gt; dict[str, Any] | BaseModel:\n    \"\"\"\n    Validate input data against the input schema. Returns instance of input_schema if it is provided.\n\n    Args:\n        input_data (Any): Input data to validate.\n\n    Raises:\n        NodeException: If input data does not match the input schema.\n    \"\"\"\n    from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n    if self.input_schema:\n        try:\n            return self.input_schema.model_validate(\n                input_data, context=kwargs | self.get_context_for_input_schema()\n            )\n        except Exception as e:\n            if kwargs.get(\"recoverable_error\", False):\n                raise RecoverableAgentException(f\"Input data validation failed: {e}\")\n            raise e\n\n    return input_data\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeDependency","title":"<code>NodeDependency</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a dependency between nodes.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The dependent node.</p> <code>option</code> <code>str | None</code> <p>Optional condition for the dependency.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeDependency(BaseModel):\n    \"\"\"\n    Represents a dependency between nodes.\n\n    Attributes:\n        node (Node): The dependent node.\n        option (str | None): Optional condition for the dependency.\n    \"\"\"\n    node: \"Node\"\n    option: str | None = None\n    condition: ChoiceCondition | None = None\n\n    def __init__(self, node: \"Node\", option: str | None = None, condition: ChoiceCondition | None = None):\n        super().__init__(node=node, option=option, condition=condition)\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        for_tracing: bool = kwargs.get(\"for_tracing\", False)\n        node_value: dict\n        if for_tracing:\n            node_value = {\"id\": self.node.id, \"name\": self.node.name, \"type\": self.node.type}\n        else:\n            node_value = self.node.to_dict(**kwargs)\n\n        return {\n            \"node\": node_value,\n            \"option\": self.option,\n            \"condition\": self.condition.model_dump() if self.condition else None,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeDependency.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    for_tracing: bool = kwargs.get(\"for_tracing\", False)\n    node_value: dict\n    if for_tracing:\n        node_value = {\"id\": self.node.id, \"name\": self.node.name, \"type\": self.node.type}\n    else:\n        node_value = self.node.to_dict(**kwargs)\n\n    return {\n        \"node\": node_value,\n        \"option\": self.option,\n        \"condition\": self.condition.model_dump() if self.condition else None,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeMetadata","title":"<code>NodeMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a node.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str | None</code> <p>Optional label for the node.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeMetadata(BaseModel):\n    \"\"\"\n    Metadata for a node.\n\n    Attributes:\n        label (str | None): Optional label for the node.\n    \"\"\"\n    label: str | None = None\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeOutputReference","title":"<code>NodeOutputReference</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a reference to a node output.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to reference.</p> <code>output_key</code> <code>str</code> <p>Key for the output.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeOutputReference(BaseModel):\n    \"\"\"\n    Represents a reference to a node output.\n\n    Attributes:\n        node (Node): The node to reference.\n        output_key (str): Key for the output.\n    \"\"\"\n\n    node: \"Node\"\n    output_key: str\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeOutputReferences","title":"<code>NodeOutputReferences</code>","text":"<p>Provides output references for a node.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to provide output references for.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeOutputReferences:\n    \"\"\"\n    Provides output references for a node.\n\n    Attributes:\n        node (Node): The node to provide output references for.\n    \"\"\"\n\n    def __init__(self, node: \"Node\"):\n        self.node = node\n\n    def __getattr__(self, key: Any):\n        return NodeOutputReference(node=self.node, output_key=key)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeReadyToRun","title":"<code>NodeReadyToRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a node ready to run with its input data and dependencies.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to be run.</p> <code>is_ready</code> <code>bool</code> <p>Whether the node is ready to run.</p> <code>input_data</code> <code>Any</code> <p>Input data for the node.</p> <code>depends_result</code> <code>dict[str, Any]</code> <p>Results of dependent nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeReadyToRun(BaseModel):\n    \"\"\"\n    Represents a node ready to run with its input data and dependencies.\n\n    Attributes:\n        node (Node): The node to be run.\n        is_ready (bool): Whether the node is ready to run.\n        input_data (Any): Input data for the node.\n        depends_result (dict[str, Any]): Results of dependent nodes.\n    \"\"\"\n    node: \"Node\"\n    is_ready: bool\n    input_data: Any = None\n    depends_result: dict[str, Any] = {}\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.OutputTransformer","title":"<code>OutputTransformer</code>","text":"<p>               Bases: <code>InputTransformer</code></p> <p>Output transformer for nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class OutputTransformer(InputTransformer):\n    \"\"\"Output transformer for nodes.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for input and output transformers.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str | None</code> <p>JSONPath for data selection.</p> <code>selector</code> <code>dict[str, str] | None</code> <p>Mapping for data transformation.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class Transformer(BaseModel):\n    \"\"\"\n    Base class for input and output transformers.\n\n    Attributes:\n        path (str | None): JSONPath for data selection.\n        selector (dict[str, str] | None): Mapping for data transformation.\n    \"\"\"\n    path: str | None = None\n    selector: dict[str, str] | None = None\n\n    def to_dict(self, for_tracing: bool = False, **kwargs) -&gt; dict | None:\n        if for_tracing and self.path is None and self.selector is None:\n            return None\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode","title":"<code>VectorStoreNode</code>","text":"<p>               Bases: <code>ConnectionNode</code>, <code>BaseVectorStoreParams</code>, <code>ABC</code></p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class VectorStoreNode(ConnectionNode, BaseVectorStoreParams, ABC):\n    vector_store: Any | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_connection_client(self):\n        if not self.vector_store and not self.connection:\n            raise ValueError(\"'connection' or 'vector_store' should be specified\")\n        return self\n\n    @property\n    @abstractmethod\n    def vector_store_cls(self):\n        raise NotImplementedError\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(BaseVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def connect_to_vector_store(self):\n        vector_store_params = self.vector_store_params\n        vector_store = self.vector_store_cls(**vector_store_params)\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: connected to {self.vector_store_cls.__name__} vector store with\"\n            f\" {vector_store_params}\"\n        )\n\n        return vector_store\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize components for the node.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        self._connection_manager = connection_manager\n\n        # Use vector_store client if it is already initialized\n        if self.vector_store:\n            self.client = self.vector_store.client\n\n        super().init_components(connection_manager)\n\n        if self.vector_store is None:\n            self.vector_store = self.connect_to_vector_store()\n\n    def is_client_closed(self) -&gt; bool:\n        \"\"\"\n        Check if the client or vector store connection is closed.\n\n        Returns:\n            bool: True if client/vector_store is closed, False otherwise\n        \"\"\"\n        if self.vector_store and hasattr(self.vector_store, \"client\"):\n            vector_store_client = self.vector_store.client\n            if vector_store_client is None:\n                return False\n            if hasattr(vector_store_client, \"closed\"):\n                return vector_store_client.closed\n\n        return super().is_client_closed()\n\n    def ensure_client(self) -&gt; None:\n        \"\"\"\n        Ensure the client and vector store are alive and reconnect if needed.\n        Automatically detects closed connections and reinitializes them.\n        \"\"\"\n        if self.is_client_closed():\n            if self.connection is None:\n                logger.debug(\n                    f\"Node {self.name} - {self.id}: Vector store client connection is closed but no connection \"\n                    f\"available for reinitialization.\"\n                )\n                return\n\n            logger.warning(f\"Node {self.name} - {self.id}: Vector store client connection is closed. Reinitializing\")\n            connection_manager = self._connection_manager or ConnectionManager()\n\n            try:\n                self.client = connection_manager.get_connection_client(connection=self.connection)\n                self.vector_store = self.connect_to_vector_store()\n                logger.info(f\"Node {self.name} - {self.id}: Vector store reinitialized successfully\")\n            except Exception as e:\n                logger.error(f\"Node {self.name} - {self.id}: Failed to reinitialize vector store: {e}\")\n                raise ConnectionManagerException(\n                    f\"Failed to reinitialize vector store for node {self.name}: {e}\"\n                ) from e\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode.ensure_client","title":"<code>ensure_client()</code>","text":"<p>Ensure the client and vector store are alive and reconnect if needed. Automatically detects closed connections and reinitializes them.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def ensure_client(self) -&gt; None:\n    \"\"\"\n    Ensure the client and vector store are alive and reconnect if needed.\n    Automatically detects closed connections and reinitializes them.\n    \"\"\"\n    if self.is_client_closed():\n        if self.connection is None:\n            logger.debug(\n                f\"Node {self.name} - {self.id}: Vector store client connection is closed but no connection \"\n                f\"available for reinitialization.\"\n            )\n            return\n\n        logger.warning(f\"Node {self.name} - {self.id}: Vector store client connection is closed. Reinitializing\")\n        connection_manager = self._connection_manager or ConnectionManager()\n\n        try:\n            self.client = connection_manager.get_connection_client(connection=self.connection)\n            self.vector_store = self.connect_to_vector_store()\n            logger.info(f\"Node {self.name} - {self.id}: Vector store reinitialized successfully\")\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: Failed to reinitialize vector store: {e}\")\n            raise ConnectionManagerException(\n                f\"Failed to reinitialize vector store for node {self.name}: {e}\"\n            ) from e\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components for the node.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize components for the node.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    self._connection_manager = connection_manager\n\n    # Use vector_store client if it is already initialized\n    if self.vector_store:\n        self.client = self.vector_store.client\n\n    super().init_components(connection_manager)\n\n    if self.vector_store is None:\n        self.vector_store = self.connect_to_vector_store()\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode.is_client_closed","title":"<code>is_client_closed()</code>","text":"<p>Check if the client or vector store connection is closed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if client/vector_store is closed, False otherwise</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def is_client_closed(self) -&gt; bool:\n    \"\"\"\n    Check if the client or vector store connection is closed.\n\n    Returns:\n        bool: True if client/vector_store is closed, False otherwise\n    \"\"\"\n    if self.vector_store and hasattr(self.vector_store, \"client\"):\n        vector_store_client = self.vector_store.client\n        if vector_store_client is None:\n            return False\n        if hasattr(vector_store_client, \"closed\"):\n            return vector_store_client.closed\n\n    return super().is_client_closed()\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ensure_config","title":"<code>ensure_config(config=None)</code>","text":"<p>Ensure that a valid RunnableConfig is provided.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RunnableConfig</code> <p>The input configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RunnableConfig</code> <code>RunnableConfig</code> <p>A valid RunnableConfig object.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def ensure_config(config: RunnableConfig = None) -&gt; RunnableConfig:\n    \"\"\"\n    Ensure that a valid RunnableConfig is provided.\n\n    Args:\n        config (RunnableConfig, optional): The input configuration. Defaults to None.\n\n    Returns:\n        RunnableConfig: A valid RunnableConfig object.\n    \"\"\"\n    if config is None:\n        return RunnableConfig(callbacks=[])\n\n    return config\n</code></pre>"},{"location":"dynamiq/nodes/types/","title":"Types","text":""},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.ActionType","title":"<code>ActionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of action types for tools and nodes used as tools. Classifies what kind of action is performed.</p> <p>Note: This is distinct from NodeGroup which classifies the node category. ActionType indicates the behavior/action being performed.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class ActionType(str, Enum):\n    \"\"\"\n    Enumeration of action types for tools and nodes used as tools.\n    Classifies what kind of action is performed.\n\n    Note: This is distinct from NodeGroup which classifies the node category.\n    ActionType indicates the behavior/action being performed.\n    \"\"\"\n\n    WEB_SEARCH = \"web_search\"\n    WEB_SCRAPE = \"web_scrape\"\n    CODE_EXECUTION = \"code_execution\"\n    FILE_OPERATION = \"file_operation\"\n    DATABASE_QUERY = \"database_query\"\n    COMPUTER_USE = \"computer_use\"\n    SEMANTIC_SEARCH = \"semantic_search\"\n</code></pre>"},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.ChoiceCondition","title":"<code>ChoiceCondition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a condition.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class ChoiceCondition(BaseModel):\n    \"\"\"Represents a condition.\"\"\"\n\n    variable: str | None = None\n    operator: ConditionOperator | None = None\n    value: Any = None\n    is_not: bool = False\n    operands: list[\"ChoiceCondition\"] | None = None\n</code></pre>"},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.ConditionOperator","title":"<code>ConditionOperator</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representing various condition operators.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class ConditionOperator(str, Enum):\n    \"\"\"Enum representing various condition operators.\"\"\"\n\n    OR = \"or\"\n    AND = \"and\"\n    BOOLEAN_EQUALS = \"boolean-equals\"\n    BOOLEAN_EQUALS_PATH = \"boolean-equals-path\"\n    NUMERIC_EQUALS = \"numeric-equals\"\n    NUMERIC_EQUALS_PATH = \"numeric-equals-path\"\n    NUMERIC_GREATER_THAN = \"numeric-greater-than\"\n    NUMERIC_GREATER_THAN_PATH = \"numeric-greater-than-path\"\n    NUMERIC_GREATER_THAN_OR_EQUALS = \"numeric-greater-than-or-equals\"\n    NUMERIC_GREATER_THAN_OR_EQUALS_PATH = \"numeric-greater-than-or-equals-path\"\n    NUMERIC_LESS_THAN = \"numeric-less-than\"\n    NUMERIC_LESS_THAN_PATH = \"numeric-less-than-path\"\n    NUMERIC_LESS_THAN_OR_EQUALS = \"numeric-less-than-or-equals\"\n    NUMERIC_LESS_THAN_OR_EQUALS_PATH = \"numeric-less-than-or-equals-path\"\n    STRING_EQUALS = \"string-equals\"\n    STRING_EQUALS_PATH = \"string-equals-path\"\n    STRING_GREATER_THAN = \"string-greater-than\"\n    STRING_GREATER_THAN_PATH = \"string-greater-than-path\"\n    STRING_GREATER_THAN_OR_EQUALS = \"string-greater-than-or-equals\"\n    STRING_GREATER_THAN_OR_EQUALS_PATH = \"string-greater-than-or-equals-path\"\n    STRING_LESS_THAN = \"string-less-than\"\n    STRING_LESS_THAN_PATH = \"string-less-than-path\"\n    STRING_LESS_THAN_OR_EQUALS = \"string-less-than-or-equals\"\n    STRING_LESS_THAN_OR_EQUALS_PATH = \"string-less-than-or-equals-path\"\n    STRING_STARTS_WITH = \"string-starts-with\"\n    STRING_ENDS_WITH = \"string-ends-with\"\n    STRING_CONTAINS = \"string-contains\"\n    STRING_REGEXP = \"string-regexp\"\n</code></pre>"},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.InferenceMode","title":"<code>InferenceMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of inference types.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class InferenceMode(str, Enum):\n    \"\"\"\n    Enumeration of inference types.\n    \"\"\"\n\n    DEFAULT = \"DEFAULT\"\n    XML = \"XML\"\n    FUNCTION_CALLING = \"FUNCTION_CALLING\"\n    STRUCTURED_OUTPUT = \"STRUCTURED_OUTPUT\"\n</code></pre>"},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.NodeGroup","title":"<code>NodeGroup</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of node groups that categorize different types of nodes.</p> <p>Each group represents a collection of related node types, providing a higher-level classification of the system's components.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class NodeGroup(str, Enum):\n    \"\"\"\n    Enumeration of node groups that categorize different types of nodes.\n\n    Each group represents a collection of related node types, providing a higher-level\n    classification of the system's components.\n    \"\"\"\n\n    LLMS = \"llms\"\n    OPERATORS = \"operators\"\n    EMBEDDERS = \"embedders\"\n    RANKERS = \"rankers\"\n    CONVERTERS = \"converters\"\n    RETRIEVERS = \"retrievers\"\n    SPLITTERS = \"splitters\"\n    WRITERS = \"writers\"\n    UTILS = \"utils\"\n    TOOLS = \"tools\"\n    AGENTS = \"agents\"\n    AUDIO = \"audio\"\n    VALIDATORS = \"validators\"\n    IMAGES = \"images\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/","title":"Agent","text":""},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>HistoryManagerMixin</code>, <code>Agent</code></p> <p>Unified Agent that uses a ReAct-style strategy for processing tasks by interacting with tools in a loop.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>class Agent(HistoryManagerMixin, BaseAgent):\n    \"\"\"Unified Agent that uses a ReAct-style strategy for processing tasks by interacting with tools in a loop.\"\"\"\n\n    name: str = \"Agent\"\n    max_loops: int = Field(default=15, ge=2)\n    inference_mode: InferenceMode = Field(default=InferenceMode.DEFAULT)\n    behaviour_on_max_loops: Behavior = Field(\n        default=Behavior.RAISE,\n        description=\"Define behavior when max loops are exceeded. Options are 'raise' or 'return'.\",\n    )\n    direct_tool_output_enabled: bool = Field(\n        default=False,\n        description=\"Enable direct tool output capability. \"\n        \"When True, the agent can return raw tool outputs directly without summarization.\",\n    )\n\n    format_schema: list = Field(default_factory=list)\n    summarization_config: SummarizationConfig = Field(default_factory=SummarizationConfig)\n    state: AgentState = Field(default_factory=AgentState, exclude=True)\n\n    _tools: list[Tool] = []\n    _response_format: dict[str, Any] | None = None\n    _requested_output_files: list[str] = []\n\n    def get_clone_attr_initializers(self) -&gt; dict[str, Callable[[Node], Any]]:\n        \"\"\"\n        Define attribute initializers for cloning.\n\n        Ensures that cloned agents get fresh instances of:\n        - _tool_cache: Independent tool execution cache\n        - state: Independent AgentState to avoid race conditions in parallel execution\n\n        Returns:\n            Dictionary mapping attribute names to initializer functions\n        \"\"\"\n        base = super().get_clone_attr_initializers()\n        base.update(\n            {\n                \"_tool_cache\": lambda _: {},\n                \"state\": lambda _: AgentState(),\n            }\n        )\n        return base\n\n    def reset_run_state(self):\n        \"\"\"Resets the agent's run state including AgentState.\"\"\"\n        super().reset_run_state()\n        self.state.reset()\n\n    def log_reasoning(self, thought: str, action: str, action_input: str, loop_num: int) -&gt; None:\n        \"\"\"\n        Logs reasoning step of agent.\n\n        Args:\n            thought (str): Reasoning about next step.\n            action (str): Chosen action.\n            action_input (str): Input to the tool chosen by action.\n            loop_num (int): Number of reasoning loop.\n        \"\"\"\n        logger.info(\n            \"\\n------------------------------------------\\n\"\n            f\"Agent {self.name}: Loop {loop_num}:\\n\"\n            f\"Thought: {thought}\\n\"\n            f\"Action: {action}\\n\"\n            f\"Action Input: {action_input}\"\n            \"\\n------------------------------------------\"\n        )\n\n    def log_final_output(self, thought: str, final_output: str, loop_num: int) -&gt; None:\n        \"\"\"\n        Logs final output of the agent.\n\n        Args:\n            final_output (str): Final output of agent.\n            loop_num (int): Number of reasoning loop\n        \"\"\"\n        logger.info(\n            \"\\n------------------------------------------\\n\"\n            f\"Agent {self.name}: Loop {loop_num}\\n\"\n            f\"Thought: {thought}\\n\"\n            f\"Final answer: {final_output}\"\n            \"\\n------------------------------------------\\n\"\n        )\n\n    def _should_delegate_final(\n        self,\n        tool: Node | None,\n        action_input: Any,\n    ) -&gt; bool:\n        \"\"\"Only Agent tools with per-call delegate_final flag can delegate.\"\"\"\n        if not self.delegation_allowed:\n            return False\n\n        if not isinstance(tool, Agent):\n            return False\n\n        if isinstance(action_input, str):\n            try:\n                action_input = json.loads(action_input)\n            except json.JSONDecodeError:\n                return False\n\n        if isinstance(action_input, Mapping):\n            return bool(action_input.get(\"delegate_final\"))\n\n        return False\n\n    @model_validator(mode=\"after\")\n    def validate_inference_mode(self):\n        \"\"\"Validate whether specified model can be inferenced in provided mode.\"\"\"\n        match self.inference_mode:\n            case InferenceMode.FUNCTION_CALLING:\n                if not supports_function_calling(model=self.llm.model):\n                    raise ValueError(f\"Model {self.llm.model} does not support function calling\")\n\n            case InferenceMode.STRUCTURED_OUTPUT:\n                params = get_supported_openai_params(model=self.llm.model)\n                if \"response_format\" not in params:\n                    raise ValueError(f\"Model {self.llm.model} does not support structured output\")\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def _ensure_context_manager_tool(self):\n        \"\"\"Automatically add ContextManagerTool when summarization is enabled.\"\"\"\n        try:\n            if self.summarization_config.enabled:\n                has_context_tool = any(isinstance(t, ContextManagerTool) for t in self.tools)\n                if not has_context_tool:\n                    context_tool = ContextManagerTool(\n                        llm=self.llm,\n                        name=\"context-manager\",\n                        token_budget_ratio=self.summarization_config.token_budget_ratio,\n                    )\n                    self.tools.append(context_tool)\n                    self._excluded_tool_ids.add(context_tool.id)\n        except Exception as e:\n            logger.error(f\"Failed to ensure ContextManagerTool: {e}\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def _ensure_todo_tools(self):\n        \"\"\"Automatically add TodoWriteTool when todo is enabled in file_store config.\"\"\"\n        try:\n            if self.file_store.enabled and self.file_store.todo_enabled:\n                has_todo_write = any(isinstance(t, TodoWriteTool) for t in self.tools)\n\n                if not has_todo_write:\n                    todo_tool = TodoWriteTool(\n                        name=\"todo-write\",\n                        file_store=self.file_store.backend,\n                    )\n                    self.tools.append(todo_tool)\n                    self._excluded_tool_ids.add(todo_tool.id)\n                    logger.info(\"Agent: Added TodoWriteTool\")\n        except Exception as e:\n            logger.error(f\"Failed to ensure TodoWriteTool: {e}\")\n        return self\n\n    def _append_recovery_instruction(\n        self,\n        *,\n        error_label: str,\n        error_detail: str,\n        llm_generated_output: str | None,\n        extra_guidance: str | None = None,\n    ) -&gt; None:\n        \"\"\"Append a correction instruction to prompt for recoverable agent errors.\"\"\"\n\n        error_context = llm_generated_output if llm_generated_output else \"No response generated\"\n\n        self._prompt.messages.append(\n            Message(role=MessageRole.ASSISTANT, content=f\"Previous response:\\n{error_context}\", static=True)\n        )\n\n        guidance_suffix = f\" {extra_guidance.strip()}\" if extra_guidance else \"\"\n\n        correction_message = (\n            \"Correction Instruction: The previous response could not be parsed due to the \"\n            f\"following error: '{error_label}: {error_detail}'. Please regenerate the response \"\n            \"strictly following the required format, ensuring all tags or labeled sections are \"\n            \"present and correctly structured, and that any JSON content is valid.\" + guidance_suffix\n        )\n\n        self._prompt.messages.append(\n            Message(role=MessageRole.USER, content=correction_message, static=True)\n        )\n\n    def _stream_agent_event(\n        self,\n        content: AgentReasoningEventMessageData | AgentToolResultEventMessageData,\n        step: str,\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Stream agent event if streaming is enabled.\n\n        Args:\n            content: The event data (reasoning or tool result).\n            step: Event type (\"reasoning\" or \"tool\").\n            config: Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if not (self.streaming.enabled and self.streaming.mode == StreamingMode.ALL):\n            return\n\n        source = content.tool.name if step == \"reasoning\" else content.name\n        self.stream_content(\n            content=content.model_dump(),\n            source=source,\n            step=step,\n            config=config,\n            **kwargs,\n        )\n\n    def _append_assistant_message(self, llm_result: Any, llm_generated_output: str) -&gt; None:\n        \"\"\"\n        Appends the assistant's message to conversation history based on inference mode.\n\n        Args:\n            llm_result: The full LLM result object (needed for function calling mode).\n            llm_generated_output: The generated text output from the LLM.\n        \"\"\"\n        if self.inference_mode == InferenceMode.FUNCTION_CALLING:\n            # For function calling, construct a message that includes the tool call\n            if \"tool_calls\" in dict[Any, Any](llm_result.output):\n                try:\n                    tool_call = list(llm_result.output[\"tool_calls\"].values())[0]\n                    function_name = tool_call[\"function\"][\"name\"]\n                    function_args = json.dumps(tool_call[\"function\"][\"arguments\"])\n                    message_content = f\"Function call: {function_name}({function_args})\"\n                    self._prompt.messages.append(\n                        Message(role=MessageRole.ASSISTANT, content=message_content, static=True)\n                    )\n                except Exception as e:\n                    logger.warning(f\"Failed to extract tool call from LLM result: {e}. Using raw output instead.\")\n                    self._prompt.messages.append(\n                        Message(\n                            role=MessageRole.ASSISTANT,\n                            content=llm_generated_output or \"Cannot extract tool call from LLM result.\",\n                            static=True,\n                        )\n                    )\n        elif llm_generated_output:\n            # For other modes, use the generated text output\n            self._prompt.messages.append(Message(role=MessageRole.ASSISTANT, content=llm_generated_output, static=True))\n\n    def _handle_default_mode(\n        self, llm_generated_output: str, loop_num: int\n    ) -&gt; tuple[str | None, str | None, dict | list | None]:\n        \"\"\"Handle DEFAULT inference mode parsing.\"\"\"\n        if not llm_generated_output or not llm_generated_output.strip():\n            self._append_recovery_instruction(\n                error_label=\"EmptyResponse\",\n                error_detail=\"The model returned an empty reply while using the Thought/Action format.\",\n                llm_generated_output=llm_generated_output,\n                extra_guidance=(\n                    \"Re-evaluate the latest observation and respond with 'Thought:' followed by either \"\n                    \"an 'Action:' plus JSON 'Action Input:' or a final 'Answer:' section.\"\n                ),\n            )\n            return None, None, None\n\n        if \"Answer:\" in llm_generated_output:\n            thought, final_answer, output_files_raw = parser.extract_default_final_answer(llm_generated_output)\n            self._requested_output_files = self._parse_output_files_csv(output_files_raw)\n            self.log_final_output(thought, final_answer, loop_num)\n            return thought, \"final_answer\", final_answer\n\n        thought, action, action_input = parser.parse_default_action(llm_generated_output)\n        self.log_reasoning(thought, action, action_input, loop_num)\n        return thought, action, action_input\n\n    def _handle_function_calling_mode(\n        self, llm_result: Any, loop_num: int\n    ) -&gt; tuple[str | None, str | None, dict | list | None] | tuple[str, str, str]:\n        \"\"\"Handle FUNCTION_CALLING inference mode parsing.\n\n        Returns:\n            tuple: (thought, action, action_input) for normal actions\n                   (thought, \"final_answer\", final_answer) for final answers\n        \"\"\"\n        if self.verbose:\n            logger.info(f\"Agent {self.name} - {self.id}: using function calling inference mode\")\n\n        if \"tool_calls\" not in dict(llm_result.output):\n            logger.error(\"Error: No function called.\")\n            raise ActionParsingException(\"Error: No function called, you need to call the correct function.\")\n\n        action = list(llm_result.output[\"tool_calls\"].values())[0][\"function\"][\"name\"].strip()\n        llm_generated_output_json = list(llm_result.output[\"tool_calls\"].values())[0][\"function\"][\"arguments\"]\n\n        thought = llm_generated_output_json[\"thought\"]\n        if action == \"provide_final_answer\":\n            final_answer = llm_generated_output_json[\"answer\"]\n            self._requested_output_files = self._parse_output_files_csv(\n                llm_generated_output_json.get(\"output_files\") or \"\"\n            )\n            self.log_final_output(thought, final_answer, loop_num)\n            return thought, \"final_answer\", final_answer\n\n        action_input = llm_generated_output_json[\"action_input\"]\n\n        if isinstance(action_input, str):\n            try:\n                action_input = json.loads(action_input)\n            except json.JSONDecodeError as e:\n                raise ActionParsingException(f\"Error parsing action_input string. {e}\", recoverable=True)\n\n        self.log_reasoning(thought, action, action_input, loop_num)\n        return thought, action, action_input\n\n    def _handle_structured_output_mode(\n        self, llm_generated_output: str | dict, loop_num: int\n    ) -&gt; tuple[str | None, str | None, dict | list | None] | tuple[str, str, str]:\n        \"\"\"Handle STRUCTURED_OUTPUT inference mode parsing.\n\n        Returns:\n            tuple: (thought, action, action_input) for normal actions\n                   (thought, \"final_answer\", final_answer) for final answers\n        \"\"\"\n        if self.verbose:\n            logger.info(f\"Agent {self.name} - {self.id}: using structured output inference mode\")\n\n        try:\n            if isinstance(llm_generated_output, str):\n                llm_generated_output_json = json.loads(llm_generated_output)\n            else:\n                llm_generated_output_json = llm_generated_output\n        except json.JSONDecodeError as e:\n            raise ActionParsingException(f\"Error parsing action. {e}\", recoverable=True)\n\n        if \"action\" not in llm_generated_output_json or \"thought\" not in llm_generated_output_json:\n            raise ActionParsingException(\"No action or thought provided.\", recoverable=True)\n\n        thought = llm_generated_output_json[\"thought\"]\n        action = llm_generated_output_json[\"action\"]\n        action_input = llm_generated_output_json[\"action_input\"]\n\n        if action == \"finish\":\n            self._requested_output_files = self._parse_output_files_csv(\n                llm_generated_output_json.get(\"output_files\") or \"\"\n            )\n            self.log_final_output(thought, action_input, loop_num)\n            return thought, \"final_answer\", action_input\n\n        try:\n            if isinstance(action_input, str):\n                action_input = json.loads(action_input)\n        except json.JSONDecodeError as e:\n            raise ActionParsingException(f\"Error parsing action_input string. {e}\", recoverable=True)\n\n        self.log_reasoning(thought, action, action_input, loop_num)\n        return thought, action, action_input\n\n    def _handle_xml_mode(\n        self, llm_generated_output: str, loop_num: int, config: RunnableConfig, **kwargs\n    ) -&gt; tuple[str | None, str | None, dict | list | None]:\n        \"\"\"Handle XML inference mode parsing.\"\"\"\n        if self.verbose:\n            logger.info(f\"Agent {self.name} - {self.id}: using XML inference mode\")\n\n        if not llm_generated_output or not llm_generated_output.strip():\n            self._append_recovery_instruction(\n                error_label=\"EmptyResponse\",\n                error_detail=\"The model returned an empty reply while XML format was required.\",\n                llm_generated_output=llm_generated_output,\n                extra_guidance=(\n                    \"Respond with &lt;thought&gt;...&lt;/thought&gt; and \"\n                    \"either &lt;action&gt;/&lt;action_input&gt; or &lt;answer&gt; tags, \"\n                    \"making sure to address the latest observation.\"\n                ),\n            )\n            return None, None, None\n\n        try:\n            parsed_data = XMLParser.parse(\n                llm_generated_output,\n                required_tags=[\"thought\", \"answer\"],\n                optional_tags=[\"output\", \"output_files\"],\n            )\n            thought = parsed_data.get(\"thought\")\n            final_answer = parsed_data.get(\"answer\")\n\n            self._requested_output_files = self._parse_output_files_csv(\n                parsed_data.get(\"output_files\") or \"\"\n            )\n\n            self.log_final_output(thought, final_answer, loop_num)\n            return thought, \"final_answer\", final_answer\n\n        except TagNotFoundError:\n            logger.debug(\"XMLParser: Not a final answer structure, trying action structure.\")\n            try:\n                parsed_data = XMLParser.parse(\n                    llm_generated_output,\n                    required_tags=[\"thought\", \"action\", \"action_input\"],\n                    optional_tags=[\"output\"],\n                    json_fields=[\"action_input\"],\n                )\n                thought = parsed_data.get(\"thought\")\n                action = parsed_data.get(\"action\")\n                action_input = parsed_data.get(\"action_input\")\n                self.log_reasoning(thought, action, action_input, loop_num)\n                return thought, action, action_input\n            except JSONParsingError as e:\n                logger.error(f\"XMLParser: Invalid JSON in action_input: {e}\")\n                raise ActionParsingException(\n                    \"The &lt;action_input&gt; value must be valid JSON. Put the whole JSON on one line; \"\n                    'use \\\\n for newlines inside strings and \\\\\" for quotes. '\n                    \"Provide &lt;thought&gt; with &lt;action&gt; and &lt;action_input&gt; again.\",\n                    recoverable=True,\n                )\n            except ParsingError as e:\n                logger.error(f\"XMLParser: Empty or invalid XML response for action parsing: {e}\")\n                raise ActionParsingException(\n                    \"The previous response was empty or invalid. \"\n                    \"Provide &lt;thought&gt; with either &lt;action&gt;/&lt;action_input&gt; or &lt;answer&gt;.\",\n                    recoverable=True,\n                )\n\n        except ParsingError as e:\n            logger.error(f\"XMLParser: Empty or invalid XML response: {e}\")\n            raise ActionParsingException(\n                \"The previous response was empty or invalid. \" \"Please provide the required XML tags.\",\n                recoverable=True,\n            )\n\n    def _setup_prompt_and_stop_sequences(\n        self,\n        input_message: Message | VisionMessage,\n        history_messages: list[Message] | None = None,\n    ) -&gt; None:\n        \"\"\"Setup the prompt with system message, history, and configure stop sequences.\n\n        Args:\n            input_message: The user's input message\n            history_messages: Optional conversation history\n        \"\"\"\n        system_message = Message(\n            role=MessageRole.SYSTEM,\n            content=self.generate_prompt(\n                tools_name=self.tool_names,\n                input_formats=schema_generator.generate_input_formats(self.tools, self.sanitize_tool_name),\n            ),\n            static=True,\n        )\n\n        if history_messages:\n            self._prompt.messages = [system_message, *history_messages, input_message]\n        else:\n            self._prompt.messages = [system_message, input_message]\n\n        self._history_offset = len(self._prompt.messages)\n\n        # Configure stop sequences based on inference mode\n        stop_sequences = []\n        if self.inference_mode == InferenceMode.DEFAULT:\n            stop_sequences.extend([\"Observation: \", \"\\nObservation:\"])\n        elif self.inference_mode == InferenceMode.XML:\n            stop_sequences.extend(\n                [\n                    \"\\nObservation:\",\n                    \"Observation:\",\n                    \"&lt;/output&gt;\\n&lt;\",\n                    \"&lt;/output&gt;&lt;\",\n                ]\n            )\n        self.llm.stop = stop_sequences\n\n    def _setup_streaming_callback(\n        self, config: RunnableConfig, loop_num: int, **kwargs\n    ) -&gt; tuple[AgentStreamingParserCallback | None, RunnableConfig, bool]:\n        \"\"\"Setup streaming callback and modify LLM config if agent streaming is enabled.\n\n        Args:\n            config: The runnable configuration\n            loop_num: Current loop iteration number\n            **kwargs: Additional parameters\n\n        Returns:\n            tuple: (streaming_callback, modified_config, original_streaming_enabled)\n        \"\"\"\n        streaming_callback = None\n        original_streaming_enabled = self.llm.streaming.enabled\n\n        if self.streaming.enabled:\n            streaming_callback = AgentStreamingParserCallback(\n                agent=self,\n                config=config,\n                loop_num=loop_num,\n                **kwargs,\n            )\n\n            if not original_streaming_enabled:\n                self.llm.streaming.enabled = True\n\n            llm_config = config.model_copy(deep=False)\n            llm_config.callbacks = [\n                callback for callback in llm_config.callbacks if not isinstance(callback, StreamingQueueCallbackHandler)\n            ]\n            llm_config.callbacks.append(streaming_callback)\n        else:\n            llm_config = config\n\n        return streaming_callback, llm_config, original_streaming_enabled\n\n    def _execute_single_tool(\n        self,\n        action: str,\n        action_input: Any,\n        thought: str,\n        loop_num: int,\n        config: RunnableConfig,\n        update_run_depends: bool = True,\n        collect_dependency: bool = False,\n        is_parallel: bool = False,\n        **kwargs,\n    ) -&gt; tuple[Any, list, bool, bool, dict | None]:\n        \"\"\"Execute a single tool with caching support.\n\n        Args:\n            update_run_depends: Whether to update self._run_depends. Set to False for parallel execution.\n            collect_dependency: Whether to collect and return the dependency dict.\n            is_parallel: Whether this tool is being executed in parallel with other tools.\n                When True, the tool will be cloned for thread-safe execution.\n\n        Returns:\n            tuple: (tool_result, tool_files, is_delegated, success, dependency)\n        \"\"\"\n        tool = self.tool_by_names.get(self.sanitize_tool_name(action))\n\n        if not tool:\n            error_message = (\n                f\"Unknown tool: {action}. Use only available tools and provide only the tool's name in the \"\n                \"action field. Do not include any additional reasoning. \"\n                \"Please correct the action field or state that you cannot answer the question.\"\n            )\n            return error_message, [], False, False, None\n\n        tool_run_id = generate_uuid()\n        tool_data = AgentToolData(\n            name=tool.name,\n            type=tool.type,\n            action_type=tool.action_type.value if tool.action_type else None,\n        )\n\n        self._stream_agent_event(\n            AgentReasoningEventMessageData(\n                tool_run_id=tool_run_id,\n                thought=thought or \"\",\n                action=action,\n                tool=tool_data,\n                action_input=action_input,\n                loop_num=loop_num,\n            ),\n            \"reasoning\",\n            config,\n            **kwargs,\n        )\n\n        try:\n            if isinstance(tool, ContextManagerTool):\n                tool_result = None\n                to_summarize, _ = self._split_history()\n                tool_input = {**(action_input if isinstance(action_input, dict) else {}), \"messages\": to_summarize}\n            else:\n                tool_cache_entry = ToolCacheEntry(action=action, action_input=action_input)\n                tool_result = self._tool_cache.get(tool_cache_entry, None)\n                tool_input = action_input\n\n            delegate_final = self._should_delegate_final(tool, action_input)\n\n            dependency: dict | None = None\n            if not tool_result:\n                tool_kwargs = kwargs.copy()\n\n                run_tool_result = self._run_tool(\n                    tool,\n                    tool_input,\n                    config,\n                    delegate_final=delegate_final,\n                    update_run_depends=update_run_depends,\n                    collect_dependency=collect_dependency,\n                    is_parallel=is_parallel,\n                    **tool_kwargs,\n                )\n                if collect_dependency:\n                    tool_result, tool_files, tool_output_meta, dependency = run_tool_result\n                else:\n                    tool_result, tool_files, tool_output_meta = run_tool_result\n\n            else:\n                logger.info(f\"Agent {self.name} - {self.id}: Cached output of {action} found.\")\n                tool_result, tool_output_meta = tool_result\n                tool_files = []\n\n            if delegate_final:\n                self.log_final_output(thought, tool_result, loop_num)\n                # Stream tool result (with files) before streaming final answer\n                self._stream_agent_event(\n                    AgentToolResultEventMessageData(\n                        tool_run_id=tool_run_id,\n                        name=tool.name,\n                        tool=tool_data,\n                        input=action_input,\n                        result=tool_result,\n                        files=tool_files,\n                        loop_num=loop_num,\n                        output=tool_output_meta,\n                    ),\n                    \"tool\",\n                    config,\n                    **kwargs,\n                )\n                if self.streaming.enabled:\n                    self.stream_content(\n                        content=tool_result,\n                        source=tool.name,\n                        step=\"answer\",\n                        config=config,\n                        **kwargs,\n                    )\n                return tool_result, tool_files, True, True, dependency\n\n            if isinstance(tool, ContextManagerTool):\n                self._compact_history(summary=tool_output_meta.get(\"summary\", tool_result))\n\n            # Stream the result\n            self._stream_agent_event(\n                AgentToolResultEventMessageData(\n                    tool_run_id=tool_run_id,\n                    name=tool.name,\n                    tool=tool_data,\n                    input=action_input,\n                    result=tool_result,\n                    files=tool_files,\n                    loop_num=loop_num,\n                    output=tool_output_meta,\n                ),\n                \"tool\",\n                config,\n                **kwargs,\n            )\n\n            return tool_result, tool_files, False, True, dependency\n\n        except RecoverableAgentException as e:\n            # Stream error result with the same tool_run_id used for reasoning\n            error_message = f\"{type(e).__name__}: {e}\"\n            self._stream_agent_event(\n                AgentToolResultEventMessageData(\n                    tool_run_id=tool_run_id,\n                    name=tool.name,\n                    tool=tool_data,\n                    input=action_input,\n                    result=error_message,\n                    files=[],\n                    loop_num=loop_num,\n                    output={},\n                ),\n                \"tool\",\n                config,\n                **kwargs,\n            )\n            return error_message, [], False, False, None\n\n    def _add_observation(self, tool_result: Any) -&gt; None:\n        \"\"\"Add observation to prompt.\n\n        Args:\n            tool_result: The result from the tool execution.\n        \"\"\"\n        observation = f\"\\nObservation: {tool_result}\\n\"\n        self._prompt.messages.append(Message(role=MessageRole.USER, content=observation, static=True))\n\n    def _validate_parallel_tool_input(self, action_input: Any) -&gt; list[dict[str, Any]] | None:\n        \"\"\"Validate and parse parallel tool input schema.\n\n        If validation fails, logs error and adds observation for agent recovery.\n\n        Args:\n            action_input: Raw input from LLM for the parallel tool.\n\n        Returns:\n            list: Validated tools list, or None if validation failed\n        \"\"\"\n        try:\n            validated = ParallelToolCallsInputSchema.model_validate(action_input).model_dump()\n            return validated[\"tools\"]\n        except Exception as e:\n            error_message = f\"Invalid parallel tool input: {e}\"\n            logger.error(error_message)\n            self._add_observation(error_message)\n            return None\n\n    def _should_skip_parallel_mode(\n        self, action: str | None, action_input: Any\n    ) -&gt; tuple[bool, str | None, Any, list[str]]:\n        \"\"\"Check if parallel mode should be skipped for ContextManagerTool.\n\n        When ContextManagerTool is detected in a parallel tool list, we filter\n        to only execute that tool to ensure safe context modification.\n\n        Args:\n            action: The action to execute\n            action_input: The action input (list for parallel mode)\n\n        Returns:\n            tuple: (skip_parallel, action_name, action_input, skipped_tools)\n                - skip_parallel: True if parallel mode should be skipped\n                - action_name: The tool name to execute\n                - action_input: The tool input to use\n                - skipped_tools: List of tool names that were skipped\n        \"\"\"\n        # Get ContextManagerTool names\n        context_manager_names = {\n            self.sanitize_tool_name(t.name) for t in self.tools if isinstance(t, ContextManagerTool)\n        }\n\n        # Check if ContextManagerTool is in a parallel tool list\n        if isinstance(action_input, list):\n            for tool_data in action_input:\n                if isinstance(tool_data, dict):\n                    tool_name = self.sanitize_tool_name(tool_data.get(\"name\", \"\"))\n                    if tool_name in context_manager_names:\n                        # Collect names of other tools that will be skipped\n                        skipped_tools = [\n                            td.get(\"name\", \"unknown\")\n                            for td in action_input\n                            if isinstance(td, dict)\n                            and self.sanitize_tool_name(td.get(\"name\", \"\")) not in context_manager_names\n                        ]\n                        logger.info(\n                            f\"Agent {self.name} - {self.id}: ContextManagerTool detected in parallel call. \"\n                            f\"Filtering to execute only ContextManagerTool. Skipped tools: {skipped_tools}\"\n                        )\n                        return True, tool_data.get(\"name\", \"\"), tool_data.get(\"input\", {}), skipped_tools\n        elif isinstance(action, str) and self.sanitize_tool_name(action) in context_manager_names:\n            # Single tool mode - ContextManagerTool detected, no tools skipped\n            return True, action, action_input, []\n\n        return False, action, action_input, []\n\n    def _execute_tools_and_update_prompt(\n        self,\n        action: str | None,\n        action_input: Any,\n        thought: str | None,\n        loop_num: int,\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; str | None:\n        \"\"\"Execute tools based on action and update prompt with observations.\n\n        Args:\n            action: The action/tool name to execute\n            action_input: Input parameters for the tool\n            thought: The agent's reasoning\n            loop_num: Current loop iteration number\n            config: Runnable configuration\n            **kwargs: Additional parameters\n\n        Returns:\n            str | None: Final answer if delegation occurred, None to continue loop\n        \"\"\"\n        if action and self.tools:\n            tool_result = None\n            skipped_tools: list[str] = []\n\n            if self.sanitize_tool_name(action) == PARALLEL_TOOL_NAME:\n                action_input = self._validate_parallel_tool_input(action_input)\n                if action_input is None:\n                    return None\n\n            # Check if ContextManagerTool is in the action - if so, skip parallel mode\n            skip_parallel, action, action_input, skipped_tools = self._should_skip_parallel_mode(action, action_input)\n\n            # Handle XML parallel mode (only for multiple tools, not for ContextManagerTool)\n            tools_data = action_input if isinstance(action_input, list) else [action_input]\n            if (\n                self.sanitize_tool_name(action) == PARALLEL_TOOL_NAME\n                and self.parallel_tool_calls_enabled\n                and not skip_parallel\n            ):\n                tool_result, _ = self._execute_tools(tools_data, thought, loop_num, config, **kwargs)\n            else:\n                tool_result, _, is_delegated, _, _ = self._execute_single_tool(\n                    action, action_input, thought, loop_num, config, **kwargs\n                )\n                if is_delegated:\n                    return tool_result\n\n            if skipped_tools:\n                skipped_notice = (\n                    f\"\\n\\n[Note: The following tools were NOT executed because context-manager \"\n                    f\"must run alone: {', '.join(skipped_tools)}. Please call them separately after \"\n                    f\"context compression completes.]\"\n                )\n                tool_result = f\"{tool_result}{skipped_notice}\" if tool_result else skipped_notice\n\n            self._add_observation(tool_result)\n\n        # else: No action or no tools available - no reasoning to stream\n\n        return None\n\n    def _inject_state_into_messages(self, messages: list[Message | VisionMessage]) -&gt; list[Message | VisionMessage]:\n        \"\"\"\n        Create a copy of messages with state injected into the last user message.\n\n        Original messages are not modified. Handles both Message and VisionMessage types.\n        \"\"\"\n        state_info = self.state.to_prompt_string()\n        if not state_info or not messages:\n            return messages\n\n        last_msg = messages[-1]\n        if last_msg.role != MessageRole.USER:\n            return messages\n\n        state_suffix = f\"\\n\\n[State: {state_info}]\"\n\n        if isinstance(last_msg, VisionMessage):\n            new_content = list(last_msg.content) + [VisionMessageTextContent(text=state_suffix)]\n            return messages[:-1] + [\n                VisionMessage(\n                    role=last_msg.role,\n                    content=new_content,\n                    static=last_msg.static,\n                )\n            ]\n\n        return messages[:-1] + [\n            Message(\n                role=last_msg.role,\n                content=f\"{last_msg.content}{state_suffix}\",\n                metadata=last_msg.metadata,\n                static=last_msg.static,\n            )\n        ]\n\n    def _run_agent(\n        self,\n        input_message: Message | VisionMessage,\n        history_messages: list[Message] | None = None,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Executes the ReAct strategy by iterating through thought, action, and observation cycles.\n        Args:\n            config (RunnableConfig | None): Configuration for the agent run.\n            **kwargs: Additional parameters for running the agent.\n        Returns:\n            str: Final answer provided by the agent.\n        Raises:\n            RuntimeError: If the maximum number of loops is reached without finding a final answer.\n            Exception: If an error occurs during execution.\n        \"\"\"\n        if self.verbose:\n            logger.info(f\"Agent {self.name} - {self.id}: Running ReAct strategy\")\n\n        self.state.max_loops = self.max_loops\n        self._requested_output_files = []\n        self._refresh_agent_state(1)\n\n        self._setup_prompt_and_stop_sequences(input_message, history_messages)\n\n        for loop_num in range(1, self.max_loops + 1):\n            if loop_num &gt; 1:\n                self._refresh_agent_state(loop_num)\n\n            try:\n                streaming_callback, llm_config, original_streaming_enabled = self._setup_streaming_callback(\n                    config, loop_num, **kwargs\n                )\n\n                # Append state to the last user message before LLM call\n                messages = self._inject_state_into_messages(self._prompt.messages)\n\n                try:\n                    llm_result = self._run_llm(\n                        messages=messages,\n                        tools=self._tools,\n                        response_format=self._response_format,\n                        config=llm_config,\n                        **kwargs,\n                    )\n                finally:\n                    if not original_streaming_enabled:\n                        try:\n                            self.llm.streaming.enabled = original_streaming_enabled\n                        except Exception:\n                            logger.error(\"Failed to restore llm.streaming.enabled state\")\n\n                action, action_input = None, None\n                llm_generated_output = \"\"\n\n                if streaming_callback and streaming_callback.accumulated_content:\n                    llm_generated_output = streaming_callback.accumulated_content\n                else:\n                    llm_generated_output = llm_result.output.get(\"content\", \"\")\n\n                llm_reasoning = (\n                    llm_generated_output[:200]\n                    if llm_generated_output\n                    else str(llm_result.output.get(\"tool_calls\", \"\"))[:200]\n                )\n                logger.info(f\"Agent {self.name} - {self.id}: Loop {loop_num}, \" f\"reasoning:\\n{llm_reasoning}...\")\n\n                # Append assistant message to conversation history BEFORE parsing\n                # This ensures the LLM can see its own output during error recovery\n                self._append_assistant_message(llm_result, llm_generated_output)\n\n                # Parse LLM output based on inference mode\n                match self.inference_mode:\n                    case InferenceMode.DEFAULT:\n                        result = self._handle_default_mode(llm_generated_output, loop_num)\n                    case InferenceMode.FUNCTION_CALLING:\n                        result = self._handle_function_calling_mode(llm_result, loop_num)\n                    case InferenceMode.STRUCTURED_OUTPUT:\n                        result = self._handle_structured_output_mode(llm_generated_output, loop_num)\n                    case InferenceMode.XML:\n                        result = self._handle_xml_mode(llm_generated_output, loop_num, config, **kwargs)\n\n                # Handle final answer\n                if result[1] == \"final_answer\":\n                    self._resolve_requested_output_files(strict=True)\n                    return result[2]\n\n                # Handle recovery (for modes that support it)\n                # Check if action is None, which indicates (None, None, None) recovery\n                if result[1] is None:\n                    continue\n\n                thought, action, action_input = result\n\n                final_answer = self._execute_tools_and_update_prompt(\n                    action, action_input, thought, loop_num, config, **kwargs\n                )\n\n                if final_answer is not None:\n                    return final_answer\n\n            except OutputFileNotFoundError as e:\n                self._requested_output_files = []\n                self._append_recovery_instruction(\n                    error_label=type(e).__name__,\n                    error_detail=str(e),\n                    llm_generated_output=llm_generated_output,\n                    extra_guidance=(\n                        \"The response format is correct, but some files could not be found. \"\n                        \"Please create the missing files or correct the file paths, \"\n                        \"then provide your final answer again.\"\n                    ),\n                )\n                continue\n\n            except ActionParsingException as e:\n                extra_guidance = None\n                if self.inference_mode == InferenceMode.XML:\n                    extra_guidance = (\n                        \"Ensure the reply contains &lt;thought&gt; along \"\n                        \"with either &lt;action&gt;/&lt;action_input&gt; or a final \"\n                        \"&lt;answer&gt; tag.\"\n                    )\n                elif self.inference_mode == InferenceMode.DEFAULT:\n                    extra_guidance = (\n                        \"Provide 'Thought:' and either 'Action:' \"\n                        \"with a JSON 'Action Input:' or a final 'Answer:' section.\"\n                    )\n\n                self._append_recovery_instruction(\n                    error_label=type(e).__name__,\n                    error_detail=str(e),\n                    llm_generated_output=llm_generated_output,\n                    extra_guidance=extra_guidance,\n                )\n                continue\n            except Exception as e:\n                logger.error(f\"Agent {self.name} - {self.id}: Error during agent execution: {e}\")\n                raise e\n\n            # Inject automatic summarization if token limit exceeded (like Context Manager Tool)\n            self._try_summarize_history(config=config, **kwargs)\n\n        if self.behaviour_on_max_loops == Behavior.RAISE:\n            error_message = (\n                f\"Agent {self.name} (ID: {self.id}) \"\n                f\"has reached the maximum loop limit of {self.max_loops} \"\n                f\"without finding a final answer. \"\n                f\"Last response: {self._prompt.messages[-1].content}\\n\"\n                f\"Consider increasing the maximum number of loops or \"\n                f\"reviewing the task complexity to ensure completion.\"\n            )\n            raise MaxLoopsExceededException(message=error_message)\n        else:\n            max_loop_final_answer = self._handle_max_loops_exceeded(input_message, config, **kwargs)\n            self._resolve_requested_output_files(strict=False)\n            if self.streaming.enabled:\n                self.stream_content(\n                    content=max_loop_final_answer,\n                    source=self.name,\n                    step=\"answer\",\n                    config=config,\n                    **kwargs,\n                )\n            return max_loop_final_answer\n\n    def _try_summarize_history(\n        self,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Check if summarization is needed and inject it automatically if token limit is exceeded.\n\n        Works like an automatic Context Manager Tool invocation.\n\n        Args:\n            config: Configuration for the agent run\n            **kwargs: Additional parameters for running the agent\n        \"\"\"\n        if not self.summarization_config.enabled:\n            return\n\n        if self.is_token_limit_exceeded():\n            logger.info(\n                f\"Agent {self.name} - {self.id}: Token limit exceeded. Automatically invoking Context Manager Tool.\"\n            )\n\n            context_tool = next((t for t in self.tools if isinstance(t, ContextManagerTool)), None)\n\n            if context_tool is None:\n                logger.error(f\"Agent {self.name} - {self.id}: Context Manager Tool not found.\")\n                return\n\n            action = self.sanitize_tool_name(context_tool.name)\n\n            self._execute_tools_and_update_prompt(\n                action=action,\n                action_input={},\n                thought=None,\n                loop_num=0,  # Use 0 for automatic invocation\n                config=config,\n                **kwargs,\n            )\n\n    @staticmethod\n    def _parse_output_files_csv(raw: str) -&gt; list[str]:\n        \"\"\"Parse a comma-separated string of file paths into a list.\n\n        Strips whitespace from each entry and drops empty entries.\n        \"\"\"\n        if not raw or not raw.strip():\n            return []\n        return [p.strip() for p in raw.split(\",\") if p.strip()]\n\n    def _resolve_requested_output_files(self, *, strict: bool = True) -&gt; None:\n        \"\"\"Resolve ``_requested_output_files`` against the file backend.\n\n        Each requested path is checked as-is first, then by basename.\n        When *strict* is ``True`` (inside the normal loop), missing files\n        raise :class:`OutputFileNotFoundError` so the agent can retry.\n        When *strict* is ``False`` (max-loops path), missing files are\n        silently dropped because there are no retries left.\n        \"\"\"\n        if not self._requested_output_files:\n            return\n\n        file_backend = self.sandbox_backend or self.file_store_backend\n        if not file_backend:\n            return\n\n        resolved: list[str] = []\n        file_not_found: list[str] = []\n        for f in self._requested_output_files:\n            basename = f.rsplit(\"/\", 1)[-1]\n            if file_backend.exists(f):\n                resolved.append(f)\n            elif f != basename and file_backend.exists(basename):\n                resolved.append(basename)\n            else:\n                file_not_found.append(f)\n\n        if file_not_found and strict:\n            raise OutputFileNotFoundError(f\"File not found: {file_not_found}.\", recoverable=True)\n\n        if file_not_found and not strict:\n            logger.warning(\n                f\"Agent {self.name} - {self.id}: \" f\"max-loops output_files not found (skipped): {file_not_found}\"\n            )\n\n        self._requested_output_files = resolved\n\n    def _handle_max_loops_exceeded(\n        self, input_message: Message | VisionMessage, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; str:\n        \"\"\"\n        Handle the case where max loops are exceeded by crafting a thoughtful response.\n        Uses XMLParser to extract the final answer from the LLM's last attempt.\n\n        Args:\n            input_message (Message | VisionMessage): Initial user message.\n            config (RunnableConfig | None): Configuration for the agent run.\n            **kwargs: Additional parameters for running the agent.\n\n        Returns:\n            str: Final answer provided by the agent.\n        \"\"\"\n        # Use model-specific max loops prompt from prompt manager\n        max_loops_prompt = self.system_prompt_manager.max_loops_prompt\n\n        system_message = Message(content=max_loops_prompt, role=MessageRole.SYSTEM, static=True)\n        conversation_history = Message(\n            content=self.aggregate_history(self._prompt.messages), role=MessageRole.USER, static=True\n        )\n        llm_final_attempt_result = self._run_llm(\n            [system_message, input_message, conversation_history], config=config, **kwargs\n        )\n        llm_final_attempt = llm_final_attempt_result.output[\"content\"]\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n        try:\n            final_answer = XMLParser.extract_first_tag_lxml(llm_final_attempt, [\"answer\"])\n            if final_answer is None:\n                logger.warning(\"Max loops handler: lxml failed to extract &lt;answer&gt;, falling back to regex.\")\n                final_answer = XMLParser.extract_first_tag_regex(llm_final_attempt, [\"answer\"])\n\n            if final_answer is None:\n                logger.error(\n                    \"Max loops handler: Failed to extract &lt;answer&gt; tag even with fallbacks. Returning raw output.\"\n                )\n                final_answer = llm_final_attempt\n\n            raw_output_files = XMLParser.extract_first_tag_lxml(llm_final_attempt, [\"output_files\"])\n            if raw_output_files is None:\n                raw_output_files = XMLParser.extract_first_tag_regex(llm_final_attempt, [\"output_files\"])\n            self._requested_output_files = self._parse_output_files_csv(raw_output_files or \"\")\n\n        except Exception as e:\n            logger.error(f\"Max loops handler: Error during final answer extraction: {e}. Returning raw output.\")\n            final_answer = llm_final_attempt\n            self._requested_output_files = []\n\n        return f\"{final_answer}\"\n\n    def _refresh_agent_state(self, loop_num: int) -&gt; None:\n        \"\"\"\n        Refresh the agent state with current values.\n\n        Args:\n            loop_num: Current loop iteration number.\n        \"\"\"\n        self.state.update_loop(loop_num)\n        todo_backend = None\n        if self.sandbox_backend:\n            todo_backend = self.sandbox_backend\n        elif self.file_store.enabled and self.file_store.todo_enabled:\n            todo_backend = self.file_store.backend\n\n        if todo_backend:\n            try:\n                from dynamiq.nodes.tools.todo_tools import TODOS_FILE_PATH\n\n                if todo_backend.exists(TODOS_FILE_PATH):\n                    content = todo_backend.retrieve(TODOS_FILE_PATH)\n                    data = json.loads(content.decode(\"utf-8\"))\n                    self.state.update_todos(data.get(\"todos\", []))\n            except Exception as e:\n                logger.debug(\"Failed to load todo state (none or invalid): %s\", e)\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initialize the prompt blocks required for the ReAct strategy.\"\"\"\n        super()._init_prompt_blocks()\n        # Delegation guidance is rendered via prompt variables managed by AgentPromptManager\n\n        # Handle function calling schema generation first\n        if self.inference_mode == InferenceMode.FUNCTION_CALLING:\n            self._tools = schema_generator.generate_function_calling_schemas(\n                self.tools, self.delegation_allowed, self.sanitize_tool_name, self.llm\n            )\n        elif self.inference_mode == InferenceMode.STRUCTURED_OUTPUT:\n            self._response_format = schema_generator.generate_structured_output_schemas(\n                self.tools, self.sanitize_tool_name, self.delegation_allowed\n            )\n\n        # Setup ReAct-specific prompts via prompt manager.\n        has_tools = bool(self.tools) or (self.skills.enabled and self.skills.source is not None)\n        self.system_prompt_manager.setup_for_react_agent(\n            inference_mode=self.inference_mode,\n            parallel_tool_calls_enabled=self.parallel_tool_calls_enabled,\n            has_tools=has_tools,\n            delegation_allowed=self.delegation_allowed,\n            context_compaction_enabled=self.summarization_config.enabled,\n            todo_management_enabled=(self.file_store.enabled and self.file_store.todo_enabled)\n            or bool(self.sandbox_backend),\n            sandbox_base_path=self.sandbox_backend.base_path if self.sandbox_backend else None,\n        )\n\n        # Only auto-wrap the entire role in a raw block if the user did not\n        # provide explicit raw/endraw markers. This allows roles to mix\n        # literal sections (via raw) with Jinja variables like {{ input }}\n        # without creating nested raw blocks.\n        if self.role:\n            if (\"{% raw %}\" in self.role) or (\"{% endraw %}\" in self.role):\n                self.system_prompt_manager.set_block(\"role\", self.role)\n            else:\n                self.system_prompt_manager.set_block(\"role\", f\"{{% raw %}}{self.role}{{% endraw %}}\")\n\n    @staticmethod\n    def _build_unique_file_key(files_map: dict[str, Any], base: str) -&gt; str:\n        key = base or \"file\"\n        if key not in files_map:\n            return key\n        suffix = 1\n        while f\"{key}_{suffix}\" in files_map:\n            suffix += 1\n        return f\"{key}_{suffix}\"\n\n    def _merge_tool_files(self, aggregated: dict[str, Any], tool_name: str, files: Any) -&gt; None:\n        if not files:\n            return\n\n        sanitized_name = self.sanitize_tool_name(tool_name) or \"tool\"\n\n        if isinstance(files, dict):\n            for key, value in files.items():\n                base_key = key or sanitized_name\n                unique_key = self._build_unique_file_key(aggregated, base_key)\n                aggregated[unique_key] = value\n        elif isinstance(files, (list, tuple)):\n            for idx, file_obj in enumerate(files):\n                base_key = getattr(file_obj, \"name\", None) or f\"{sanitized_name}_{idx}\"\n                unique_key = self._build_unique_file_key(aggregated, base_key)\n                aggregated[unique_key] = file_obj\n        else:\n            unique_key = self._build_unique_file_key(aggregated, sanitized_name)\n            aggregated[unique_key] = files\n\n    def _is_tool_parallel_eligible(self, tool_name: str) -&gt; bool:\n        \"\"\"Check if a tool is eligible for parallel execution based on its is_parallel_execution_allowed flag.\"\"\"\n        tool = self.tool_by_names.get(self.sanitize_tool_name(tool_name))\n        return tool.is_parallel_execution_allowed if tool else False\n\n    def _execute_tools(\n        self,\n        tools_data: list[dict[str, Any]],\n        thought: str | None,\n        loop_num: int,\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; tuple[str, dict[str, Any]]:\n        \"\"\"\n        Execute one or more tools and gather their results.\n\n        Tools are split into two groups based on their ``is_parallel_execution_allowed`` flag:\n        parallel-eligible tools run concurrently first, then sequential-only\n        tools run one-by-one.\n\n        Args:\n            tools_data (list): List of dictionaries containing name and input for each tool\n            thought: The agent's reasoning\n            loop_num: Current loop iteration number\n            config (RunnableConfig): Configuration for the runnable\n            **kwargs: Additional arguments for tool execution\n\n        Returns:\n            tuple: (combined_observation, aggregated_files)\n        \"\"\"\n        all_results: list[dict[str, Any]] = []\n\n        if not tools_data:\n            return \"\", {}\n\n        prepared_tools: list[dict[str, Any]] = []\n\n        for idx, td in enumerate(tools_data):\n            tool_name = td.get(\"name\")\n            tool_input = td.get(\"input\")\n            if tool_name is None or tool_input is None:\n                error_message = \"Invalid tool payload: missing 'name' or 'input'\"\n                logger.error(error_message)\n                all_results.append(\n                    {\n                        \"order\": idx,\n                        \"tool_name\": tool_name or UNKNOWN_TOOL_NAME,\n                        \"success\": False,\n                        \"result\": error_message,\n                        \"files\": [],\n                    }\n                )\n                continue\n            prepared_tools.append({\"order\": idx, \"name\": tool_name, \"input\": tool_input})\n\n        def _execute_single_tool_to_result(tool_payload: dict[str, Any], **extra) -&gt; dict[str, Any]:\n            \"\"\"Execute a single tool and wrap the result as a dict.\"\"\"\n            tool_result, tool_files, _, success, dependency = self._execute_single_tool(\n                tool_payload[\"name\"],\n                tool_payload[\"input\"],\n                thought or \"\",\n                loop_num,\n                config,\n                collect_dependency=True,\n                **extra,\n                **kwargs,\n            )\n            return {\n                \"order\": tool_payload[\"order\"],\n                \"tool_name\": tool_payload[\"name\"],\n                \"success\": success,\n                \"result\": tool_result,\n                \"files\": tool_files,\n                \"dependency\": dependency,\n            }\n\n        if prepared_tools:\n            if len(prepared_tools) == 1:\n                all_results.append(_execute_single_tool_to_result(prepared_tools[0], update_run_depends=True))\n            else:\n                parallel_group = [tp for tp in prepared_tools if self._is_tool_parallel_eligible(tp[\"name\"])]\n                sequential_group = [tp for tp in prepared_tools if not self._is_tool_parallel_eligible(tp[\"name\"])]\n\n                if sequential_group:\n                    seq_names = [tp[\"name\"] for tp in sequential_group]\n                    logger.info(\n                        f\"Agent {self.name} - {self.id}: tools excluded from parallel execution \"\n                        f\"(is_parallel_execution_allowed=False): {seq_names}\"\n                    )\n\n                # Phase 1: run parallel-eligible tools concurrently\n                if len(parallel_group) &gt; 1:\n                    max_workers = len(parallel_group)\n                    with ContextAwareThreadPoolExecutor(max_workers=max_workers) as executor:\n                        future_map = {}\n                        for tool_payload in parallel_group:\n                            future = executor.submit(\n                                _execute_single_tool_to_result,\n                                tool_payload,\n                                is_parallel=True,\n                                update_run_depends=False,\n                            )\n                            future_map[future] = tool_payload\n\n                        for future in as_completed(future_map.keys()):\n                            all_results.append(future.result())\n                elif len(parallel_group) == 1:\n                    all_results.append(_execute_single_tool_to_result(parallel_group[0], update_run_depends=False))\n\n                # Phase 2: run sequential-only tools one-by-one\n                for tool_payload in sequential_group:\n                    all_results.append(_execute_single_tool_to_result(tool_payload, update_run_depends=False))\n\n        observation_parts: list[str] = []\n        aggregated_files: dict[str, Any] = {}\n\n        ordered_results = sorted(all_results, key=lambda r: r.get(\"order\", 0))\n\n        for result in ordered_results:\n            tool_name = result.get(\"tool_name\", UNKNOWN_TOOL_NAME)\n            result_content = result.get(\"result\", \"\")\n            success_status = \"SUCCESS\" if result.get(\"success\") else \"ERROR\"\n            observation_parts.append(f\"--- {tool_name} has resulted in {success_status} ---\\n{result_content}\")\n\n            self._merge_tool_files(aggregated_files, tool_name, result.get(\"files\"))\n\n        # Collect dependencies from results (for tracing)\n        dependencies = [result.get(\"dependency\") for result in ordered_results if result.get(\"dependency\")]\n\n        # Set run_depends after parallel execution completes\n        if dependencies:\n            self._run_depends = dependencies\n\n        combined_observation = \"\\n\\n\".join(observation_parts)\n\n        return combined_observation, aggregated_files\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent.get_clone_attr_initializers","title":"<code>get_clone_attr_initializers()</code>","text":"<p>Define attribute initializers for cloning.</p> <p>Ensures that cloned agents get fresh instances of: - _tool_cache: Independent tool execution cache - state: Independent AgentState to avoid race conditions in parallel execution</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[Node], Any]]</code> <p>Dictionary mapping attribute names to initializer functions</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def get_clone_attr_initializers(self) -&gt; dict[str, Callable[[Node], Any]]:\n    \"\"\"\n    Define attribute initializers for cloning.\n\n    Ensures that cloned agents get fresh instances of:\n    - _tool_cache: Independent tool execution cache\n    - state: Independent AgentState to avoid race conditions in parallel execution\n\n    Returns:\n        Dictionary mapping attribute names to initializer functions\n    \"\"\"\n    base = super().get_clone_attr_initializers()\n    base.update(\n        {\n            \"_tool_cache\": lambda _: {},\n            \"state\": lambda _: AgentState(),\n        }\n    )\n    return base\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent.log_final_output","title":"<code>log_final_output(thought, final_output, loop_num)</code>","text":"<p>Logs final output of the agent.</p> <p>Parameters:</p> Name Type Description Default <code>final_output</code> <code>str</code> <p>Final output of agent.</p> required <code>loop_num</code> <code>int</code> <p>Number of reasoning loop</p> required Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def log_final_output(self, thought: str, final_output: str, loop_num: int) -&gt; None:\n    \"\"\"\n    Logs final output of the agent.\n\n    Args:\n        final_output (str): Final output of agent.\n        loop_num (int): Number of reasoning loop\n    \"\"\"\n    logger.info(\n        \"\\n------------------------------------------\\n\"\n        f\"Agent {self.name}: Loop {loop_num}\\n\"\n        f\"Thought: {thought}\\n\"\n        f\"Final answer: {final_output}\"\n        \"\\n------------------------------------------\\n\"\n    )\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent.log_reasoning","title":"<code>log_reasoning(thought, action, action_input, loop_num)</code>","text":"<p>Logs reasoning step of agent.</p> <p>Parameters:</p> Name Type Description Default <code>thought</code> <code>str</code> <p>Reasoning about next step.</p> required <code>action</code> <code>str</code> <p>Chosen action.</p> required <code>action_input</code> <code>str</code> <p>Input to the tool chosen by action.</p> required <code>loop_num</code> <code>int</code> <p>Number of reasoning loop.</p> required Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def log_reasoning(self, thought: str, action: str, action_input: str, loop_num: int) -&gt; None:\n    \"\"\"\n    Logs reasoning step of agent.\n\n    Args:\n        thought (str): Reasoning about next step.\n        action (str): Chosen action.\n        action_input (str): Input to the tool chosen by action.\n        loop_num (int): Number of reasoning loop.\n    \"\"\"\n    logger.info(\n        \"\\n------------------------------------------\\n\"\n        f\"Agent {self.name}: Loop {loop_num}:\\n\"\n        f\"Thought: {thought}\\n\"\n        f\"Action: {action}\\n\"\n        f\"Action Input: {action_input}\"\n        \"\\n------------------------------------------\"\n    )\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Resets the agent's run state including AgentState.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Resets the agent's run state including AgentState.\"\"\"\n    super().reset_run_state()\n    self.state.reset()\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.Agent.validate_inference_mode","title":"<code>validate_inference_mode()</code>","text":"<p>Validate whether specified model can be inferenced in provided mode.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_inference_mode(self):\n    \"\"\"Validate whether specified model can be inferenced in provided mode.\"\"\"\n    match self.inference_mode:\n        case InferenceMode.FUNCTION_CALLING:\n            if not supports_function_calling(model=self.llm.model):\n                raise ValueError(f\"Model {self.llm.model} does not support function calling\")\n\n        case InferenceMode.STRUCTURED_OUTPUT:\n            params = get_supported_openai_params(model=self.llm.model)\n            if \"response_format\" not in params:\n                raise ValueError(f\"Model {self.llm.model} does not support structured output\")\n\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.AgentState","title":"<code>AgentState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates the dynamic state of an agent during execution.</p> <p>Tracks loop progress and todos. Provides its own serialization to string for injection into observations.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>class AgentState(BaseModel):\n    \"\"\"\n    Encapsulates the dynamic state of an agent during execution.\n\n    Tracks loop progress and todos. Provides its own serialization\n    to string for injection into observations.\n    \"\"\"\n\n    current_loop: int = 0\n    max_loops: int = 0\n    todos: list[TodoItem] = Field(default_factory=list)\n\n    def reset(self, max_loops: int = 0) -&gt; None:\n        \"\"\"Reset state for a new execution.\"\"\"\n        self.current_loop = 0\n        self.max_loops = max_loops\n        self.todos = []\n\n    def update_loop(self, current: int) -&gt; None:\n        \"\"\"Update current loop number.\"\"\"\n        self.current_loop = current\n\n    def update_todos(self, todos: list[dict | TodoItem]) -&gt; None:\n        \"\"\"Update todo list from dicts or TodoItem objects.\"\"\"\n        self.todos = [t if isinstance(t, TodoItem) else TodoItem(**t) for t in todos]\n\n    def to_prompt_string(self) -&gt; str:\n        \"\"\"\n        Serialize state to a string for observation injection.\n\n        Returns:\n            str: Formatted state string, or empty string if no state to show.\n        \"\"\"\n        sections = []\n\n        if self.current_loop &gt; 0:\n            sections.append(f\"Progress: Loop {self.current_loop}/{self.max_loops}\")\n\n        if self.todos:\n            todo_lines = [t.to_display_string() for t in self.todos]\n            sections.append(\"Todos:\\n\" + \"\\n\".join(todo_lines))\n\n        return \"\\n\".join(sections) if sections else \"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.AgentState.reset","title":"<code>reset(max_loops=0)</code>","text":"<p>Reset state for a new execution.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def reset(self, max_loops: int = 0) -&gt; None:\n    \"\"\"Reset state for a new execution.\"\"\"\n    self.current_loop = 0\n    self.max_loops = max_loops\n    self.todos = []\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.AgentState.to_prompt_string","title":"<code>to_prompt_string()</code>","text":"<p>Serialize state to a string for observation injection.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted state string, or empty string if no state to show.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def to_prompt_string(self) -&gt; str:\n    \"\"\"\n    Serialize state to a string for observation injection.\n\n    Returns:\n        str: Formatted state string, or empty string if no state to show.\n    \"\"\"\n    sections = []\n\n    if self.current_loop &gt; 0:\n        sections.append(f\"Progress: Loop {self.current_loop}/{self.max_loops}\")\n\n    if self.todos:\n        todo_lines = [t.to_display_string() for t in self.todos]\n        sections.append(\"Todos:\\n\" + \"\\n\".join(todo_lines))\n\n    return \"\\n\".join(sections) if sections else \"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.AgentState.update_loop","title":"<code>update_loop(current)</code>","text":"<p>Update current loop number.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def update_loop(self, current: int) -&gt; None:\n    \"\"\"Update current loop number.\"\"\"\n    self.current_loop = current\n</code></pre>"},{"location":"dynamiq/nodes/agents/agent/#dynamiq.nodes.agents.agent.AgentState.update_todos","title":"<code>update_todos(todos)</code>","text":"<p>Update todo list from dicts or TodoItem objects.</p> Source code in <code>dynamiq/nodes/agents/agent.py</code> <pre><code>def update_todos(self, todos: list[dict | TodoItem]) -&gt; None:\n    \"\"\"Update todo list from dicts or TodoItem objects.\"\"\"\n    self.todos = [t if isinstance(t, TodoItem) else TodoItem(**t) for t in todos]\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/","title":"Base","text":""},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>Node</code></p> <p>Base class for an AI Agent that interacts with a Language Model and tools.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class Agent(Node):\n    \"\"\"Base class for an AI Agent that interacts with a Language Model and tools.\"\"\"\n\n    AGENT_PROMPT_TEMPLATE: ClassVar[str] = AGENT_PROMPT_TEMPLATE\n\n    llm: BaseLLM = Field(..., description=\"LLM used by the agent.\")\n    group: NodeGroup = NodeGroup.AGENTS\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=3600))\n    tools: list[Node] = []\n    files: list[io.BytesIO | bytes] | None = None\n    images: list[str | bytes | io.BytesIO] = None\n    name: str = \"Agent\"\n    max_loops: int = 1\n    tool_output_max_length: int = TOOL_MAX_TOKENS\n    tool_output_truncate_enabled: bool = True\n    delegation_allowed: bool = Field(\n        default=False,\n        description=\"Allow returning a child agent tool's output directly via delegate_final flag.\",\n    )\n    parallel_tool_calls_enabled: bool = Field(\n        default=False,\n        description=\"Enable multi-tool execution in a single step. \"\n        \"When True, the agent can call multiple tools in parallel.\",\n    )\n    memory: Memory | None = Field(None, description=\"Memory node for the agent.\")\n    memory_limit: int = Field(100, description=\"Maximum number of messages to retrieve from memory\")\n    memory_retrieval_strategy: MemoryRetrievalStrategy | None = MemoryRetrievalStrategy.ALL\n    verbose: bool = Field(False, description=\"Whether to print verbose logs.\")\n    file_store: FileStoreConfig = Field(\n        default_factory=lambda: FileStoreConfig(enabled=False, backend=InMemoryFileStore()),\n        description=\"Configuration for file storage used by the agent.\",\n    )\n    sandbox: SandboxConfig | None = Field(default=None, description=\"Configuration for sandbox used by the agent.\")\n    file_attachment_preview_bytes: int = Field(\n        default=512,\n        description=\"Maximum number of bytes/characters from each uploaded file to surface as an inline preview.\",\n    )\n    skills: SkillsConfig = Field(\n        default_factory=SkillsConfig,\n        description=\"Skills config. When enabled and source registry is set, skills are on (Dynamiq or FileSystem).\",\n    )\n\n    input_message: Message | VisionMessage | None = None\n    role: str | None = Field(\n        default=None,\n        description=\"\"\"Agent basic instructions.\n            Can be used to provide additional context or instructions to the agent.\n            Accepts Jinja templates to provide additional parameters.\"\"\",\n    )\n    description: str | None = Field(default=None, description=\"Short human-readable description of the agent.\")\n    _mcp_servers: list[MCPServer] = PrivateAttr(default_factory=list)\n    _excluded_tool_ids: set[str] = PrivateAttr(default_factory=set)\n    _tool_cache: dict[ToolCacheEntry, Any] = {}\n    _history_offset: int = PrivateAttr(\n        default=2,  # Offset to the first message (default: 2 \u2014 system and initial user messages).\n    )\n    system_prompt_manager: AgentPromptManager = Field(default_factory=AgentPromptManager)\n    _current_call_context: dict[str, Any] | None = PrivateAttr(default=None)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[AgentInputSchema]] = AgentInputSchema\n    _json_schema_fields: ClassVar[list[str]] = [\"role\", \"description\"]\n\n    @classmethod\n    def _generate_json_schema(\n        cls, llms: dict[type[BaseLLM], list[str]] = {}, tools=list[type[Node]], **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Generates full json schema for Agent with provided llms and tools.\n        This schema is designed for compatibility with the WorkflowYamlParser,\n        containing enough partial information to instantiate an Agent.\n        Parameters name to be included in the schema are either defined in the _json_schema_fields class variable or\n        passed via the fields parameter.\n\n        It generates a schema using the provided LLMs and tools.\n\n        Args:\n            llms (dict[type[BaseLLM], list[str]]): Available llm providers and models.\n            tools (list[type[Node]]): List of tools.\n\n        Returns:\n            dict[str, Any]: Generated json schema.\n        \"\"\"\n        schema = super()._generate_json_schema(**kwargs)\n        schema[\"properties\"][\"llm\"] = {\n            \"anyOf\": [\n                {\n                    \"type\": \"object\",\n                    **llm._generate_json_schema(models=models, fields=[\"model\", \"temperature\", \"max_tokens\"]),\n                }\n                for llm, models in llms.items()\n            ],\n            \"additionalProperties\": False,\n        }\n\n        schema[\"properties\"][\"tools\"] = {\n            \"type\": \"array\",\n            \"items\": {\"anyOf\": [{\"type\": \"object\", **tool._generate_json_schema()} for tool in tools]},\n        }\n\n        schema[\"required\"] += [\"tools\", \"llm\"]\n        return schema\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._run_depends: list[dict] = []\n        self._prompt = Prompt(messages=[])\n\n        expanded_tools = []\n        for tool in self.tools:\n            if isinstance(tool, MCPServer):\n                self._mcp_servers.append(tool)\n                subtools = tool.get_mcp_tools()\n                expanded_tools.extend(subtools)\n                self._excluded_tool_ids.update(subtool.id for subtool in subtools)\n            else:\n                expanded_tools.append(tool)\n\n        self.tools = expanded_tools\n        if self.file_store_backend and self.sandbox_backend:\n            raise ValueError(\"file_store and sandbox cannot both be enabled for an Agent at the same time\")\n\n        if self.sandbox_backend:\n            # Add sandbox tools when sandbox is enabled (not serialized; recreated from sandbox config on load)\n            sandbox_tools = self.sandbox_backend.get_tools(llm=self.llm)\n            self._excluded_tool_ids.update(t.id for t in sandbox_tools)\n            self.tools.extend(sandbox_tools)\n\n        elif self.file_store_backend:\n            # Add file tools when file store is enabled\n            self.tools.extend(\n                [\n                    FileReadTool(file_store=self.file_store_backend, llm=self.llm),\n                    FileSearchTool(file_store=self.file_store_backend),\n                    FileListTool(file_store=self.file_store_backend),\n                ]\n            )\n            if self.file_store.agent_file_write_enabled:\n                self.tools.append(FileWriteTool(file_store=self.file_store_backend))\n\n        if self.parallel_tool_calls_enabled:\n            # Filter out any user tools with the reserved parallel tool name\n            self.tools = [t for t in self.tools if t.name != PARALLEL_TOOL_NAME]\n            self.tools.append(ParallelToolCallsTool())\n\n        if self._skills_should_init():\n            self._init_skills()\n        self._init_prompt_blocks()\n        if self._skills_should_init():\n            self._apply_skills_to_prompt()\n\n    @model_validator(mode=\"after\")\n    def validate_input_fields(self):\n        if self.input_message:\n            self.input_message.role = MessageRole.USER\n\n        return self\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n        role_for_validation = self.role or \"\"\n        if role_for_validation and (\n            \"{% raw %}\" not in role_for_validation and \"{% endraw %}\" not in role_for_validation\n        ):\n            role_for_validation = f\"{{% raw %}}{role_for_validation}{{% endraw %}}\"\n        return {\"input_message\": self.input_message, \"role\": role_for_validation}\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\n            \"llm\": True,\n            \"tools\": True,\n            \"memory\": True,\n            \"files\": True,\n            \"images\": True,\n            \"file_store\": True,\n            \"skills\": True,\n            \"sandbox\": True,\n            \"system_prompt_manager\": True,  # Runtime state container, not serializable\n        }\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n\n        tools_to_serialize = [t for t in self.tools if t.id not in self._excluded_tool_ids]\n        data[\"tools\"] = [tool.to_dict(**kwargs) for tool in tools_to_serialize]\n        data[\"tools\"] = data[\"tools\"] + [mcp_server.to_dict(**kwargs) for mcp_server in self._mcp_servers]\n\n        data[\"memory\"] = self.memory.to_dict(**kwargs) if self.memory else None\n        if self.files:\n            data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n        if self.images:\n            data[\"images\"] = [{\"name\": getattr(f, \"name\", f\"image_{i}\")} for i, f in enumerate(self.images)]\n\n        data[\"file_store\"] = self.file_store.to_dict(**kwargs) if self.file_store else None\n        data[\"sandbox\"] = self.sandbox.to_dict(**kwargs) if self.sandbox else None\n        data[\"skills\"] = self.skills.to_dict(**kwargs)\n\n        return data\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize components for the manager and agents.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n        for tool in self.tools:\n            if tool.is_postponed_component_init:\n                tool.init_components(connection_manager)\n            tool.is_optimized_for_agents = True\n\n        self._ensure_skills_ingested_for_sandbox()\n\n    def _ensure_skills_ingested_for_sandbox(self) -&gt; None:\n        \"\"\"When skills source is Dynamiq with sandbox_skills_base_path and sandbox is enabled, ingest skills at init.\"\"\"\n        if not self.sandbox_backend or not self._skills_should_init():\n            return\n        source = self.skills.source\n        if source is None:\n            return\n\n        if not isinstance(source, Dynamiq) or not source.sandbox_skills_base_path:\n            return\n        try:\n            if hasattr(self.sandbox_backend, \"_ensure_sandbox\"):\n                self.sandbox_backend._ensure_sandbox()\n            ingest_skills_into_sandbox(\n                self.sandbox_backend,\n                source,\n                sandbox_skills_base_path=source.sandbox_skills_base_path,\n            )\n            logger.info(\"Agent %s: skills ingested into sandbox at init\", self.name)\n        except Exception as e:\n            logger.warning(\"Agent %s: skills ingestion into sandbox failed: %s\", self.name, e)\n\n    def sanitize_tool_name(self, s: str):\n        \"\"\"Sanitize tool name to follow [^a-zA-Z0-9_-].\"\"\"\n        s = s.replace(\" \", \"-\")\n        sanitized = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", s)\n        return sanitized\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initializes default prompt blocks and variables.\"\"\"\n        model_name = getattr(self.llm, \"model\", None)\n\n        self.system_prompt_manager = AgentPromptManager(model_name=model_name, tool_description=self.tool_description)\n        self.system_prompt_manager.setup_for_base_agent()\n\n    def _skills_should_init(self) -&gt; bool:\n        \"\"\"True if skills support should be initialized (enabled and source set).\"\"\"\n        return self.skills.enabled and self.skills.source is not None\n\n    def _init_skills(self) -&gt; None:\n        \"\"\"Add SkillsTool to self.tools so it is included in function-calling and structured-output schemas.\"\"\"\n\n        source = self.skills.source\n        if source is None:\n            logger.warning(\"Skills config missing or invalid (source required); skipping skills init\")\n            return\n        skills_tool = SkillsTool(skill_registry=source)\n        self.tools.append(skills_tool)\n        self._excluded_tool_ids.add(skills_tool.id)\n\n    def _apply_skills_to_prompt(self) -&gt; None:\n        \"\"\"Set skills block and tool_description on the prompt manager after _init_prompt_blocks().\"\"\"\n        source = self.skills.source\n        if source is None:\n            return\n        metadata = self.skills.get_skills_metadata()\n        sandbox_base = normalize_sandbox_skills_base_path(getattr(source, \"sandbox_skills_base_path\", None))\n        skills_summary = self._format_skills_summary(\n            metadata, sandbox_skills_base_path=sandbox_base if sandbox_base else None\n        )\n        self.system_prompt_manager.set_block(\"skills\", skills_summary)\n        self.system_prompt_manager.set_initial_variable(\"tool_description\", self.tool_description)\n        if sandbox_base:\n            self.system_prompt_manager.set_initial_variable(\"sandbox_skills_base_path\", sandbox_base)\n        logger.info(\n            f\"Agent {self.name} - {self.id}: initialized with {len(metadata)} skills \"\n            f\"(source={source.__class__.__name__})\"\n        )\n\n    def _format_skills_summary(self, metadata: list[SkillMetadata], sandbox_skills_base_path: str | None = None) -&gt; str:\n        \"\"\"Format skills summary for prompt.\n\n        When sandbox_skills_base_path is set (caller must pass an already-normalized path or None),\n        each line includes the path to read the skill in the sandbox so the agent can go straight\n        to SandboxShellTool without calling SkillsTool list.\n        \"\"\"\n        if not metadata:\n            return \"\"\n\n        base = sandbox_skills_base_path or \"\"\n        lines = []\n        for skill in metadata:\n            if base:\n                skill_path = f\"{base}/{skill.name}/SKILL.md\"\n                lines.append(f\"- **{skill.name}**: {skill.description} \u2014 read: `{skill_path}`\")\n            else:\n                lines.append(f\"- **{skill.name}**: {skill.description}\")\n        return \"\\n\".join(lines)\n\n    def set_block(self, block_name: str, content: str):\n        \"\"\"Adds or updates a prompt block.\"\"\"\n        self.system_prompt_manager.set_block(block_name, content)\n\n    def set_prompt_variable(self, variable_name: str, value: Any):\n        \"\"\"Sets or updates a prompt variable.\"\"\"\n        self.system_prompt_manager.set_variable(variable_name, value)\n\n    def _prepare_metadata(self, input_data: AgentInputSchema) -&gt; dict:\n        \"\"\"\n        Prepare metadata from input data.\n\n        Args:\n            input_data: Agent input schema containing user information\n\n        Returns:\n            dict: Processed metadata\n        \"\"\"\n        custom_metadata = input_data.metadata.copy()\n\n        # Add extra fields that were provided (model allows extra fields with ConfigDict extra=\"allow\")\n        if input_data.model_extra:\n            custom_metadata.update(input_data.model_extra)\n\n        # Clean up any leaked fields\n        if \"files\" in custom_metadata:\n            del custom_metadata[\"files\"]\n        if \"images\" in custom_metadata:\n            del custom_metadata[\"images\"]\n        if \"tool_params\" in custom_metadata:\n            del custom_metadata[\"tool_params\"]\n\n        if input_data.user_id:\n            custom_metadata[\"user_id\"] = input_data.user_id\n        if input_data.session_id:\n            custom_metadata[\"session_id\"] = input_data.session_id\n\n        return custom_metadata\n\n    def execute(\n        self,\n        input_data: AgentInputSchema,\n        input_message: Message | VisionMessage | None = None,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the agent with the given input data.\n        \"\"\"\n        # Convert to dict only for logging (to avoid logging BytesIO objects)\n        log_data = input_data.model_dump()\n        if log_data.get(\"images\"):\n            log_data[\"images\"] = [f\"image_{i}\" for i in range(len(log_data[\"images\"]))]\n        if log_data.get(\"files\"):\n            log_data[\"files\"] = [f\"file_{i}\" for i in range(len(log_data[\"files\"]))]\n\n        logger.info(f\"Agent {self.name} - {self.id}: started with input {log_data}\")\n        self.reset_run_state()\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        custom_metadata = self._prepare_metadata(input_data)\n        self._current_call_context = {\n            \"user_id\": input_data.user_id,\n            \"session_id\": input_data.session_id,\n            \"metadata\": custom_metadata,\n        }\n\n        input_message = input_message or self.input_message or Message(role=MessageRole.USER, content=input_data.input)\n        # Convert to dict for format_message, excluding fields that are unsafe for templates\n        # (binary data like files/images, complex objects like tool_params, and input which is already handled)\n        standard_fields = set(AgentInputSchema.model_fields.keys())\n        extra_fields = input_data.model_dump(exclude=standard_fields)\n        input_message = input_message.format_message(**extra_fields)\n\n        use_memory = self.memory and (input_data.user_id or input_data.session_id)\n\n        if use_memory:\n            history_messages = self._retrieve_memory(input_data)\n            if len(history_messages) &gt; 0:\n                history_messages.insert(\n                    0,\n                    Message(\n                        role=MessageRole.SYSTEM,\n                        content=\"Below is the previous conversation history. \"\n                        \"Use this context to inform your response.\",\n                    ),\n                )\n            if isinstance(input_message, Message):\n                memory_content = input_message.content\n            else:\n                text_parts = [\n                    content.text for content in input_message.content if isinstance(content, VisionMessageTextContent)\n                ]\n                memory_content = \" \".join(text_parts) if text_parts else \"Image input\"\n            self.memory.add(role=MessageRole.USER, content=memory_content, metadata=custom_metadata)\n        else:\n            history_messages = None\n\n        files = input_data.files\n        if files:\n            normalized_files = self._ensure_named_files(files)\n            if self.sandbox_backend:\n                self._upload_files_to_sandbox(normalized_files)\n            else:\n                if not self.file_store_backend:\n                    self._setup_in_memory_file_store_and_tools()\n                if self.file_store_backend:\n                    self._upload_files_to_file_store(normalized_files)\n            input_message = self._inject_attached_files_into_message(input_message, normalized_files)\n\n        if input_data.tool_params:\n            kwargs[\"tool_params\"] = input_data.tool_params\n\n        self.system_prompt_manager.update_variables(dict(input_data))\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n\n        try:\n            result = self._run_agent(input_message, history_messages, config=config, **kwargs)\n        finally:\n            self._current_call_context = None\n\n        if use_memory:\n            self.memory.add(role=MessageRole.ASSISTANT, content=result, metadata=custom_metadata)\n\n        execution_result = {\n            \"content\": result,\n        }\n\n        requested_paths = getattr(self, \"_requested_output_files\", None)\n\n        if self.file_store_backend and requested_paths:\n            try:\n                stored_files = self.file_store_backend.list_files_bytes(requested_paths)\n            except Exception as e:\n                logger.warning(f\"Agent {self.name} - {self.id}: failed to collect files from file store: {e}\")\n                stored_files = []\n            if stored_files:\n                execution_result[\"files\"] = stored_files\n                logger.info(\n                    f\"Agent {self.name} - {self.id}: \"\n                    f\"returning {len(stored_files)} requested file(s) from file store\"\n                )\n\n        if self.sandbox_backend and requested_paths:\n            try:\n                sandbox_files = self.sandbox_backend.collect_files(file_paths=requested_paths)\n            except Exception as e:\n                logger.warning(f\"Agent {self.name} - {self.id}: failed to collect files from sandbox: {e}\")\n                sandbox_files = []\n            if sandbox_files:\n                existing_files = execution_result.get(\"files\", [])\n                execution_result[\"files\"] = existing_files + sandbox_files\n                logger.info(\n                    f\"Agent {self.name} - {self.id}: \"\n                    f\"returning {len(sandbox_files)} requested file(s) from sandbox\"\n                )\n\n        logger.info(f\"Node {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n        return execution_result\n\n    def retrieve_conversation_history(\n        self,\n        user_query: str = None,\n        user_id: str = None,\n        session_id: str = None,\n        limit: int = None,\n        strategy: MemoryRetrievalStrategy = MemoryRetrievalStrategy.ALL,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Retrieves conversation history for the agent using the specified strategy.\n\n        Args:\n            user_query: Current user input to find relevant context (for RELEVANT/HYBRID strategies)\n            user_id: Optional user identifier\n            session_id: Optional session identifier\n            limit: Maximum number of messages to return (defaults to memory_limit)\n            strategy: Which retrieval strategy to use (ALL, RELEVANT, or HYBRID)\n\n        Returns:\n            List of messages forming a valid conversation context\n        \"\"\"\n        if not self.memory or not (user_id or session_id):\n            return []\n\n        filters = {}\n        if user_id:\n            filters[\"user_id\"] = user_id\n        if session_id:\n            filters[\"session_id\"] = session_id\n\n        limit = limit or self.memory_limit\n\n        if strategy == MemoryRetrievalStrategy.RELEVANT and not user_query:\n            logger.warning(\"RELEVANT strategy selected but no user_query provided - falling back to ALL\")\n            strategy = MemoryRetrievalStrategy.ALL\n\n        conversation = self.memory.get_agent_conversation(\n            query=user_query,\n            limit=limit,\n            filters=filters,\n            strategy=strategy,\n        )\n        return conversation\n\n    def _retrieve_memory(self, input_data: AgentInputSchema) -&gt; list[Message]:\n        \"\"\"\n        Args:\n            input_data: Agent input schema containing user information\n\n        Returns:\n            list[Message]: List of messages forming a valid conversation context\n        Retrieves memory messages when user_id and/or session_id are provided.\n        \"\"\"\n        history_messages = self.retrieve_conversation_history(\n            user_query=input_data.input,\n            user_id=input_data.user_id,\n            session_id=input_data.session_id,\n            strategy=self.memory_retrieval_strategy,\n        )\n        logger.info(\"Agent %s - %s: retrieved %d messages from memory\", self.name, self.id, len(history_messages))\n        return history_messages\n\n    def _run_llm(\n        self, messages: list[Message | VisionMessage], config: RunnableConfig | None = None, **kwargs\n    ) -&gt; RunnableResult:\n        \"\"\"Runs the LLM with a given prompt and handles streaming or full responses.\n\n        Args:\n            messages (list[Message | VisionMessage]): Input messages for llm.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Generated response.\n        \"\"\"\n        try:\n            llm_result = self.llm.run(\n                input_data={},\n                config=config,\n                prompt=Prompt(messages=messages),\n                run_depends=deepcopy(self._run_depends),\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n            if llm_result.status != RunnableStatus.SUCCESS:\n                error_message = f\"LLM '{self.llm.name}' failed: {llm_result.error.message}\"\n                raise ValueError({error_message})\n\n            return llm_result\n\n        except Exception as e:\n            raise e\n\n    def stream_content(\n        self,\n        content: str | dict,\n        source: str,\n        step: str,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; str | dict:\n        \"\"\"\n        Streams data.\n\n        Args:\n            content (str | dict): Data that will be streamed.\n            source (str): Source of the content.\n            step (str): Description of the step.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str | dict: Streamed data.\n        \"\"\"\n        if not isinstance(source, str):\n            raise ValueError(\n                f\"stream_content source parameter must be a string, got {type(source).__name__}: {source}. \"\n                f\"This likely indicates incorrect parameter passing from the calling code.\"\n            )\n\n        return self.stream_response(content=content, source=source, step=step, config=config, **kwargs)\n\n    def stream_response(\n        self, content: str | dict, source: str, step: str, config: RunnableConfig | None = None, **kwargs\n    ):\n        if not isinstance(source, str):\n            raise ValueError(\n                f\"stream_response source parameter must be a string, got {type(source).__name__}: {source}. \"\n                f\"This likely indicates a parameter ordering issue in the calling code.\"\n            )\n\n        response_for_stream = StreamChunk(\n            choices=[StreamChunkChoice(delta=StreamChunkChoiceDelta(content=content, source=source, step=step))]\n        )\n\n        self.run_on_node_execute_stream(\n            callbacks=config.callbacks,\n            chunk=response_for_stream.model_dump(),\n            **kwargs,\n        )\n        return content\n\n    def _run_agent(\n        self,\n        input_message: Message | VisionMessage,\n        history_messages: list[Message] | None = None,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Runs the agent with the generated prompt and handles exceptions.\"\"\"\n        formatted_prompt = self.generate_prompt()\n        system_message = Message(role=MessageRole.SYSTEM, content=formatted_prompt)\n        if history_messages:\n            self._prompt.messages = [system_message, *history_messages, input_message]\n        else:\n            self._prompt.messages = [system_message, input_message]\n\n        try:\n            llm_result = self._run_llm(self._prompt.messages, config=config, **kwargs).output[\"content\"]\n            self._prompt.messages.append(Message(role=MessageRole.ASSISTANT, content=llm_result))\n\n            if self.streaming.enabled:\n                return self.stream_content(\n                    content=llm_result,\n                    source=self.name,\n                    step=\"answer\",\n                    config=config,\n                    **kwargs,\n                )\n            return llm_result\n\n        except Exception as e:\n            raise e\n\n    def _get_tool(self, action: str) -&gt; Node:\n        \"\"\"Retrieves the tool corresponding to the given action.\"\"\"\n        tool = self.tool_by_names.get(self.sanitize_tool_name(action))\n        if not tool:\n            raise AgentUnknownToolException(\n                f\"Unknown tool: {action}.\"\n                \"Use only available tools and provide only the tool's name in the action field. \"\n                \"Do not include any additional reasoning. \"\n                \"Please correct the action field or state that you cannot answer the question.\"\n            )\n        return tool\n\n    def _apply_parameters(self, merged_input: dict, params: dict, source: str, debug_info: list = None):\n        \"\"\"Apply parameters from the specified source to the merged input.\"\"\"\n        if debug_info is None:\n            debug_info = []\n        for key, value in params.items():\n            if key in merged_input and isinstance(value, dict) and isinstance(merged_input[key], dict):\n                merged_nested = merged_input[key].copy()\n                merged_input[key] = deep_merge(value, merged_nested)\n                debug_info.append(f\"  - From {source}: Merged nested {key}\")\n            else:\n                merged_input[key] = value\n                debug_info.append(f\"  - From {source}: Set {key}={value}\")\n\n    def _regenerate_node_ids(self, obj: Any) -&gt; Any:\n        \"\"\"Recursively assign new IDs to cloned nodes and nested models.\"\"\"\n        if isinstance(obj, BaseModel):\n            if hasattr(obj, \"id\"):\n                setattr(obj, \"id\", str(uuid4()))\n\n            for field_name in getattr(obj, \"model_fields\", {}):\n                value = getattr(obj, field_name)\n                if isinstance(value, list):\n                    setattr(obj, field_name, [self._regenerate_node_ids(item) for item in value])\n                elif isinstance(value, dict):\n                    setattr(obj, field_name, {k: self._regenerate_node_ids(v) for k, v in value.items()})\n                else:\n                    setattr(obj, field_name, self._regenerate_node_ids(value))\n            return obj\n        if isinstance(obj, list):\n            return [self._regenerate_node_ids(item) for item in obj]\n        if isinstance(obj, dict):\n            return {k: self._regenerate_node_ids(v) for k, v in obj.items()}\n        return obj\n\n    def _clone_tool_for_execution(self, tool: Node, config: RunnableConfig | None) -&gt; tuple[Node, RunnableConfig]:\n        \"\"\"Clone tool and align config overrides so each execution is isolated.\"\"\"\n        base_config = ensure_config(config)\n        try:\n            tool_copy = self._regenerate_node_ids(tool.clone())\n        except Exception as e:\n            logger.warning(f\"Agent {self.name} - {self.id}: failed to clone tool {tool.name}: {e}\")\n            return tool, base_config\n\n        local_config = base_config\n        try:\n            local_config = base_config.model_copy(deep=False)\n            original_override = base_config.nodes_override.get(tool.id)\n            if original_override:\n                local_config.nodes_override[tool_copy.id] = original_override\n        except Exception as e:\n            logger.warning(\n                f\"Agent {self.name} - {self.id}: failed to prepare config override for cloned tool {tool.name}: {e}\"\n            )\n            local_config = base_config\n\n        return tool_copy, local_config\n\n    @staticmethod\n    def _extract_file_paths_from_input(tool: Node, merged_input: dict[str, Any]) -&gt; list[str] | None:\n        \"\"\"Extract file path references from map_from_storage fields in tool input.\n\n        Scans the tool's input schema for fields tagged with ``map_from_storage``\n        and collects any string values the LLM provided for those fields.\n\n        Returns:\n            List of file path strings, or None if no paths were found.\n        \"\"\"\n        paths: list[str] = []\n        for field_name, field in tool.input_schema.model_fields.items():\n            if not (field.json_schema_extra and field.json_schema_extra.get(\"map_from_storage\", False)):\n                continue\n            value = merged_input.get(field_name)\n            if value is None:\n                continue\n            if isinstance(value, str):\n                paths.append(value)\n            elif isinstance(value, (list, tuple)):\n                paths.extend(v for v in value if isinstance(v, str))\n            elif isinstance(value, dict):\n                paths.extend(v for v in value.values() if isinstance(v, str))\n        return paths or None\n\n    def _inject_files_into_tool(self, tool: Node, merged_input: dict[str, Any]) -&gt; None:\n        \"\"\"Inject files from file store or sandbox into tool input when applicable.\n\n        Sandbox files are only collected when the LLM explicitly references paths\n        in ``map_from_storage`` fields.  Otherwise the file store is used, which\n        means ``Python`` and ``PythonCodeExecutor`` tools always receive files\n        from the file store (never from the sandbox).\n        \"\"\"\n        if not tool.is_files_allowed:\n            return\n\n        file_paths = self._extract_file_paths_from_input(tool, merged_input)\n\n        if self.sandbox_backend and file_paths:\n            files = self.sandbox_backend.collect_files(file_paths=file_paths)\n            files_map = {path: file for path, file in zip(file_paths, files)}\n        elif self.file_store_backend:\n            files = self.file_store_backend.list_files_bytes()\n            files_map = {getattr(f, \"name\", f\"file_{id(f)}\"): f for f in files}\n        else:\n            return\n\n        if not files:\n            return\n\n        for field_name, field in tool.input_schema.model_fields.items():\n            if not (field.json_schema_extra and field.json_schema_extra.get(\"map_from_storage\", False)):\n                continue\n            value = merged_input.get(field_name)\n            if value is None:\n                merged_input[field_name] = files\n            elif isinstance(value, dict):\n                merged_input[field_name] = {\n                    k: files_map.get(v, v) if isinstance(v, str) else v for k, v in value.items()\n                }\n            elif isinstance(value, (list, tuple)):\n                merged_input[field_name] = [files_map.get(v, v) if isinstance(v, str) else v for v in value]\n            elif isinstance(value, str):\n                merged_input[field_name] = files_map.get(value, value)\n\n        if isinstance(tool, Python):\n            merged_input[\"files\"] = files\n\n        if isinstance(tool, PythonCodeExecutor) and not tool.file_store and self.file_store_backend:\n            tool.file_store = self.file_store_backend\n            logger.debug(f\"Agent {self.name} - {self.id}: injected file_store into PythonCodeExecutor tool {tool.name}\")\n\n    def _run_tool(\n        self,\n        tool: Node,\n        tool_input: dict,\n        config,\n        update_run_depends: bool = True,\n        collect_dependency: bool = False,\n        delegate_final: bool = False,\n        is_parallel: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Runs a specific tool with the given input.\"\"\"\n        merged_input = tool_input.copy() if isinstance(tool_input, dict) else {\"input\": tool_input}\n\n        if not self.delegation_allowed:\n            if delegate_final and self.verbose:\n                logger.debug(\n                    \"Agent %s - %s: delegate_final ignored because delegation_allowed is False\",\n                    self.name,\n                    self.id,\n                )\n            delegate_final = False\n            if isinstance(merged_input, dict) and \"delegate_final\" in merged_input:\n                if self.verbose:\n                    logger.debug(\n                        \"Agent %s - %s: delegate_final removed from tool input because delegation_allowed is False\",\n                        self.name,\n                        self.id,\n                    )\n                merged_input.pop(\"delegate_final\", None)\n\n        raw_tool_params = kwargs.get(\"tool_params\", ToolParams())\n        tool_params = (\n            ToolParams.model_validate(raw_tool_params) if isinstance(raw_tool_params, dict) else raw_tool_params\n        )\n\n        self._inject_files_into_tool(tool, merged_input)\n\n        if tool_params:\n            debug_info = []\n            if self.verbose:\n                debug_info.append(f\"Tool parameter merging for {tool.name} (ID: {tool.id}):\")\n                debug_info.append(f\"Starting with input: {merged_input}\")\n\n            # 1. Apply global parameters (lowest priority)\n            global_params = tool_params.global_params\n            if global_params:\n                self._apply_parameters(merged_input, global_params, \"global\", debug_info)\n\n            # 2. Apply parameters by tool name (medium priority)\n            name_params_any = tool_params.by_name_params.get(tool.name) or tool_params.by_name_params.get(\n                self.sanitize_tool_name(tool.name)\n            )\n            if name_params_any:\n                if isinstance(name_params_any, ToolParams):\n                    if self.verbose:\n                        debug_info.append(\n                            f\"  - From name:{tool.name}: encountered nested ToolParams (ignored for non-agent tool)\"\n                        )\n                elif isinstance(name_params_any, dict):\n                    self._apply_parameters(merged_input, name_params_any, f\"name:{tool.name}\", debug_info)\n\n            # 3. Apply parameters by tool ID (highest priority)\n            id_params_any = tool_params.by_id_params.get(tool.id)\n            if id_params_any:\n                if isinstance(id_params_any, ToolParams):\n                    if self.verbose:\n                        debug_info.append(\n                            f\"  - From id:{tool.id}: encountered nested ToolParams (ignored for non-agent tool)\"\n                        )\n                elif isinstance(id_params_any, dict):\n                    self._apply_parameters(merged_input, id_params_any, f\"id:{tool.id}\", debug_info)\n\n            if self.verbose and debug_info:\n                logger.debug(\"\\n\".join(debug_info))\n\n        child_kwargs = kwargs | {\"recoverable_error\": True}\n        is_child_agent = isinstance(tool, Agent)\n\n        if is_child_agent and self._current_call_context:\n            child_context = self._build_child_agent_context(tool)\n            for ctx_key in (\"user_id\", \"session_id\"):\n                if ctx_key not in merged_input and child_context.get(ctx_key):\n                    merged_input[ctx_key] = child_context[ctx_key]\n            if \"metadata\" not in merged_input and child_context.get(\"metadata\"):\n                merged_input[\"metadata\"] = child_context[\"metadata\"]\n\n        if is_child_agent and tool_params:\n            nested_any = (\n                tool_params.by_id_params.get(getattr(tool, \"id\", \"\"))\n                or tool_params.by_name_params.get(getattr(tool, \"name\", \"\"))\n                or tool_params.by_name_params.get(self.sanitize_tool_name(getattr(tool, \"name\", \"\")))\n            )\n            if nested_any:\n                if isinstance(nested_any, ToolParams):\n                    nested_tp = nested_any\n                elif isinstance(nested_any, dict):\n                    nested_tp = ToolParams.model_validate(nested_any)\n                else:\n                    nested_tp = None\n                if nested_tp:\n                    child_kwargs = child_kwargs | {\"tool_params\": nested_tp}\n\n        effective_delegate_final = delegate_final and is_child_agent\n        if is_child_agent and isinstance(merged_input, dict) and \"delegate_final\" in merged_input:\n            effective_delegate_final = effective_delegate_final or bool(merged_input.pop(\"delegate_final\"))\n\n        tool_to_run = tool\n        tool_config = ensure_config(config)\n        if is_parallel:\n            tool_to_run, tool_config = self._clone_tool_for_execution(tool, tool_config)\n\n        tool_result = tool_to_run.run(\n            input_data=merged_input,\n            config=tool_config,\n            run_depends=deepcopy(self._run_depends),\n            **child_kwargs,\n        )\n        dependency_node = tool_to_run if tool_to_run is not tool else tool\n        dependency_dict = NodeDependency(node=dependency_node).to_dict(for_tracing=True)\n        if update_run_depends:\n            self._run_depends = [dependency_dict]\n        if tool_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"Tool '{tool.name}' failed: {tool_result.error.to_dict()}\"\n            if tool_result.error.recoverable:\n                raise ToolExecutionException({error_message})\n            else:\n                raise ValueError({error_message})\n        tool_result_output_content = tool_result.output.get(\"content\")\n\n        self._handle_tool_generated_files(tool, tool_result)\n\n        tool_result_content_processed = process_tool_output_for_agent(\n            content=tool_result_output_content,\n            max_tokens=self.tool_output_max_length,\n            truncate=self.tool_output_truncate_enabled and not effective_delegate_final,\n        )\n\n        output_files = tool_result.output.get(\"files\", [])\n        tool_output_meta = {k: v for k, v in tool_result.output.items() if k not in (\"content\", \"files\")}\n\n        if not isinstance(tool, ContextManagerTool):\n            self._tool_cache[ToolCacheEntry(action=tool.name, action_input=tool_input)] = (\n                tool_result_content_processed,\n                tool_output_meta,\n            )\n        if collect_dependency:\n            return tool_result_content_processed, output_files, tool_output_meta, dependency_dict\n\n        return tool_result_content_processed, output_files, tool_output_meta\n\n    def _ensure_named_files(self, files: list[io.BytesIO | bytes]) -&gt; list[io.BytesIO | bytes]:\n        \"\"\"Ensure all uploaded files have name and description attributes and store them in storage backend.\"\"\"\n        named = []\n        for i, f in enumerate(files):\n            if isinstance(f, bytes):\n                bio = io.BytesIO(f)\n                bio.name = f\"file_{i}.bin\"\n                bio.description = \"User-provided file\"\n                named.append(bio)\n            elif isinstance(f, io.BytesIO):\n                if not hasattr(f, \"name\"):\n                    f.name = f\"file_{i}\"\n                if not hasattr(f, \"description\"):\n                    f.description = \"User-provided file\"\n                named.append(f)\n            else:\n                named.append(f)\n        return named\n\n    def _handle_tool_generated_files(self, tool: Node, tool_result: RunnableResult) -&gt; None:\n        \"\"\"\n        Handle files generated by tools and store them in the file store and/or sandbox.\n\n        Args:\n            tool: The tool that generated the files\n            tool_result: The result from the tool execution\n        \"\"\"\n        if not self.file_store_backend and not self.sandbox_backend:\n            return\n\n        if not (isinstance(tool_result.output, dict) and \"files\" in tool_result.output):\n            return\n\n        tool_files = tool_result.output.get(\"files\", [])\n        if not tool_files:\n            return\n\n        stored_files = []\n        for file in tool_files:\n            if isinstance(file, io.BytesIO):\n                file_name = getattr(file, \"name\", f\"file_{id(file)}.bin\")\n                file_description = getattr(file, \"description\", \"Tool-generated file\")\n                content_type = getattr(file, \"content_type\", \"application/octet-stream\")\n\n                content = file.read()\n                file.seek(0)\n\n                if self.sandbox_backend:\n                    try:\n                        dest = f\"{self.sandbox_backend.base_path}/{file_name}\"\n                        self.sandbox_backend.upload_file(file_name, content, destination_path=dest)\n                        stored_files.append(file_name)\n                    except Exception as e:\n                        logger.warning(f\"Failed to upload tool file '{file_name}' to sandbox: {e}\")\n                elif self.file_store_backend:\n                    self.file_store_backend.store(\n                        file_path=file_name,\n                        content=content,\n                        content_type=content_type,\n                        metadata={\"description\": file_description, \"source\": \"tool_generated\"},\n                        overwrite=True,\n                    )\n                    stored_files.append(file_name)\n\n            elif isinstance(file, bytes):\n                file_name = f\"file_{id(file)}.bin\"\n                file_description = f\"Tool-{tool.name}-generated file\"\n                content_type = \"application/octet-stream\"\n\n                if self.sandbox_backend:\n                    try:\n                        dest = f\"{self.sandbox_backend.base_path}/{file_name}\"\n                        self.sandbox_backend.upload_file(file_name, file, destination_path=dest)\n                        stored_files.append(file_name)\n                    except Exception as e:\n                        logger.warning(f\"Failed to upload tool file '{file_name}' to sandbox: {e}\")\n                elif self.file_store_backend:\n                    self.file_store_backend.store(\n                        file_path=file_name,\n                        content=file,\n                        content_type=content_type,\n                        metadata={\"description\": file_description, \"source\": \"tool_generated\"},\n                        overwrite=True,\n                    )\n                    stored_files.append(file_name)\n            else:\n                logger.warning(f\"Unsupported file type from tool '{tool.name}': {type(file)}\")\n\n        if stored_files:\n            logger.info(f\"Tool '{tool.name}' generated {len(stored_files)} file(s): {stored_files}\")\n\n    def _upload_files_to_sandbox(self, normalized_files: list) -&gt; None:\n        \"\"\"Upload file-like objects to the sandbox backend.\"\"\"\n        for file_obj in normalized_files:\n            file_name = getattr(file_obj, \"name\", None)\n            if file_name and hasattr(file_obj, \"read\"):\n                try:\n                    if hasattr(file_obj, \"seek\"):\n                        file_obj.seek(0)\n                    content = file_obj.read()\n                    if isinstance(content, str):\n                        content = content.encode(\"utf-8\")\n                    self.sandbox_backend.upload_file(file_name, content)\n                except Exception as e:\n                    logger.warning(f\"Failed to upload file {file_name} to sandbox: {e}\")\n\n    def _upload_files_to_file_store(self, normalized_files: list) -&gt; None:\n        \"\"\"Store file-like objects in the file store backend.\"\"\"\n        for file_obj in normalized_files:\n            file_name = getattr(file_obj, \"name\", None)\n            if not file_name or not hasattr(file_obj, \"read\"):\n                continue\n            try:\n                if hasattr(file_obj, \"seek\"):\n                    file_obj.seek(0)\n                content = file_obj.read()\n                if isinstance(content, str):\n                    content = content.encode(\"utf-8\")\n                description = getattr(file_obj, \"description\", \"User-provided file\")\n                self.file_store_backend.store(\n                    file_path=file_name,\n                    content=content,\n                    content_type=\"application/octet-stream\",\n                    metadata={\"description\": description, \"source\": \"user_upload\"},\n                    overwrite=True,\n                )\n            except Exception as e:\n                logger.warning(f\"Failed to store file {file_name} in file store: {e}\")\n\n    def _setup_in_memory_file_store_and_tools(self) -&gt; None:\n        \"\"\"Create in-memory file store and file tools when files are uploaded and no sandbox/file store exists.\"\"\"\n        self.file_store = FileStoreConfig(enabled=True, backend=InMemoryFileStore())\n        self.tools.extend(\n            [\n                FileReadTool(file_store=self.file_store_backend, llm=self.llm),\n                FileSearchTool(file_store=self.file_store_backend),\n                FileListTool(file_store=self.file_store_backend),\n            ]\n        )\n        new_tool_description = self.tool_description\n        self.system_prompt_manager.set_initial_variable(\"tool_description\", new_tool_description)\n        if self.system_prompt_manager._prompt_blocks.get(\"tools\") == \"\":\n            from dynamiq.nodes.agents.agent import Agent\n\n            if isinstance(self, Agent):\n                self.system_prompt_manager.setup_for_react_agent(\n                    inference_mode=self.inference_mode,\n                    parallel_tool_calls_enabled=self.parallel_tool_calls_enabled,\n                    has_tools=True,\n                    delegation_allowed=self.delegation_allowed,\n                    context_compaction_enabled=self.summarization_config.enabled,\n                    todo_management_enabled=(self.file_store.enabled and self.file_store.todo_enabled)\n                    or bool(self.sandbox_backend),\n                )\n\n    def _inject_attached_files_into_message(\n        self, input_message: Message | VisionMessage, files: list[io.BytesIO]\n    ) -&gt; Message | VisionMessage:\n        if not files:\n            return input_message\n\n        if not isinstance(input_message, Message):\n            return input_message\n\n        file_lines = []\n\n        for f in files:\n            name = getattr(f, \"name\", None) or \"unnamed_file\"\n            description = getattr(f, \"description\", \"\") or \"\"\n            description = description.strip()\n            if description:\n                file_lines.append(f\"- {name}: {description}\")\n            else:\n                file_lines.append(f\"- {name}\")\n\n        if not file_lines:\n            return input_message\n\n        file_section = \"\\n\".join([\"\\nAttached files available to you:\"] + file_lines) + \"\\n\"\n        preview_section = self._build_file_previews_section(files)\n        if preview_section:\n            file_section = f\"{file_section}{preview_section}\"\n\n        if isinstance(input_message.content, str):\n            input_message.content = f\"{input_message.content.rstrip()}{file_section}\"\n        else:\n            input_message.content = input_message.content + file_section\n\n        return input_message\n\n    def _build_file_previews_section(self, files: list[io.BytesIO]) -&gt; str:\n        \"\"\"Build a short, truncated preview section for uploaded files.\"\"\"\n        if not files or self.file_attachment_preview_bytes &lt;= 0:\n            return \"\"\n\n        previews: list[str] = []\n        max_bytes = max(1, self.file_attachment_preview_bytes)\n        for file_obj in files:\n            preview = self._extract_file_preview(file_obj, max_bytes)\n            if preview:\n                previews.append(preview)\n\n        if not previews:\n            return \"\"\n\n        return \"\\n\".join([\"File previews (truncated, may be incomplete):\", *previews]) + \"\\n\"\n\n    @staticmethod\n    def _extract_file_preview(file_obj: io.BytesIO, max_bytes: int) -&gt; str:\n        \"\"\"Extract a textual/hex preview from a BytesIO without consuming it.\"\"\"\n        if not hasattr(file_obj, \"read\"):\n            return \"\"\n\n        seekable = hasattr(file_obj, \"seek\")\n        position = 0\n        if seekable:\n            try:\n                position = file_obj.tell()\n            except Exception:\n                seekable = False\n\n        try:\n            if seekable:\n                file_obj.seek(0)\n            snippet = file_obj.read(max_bytes)\n        except Exception:\n            return \"\"\n        finally:\n            if seekable:\n                try:\n                    file_obj.seek(position)\n                except Exception as exc:\n                    logger.debug(\n                        \"Failed to restore file pointer for preview on %s: %s\", getattr(file_obj, \"name\", \"\"), exc\n                    )\n\n        if not snippet:\n            return \"\"\n\n        try:\n            preview_text = snippet.decode(\"utf-8\")\n            descriptor = \"text\"\n        except UnicodeDecodeError:\n            preview_text = snippet.hex()\n            descriptor = \"hex\"\n\n        suffix = \"...\" if len(snippet) &gt;= max_bytes else \"\"\n        name = getattr(file_obj, \"name\", \"uploaded_file\")\n        return f\"- {name} ({descriptor} preview): {preview_text}{suffix}\"\n\n    @property\n    def file_store_backend(self) -&gt; FileStore | None:\n        \"\"\"Get the file store backend from the configuration if enabled.\"\"\"\n        return self.file_store.backend if self.file_store.enabled else None\n\n    @property\n    def sandbox_backend(self) -&gt; Sandbox | None:\n        \"\"\"Get the sandbox backend from the configuration if enabled.\"\"\"\n        return self.sandbox.backend if self.sandbox and self.sandbox.enabled else None\n\n    @property\n    def tool_description(self) -&gt; str:\n        \"\"\"Returns a description of the tools available to the agent.\"\"\"\n        return \"\\n\".join([f\"- {tool.name}: {tool.description.strip()}\" for tool in self.tools]) if self.tools else \"\"\n\n    @property\n    def tool_names(self) -&gt; str:\n        \"\"\"Returns a comma-separated list of tool names available to the agent.\"\"\"\n        return \",\".join([self.sanitize_tool_name(tool.name) for tool in self.tools])\n\n    @property\n    def tool_by_names(self) -&gt; dict[str, Node]:\n        \"\"\"Returns a dictionary mapping tool names to their corresponding Node objects.\"\"\"\n        return {self.sanitize_tool_name(tool.name): tool for tool in self.tools}\n\n    def reset_run_state(self):\n        \"\"\"Resets the agent's run state.\"\"\"\n        self._run_depends = []\n        self._tool_cache: dict[ToolCacheEntry, Any] = {}\n        self.system_prompt_manager.reset()\n\n    def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n        \"\"\"Generates the prompt using specified blocks and variables.\"\"\"\n        return self.system_prompt_manager.generate_prompt(block_names=block_names, **kwargs)\n\n    def _build_child_agent_context(self, child_agent: \"Agent\") -&gt; dict[str, Any]:\n        \"\"\"Return context for child agents with per-agent ids to isolate their memory.\"\"\"\n        if not self._current_call_context:\n            return {}\n\n        suffix_raw = getattr(child_agent, \"name\", None) or getattr(child_agent, \"id\", None) or \"subagent\"\n        suffix_clean = self.sanitize_tool_name(str(suffix_raw)) or \"subagent\"\n        child_context: dict[str, Any] = {}\n\n        for ctx_key in (\"user_id\", \"session_id\"):\n            base_val = self._current_call_context.get(ctx_key)\n            if base_val:\n                child_context[ctx_key] = f\"{base_val}:{suffix_clean}\"\n\n        if metadata := self._current_call_context.get(\"metadata\"):\n            child_context[\"metadata\"] = metadata\n\n        return child_context\n\n    def get_clone_attr_initializers(self) -&gt; dict[str, Callable[[Node], Any]]:\n        base = super().get_clone_attr_initializers()\n        from dynamiq.prompts import Prompt\n\n        base.update(\n            {\n                \"_prompt\": (lambda _self: Prompt(messages=[]) if Prompt else None),\n            }\n        )\n        return base\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.file_store_backend","title":"<code>file_store_backend: FileStore | None</code>  <code>property</code>","text":"<p>Get the file store backend from the configuration if enabled.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.sandbox_backend","title":"<code>sandbox_backend: Sandbox | None</code>  <code>property</code>","text":"<p>Get the sandbox backend from the configuration if enabled.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_by_names","title":"<code>tool_by_names: dict[str, Node]</code>  <code>property</code>","text":"<p>Returns a dictionary mapping tool names to their corresponding Node objects.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_description","title":"<code>tool_description: str</code>  <code>property</code>","text":"<p>Returns a description of the tools available to the agent.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_names","title":"<code>tool_names: str</code>  <code>property</code>","text":"<p>Returns a comma-separated list of tool names available to the agent.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.execute","title":"<code>execute(input_data, input_message=None, config=None, **kwargs)</code>","text":"<p>Executes the agent with the given input data.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def execute(\n    self,\n    input_data: AgentInputSchema,\n    input_message: Message | VisionMessage | None = None,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the agent with the given input data.\n    \"\"\"\n    # Convert to dict only for logging (to avoid logging BytesIO objects)\n    log_data = input_data.model_dump()\n    if log_data.get(\"images\"):\n        log_data[\"images\"] = [f\"image_{i}\" for i in range(len(log_data[\"images\"]))]\n    if log_data.get(\"files\"):\n        log_data[\"files\"] = [f\"file_{i}\" for i in range(len(log_data[\"files\"]))]\n\n    logger.info(f\"Agent {self.name} - {self.id}: started with input {log_data}\")\n    self.reset_run_state()\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    custom_metadata = self._prepare_metadata(input_data)\n    self._current_call_context = {\n        \"user_id\": input_data.user_id,\n        \"session_id\": input_data.session_id,\n        \"metadata\": custom_metadata,\n    }\n\n    input_message = input_message or self.input_message or Message(role=MessageRole.USER, content=input_data.input)\n    # Convert to dict for format_message, excluding fields that are unsafe for templates\n    # (binary data like files/images, complex objects like tool_params, and input which is already handled)\n    standard_fields = set(AgentInputSchema.model_fields.keys())\n    extra_fields = input_data.model_dump(exclude=standard_fields)\n    input_message = input_message.format_message(**extra_fields)\n\n    use_memory = self.memory and (input_data.user_id or input_data.session_id)\n\n    if use_memory:\n        history_messages = self._retrieve_memory(input_data)\n        if len(history_messages) &gt; 0:\n            history_messages.insert(\n                0,\n                Message(\n                    role=MessageRole.SYSTEM,\n                    content=\"Below is the previous conversation history. \"\n                    \"Use this context to inform your response.\",\n                ),\n            )\n        if isinstance(input_message, Message):\n            memory_content = input_message.content\n        else:\n            text_parts = [\n                content.text for content in input_message.content if isinstance(content, VisionMessageTextContent)\n            ]\n            memory_content = \" \".join(text_parts) if text_parts else \"Image input\"\n        self.memory.add(role=MessageRole.USER, content=memory_content, metadata=custom_metadata)\n    else:\n        history_messages = None\n\n    files = input_data.files\n    if files:\n        normalized_files = self._ensure_named_files(files)\n        if self.sandbox_backend:\n            self._upload_files_to_sandbox(normalized_files)\n        else:\n            if not self.file_store_backend:\n                self._setup_in_memory_file_store_and_tools()\n            if self.file_store_backend:\n                self._upload_files_to_file_store(normalized_files)\n        input_message = self._inject_attached_files_into_message(input_message, normalized_files)\n\n    if input_data.tool_params:\n        kwargs[\"tool_params\"] = input_data.tool_params\n\n    self.system_prompt_manager.update_variables(dict(input_data))\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    kwargs.pop(\"run_depends\", None)\n\n    try:\n        result = self._run_agent(input_message, history_messages, config=config, **kwargs)\n    finally:\n        self._current_call_context = None\n\n    if use_memory:\n        self.memory.add(role=MessageRole.ASSISTANT, content=result, metadata=custom_metadata)\n\n    execution_result = {\n        \"content\": result,\n    }\n\n    requested_paths = getattr(self, \"_requested_output_files\", None)\n\n    if self.file_store_backend and requested_paths:\n        try:\n            stored_files = self.file_store_backend.list_files_bytes(requested_paths)\n        except Exception as e:\n            logger.warning(f\"Agent {self.name} - {self.id}: failed to collect files from file store: {e}\")\n            stored_files = []\n        if stored_files:\n            execution_result[\"files\"] = stored_files\n            logger.info(\n                f\"Agent {self.name} - {self.id}: \"\n                f\"returning {len(stored_files)} requested file(s) from file store\"\n            )\n\n    if self.sandbox_backend and requested_paths:\n        try:\n            sandbox_files = self.sandbox_backend.collect_files(file_paths=requested_paths)\n        except Exception as e:\n            logger.warning(f\"Agent {self.name} - {self.id}: failed to collect files from sandbox: {e}\")\n            sandbox_files = []\n        if sandbox_files:\n            existing_files = execution_result.get(\"files\", [])\n            execution_result[\"files\"] = existing_files + sandbox_files\n            logger.info(\n                f\"Agent {self.name} - {self.id}: \"\n                f\"returning {len(sandbox_files)} requested file(s) from sandbox\"\n            )\n\n    logger.info(f\"Node {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n    return execution_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.generate_prompt","title":"<code>generate_prompt(block_names=None, **kwargs)</code>","text":"<p>Generates the prompt using specified blocks and variables.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n    \"\"\"Generates the prompt using specified blocks and variables.\"\"\"\n    return self.system_prompt_manager.generate_prompt(block_names=block_names, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.get_context_for_input_schema","title":"<code>get_context_for_input_schema()</code>","text":"<p>Provides context for input schema that is required for proper validation.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def get_context_for_input_schema(self) -&gt; dict:\n    \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n    role_for_validation = self.role or \"\"\n    if role_for_validation and (\n        \"{% raw %}\" not in role_for_validation and \"{% endraw %}\" not in role_for_validation\n    ):\n        role_for_validation = f\"{{% raw %}}{role_for_validation}{{% endraw %}}\"\n    return {\"input_message\": self.input_message, \"role\": role_for_validation}\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components for the manager and agents.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize components for the manager and agents.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n\n    for tool in self.tools:\n        if tool.is_postponed_component_init:\n            tool.init_components(connection_manager)\n        tool.is_optimized_for_agents = True\n\n    self._ensure_skills_ingested_for_sandbox()\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Resets the agent's run state.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Resets the agent's run state.\"\"\"\n    self._run_depends = []\n    self._tool_cache: dict[ToolCacheEntry, Any] = {}\n    self.system_prompt_manager.reset()\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.retrieve_conversation_history","title":"<code>retrieve_conversation_history(user_query=None, user_id=None, session_id=None, limit=None, strategy=MemoryRetrievalStrategy.ALL)</code>","text":"<p>Retrieves conversation history for the agent using the specified strategy.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>Current user input to find relevant context (for RELEVANT/HYBRID strategies)</p> <code>None</code> <code>user_id</code> <code>str</code> <p>Optional user identifier</p> <code>None</code> <code>session_id</code> <code>str</code> <p>Optional session identifier</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of messages to return (defaults to memory_limit)</p> <code>None</code> <code>strategy</code> <code>MemoryRetrievalStrategy</code> <p>Which retrieval strategy to use (ALL, RELEVANT, or HYBRID)</p> <code>ALL</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages forming a valid conversation context</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def retrieve_conversation_history(\n    self,\n    user_query: str = None,\n    user_id: str = None,\n    session_id: str = None,\n    limit: int = None,\n    strategy: MemoryRetrievalStrategy = MemoryRetrievalStrategy.ALL,\n) -&gt; list[Message]:\n    \"\"\"\n    Retrieves conversation history for the agent using the specified strategy.\n\n    Args:\n        user_query: Current user input to find relevant context (for RELEVANT/HYBRID strategies)\n        user_id: Optional user identifier\n        session_id: Optional session identifier\n        limit: Maximum number of messages to return (defaults to memory_limit)\n        strategy: Which retrieval strategy to use (ALL, RELEVANT, or HYBRID)\n\n    Returns:\n        List of messages forming a valid conversation context\n    \"\"\"\n    if not self.memory or not (user_id or session_id):\n        return []\n\n    filters = {}\n    if user_id:\n        filters[\"user_id\"] = user_id\n    if session_id:\n        filters[\"session_id\"] = session_id\n\n    limit = limit or self.memory_limit\n\n    if strategy == MemoryRetrievalStrategy.RELEVANT and not user_query:\n        logger.warning(\"RELEVANT strategy selected but no user_query provided - falling back to ALL\")\n        strategy = MemoryRetrievalStrategy.ALL\n\n    conversation = self.memory.get_agent_conversation(\n        query=user_query,\n        limit=limit,\n        filters=filters,\n        strategy=strategy,\n    )\n    return conversation\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.sanitize_tool_name","title":"<code>sanitize_tool_name(s)</code>","text":"<p>Sanitize tool name to follow [^a-zA-Z0-9_-].</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def sanitize_tool_name(self, s: str):\n    \"\"\"Sanitize tool name to follow [^a-zA-Z0-9_-].\"\"\"\n    s = s.replace(\" \", \"-\")\n    sanitized = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", s)\n    return sanitized\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.set_block","title":"<code>set_block(block_name, content)</code>","text":"<p>Adds or updates a prompt block.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def set_block(self, block_name: str, content: str):\n    \"\"\"Adds or updates a prompt block.\"\"\"\n    self.system_prompt_manager.set_block(block_name, content)\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.set_prompt_variable","title":"<code>set_prompt_variable(variable_name, value)</code>","text":"<p>Sets or updates a prompt variable.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def set_prompt_variable(self, variable_name: str, value: Any):\n    \"\"\"Sets or updates a prompt variable.\"\"\"\n    self.system_prompt_manager.set_variable(variable_name, value)\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.stream_content","title":"<code>stream_content(content, source, step, config=None, **kwargs)</code>","text":"<p>Streams data.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | dict</code> <p>Data that will be streamed.</p> required <code>source</code> <code>str</code> <p>Source of the content.</p> required <code>step</code> <code>str</code> <p>Description of the step.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | dict</code> <p>str | dict: Streamed data.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def stream_content(\n    self,\n    content: str | dict,\n    source: str,\n    step: str,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; str | dict:\n    \"\"\"\n    Streams data.\n\n    Args:\n        content (str | dict): Data that will be streamed.\n        source (str): Source of the content.\n        step (str): Description of the step.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str | dict: Streamed data.\n    \"\"\"\n    if not isinstance(source, str):\n        raise ValueError(\n            f\"stream_content source parameter must be a string, got {type(source).__name__}: {source}. \"\n            f\"This likely indicates incorrect parameter passing from the calling code.\"\n        )\n\n    return self.stream_response(content=content, source=source, step=step, config=config, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n\n    tools_to_serialize = [t for t in self.tools if t.id not in self._excluded_tool_ids]\n    data[\"tools\"] = [tool.to_dict(**kwargs) for tool in tools_to_serialize]\n    data[\"tools\"] = data[\"tools\"] + [mcp_server.to_dict(**kwargs) for mcp_server in self._mcp_servers]\n\n    data[\"memory\"] = self.memory.to_dict(**kwargs) if self.memory else None\n    if self.files:\n        data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n    if self.images:\n        data[\"images\"] = [{\"name\": getattr(f, \"name\", f\"image_{i}\")} for i, f in enumerate(self.images)]\n\n    data[\"file_store\"] = self.file_store.to_dict(**kwargs) if self.file_store else None\n    data[\"sandbox\"] = self.sandbox.to_dict(**kwargs) if self.sandbox else None\n    data[\"skills\"] = self.skills.to_dict(**kwargs)\n\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager","title":"<code>AgentManager</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Manager class that extends the Agent class to include specific actions.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class AgentManager(Agent):\n    \"\"\"Manager class that extends the Agent class to include specific actions.\"\"\"\n\n    _actions: dict[str, Callable] = PrivateAttr(default_factory=dict)\n    name: str = \"Agent Manager\"\n    input_schema: ClassVar[type[AgentManagerInputSchema]] = AgentManagerInputSchema\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._init_actions()\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"_actions\"] = {\n            k: getattr(action, \"__name__\", str(action))\n            for k, action in self._actions.items()\n        }\n        return data\n\n    def _init_actions(self):\n        \"\"\"Initializes the default actions for the manager.\"\"\"\n        self._actions = {\n            \"plan\": self._plan,\n            \"assign\": self._assign,\n            \"final\": self._final,\n            \"handle_input\": self._handle_input,\n        }\n\n    def add_action(self, name: str, action: Callable):\n        \"\"\"Adds a custom action to the manager.\"\"\"\n        self._actions[name] = action\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n        return {\"actions\": list(self._actions.keys())}\n\n    def execute(\n        self, input_data: AgentManagerInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Executes the manager agent with the given input data and action.\"\"\"\n        log_data = dict(input_data).copy()\n\n        if log_data.get(\"images\"):\n            log_data[\"images\"] = [f\"image_{i}\" for i in range(len(log_data[\"images\"]))]\n\n        if log_data.get(\"files\"):\n            log_data[\"files\"] = [f\"file_{i}\" for i in range(len(log_data[\"files\"]))]\n\n        logger.info(f\"Agent {self.name} - {self.id}: started with input {log_data}\")\n        self.reset_run_state()\n        config = config or RunnableConfig()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        action = input_data.action\n\n        self.system_prompt_manager.update_variables(dict(input_data))\n\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n        _result_llm = self._actions[action](config=config, **kwargs)\n        result = {\"action\": action, \"result\": _result_llm}\n\n        execution_result = {\n            \"content\": result,\n        }\n        logger.info(f\"Agent {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n        return execution_result\n\n    def _plan(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'plan' action.\"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"plan\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n\n        return llm_result\n\n    def _assign(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'assign' action.\"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"assign\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n\n        return llm_result\n\n    def _final(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'final' action.\"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"final\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n        if self.streaming.enabled:\n            return self.stream_content(\n                content=llm_result,\n                step=\"manager_final_output\",\n                source=self.name,\n                config=config,\n                **kwargs,\n            )\n        return llm_result\n\n    def _handle_input(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Executes the single 'handle_input' action to either respond or plan\n        based on user request complexity.\n        \"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"handle_input\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n        return llm_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.add_action","title":"<code>add_action(name, action)</code>","text":"<p>Adds a custom action to the manager.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def add_action(self, name: str, action: Callable):\n    \"\"\"Adds a custom action to the manager.\"\"\"\n    self._actions[name] = action\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the manager agent with the given input data and action.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def execute(\n    self, input_data: AgentManagerInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Executes the manager agent with the given input data and action.\"\"\"\n    log_data = dict(input_data).copy()\n\n    if log_data.get(\"images\"):\n        log_data[\"images\"] = [f\"image_{i}\" for i in range(len(log_data[\"images\"]))]\n\n    if log_data.get(\"files\"):\n        log_data[\"files\"] = [f\"file_{i}\" for i in range(len(log_data[\"files\"]))]\n\n    logger.info(f\"Agent {self.name} - {self.id}: started with input {log_data}\")\n    self.reset_run_state()\n    config = config or RunnableConfig()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    action = input_data.action\n\n    self.system_prompt_manager.update_variables(dict(input_data))\n\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    kwargs.pop(\"run_depends\", None)\n    _result_llm = self._actions[action](config=config, **kwargs)\n    result = {\"action\": action, \"result\": _result_llm}\n\n    execution_result = {\n        \"content\": result,\n    }\n    logger.info(f\"Agent {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n    return execution_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.get_context_for_input_schema","title":"<code>get_context_for_input_schema()</code>","text":"<p>Provides context for input schema that is required for proper validation.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def get_context_for_input_schema(self) -&gt; dict:\n    \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n    return {\"actions\": list(self._actions.keys())}\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"_actions\"] = {\n        k: getattr(action, \"__name__\", str(action))\n        for k, action in self._actions.items()\n    }\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentStatus","title":"<code>AgentStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Represents the status of an agent's execution.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class AgentStatus(str, Enum):\n    \"\"\"Represents the status of an agent's execution.\"\"\"\n\n    SUCCESS = \"success\"\n    FAIL = \"fail\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for streaming chunks with choices containing delta updates.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunk(BaseModel):\n    \"\"\"Model for streaming chunks with choices containing delta updates.\"\"\"\n\n    choices: list[StreamChunkChoice]\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoice","title":"<code>StreamChunkChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stream chunk choice model.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunkChoice(BaseModel):\n    \"\"\"Stream chunk choice model.\"\"\"\n\n    delta: StreamChunkChoiceDelta\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoiceDelta","title":"<code>StreamChunkChoiceDelta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Delta model for content chunks.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunkChoiceDelta(BaseModel):\n    \"\"\"Delta model for content chunks.\"\"\"\n    content: str | dict\n    source: str\n    step: str\n\n    @field_validator('source')\n    @classmethod\n    def validate_source(cls, v):\n        \"\"\"Ensure source is always a string.\"\"\"\n        if not isinstance(v, str):\n            raise ValueError(f\"source must be a string, got {type(v).__name__}: {v}\")\n        return v\n\n    def _recursive_serialize(self, obj, key_path: str = \"\", index: int = None):\n        \"\"\"Recursively serialize an object, converting any BytesIO objects to FileInfo objects.\"\"\"\n        if isinstance(obj, io.BytesIO):\n            return convert_bytesio_to_file_info(obj, key_path, index).model_dump()\n\n        elif isinstance(obj, dict):\n            result = {}\n            for k, v in obj.items():\n                new_key_path = f\"{key_path}.{k}\" if key_path else k\n                result[k] = self._recursive_serialize(v, new_key_path)\n            return result\n\n        elif isinstance(obj, list):\n            result = []\n\n            for i, item in enumerate(obj):\n                new_key_path = f\"{key_path}[{i}]\" if key_path else f\"item_{i}\"\n                result.append(self._recursive_serialize(item, new_key_path, i))\n            return result\n\n        else:\n            return obj\n\n    @model_serializer\n    def serialize_content(self):\n        \"\"\"Serialize content dict, converting any BytesIO objects to base64 strings while preserving key structure.\"\"\"\n        if self.content is None or not isinstance(self.content, dict):\n            return {\"content\": self.content, \"source\": self.source, \"step\": self.step}\n\n        serialized_content = self._recursive_serialize(self.content)\n\n        result = {\n            \"content\": serialized_content,\n            \"source\": self.source,\n            \"step\": self.step,\n        }\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoiceDelta.serialize_content","title":"<code>serialize_content()</code>","text":"<p>Serialize content dict, converting any BytesIO objects to base64 strings while preserving key structure.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>@model_serializer\ndef serialize_content(self):\n    \"\"\"Serialize content dict, converting any BytesIO objects to base64 strings while preserving key structure.\"\"\"\n    if self.content is None or not isinstance(self.content, dict):\n        return {\"content\": self.content, \"source\": self.source, \"step\": self.step}\n\n    serialized_content = self._recursive_serialize(self.content)\n\n    result = {\n        \"content\": serialized_content,\n        \"source\": self.source,\n        \"step\": self.step,\n    }\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoiceDelta.validate_source","title":"<code>validate_source(v)</code>  <code>classmethod</code>","text":"<p>Ensure source is always a string.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>@field_validator('source')\n@classmethod\ndef validate_source(cls, v):\n    \"\"\"Ensure source is always a string.\"\"\"\n    if not isinstance(v, str):\n        raise ValueError(f\"source must be a string, got {type(v).__name__}: {v}\")\n    return v\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.ActionParsingException","title":"<code>ActionParsingException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when an action cannot be parsed. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class ActionParsingException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when an action cannot be parsed. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.AgentUnknownToolException","title":"<code>AgentUnknownToolException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when a unknown tool is requested. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class AgentUnknownToolException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when a unknown tool is requested. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.InvalidActionException","title":"<code>InvalidActionException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when invalid action is chosen. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class InvalidActionException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when invalid action is chosen. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.JSONParsingError","title":"<code>JSONParsingError</code>","text":"<p>               Bases: <code>ParsingError</code></p> <p>Exception raised when expected JSON content within XML is invalid.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class JSONParsingError(ParsingError):\n    \"\"\"Exception raised when expected JSON content within XML is invalid.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.MaxLoopsExceededException","title":"<code>MaxLoopsExceededException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when the agent exceeds the maximum number of allowed loops.</p> <p>This exception is recoverable, meaning the agent can continue after catching this exception.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class MaxLoopsExceededException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when the agent exceeds the maximum number of allowed loops.\n\n    This exception is recoverable, meaning the agent can continue after catching this exception.\n    \"\"\"\n\n    def __init__(\n        self, message: str = \"Maximum number of loops reached without finding a final answer.\", recoverable: bool = True\n    ):\n        super().__init__(message, recoverable=recoverable)\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.OutputFileNotFoundError","title":"<code>OutputFileNotFoundError</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when files listed in  do not exist on the backend. Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class OutputFileNotFoundError(RecoverableAgentException):\n    \"\"\"Exception raised when files listed in &lt;output_files&gt; do not exist on the backend.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.ParsingError","title":"<code>ParsingError</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Base class for parsing errors.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class ParsingError(RecoverableAgentException):\n    \"\"\"Base class for parsing errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.RecoverableAgentException","title":"<code>RecoverableAgentException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for recoverable agent errors.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class RecoverableAgentException(Exception):\n    \"\"\"\n    Base exception class for recoverable agent errors.\n    \"\"\"\n\n    def __init__(self, *args, recoverable: bool = True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.recoverable = recoverable\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.TagNotFoundError","title":"<code>TagNotFoundError</code>","text":"<p>               Bases: <code>ParsingError</code></p> <p>Exception raised when required XML tags are missing.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class TagNotFoundError(ParsingError):\n    \"\"\"Exception raised when required XML tags are missing.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.ToolExecutionException","title":"<code>ToolExecutionException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when a tools fails to execute. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class ToolExecutionException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when a tools fails to execute. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.XMLParsingError","title":"<code>XMLParsingError</code>","text":"<p>               Bases: <code>ParsingError</code></p> <p>Exception raised when XML structure is invalid or cannot be parsed.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class XMLParsingError(ParsingError):\n    \"\"\"Exception raised when XML structure is invalid or cannot be parsed.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/","title":"Utils","text":""},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.FileMappedInput","title":"<code>FileMappedInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structure for storing file mapped inputs.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>class FileMappedInput(BaseModel):\n    \"\"\"Structure for storing file mapped inputs.\"\"\"\n\n    input: Any\n    files: list[io.BytesIO]  # List of BytesIO objects or FileInfo objects\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_serializer(\"files\")\n    def serialize_files(self, files: list[io.BytesIO]) -&gt; list[str]:\n        return [getattr(file, \"name\", f\"file_{i}\") for i, file in enumerate(files)]\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.SummarizationConfig","title":"<code>SummarizationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for agent history summarization.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether summarization is enabled. Defaults to False.</p> <code>max_token_context_length</code> <code>int | None</code> <p>Maximum number of tokens in prompt after which summarization will be applied. Defaults to None.</p> <code>context_usage_ratio</code> <code>float</code> <p>Relative percentage of tokens in prompt after which summarization will be applied.</p> <code>preserve_last_messages</code> <code>int</code> <p>Number of most recent messages to keep verbatim (not summarized). These are excluded from summarization and re-appended after the older history is replaced with a summary. 0 means all messages are summarized.</p> <code>token_budget_ratio</code> <code>float</code> <p>Fraction of the LLM context window used for input messages during summarization. The remainder is reserved for the summarization prompt and generated output. Defaults to 0.75.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>class SummarizationConfig(BaseModel):\n    \"\"\"Configuration for agent history summarization.\n\n    Attributes:\n        enabled (bool): Whether summarization is enabled. Defaults to False.\n        max_token_context_length (int | None): Maximum number of tokens in prompt after\n          which summarization will be applied. Defaults to None.\n        context_usage_ratio (float): Relative percentage of tokens in prompt after which summarization will be applied.\n        preserve_last_messages (int): Number of most recent messages to keep verbatim\n          (not summarized). These are excluded from summarization and re-appended after\n          the older history is replaced with a summary. 0 means all messages are summarized.\n        token_budget_ratio (float): Fraction of the LLM context window used for input\n          messages during summarization. The remainder is reserved for the summarization\n          prompt and generated output. Defaults to 0.75.\n    \"\"\"\n\n    enabled: bool = False\n    max_token_context_length: int | None = None\n    context_usage_ratio: float = Field(default=0.8, gt=0, le=1)\n    preserve_last_messages: int = Field(default=4, ge=0)\n    token_budget_ratio: float = Field(default=0.75, gt=0, lt=1)\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.ToolCacheEntry","title":"<code>ToolCacheEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single key entry in tool cache.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>class ToolCacheEntry(BaseModel):\n    \"\"\"Single key entry in tool cache.\"\"\"\n\n    action: str\n    action_input: dict | str\n\n    model_config = ConfigDict(frozen=True)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def convert_to_str(cls, data):\n        if isinstance(data, dict):\n            data[\"action_input\"] = json.dumps(data.get('action_input'), sort_keys=True)\n        return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser","title":"<code>XMLParser</code>","text":"<p>Utility class for parsing XML-like output, often generated by LLMs. Prioritizes lxml for robustness, with fallbacks for common issues.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>class XMLParser:\n    \"\"\"\n    Utility class for parsing XML-like output, often generated by LLMs.\n    Prioritizes lxml for robustness, with fallbacks for common issues.\n    \"\"\"\n\n    DEFAULT_PRESERVE_TAGS = [\"answer\", \"thought\"]\n\n    @staticmethod\n    def _clean_content(text: str) -&gt; str:\n        \"\"\"\n        Cleans the input string to remove common LLM artifacts and isolate XML.\n\n        Args:\n            text (str): The input string to be cleaned\n\n        Returns:\n            str: Cleaned string containing only the XML content\n        \"\"\"\n        if not isinstance(text, str):\n            return \"\"\n\n        cleaned = text.strip()\n\n        if cleaned.startswith(\"```\") and cleaned.endswith(\"```\"):\n            cleaned = re.sub(r\"^```.*?\\n\", \"\", cleaned)\n            cleaned = re.sub(r\"\\n```$\", \"\", cleaned)\n\n        xml_matches = list(re.finditer(r\"&lt;(\\w+)\\b[^&gt;]*&gt;.*?&lt;/\\1&gt;\", cleaned, re.DOTALL))\n        if xml_matches:\n            for match in reversed(xml_matches):\n                candidate = match.group(0)\n                if \"&lt;answer\" in candidate:\n                    cleaned = candidate\n                    break\n            else:\n                cleaned = xml_matches[-1].group(0)\n\n        return cleaned\n\n    @staticmethod\n    def extract_content_with_regex_fallback(text: str, tag_name: str) -&gt; str | None:\n        \"\"\"\n        Extract tag content using regex as a fallback when XML parsing fails.\n        Works with both complete and incomplete tags.\n\n        Args:\n            text (str): The XML-like text to extract content from\n            tag_name (str): The name of the tag to extract content from\n\n        Returns:\n            str | None: The extracted content if found, None otherwise\n        \"\"\"\n        complete_pattern = f\"&lt;{tag_name}[^&gt;]*&gt;(.*?)&lt;/{tag_name}&gt;\"\n        complete_match = re.search(complete_pattern, text, re.DOTALL)\n\n        if complete_match:\n            content = complete_match.group(1).strip()\n            if content:\n                return content\n            return None\n\n        incomplete_pattern = f\"&lt;{tag_name}[^&gt;]*&gt;(.*?)(?=&lt;(?!/{tag_name})|$)\"\n        incomplete_match = re.search(incomplete_pattern, text, re.DOTALL)\n\n        if incomplete_match:\n            content = incomplete_match.group(1).strip()\n            if content:\n                return content\n\n        return None\n\n    @staticmethod\n    def preprocess_xml_content(\n        text: str, required_tags: Sequence[str] = None, optional_tags: Sequence[str] = None\n    ) -&gt; dict[str, tuple[str, str]]:\n        \"\"\"\n        Extract raw content for all tags that need special handling before parsing.\n        Filters tags based on required and optional tags if provided.\n\n        Args:\n            text (str): The XML-like text to preprocess\n            required_tags (Sequence[str], optional): List of tags that must be present\n            optional_tags (Sequence[str], optional): List of tags that may be present\n\n        Returns:\n            dict[str, tuple[str, str]]: Dictionary mapping tag names to (modified_text, raw_content) tuples\n        \"\"\"\n        extracted_contents = {}\n        tags_to_process = set(XMLParser.DEFAULT_PRESERVE_TAGS)\n\n        # Add required and optional tags to the set if provided\n        if required_tags:\n            tags_to_process.update(required_tags)\n        if optional_tags:\n            tags_to_process.update(optional_tags)\n\n        text = XMLParser._escape_unbalanced_reserved_tags(text, tags_to_process)\n\n        for tag in tags_to_process:\n            content = XMLParser.extract_content_with_regex_fallback(text, tag)\n            if content:\n                tag_pattern = f\"&lt;{tag}[^&gt;]*&gt;.*?&lt;/{tag}&gt;\"\n                modified_text = re.sub(tag_pattern, f\"&lt;{tag}&gt;CONTENT_PLACEHOLDER_{tag}&lt;/{tag}&gt;\", text, flags=re.DOTALL)\n                extracted_contents[tag] = (modified_text, content)\n                text = modified_text\n\n        return extracted_contents\n\n    @staticmethod\n    def _escape_unbalanced_reserved_tags(text: str, tags: Sequence[str]) -&gt; str:\n        \"\"\"Escape reserved-tag tokens that appear in prose instead of well-formed XML.\"\"\"\n\n        if not text:\n            return text\n\n        reserved = set(tags)\n        token_pattern = re.compile(r\"&lt;/?([A-Za-z_][\\w\\-]*)\\b[^&gt;]*&gt;\")\n        stack: list[tuple[str, int, int]] = []\n        spans_to_escape: list[tuple[int, int]] = []\n\n        for match in token_pattern.finditer(text):\n            tag = match.group(1)\n            if tag not in reserved:\n                continue\n\n            token = match.group(0)\n            is_closing = token.startswith(\"&lt;/\")\n\n            if not is_closing:\n                stack.append((tag, match.start(), match.end()))\n                continue\n\n            if stack and stack[-1][0] == tag:\n                stack.pop()\n                continue\n\n            found_matching_open = any(open_tag == tag for open_tag, _, _ in stack)\n            if found_matching_open:\n                while stack and stack[-1][0] != tag:\n                    _, open_start, open_end = stack.pop()\n                    spans_to_escape.append((open_start, open_end))\n                if stack and stack[-1][0] == tag:\n                    stack.pop()\n            else:\n                spans_to_escape.append((match.start(), match.end()))\n\n        for _, open_start, open_end in stack:\n            spans_to_escape.append((open_start, open_end))\n\n        if not spans_to_escape:\n            return text\n\n        for start, end in sorted(spans_to_escape, reverse=True):\n            token = text[start:end]\n            escaped = token.replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n            text = text[:start] + escaped + text[end:]\n\n        return text\n\n    @staticmethod\n    def _parse_with_lxml(cleaned_text: str) -&gt; LET._Element | None:\n        \"\"\"\n        Attempts to parse the cleaned text using lxml with recovery.\n\n        Args:\n            cleaned_text (str): The cleaned XML text to parse\n\n        Returns:\n            LET._Element | None: The parsed XML element if successful, None otherwise\n        \"\"\"\n        if not cleaned_text:\n            return None\n        try:\n            tags_to_check = [\"thought\", \"answer\", \"action\", \"action_input\", \"output\"]\n            fixed_text = cleaned_text\n\n            for tag in tags_to_check:\n                opening_count = len(re.findall(f\"&lt;{tag}[^&gt;]*&gt;\", fixed_text))\n                closing_count = len(re.findall(f\"&lt;/{tag}&gt;\", fixed_text))\n\n                if opening_count &gt; closing_count:\n                    logger.debug(f\"XMLParser: Adding missing &lt;/{tag}&gt; tags\")\n                    if tag == \"output\":\n                        fixed_text += f\"&lt;/{tag}&gt;\"\n                    else:\n                        pos = fixed_text.find(f\"&lt;{tag}\")\n                        if pos &gt;= 0:\n                            next_tag_pos = fixed_text.find(\"&lt;\", pos + 1)\n                            if next_tag_pos &gt; 0 and f\"&lt;/{tag}&gt;\" not in fixed_text[pos:next_tag_pos]:\n                                fixed_text = fixed_text[:next_tag_pos] + f\"&lt;/{tag}&gt;\" + fixed_text[next_tag_pos:]\n\n            parser = LET.XMLParser(recover=True, encoding=\"utf-8\")\n            root = LET.fromstring(fixed_text.encode(\"utf-8\"), parser=parser)  # nosec: B320\n            return root\n        except LET.XMLSyntaxError as e:\n            logger.warning(f\"XMLParser: lxml parsing failed with recovery: {e}. Content: {cleaned_text[:200]}...\")\n            return None\n        except Exception as e:\n            logger.error(f\"XMLParser: Unexpected error during parsing: {e}. Content: {cleaned_text[:200]}...\")\n            return None\n\n    @staticmethod\n    def _extract_data_lxml(\n        root: LET._Element,\n        required_tags: Sequence[str],\n        optional_tags: Sequence[str] = None,\n        preserve_format_tags: Sequence[str] = None,\n    ) -&gt; dict[str, str]:\n        \"\"\"\n        Extracts text content from specified tags using XPath.\n\n        Args:\n            root (LET._Element): The root XML element to extract data from\n            required_tags (Sequence[str]): Tags that must be present in the output\n            optional_tags (Sequence[str], optional): Tags to extract if present\n            preserve_format_tags (Sequence[str], optional): Tags where original formatting should be preserved\n\n        Returns:\n            dict[str, str]: Dictionary mapping tag names to their extracted content\n\n        Raises:\n            TagNotFoundError: If a required tag is missing or empty\n        \"\"\"\n        data = {}\n        optional_tags = optional_tags or []\n        preserve_format_tags = list(preserve_format_tags or [])\n\n        for tag in XMLParser.DEFAULT_PRESERVE_TAGS:\n            if tag not in preserve_format_tags:\n                preserve_format_tags.append(tag)\n\n        all_tags = list(required_tags) + list(optional_tags)\n\n        for tag in all_tags:\n            tag_content = None\n            element_found = False\n            elements = root.xpath(f\".//{tag}\")\n            if elements:\n                element_found = True\n                for elem in elements:\n                    if tag in preserve_format_tags:\n                        xml_content = LET.tostring(elem, encoding=\"unicode\", method=\"xml\")\n                        tag_pattern = re.compile(f\"&lt;{tag}[^&gt;]*&gt;(.*?)&lt;/{tag}&gt;\", re.DOTALL)\n                        match = tag_pattern.search(xml_content)\n                        text = match.group(1) if match else \"\"\n                    else:\n                        text = \"\".join(elem.itertext()).strip()\n\n                    if not text and tag in required_tags:\n                        raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; found but contains no text content.\")\n\n                    if text:\n                        if text.startswith(\"CONTENT_PLACEHOLDER_\"):\n                            continue\n                        tag_content = text\n                        break\n\n            if not element_found:\n                try:\n                    if root.getparent() is not None:\n                        parent_elements = root.xpath(f\"../{tag}\")\n                        if parent_elements:\n                            element_found = True\n                            for elem in parent_elements:\n                                text_parent_child = \"\".join(elem.itertext()).strip()\n                                if text_parent_child:\n                                    tag_content = text_parent_child\n                                    break\n                except (AttributeError, Exception) as e:\n                    logger.debug(f\"XMLParser: Error checking parent for tag '{tag}': {e}\")\n                    pass\n\n            if tag_content is not None:\n                data[tag] = tag_content\n            elif element_found and tag in required_tags and tag_content is None:\n                raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; found but contains no text content.\")\n            elif not element_found and tag in required_tags:\n                raise TagNotFoundError(\n                    f\"Required tag &lt;{tag}&gt; not found in the XML structure \"\n                    f\"relative to the parsed root element ('{root.tag}') or its parent.\"\n                )\n\n        missing_required_after_all = [tag for tag in required_tags if tag not in data]\n        if missing_required_after_all:\n            raise TagNotFoundError(f\"Required tags missing after extraction: {', '.join(missing_required_after_all)}\")\n\n        return data\n\n    @staticmethod\n    def _repair_json_newlines_in_strings(text: str) -&gt; str:\n        \"\"\"\n        Replace literal newlines inside double-quoted JSON string values with\n        the escape sequence \\\\n so that json.loads can parse LLM output that\n        used real line breaks instead of escaped newlines.\n        \"\"\"\n        result = []\n        i = 0\n        in_double_quote = False\n        escape_next = False\n        while i &lt; len(text):\n            c = text[i]\n            if escape_next:\n                result.append(c)\n                escape_next = False\n                i += 1\n                continue\n            if c == \"\\\\\" and in_double_quote:\n                escape_next = True\n                result.append(c)\n                i += 1\n                continue\n            if c == '\"' and not escape_next:\n                in_double_quote = not in_double_quote\n                result.append(c)\n                i += 1\n                continue\n            if in_double_quote and c in (\"\\r\", \"\\n\"):\n                result.append(\"\\\\n\")\n                if c == \"\\r\" and i + 1 &lt; len(text) and text[i + 1] == \"\\n\":\n                    i += 1\n                i += 1\n                continue\n            result.append(c)\n            i += 1\n        return \"\".join(result)\n\n    @staticmethod\n    def _unescape_html_in_json_values(obj: Any) -&gt; Any:\n        \"\"\"Recursively unescape &amp;lt; &amp;gt; &amp;amp; in string values so tool inputs get real &lt; &gt; &amp;.\"\"\"\n        if isinstance(obj, str):\n            return html.unescape(obj)\n        if isinstance(obj, dict):\n            return {k: XMLParser._unescape_html_in_json_values(v) for k, v in obj.items()}\n        if isinstance(obj, list):\n            return [XMLParser._unescape_html_in_json_values(v) for v in obj]\n        return obj\n\n    @staticmethod\n    def _parse_json_fields(data: dict[str, str], json_fields: Sequence[str]) -&gt; dict[str, Any]:\n        \"\"\"\n        Parses specified fields in the data dictionary as JSON.\n\n        Args:\n            data (dict[str, str]): Dictionary of extracted tag contents\n            json_fields (Sequence[str]): List of field names to parse as JSON\n\n        Returns:\n            dict[str, Any]: Dictionary with specified fields parsed as JSON objects\n\n        Raises:\n            JSONParsingError: If a JSON field cannot be parsed correctly\n        \"\"\"\n        parsed_data = data.copy()\n        for field in json_fields:\n            if field in parsed_data:\n                json_string = re.sub(r\"^```(?:json)?\\s*|```$\", \"\", parsed_data[field].strip())\n                try:\n                    try:\n                        parsed_data[field] = json.loads(json_string)\n                    except json.JSONDecodeError:\n                        repaired = XMLParser._repair_json_newlines_in_strings(json_string)\n                        parsed_data[field] = json.loads(repaired)\n                    parsed_data[field] = XMLParser._unescape_html_in_json_values(parsed_data[field])\n                except json.JSONDecodeError as e:\n                    error_message = (\n                        f\"Failed to parse JSON content for field '{field}'. \"\n                        f\"Error: {e}. Original content: '{parsed_data[field][:100]}...'\"\n                    )\n                    guidance = (\n                        \" Ensure the value is valid JSON with double quotes for keys and strings, \"\n                        'and proper escaping for special characters (e.g., \\\\n for newlines, \\\\\" for quotes).'\n                    )\n                    raise JSONParsingError(error_message + guidance)\n                except Exception as e:\n                    raise JSONParsingError(f\"Unexpected error parsing JSON for field '{field}': {e}\")\n        return parsed_data\n\n    @staticmethod\n    def parse(\n        text: str,\n        required_tags: Sequence[str],\n        optional_tags: Sequence[str] = None,\n        json_fields: Sequence[str] = None,\n        preserve_format_tags: Sequence[str] = None,\n        attempt_wrap: bool = True,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Parse XML-like text to extract structured data from specified tags.\n\n        This function employs a multi-stage parsing strategy:\n        1. First cleans and preprocesses the input text\n        2. Attempts extraction using regex for tags needing special handling\n        3. Tries robust XML parsing with lxml (with auto-repair capabilities)\n        4. Falls back to regex extraction for any remaining tags\n        5. Processes JSON fields if specified\n\n        Each stage has fallback mechanisms to handle various edge cases commonly\n        seen in LLM-generated XML, including malformed tags, missing closing tags,\n        and inconsistent formatting.\n\n        Args:\n            text (str): The XML-like text to parse\n            required_tags (Sequence[str]): Tags that must be present in the output\n            optional_tags (Sequence[str], optional): Tags to extract if present\n            json_fields (Sequence[str], optional): Fields to parse as JSON objects\n            preserve_format_tags (Sequence[str], optional): Tags where original formatting\n                                                           should be preserved\n            attempt_wrap (bool, optional): Whether to try wrapping content in a root tag\n                                          if initial parsing fails\n\n        Returns:\n            dict[str, Any]: Dictionary mapping tag names to their extracted content\n\n        Raises:\n            ParsingError: If the input is empty or invalid\n            TagNotFoundError: If a required tag is missing or empty\n            JSONParsingError: If a JSON field cannot be parsed\n        \"\"\"\n        optional_tags = optional_tags or []\n        json_fields = json_fields or []\n        preserve_format_tags = list(preserve_format_tags or [])\n\n        required_optional_set = set(required_tags) | set(optional_tags)\n\n        for tag in XMLParser.DEFAULT_PRESERVE_TAGS:\n            if tag not in preserve_format_tags:\n                preserve_format_tags.append(tag)\n        cleaned_text = XMLParser._clean_content(text)\n        if not cleaned_text:\n            if text and text.strip():\n                cleaned_text = text.strip()\n            else:\n                if required_tags:\n                    raise ParsingError(\"Input text is empty or became empty after cleaning.\")\n                else:\n                    return {}\n\n        tags_to_process = set(XMLParser.DEFAULT_PRESERVE_TAGS)\n        if required_tags:\n            tags_to_process.update(required_tags)\n        if optional_tags:\n            tags_to_process.update(optional_tags)\n\n        extracted_contents = XMLParser.preprocess_xml_content(cleaned_text, required_tags, optional_tags)\n\n        modified_text = cleaned_text\n        if extracted_contents:\n            for tag, (tag_modified_text, _) in extracted_contents.items():\n                modified_text = tag_modified_text\n\n        result = {}\n        for tag, (_, content) in extracted_contents.items():\n            if tag in required_optional_set:\n                result[tag] = content\n\n        remaining_required = [tag for tag in required_tags if tag not in result]\n        remaining_optional = [tag for tag in optional_tags if tag not in result]\n\n        try:\n            root = XMLParser._parse_with_lxml(modified_text)\n\n            if root is None and attempt_wrap:\n                wrapped_text = f\"&lt;root&gt;{modified_text}&lt;/root&gt;\"\n                root = XMLParser._parse_with_lxml(wrapped_text)\n\n            if root is not None and (remaining_required or remaining_optional):\n                xml_data = XMLParser._extract_data_lxml(\n                    root, remaining_required, remaining_optional, preserve_format_tags\n                )\n                result.update(xml_data)\n                remaining_required = [tag for tag in remaining_required if tag not in result]\n                remaining_optional = [tag for tag in remaining_optional if tag not in result]\n        except TagNotFoundError as e:\n            logger.debug(\n                \"XMLParser: lxml extraction missing tag (will retry via fallback): %s\",\n                e,\n            )\n        except Exception as e:\n            logger.warning(f\"XMLParser: XML parsing failed: {e}\")\n\n        for tag in remaining_required:\n            content = XMLParser.extract_content_with_regex_fallback(text, tag)\n            if content:\n                result[tag] = content\n            else:\n                empty_tag_pattern = f\"&lt;{tag}[^&gt;]*&gt;\\\\s*&lt;/{tag}&gt;\"\n                if re.search(empty_tag_pattern, text):\n                    raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; found but contains no text content.\")\n                else:\n                    raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; not found even with fallback methods\")\n\n        for tag in remaining_optional:\n            content = XMLParser.extract_content_with_regex_fallback(text, tag)\n            if content:\n                result[tag] = content\n\n        if result:\n            result = XMLParser._restore_placeholder_mentions(result, tags_to_process)\n\n        if json_fields and result:\n            result = XMLParser._parse_json_fields(result, json_fields)\n\n        return result\n\n    @staticmethod\n    def _restore_placeholder_mentions(data: dict[str, Any], tags: Sequence[str]) -&gt; dict[str, Any]:\n        \"\"\"Replace placeholder artifacts with escaped tag mentions.\"\"\"\n\n        restored = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                for tag in tags:\n                    placeholder = f\"&lt;{tag}&gt;CONTENT_PLACEHOLDER_{tag}&lt;/{tag}&gt;\"\n                    if placeholder in value:\n                        value = value.replace(placeholder, f\"&amp;lt;{tag}&amp;gt;\")\n                    # Escape any remaining raw occurrences of reserved tags\n                    value = value.replace(f\"&lt;{tag}&gt;\", f\"&amp;lt;{tag}&amp;gt;\")\n                    value = value.replace(f\"&lt;/{tag}&gt;\", f\"&amp;lt;/{tag}&amp;gt;\")\n                restored[key] = value\n            else:\n                restored[key] = value\n\n        return restored\n\n    @staticmethod\n    def extract_first_tag_lxml(text: str, tags: Sequence[str]) -&gt; str | None:\n        \"\"\"\n        Extracts the text content of the first tag found from the list using lxml.\n        Useful for simple cases like extracting just the final answer.\n\n        Args:\n            text (str): The XML-like text to extract content from\n            tags (Sequence[str]): Ordered list of tags to look for\n\n        Returns:\n            str | None: Content of the first found tag, or None if no tags are found\n        \"\"\"\n        cleaned_text = XMLParser._clean_content(text)\n        if not cleaned_text:\n            return None\n\n        root = XMLParser._parse_with_lxml(cleaned_text)\n\n        if root is None:\n            wrapped_text = f\"&lt;root&gt;{cleaned_text}&lt;/root&gt;\"\n            root = XMLParser._parse_with_lxml(wrapped_text)\n\n        if root is None:\n            logger.warning(f\"XMLParser: extract_first_tag_lxml failed to parse: {cleaned_text[:200]}...\")\n            return None\n\n        for tag in tags:\n            elements = root.xpath(f\".//{tag}\")\n            if elements:\n                for elem in elements:\n                    content = \"\".join(elem.itertext()).strip()\n                    if content:\n                        return content\n        return None\n\n    @staticmethod\n    def extract_first_tag_regex(text: str, tags: Sequence[str]) -&gt; str | None:\n        \"\"\"\n        Fallback method: Extracts the text content of the first tag found using regex.\n        Less reliable than lxml, use only when lxml fails completely.\n\n        Args:\n            text (str): The XML-like text to extract content from\n            tags (Sequence[str]): Ordered list of tags to look for\n\n        Returns:\n            str | None: Content of the first found tag, or None if no tags are found\n        \"\"\"\n        if not isinstance(text, str):\n            return None\n\n        for tag in tags:\n            match = re.search(f\"&lt;{tag}\\\\b[^&gt;]*&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL | re.IGNORECASE)\n            if match:\n                content = match.group(1).strip()\n                if content:\n                    return content\n        return None\n\n    @staticmethod\n    def parse_unified_xml_format(text: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Parse XML content using a unified structure for both tool calls and final answers.\n\n        Args:\n            text (str): The XML content to parse\n\n        Returns:\n            dict: A dictionary containing parsed data with the following structure:\n                For tool calls:\n                {\n                    \"is_final\": False,\n                    \"thought\": \"extracted thought text\",\n                    \"tools\": [\n                        {\"name\": \"tool name\", \"input\": parsed_json_input},\n                        {\"name\": \"tool name\", \"input\": parsed_json_input},\n                        ...\n                    ]\n                }\n\n                For final answer:\n                {\n                    \"is_final\": True,\n                    \"thought\": \"extracted thought text\",\n                    \"answer\": \"final answer text\"\n                }\n\n        Raises:\n            XMLParsingError: If the XML cannot be parsed\n            TagNotFoundError: If required tags are missing\n        \"\"\"\n        try:\n            cleaned_text = XMLParser._clean_content(text)\n            if not cleaned_text:\n                raise XMLParsingError(\"Empty or invalid XML content\")\n\n            try:\n                parsed_data = XMLParser.parse(cleaned_text, required_tags=[\"thought\", \"answer\"], optional_tags=[\"o\"])\n                return {\"is_final\": True, \"thought\": parsed_data.get(\"thought\"), \"answer\": parsed_data.get(\"answer\")}\n            except TagNotFoundError:\n                pass\n\n            root = XMLParser._parse_with_lxml(cleaned_text)\n            if root is None:\n\n                wrapped_text = f\"&lt;root&gt;{cleaned_text}&lt;/root&gt;\"\n                root = XMLParser._parse_with_lxml(wrapped_text)\n\n            if root is None:\n                raise XMLParsingError(\"Failed to parse XML content with lxml\")\n\n            thought_elems = root.xpath(\".//thought\")\n            if not thought_elems:\n                raise TagNotFoundError(\"Required tag &lt;thought&gt; not found\")\n\n            thought = \"\".join(thought_elems[0].itertext()).strip()\n\n            tool_calls_elem = None\n            for tag_name in [\"tool_calls\", \"tools\"]:\n                elems = root.xpath(f\".//{tag_name}\")\n                if elems:\n                    tool_calls_elem = elems[0]\n                    break\n\n            if tool_calls_elem is None:\n                raise TagNotFoundError(\"Required tag &lt;tool_calls&gt; or &lt;tools&gt; not found\")\n\n            tools = []\n            for tool_elem in tool_calls_elem.xpath(\".//tool\"):\n\n                name_elem = None\n                for name_tag in [\"n\", \"name\", \"tool_name\"]:\n                    name_elems = tool_elem.xpath(f\".//{name_tag}\")\n                    if name_elems:\n                        name_elem = name_elems[0]\n                        break\n\n                if name_elem is None:\n                    continue\n\n                input_elem = None\n                for input_tag in [\"input\", \"tool_input\"]:\n                    input_elems = tool_elem.xpath(f\".//{input_tag}\")\n                    if input_elems:\n                        input_elem = input_elems[0]\n                        break\n\n                if input_elem is None:\n                    continue\n\n                tool_name = \"\".join(name_elem.itertext()).strip()\n                input_json_str = \"\".join(input_elem.itertext()).strip()\n\n                try:\n\n                    input_json_str = re.sub(r\"^```(?:json)?\\s*|```$\", \"\", input_json_str.strip())\n                    tool_input = json.loads(input_json_str)\n                except json.JSONDecodeError as e:\n                    logger.warning(f\"Failed to parse JSON for tool {tool_name}: {e}\")\n                    continue\n\n                tools.append({\"name\": tool_name, \"input\": tool_input})\n\n            if not tools:\n                raise TagNotFoundError(\"No valid tool elements found with both name and input tags\")\n\n            return {\"is_final\": False, \"thought\": thought, \"tools\": tools}\n\n        except (XMLParsingError, TagNotFoundError):\n            raise\n        except Exception as e:\n            raise XMLParsingError(f\"Error parsing XML: {str(e)}\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.extract_content_with_regex_fallback","title":"<code>extract_content_with_regex_fallback(text, tag_name)</code>  <code>staticmethod</code>","text":"<p>Extract tag content using regex as a fallback when XML parsing fails. Works with both complete and incomplete tags.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML-like text to extract content from</p> required <code>tag_name</code> <code>str</code> <p>The name of the tag to extract content from</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The extracted content if found, None otherwise</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef extract_content_with_regex_fallback(text: str, tag_name: str) -&gt; str | None:\n    \"\"\"\n    Extract tag content using regex as a fallback when XML parsing fails.\n    Works with both complete and incomplete tags.\n\n    Args:\n        text (str): The XML-like text to extract content from\n        tag_name (str): The name of the tag to extract content from\n\n    Returns:\n        str | None: The extracted content if found, None otherwise\n    \"\"\"\n    complete_pattern = f\"&lt;{tag_name}[^&gt;]*&gt;(.*?)&lt;/{tag_name}&gt;\"\n    complete_match = re.search(complete_pattern, text, re.DOTALL)\n\n    if complete_match:\n        content = complete_match.group(1).strip()\n        if content:\n            return content\n        return None\n\n    incomplete_pattern = f\"&lt;{tag_name}[^&gt;]*&gt;(.*?)(?=&lt;(?!/{tag_name})|$)\"\n    incomplete_match = re.search(incomplete_pattern, text, re.DOTALL)\n\n    if incomplete_match:\n        content = incomplete_match.group(1).strip()\n        if content:\n            return content\n\n    return None\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.extract_first_tag_lxml","title":"<code>extract_first_tag_lxml(text, tags)</code>  <code>staticmethod</code>","text":"<p>Extracts the text content of the first tag found from the list using lxml. Useful for simple cases like extracting just the final answer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML-like text to extract content from</p> required <code>tags</code> <code>Sequence[str]</code> <p>Ordered list of tags to look for</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Content of the first found tag, or None if no tags are found</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef extract_first_tag_lxml(text: str, tags: Sequence[str]) -&gt; str | None:\n    \"\"\"\n    Extracts the text content of the first tag found from the list using lxml.\n    Useful for simple cases like extracting just the final answer.\n\n    Args:\n        text (str): The XML-like text to extract content from\n        tags (Sequence[str]): Ordered list of tags to look for\n\n    Returns:\n        str | None: Content of the first found tag, or None if no tags are found\n    \"\"\"\n    cleaned_text = XMLParser._clean_content(text)\n    if not cleaned_text:\n        return None\n\n    root = XMLParser._parse_with_lxml(cleaned_text)\n\n    if root is None:\n        wrapped_text = f\"&lt;root&gt;{cleaned_text}&lt;/root&gt;\"\n        root = XMLParser._parse_with_lxml(wrapped_text)\n\n    if root is None:\n        logger.warning(f\"XMLParser: extract_first_tag_lxml failed to parse: {cleaned_text[:200]}...\")\n        return None\n\n    for tag in tags:\n        elements = root.xpath(f\".//{tag}\")\n        if elements:\n            for elem in elements:\n                content = \"\".join(elem.itertext()).strip()\n                if content:\n                    return content\n    return None\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.extract_first_tag_regex","title":"<code>extract_first_tag_regex(text, tags)</code>  <code>staticmethod</code>","text":"<p>Fallback method: Extracts the text content of the first tag found using regex. Less reliable than lxml, use only when lxml fails completely.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML-like text to extract content from</p> required <code>tags</code> <code>Sequence[str]</code> <p>Ordered list of tags to look for</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Content of the first found tag, or None if no tags are found</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef extract_first_tag_regex(text: str, tags: Sequence[str]) -&gt; str | None:\n    \"\"\"\n    Fallback method: Extracts the text content of the first tag found using regex.\n    Less reliable than lxml, use only when lxml fails completely.\n\n    Args:\n        text (str): The XML-like text to extract content from\n        tags (Sequence[str]): Ordered list of tags to look for\n\n    Returns:\n        str | None: Content of the first found tag, or None if no tags are found\n    \"\"\"\n    if not isinstance(text, str):\n        return None\n\n    for tag in tags:\n        match = re.search(f\"&lt;{tag}\\\\b[^&gt;]*&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL | re.IGNORECASE)\n        if match:\n            content = match.group(1).strip()\n            if content:\n                return content\n    return None\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.parse","title":"<code>parse(text, required_tags, optional_tags=None, json_fields=None, preserve_format_tags=None, attempt_wrap=True)</code>  <code>staticmethod</code>","text":"<p>Parse XML-like text to extract structured data from specified tags.</p> <p>This function employs a multi-stage parsing strategy: 1. First cleans and preprocesses the input text 2. Attempts extraction using regex for tags needing special handling 3. Tries robust XML parsing with lxml (with auto-repair capabilities) 4. Falls back to regex extraction for any remaining tags 5. Processes JSON fields if specified</p> <p>Each stage has fallback mechanisms to handle various edge cases commonly seen in LLM-generated XML, including malformed tags, missing closing tags, and inconsistent formatting.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML-like text to parse</p> required <code>required_tags</code> <code>Sequence[str]</code> <p>Tags that must be present in the output</p> required <code>optional_tags</code> <code>Sequence[str]</code> <p>Tags to extract if present</p> <code>None</code> <code>json_fields</code> <code>Sequence[str]</code> <p>Fields to parse as JSON objects</p> <code>None</code> <code>preserve_format_tags</code> <code>Sequence[str]</code> <p>Tags where original formatting                                            should be preserved</p> <code>None</code> <code>attempt_wrap</code> <code>bool</code> <p>Whether to try wrapping content in a root tag                           if initial parsing fails</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary mapping tag names to their extracted content</p> <p>Raises:</p> Type Description <code>ParsingError</code> <p>If the input is empty or invalid</p> <code>TagNotFoundError</code> <p>If a required tag is missing or empty</p> <code>JSONParsingError</code> <p>If a JSON field cannot be parsed</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef parse(\n    text: str,\n    required_tags: Sequence[str],\n    optional_tags: Sequence[str] = None,\n    json_fields: Sequence[str] = None,\n    preserve_format_tags: Sequence[str] = None,\n    attempt_wrap: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Parse XML-like text to extract structured data from specified tags.\n\n    This function employs a multi-stage parsing strategy:\n    1. First cleans and preprocesses the input text\n    2. Attempts extraction using regex for tags needing special handling\n    3. Tries robust XML parsing with lxml (with auto-repair capabilities)\n    4. Falls back to regex extraction for any remaining tags\n    5. Processes JSON fields if specified\n\n    Each stage has fallback mechanisms to handle various edge cases commonly\n    seen in LLM-generated XML, including malformed tags, missing closing tags,\n    and inconsistent formatting.\n\n    Args:\n        text (str): The XML-like text to parse\n        required_tags (Sequence[str]): Tags that must be present in the output\n        optional_tags (Sequence[str], optional): Tags to extract if present\n        json_fields (Sequence[str], optional): Fields to parse as JSON objects\n        preserve_format_tags (Sequence[str], optional): Tags where original formatting\n                                                       should be preserved\n        attempt_wrap (bool, optional): Whether to try wrapping content in a root tag\n                                      if initial parsing fails\n\n    Returns:\n        dict[str, Any]: Dictionary mapping tag names to their extracted content\n\n    Raises:\n        ParsingError: If the input is empty or invalid\n        TagNotFoundError: If a required tag is missing or empty\n        JSONParsingError: If a JSON field cannot be parsed\n    \"\"\"\n    optional_tags = optional_tags or []\n    json_fields = json_fields or []\n    preserve_format_tags = list(preserve_format_tags or [])\n\n    required_optional_set = set(required_tags) | set(optional_tags)\n\n    for tag in XMLParser.DEFAULT_PRESERVE_TAGS:\n        if tag not in preserve_format_tags:\n            preserve_format_tags.append(tag)\n    cleaned_text = XMLParser._clean_content(text)\n    if not cleaned_text:\n        if text and text.strip():\n            cleaned_text = text.strip()\n        else:\n            if required_tags:\n                raise ParsingError(\"Input text is empty or became empty after cleaning.\")\n            else:\n                return {}\n\n    tags_to_process = set(XMLParser.DEFAULT_PRESERVE_TAGS)\n    if required_tags:\n        tags_to_process.update(required_tags)\n    if optional_tags:\n        tags_to_process.update(optional_tags)\n\n    extracted_contents = XMLParser.preprocess_xml_content(cleaned_text, required_tags, optional_tags)\n\n    modified_text = cleaned_text\n    if extracted_contents:\n        for tag, (tag_modified_text, _) in extracted_contents.items():\n            modified_text = tag_modified_text\n\n    result = {}\n    for tag, (_, content) in extracted_contents.items():\n        if tag in required_optional_set:\n            result[tag] = content\n\n    remaining_required = [tag for tag in required_tags if tag not in result]\n    remaining_optional = [tag for tag in optional_tags if tag not in result]\n\n    try:\n        root = XMLParser._parse_with_lxml(modified_text)\n\n        if root is None and attempt_wrap:\n            wrapped_text = f\"&lt;root&gt;{modified_text}&lt;/root&gt;\"\n            root = XMLParser._parse_with_lxml(wrapped_text)\n\n        if root is not None and (remaining_required or remaining_optional):\n            xml_data = XMLParser._extract_data_lxml(\n                root, remaining_required, remaining_optional, preserve_format_tags\n            )\n            result.update(xml_data)\n            remaining_required = [tag for tag in remaining_required if tag not in result]\n            remaining_optional = [tag for tag in remaining_optional if tag not in result]\n    except TagNotFoundError as e:\n        logger.debug(\n            \"XMLParser: lxml extraction missing tag (will retry via fallback): %s\",\n            e,\n        )\n    except Exception as e:\n        logger.warning(f\"XMLParser: XML parsing failed: {e}\")\n\n    for tag in remaining_required:\n        content = XMLParser.extract_content_with_regex_fallback(text, tag)\n        if content:\n            result[tag] = content\n        else:\n            empty_tag_pattern = f\"&lt;{tag}[^&gt;]*&gt;\\\\s*&lt;/{tag}&gt;\"\n            if re.search(empty_tag_pattern, text):\n                raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; found but contains no text content.\")\n            else:\n                raise TagNotFoundError(f\"Required tag &lt;{tag}&gt; not found even with fallback methods\")\n\n    for tag in remaining_optional:\n        content = XMLParser.extract_content_with_regex_fallback(text, tag)\n        if content:\n            result[tag] = content\n\n    if result:\n        result = XMLParser._restore_placeholder_mentions(result, tags_to_process)\n\n    if json_fields and result:\n        result = XMLParser._parse_json_fields(result, json_fields)\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.parse_unified_xml_format","title":"<code>parse_unified_xml_format(text)</code>  <code>staticmethod</code>","text":"<p>Parse XML content using a unified structure for both tool calls and final answers.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML content to parse</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing parsed data with the following structure: For tool calls: {     \"is_final\": False,     \"thought\": \"extracted thought text\",     \"tools\": [         {\"name\": \"tool name\", \"input\": parsed_json_input},         {\"name\": \"tool name\", \"input\": parsed_json_input},         ...     ] }</p> <p>For final answer: {     \"is_final\": True,     \"thought\": \"extracted thought text\",     \"answer\": \"final answer text\" }</p> <p>Raises:</p> Type Description <code>XMLParsingError</code> <p>If the XML cannot be parsed</p> <code>TagNotFoundError</code> <p>If required tags are missing</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef parse_unified_xml_format(text: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Parse XML content using a unified structure for both tool calls and final answers.\n\n    Args:\n        text (str): The XML content to parse\n\n    Returns:\n        dict: A dictionary containing parsed data with the following structure:\n            For tool calls:\n            {\n                \"is_final\": False,\n                \"thought\": \"extracted thought text\",\n                \"tools\": [\n                    {\"name\": \"tool name\", \"input\": parsed_json_input},\n                    {\"name\": \"tool name\", \"input\": parsed_json_input},\n                    ...\n                ]\n            }\n\n            For final answer:\n            {\n                \"is_final\": True,\n                \"thought\": \"extracted thought text\",\n                \"answer\": \"final answer text\"\n            }\n\n    Raises:\n        XMLParsingError: If the XML cannot be parsed\n        TagNotFoundError: If required tags are missing\n    \"\"\"\n    try:\n        cleaned_text = XMLParser._clean_content(text)\n        if not cleaned_text:\n            raise XMLParsingError(\"Empty or invalid XML content\")\n\n        try:\n            parsed_data = XMLParser.parse(cleaned_text, required_tags=[\"thought\", \"answer\"], optional_tags=[\"o\"])\n            return {\"is_final\": True, \"thought\": parsed_data.get(\"thought\"), \"answer\": parsed_data.get(\"answer\")}\n        except TagNotFoundError:\n            pass\n\n        root = XMLParser._parse_with_lxml(cleaned_text)\n        if root is None:\n\n            wrapped_text = f\"&lt;root&gt;{cleaned_text}&lt;/root&gt;\"\n            root = XMLParser._parse_with_lxml(wrapped_text)\n\n        if root is None:\n            raise XMLParsingError(\"Failed to parse XML content with lxml\")\n\n        thought_elems = root.xpath(\".//thought\")\n        if not thought_elems:\n            raise TagNotFoundError(\"Required tag &lt;thought&gt; not found\")\n\n        thought = \"\".join(thought_elems[0].itertext()).strip()\n\n        tool_calls_elem = None\n        for tag_name in [\"tool_calls\", \"tools\"]:\n            elems = root.xpath(f\".//{tag_name}\")\n            if elems:\n                tool_calls_elem = elems[0]\n                break\n\n        if tool_calls_elem is None:\n            raise TagNotFoundError(\"Required tag &lt;tool_calls&gt; or &lt;tools&gt; not found\")\n\n        tools = []\n        for tool_elem in tool_calls_elem.xpath(\".//tool\"):\n\n            name_elem = None\n            for name_tag in [\"n\", \"name\", \"tool_name\"]:\n                name_elems = tool_elem.xpath(f\".//{name_tag}\")\n                if name_elems:\n                    name_elem = name_elems[0]\n                    break\n\n            if name_elem is None:\n                continue\n\n            input_elem = None\n            for input_tag in [\"input\", \"tool_input\"]:\n                input_elems = tool_elem.xpath(f\".//{input_tag}\")\n                if input_elems:\n                    input_elem = input_elems[0]\n                    break\n\n            if input_elem is None:\n                continue\n\n            tool_name = \"\".join(name_elem.itertext()).strip()\n            input_json_str = \"\".join(input_elem.itertext()).strip()\n\n            try:\n\n                input_json_str = re.sub(r\"^```(?:json)?\\s*|```$\", \"\", input_json_str.strip())\n                tool_input = json.loads(input_json_str)\n            except json.JSONDecodeError as e:\n                logger.warning(f\"Failed to parse JSON for tool {tool_name}: {e}\")\n                continue\n\n            tools.append({\"name\": tool_name, \"input\": tool_input})\n\n        if not tools:\n            raise TagNotFoundError(\"No valid tool elements found with both name and input tags\")\n\n        return {\"is_final\": False, \"thought\": thought, \"tools\": tools}\n\n    except (XMLParsingError, TagNotFoundError):\n        raise\n    except Exception as e:\n        raise XMLParsingError(f\"Error parsing XML: {str(e)}\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.XMLParser.preprocess_xml_content","title":"<code>preprocess_xml_content(text, required_tags=None, optional_tags=None)</code>  <code>staticmethod</code>","text":"<p>Extract raw content for all tags that need special handling before parsing. Filters tags based on required and optional tags if provided.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The XML-like text to preprocess</p> required <code>required_tags</code> <code>Sequence[str]</code> <p>List of tags that must be present</p> <code>None</code> <code>optional_tags</code> <code>Sequence[str]</code> <p>List of tags that may be present</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, tuple[str, str]]</code> <p>dict[str, tuple[str, str]]: Dictionary mapping tag names to (modified_text, raw_content) tuples</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>@staticmethod\ndef preprocess_xml_content(\n    text: str, required_tags: Sequence[str] = None, optional_tags: Sequence[str] = None\n) -&gt; dict[str, tuple[str, str]]:\n    \"\"\"\n    Extract raw content for all tags that need special handling before parsing.\n    Filters tags based on required and optional tags if provided.\n\n    Args:\n        text (str): The XML-like text to preprocess\n        required_tags (Sequence[str], optional): List of tags that must be present\n        optional_tags (Sequence[str], optional): List of tags that may be present\n\n    Returns:\n        dict[str, tuple[str, str]]: Dictionary mapping tag names to (modified_text, raw_content) tuples\n    \"\"\"\n    extracted_contents = {}\n    tags_to_process = set(XMLParser.DEFAULT_PRESERVE_TAGS)\n\n    # Add required and optional tags to the set if provided\n    if required_tags:\n        tags_to_process.update(required_tags)\n    if optional_tags:\n        tags_to_process.update(optional_tags)\n\n    text = XMLParser._escape_unbalanced_reserved_tags(text, tags_to_process)\n\n    for tag in tags_to_process:\n        content = XMLParser.extract_content_with_regex_fallback(text, tag)\n        if content:\n            tag_pattern = f\"&lt;{tag}[^&gt;]*&gt;.*?&lt;/{tag}&gt;\"\n            modified_text = re.sub(tag_pattern, f\"&lt;{tag}&gt;CONTENT_PLACEHOLDER_{tag}&lt;/{tag}&gt;\", text, flags=re.DOTALL)\n            extracted_contents[tag] = (modified_text, content)\n            text = modified_text\n\n    return extracted_contents\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.bytes_to_data_url","title":"<code>bytes_to_data_url(image_bytes)</code>","text":"<p>Convert image bytes to a data URL</p> <p>Parameters:</p> Name Type Description Default <code>image_bytes</code> <code>bytes</code> <p>Raw image bytes</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Data URL string (format: data:image/jpeg;base64,...)</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>def bytes_to_data_url(image_bytes: bytes) -&gt; str:\n    \"\"\"\n    Convert image bytes to a data URL\n\n    Args:\n        image_bytes (bytes): Raw image bytes\n\n    Returns:\n        str: Data URL string (format: data:image/jpeg;base64,...)\n    \"\"\"\n    try:\n        mime_type = filetype.guess_mime(image_bytes)\n        if not mime_type:\n            if image_bytes[:2] == b\"\\xff\\xd8\":\n                mime_type = \"image/jpeg\"\n            elif image_bytes[:8] == b\"\\x89PNG\\r\\n\\x1a\\n\":\n                mime_type = \"image/png\"\n            elif image_bytes[:6] in (b\"GIF87a\", b\"GIF89a\"):\n                mime_type = \"image/gif\"\n            else:\n                mime_type = \"application/octet-stream\"\n\n        encoded = base64.b64encode(image_bytes).decode(\"utf-8\")\n        return f\"data:{mime_type};base64,{encoded}\"\n    except Exception as e:\n        logger.error(f\"Error converting image to data URL: {str(e)}\")\n        raise ValueError(f\"Failed to convert image to data URL: {str(e)}\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.convert_bytesio_to_file_info","title":"<code>convert_bytesio_to_file_info(bytesio_obj, key, index=None)</code>","text":"<p>Convert a BytesIO object to a FileInfo object with base64 encoded content.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>def convert_bytesio_to_file_info(bytesio_obj: io.BytesIO, key: str, index: int = None) -&gt; FileInfo:\n    \"\"\"Convert a BytesIO object to a FileInfo object with base64 encoded content.\"\"\"\n    content_bytes = bytesio_obj.getvalue()\n\n    encoded = base64.b64encode(content_bytes).decode(\"utf-8\")\n\n    name = getattr(bytesio_obj, \"name\", f\"file_{key}\" if index is None else f\"file_{key}_{index}\")\n    content_type = getattr(bytesio_obj, \"content_type\", \"unknown\")\n    description = getattr(bytesio_obj, \"description\", \"\")\n\n    # Create a path based on the name\n    path = f\"/{name}\" if not name.startswith(\"/\") else name\n\n    return FileInfo(\n        content=encoded,\n        path=path,\n        name=name,\n        content_type=content_type,\n        metadata={\"description\": description},\n        size=len(content_bytes),\n    )\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.create_message_from_input","title":"<code>create_message_from_input(input_data)</code>","text":"<p>Create appropriate message type based on input data, automatically detecting and handling images from either images or files fields</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input data dictionary containing: - 'input': Text input string - 'images': List of image data (URLs, bytes, or BytesIO objects) - 'files': List of file data (bytes or BytesIO objects)</p> required <p>Returns:</p> Type Description <code>Message | VisionMessage</code> <p>Message or VisionMessage: Appropriate message type for the input</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>def create_message_from_input(input_data: dict) -&gt; Message | VisionMessage:\n    \"\"\"\n    Create appropriate message type based on input data,\n    automatically detecting and handling images from either images or files fields\n\n    Args:\n        input_data (dict): Input data dictionary containing:\n            - 'input': Text input string\n            - 'images': List of image data (URLs, bytes, or BytesIO objects)\n            - 'files': List of file data (bytes or BytesIO objects)\n\n    Returns:\n        Message or VisionMessage: Appropriate message type for the input\n    \"\"\"\n    text_input = input_data.get(\"input\", \"\")\n    images = input_data.get(\"images\", []) or []\n    files = input_data.get(\"files\", []) or []\n\n    if not isinstance(images, list):\n        images = [images]\n    else:\n        images = list(images)\n\n    for file in files:\n        if is_image_file(file):\n            logger.debug(f\"File detected as image, adding to vision processing: {getattr(file, 'name', 'unnamed')}\")\n            images.append(file)\n\n    if not images:\n        return Message(role=MessageRole.USER, content=text_input)\n\n    content = []\n\n    if text_input:\n        content.append(VisionMessageTextContent(text=text_input))\n\n    for image in images:\n        try:\n            if isinstance(image, str):\n                if image.startswith((\"http://\", \"https://\", \"data:\")):\n                    image_url = image\n                else:\n                    with open(image, \"rb\") as file:\n                        image_bytes = file.read()\n                        image_url = bytes_to_data_url(image_bytes)\n            else:\n                if isinstance(image, io.BytesIO):\n                    image_bytes = image.getvalue()\n                else:\n                    image_bytes = image\n                image_url = bytes_to_data_url(image_bytes)\n\n            content.append(VisionMessageImageContent(image_url=VisionMessageImageURL(url=image_url)))\n        except Exception as e:\n            logger.error(f\"Error processing image: {str(e)}\")\n\n    return VisionMessage(content=content, role=MessageRole.USER)\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.is_image_file","title":"<code>is_image_file(file)</code>","text":"<p>Determine if a file is an image by examining its content</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <p>File-like object or bytes</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file is an image, False otherwise</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>def is_image_file(file) -&gt; bool:\n    \"\"\"\n    Determine if a file is an image by examining its content\n\n    Args:\n        file: File-like object or bytes\n\n    Returns:\n        bool: True if the file is an image, False otherwise\n    \"\"\"\n    try:\n        if isinstance(file, io.BytesIO):\n            pos = file.tell()\n            file.seek(0)\n            file_bytes = file.read(32)\n            file.seek(pos)\n        elif isinstance(file, bytes):\n            file_bytes = file[:32]\n        else:\n            return False\n\n        signatures = {\n            b\"\\xff\\xd8\\xff\": \"jpg/jpeg\",  # JPEG\n            b\"\\x89PNG\\r\\n\\x1a\\n\": \"png\",  # PNG\n            b\"GIF87a\": \"gif\",  # GIF87a\n            b\"GIF89a\": \"gif\",  # GIF89a\n            b\"RIFF\": \"webp\",  # WebP\n            b\"MM\\x00*\": \"tiff\",  # TIFF (big endian)\n            b\"II*\\x00\": \"tiff\",  # TIFF (little endian)\n            b\"BM\": \"bmp\",  # BMP\n        }\n\n        for sig, fmt in signatures.items():\n            if file_bytes.startswith(sig):\n                return True\n\n        if isinstance(file, io.BytesIO):\n            pos = file.tell()\n            file.seek(0)\n            mime = filetype.guess_mime(file.read(4096))\n            file.seek(pos)\n            return mime is not None and mime.startswith(\"image/\")\n        elif isinstance(file, bytes):\n            mime = filetype.guess_mime(file)\n            return mime is not None and mime.startswith(\"image/\")\n\n        return False\n    except Exception as e:\n        logger.error(f\"Error checking if file is an image: {str(e)}\")\n        return False\n</code></pre>"},{"location":"dynamiq/nodes/agents/utils/#dynamiq.nodes.agents.utils.process_tool_output_for_agent","title":"<code>process_tool_output_for_agent(content, max_tokens=TOOL_MAX_TOKENS, truncate=True)</code>","text":"<p>Process tool output for agent consumption.</p> <p>This function converts various types of tool outputs into a string representation. It handles dictionaries (with or without a 'content' key), lists, tuples, and other types by converting them to a string. If the resulting string exceeds the maximum allowed length, it truncates the content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Any</code> <p>The output from tool execution, which can be of various types.</p> required <code>max_tokens</code> <code>int</code> <p>Maximum allowed token count for the content. The effective character limit is computed as max_tokens * CHARS_PER_TOKEN (assuming ~4 characters per token).</p> <code>TOOL_MAX_TOKENS</code> <code>truncate</code> <code>bool</code> <p>Whether to truncate the content if it exceeds the maximum length.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A processed string suitable for agent consumption.</p> Source code in <code>dynamiq/nodes/agents/utils.py</code> <pre><code>def process_tool_output_for_agent(content: Any, max_tokens: int = TOOL_MAX_TOKENS, truncate: bool = True) -&gt; str:\n    \"\"\"\n    Process tool output for agent consumption.\n\n    This function converts various types of tool outputs into a string representation.\n    It handles dictionaries (with or without a 'content' key), lists, tuples, and other\n    types by converting them to a string. If the resulting string exceeds the maximum\n    allowed length, it truncates the content.\n\n    Args:\n        content: The output from tool execution, which can be of various types.\n        max_tokens: Maximum allowed token count for the content. The effective character\n            limit is computed as max_tokens * CHARS_PER_TOKEN (assuming ~4 characters per token).\n        truncate: Whether to truncate the content if it exceeds the maximum length.\n\n    Returns:\n        A processed string suitable for agent consumption.\n    \"\"\"\n    if not isinstance(content, str):\n        if isinstance(content, dict):\n            filtered_content = {k: v for k, v in content.items() if k != \"files\"}\n\n            if \"content\" in filtered_content:\n                inner_content = filtered_content[\"content\"]\n                content = inner_content if isinstance(inner_content, str) else json.dumps(inner_content, indent=2)\n            else:\n                content = json.dumps(filtered_content, indent=2) if filtered_content else \"\"\n        elif isinstance(content, (list, tuple)):\n            content = \"\\n\".join(str(item) for item in content)\n        else:\n            content = str(content)\n\n    max_len_in_char: int = max_tokens * CHARS_PER_TOKEN\n    content = re.sub(r\"\\{\\{\\s*(.*?)\\s*\\}\\}\", r\"\\1\", content)\n\n    if len(content) &gt; max_len_in_char and truncate:\n        half_length: int = (max_len_in_char - 100) // 2\n        truncation_message: str = \"\\n...[Content truncated]...\\n\"\n        content = content[:half_length] + truncation_message + content[-half_length:]\n\n    return content\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/history_manager/","title":"History manager","text":"<p>History and context management mixin for agents.</p>"},{"location":"dynamiq/nodes/agents/components/history_manager/#dynamiq.nodes.agents.components.history_manager.HistoryManagerMixin","title":"<code>HistoryManagerMixin</code>","text":"<p>Mixin that provides conversation history and token limit management.</p> <p>This mixin expects the class using it to have the following attributes: - _prompt: Prompt object with messages - llm: LLM object with model and get_token_limit() - summarization_config: SummarizationConfig object - _history_offset: int offset for history - name: str agent name - id: str agent id</p> Source code in <code>dynamiq/nodes/agents/components/history_manager.py</code> <pre><code>class HistoryManagerMixin:\n    \"\"\"Mixin that provides conversation history and token limit management.\n\n    This mixin expects the class using it to have the following attributes:\n    - _prompt: Prompt object with messages\n    - llm: LLM object with model and get_token_limit()\n    - summarization_config: SummarizationConfig object\n    - _history_offset: int offset for history\n    - name: str agent name\n    - id: str agent id\n    \"\"\"\n\n    def is_token_limit_exceeded(self) -&gt; bool:\n        \"\"\"\n        Check whether token limit for summarization is exceeded.\n\n        Returns:\n            bool: Whether token limit is exceeded\n        \"\"\"\n        prompt_tokens = self._prompt.count_tokens(self.llm.model)\n\n        return (\n            self.summarization_config.max_token_context_length\n            and prompt_tokens &gt; self.summarization_config.max_token_context_length\n        ) or (prompt_tokens / self.llm.get_token_limit() &gt; self.summarization_config.context_usage_ratio)\n\n    def _split_history(\n        self,\n    ) -&gt; tuple[list[Message | VisionMessage], list[Message | VisionMessage]]:\n        \"\"\"Split conversation history into messages to summarize and messages to preserve.\n\n        Uses ``preserve_last_messages`` from summarization config to determine\n        the split point.  When the history is too short to split, all messages\n        go to the summarize bucket and nothing is preserved.\n\n        Returns:\n            Tuple of (to_summarize, to_preserve).\n        \"\"\"\n        preserve_count = self.summarization_config.preserve_last_messages\n        conversation_history = self._prompt.messages[self._history_offset :]\n\n        if preserve_count &gt; 0 and len(conversation_history) &gt; preserve_count:\n            to_summarize = conversation_history[:-preserve_count]\n            to_preserve = [m.copy() for m in conversation_history[-preserve_count:]]\n        else:\n            to_summarize = conversation_history\n            to_preserve = []\n\n        return to_summarize, to_preserve\n\n    def _compact_history(self, summary: str | None = None) -&gt; None:\n        \"\"\"Compact history, optionally inserting a summary before preserved messages.\n\n        Replaces the conversation history with::\n\n            [prefix] [summary] [preserved msg 1] [preserved msg 2]\n\n        The last ``preserve_last_messages`` messages are kept verbatim.\n        If *summary* is provided it is inserted as a user message between the\n        prefix and the preserved messages.\n\n        Args:\n            summary: Optional summary text to insert after prefix.\n        \"\"\"\n        _, preserved = self._split_history()\n\n        self._prompt.messages = self._prompt.messages[: self._history_offset]\n\n        if summary:\n            self._prompt.messages.append(\n                Message(role=MessageRole.USER, content=f\"\\nObservation: {summary}\\n\", static=True)\n            )\n\n        self._prompt.messages.extend(preserved)\n        logger.info(\n            f\"Agent {self.name} - {self.id}: History compacted. \"\n            f\"Summary: {'yes' if summary else 'no'}, preserved: {len(preserved)} messages.\"\n        )\n\n    @staticmethod\n    def aggregate_history(messages: list[Message | VisionMessage]) -&gt; str:\n        \"\"\"\n        Concatenates multiple history messages into one unified string.\n\n        Args:\n            messages: List of messages to aggregate\n\n        Returns:\n            str: Aggregated content\n        \"\"\"\n        history = \"\"\n\n        for message in messages:\n            if isinstance(message, VisionMessage):\n                for content in message.content:\n                    if isinstance(content, VisionMessageTextContent):\n                        history += content.text\n            else:\n                if message.role == MessageRole.ASSISTANT:\n                    history += f\"-TOOL DESCRIPTION START-\\n{message.content}\\n-TOOL DESCRIPTION END-\\n\"\n                elif message.role == MessageRole.USER:\n                    history += f\"-TOOL OUTPUT START-\\n{message.content}\\n-TOOL OUTPUT END-\\n\"\n\n        return history\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/history_manager/#dynamiq.nodes.agents.components.history_manager.HistoryManagerMixin.aggregate_history","title":"<code>aggregate_history(messages)</code>  <code>staticmethod</code>","text":"<p>Concatenates multiple history messages into one unified string.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message | VisionMessage]</code> <p>List of messages to aggregate</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Aggregated content</p> Source code in <code>dynamiq/nodes/agents/components/history_manager.py</code> <pre><code>@staticmethod\ndef aggregate_history(messages: list[Message | VisionMessage]) -&gt; str:\n    \"\"\"\n    Concatenates multiple history messages into one unified string.\n\n    Args:\n        messages: List of messages to aggregate\n\n    Returns:\n        str: Aggregated content\n    \"\"\"\n    history = \"\"\n\n    for message in messages:\n        if isinstance(message, VisionMessage):\n            for content in message.content:\n                if isinstance(content, VisionMessageTextContent):\n                    history += content.text\n        else:\n            if message.role == MessageRole.ASSISTANT:\n                history += f\"-TOOL DESCRIPTION START-\\n{message.content}\\n-TOOL DESCRIPTION END-\\n\"\n            elif message.role == MessageRole.USER:\n                history += f\"-TOOL OUTPUT START-\\n{message.content}\\n-TOOL OUTPUT END-\\n\"\n\n    return history\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/history_manager/#dynamiq.nodes.agents.components.history_manager.HistoryManagerMixin.is_token_limit_exceeded","title":"<code>is_token_limit_exceeded()</code>","text":"<p>Check whether token limit for summarization is exceeded.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether token limit is exceeded</p> Source code in <code>dynamiq/nodes/agents/components/history_manager.py</code> <pre><code>def is_token_limit_exceeded(self) -&gt; bool:\n    \"\"\"\n    Check whether token limit for summarization is exceeded.\n\n    Returns:\n        bool: Whether token limit is exceeded\n    \"\"\"\n    prompt_tokens = self._prompt.count_tokens(self.llm.model)\n\n    return (\n        self.summarization_config.max_token_context_length\n        and prompt_tokens &gt; self.summarization_config.max_token_context_length\n    ) or (prompt_tokens / self.llm.get_token_limit() &gt; self.summarization_config.context_usage_ratio)\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/parser/","title":"Parser","text":"<p>Parsing logic for Agent LLM outputs in different formats.</p>"},{"location":"dynamiq/nodes/agents/components/parser/#dynamiq.nodes.agents.components.parser.extract_default_final_answer","title":"<code>extract_default_final_answer(output)</code>","text":"<p>Extracts the final thought, optional output files, and answer from the output string in default inference mode format.</p> <p>The expected format is::</p> <pre><code>Thought: &lt;reasoning&gt;\nOutput Files: &lt;comma-separated paths&gt;   (optional)\nAnswer: &lt;response&gt;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The LLM output string containing Thought and Answer</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str, str, str]</code> <p>(thought, answer, output_files_raw) where all are strings    (empty strings if not found). output_files_raw is the raw    comma-separated value, or \"\" when the field is absent.</p> Source code in <code>dynamiq/nodes/agents/components/parser.py</code> <pre><code>def extract_default_final_answer(output: str) -&gt; tuple[str, str, str]:\n    \"\"\"\n    Extracts the final thought, optional output files, and answer from\n    the output string in default inference mode format.\n\n    The expected format is::\n\n        Thought: &lt;reasoning&gt;\n        Output Files: &lt;comma-separated paths&gt;   (optional)\n        Answer: &lt;response&gt;\n\n    Args:\n        output: The LLM output string containing Thought and Answer\n\n    Returns:\n        tuple: (thought, answer, output_files_raw) where all are strings\n               (empty strings if not found). *output_files_raw* is the raw\n               comma-separated value, or \"\" when the field is absent.\n    \"\"\"\n    match = re.search(r\"Thought:\\s*(.*?)\\s*Answer:\\s*(.*)\", output, re.DOTALL)\n    if not match:\n        return \"\", \"\", \"\"\n\n    raw_thought = match.group(1).strip()\n    answer = match.group(2).strip()\n\n    files_match = re.search(r\"^(.*)\\nOutput Files:\\s*(.*)$\", raw_thought, re.DOTALL)\n    if files_match:\n        thought = files_match.group(1).strip()\n        output_files_raw = files_match.group(2).strip()\n    else:\n        thought = raw_thought\n        output_files_raw = \"\"\n\n    return thought, answer, output_files_raw\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/parser/#dynamiq.nodes.agents.components.parser.parse_default_action","title":"<code>parse_default_action(output)</code>","text":"<p>Parses the action(s), input(s), and thought from the output string in default inference mode format.</p> <p>Supports both single tool actions and multiple sequential tool calls when multi-tool is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output string from the LLM containing Thought, Action, and Action Input</p> required <code>parallel_tool_calls_enabled</code> <p>Whether parallel tool calls are enabled</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str | None, str | None, dict | list | None]</code> <p>(thought, action, action_input) where: - thought is the extracted reasoning - action is the tool name (first action if multiple are found) - action_input is the tool input dict</p> <p>Raises:</p> Type Description <code>ActionParsingException</code> <p>If parsing fails or format is invalid</p> Source code in <code>dynamiq/nodes/agents/components/parser.py</code> <pre><code>def parse_default_action(output: str) -&gt; tuple[str | None, str | None, dict | list | None]:\n    \"\"\"\n    Parses the action(s), input(s), and thought from the output string in default inference mode format.\n\n    Supports both single tool actions and multiple sequential tool calls\n    when multi-tool is enabled.\n\n    Args:\n        output: The output string from the LLM containing Thought, Action, and Action Input\n        parallel_tool_calls_enabled: Whether parallel tool calls are enabled\n\n    Returns:\n        tuple: (thought, action, action_input) where:\n            - thought is the extracted reasoning\n            - action is the tool name (first action if multiple are found)\n            - action_input is the tool input dict\n\n    Raises:\n        ActionParsingException: If parsing fails or format is invalid\n    \"\"\"\n    try:\n        thought = parse_default_thought(output) or None\n\n        actions = []\n        action_input_matches = list(re.finditer(r\"Action Input:\", output))\n\n        for input_match in action_input_matches:\n            preceding_text = output[: input_match.start()]\n\n            # Find the last \"Action:\" before this \"Action Input:\"\n            # Match action name up to newline or end of string\n            all_actions = list(re.finditer(r\"Action:\\s*(.+?)(?=\\n|$|Action)\", preceding_text))\n\n            if not all_actions:\n                continue\n\n            action_match = all_actions[-1]\n            action_name = action_match.group(1).strip()\n\n            # Extract JSON starting after \"Action Input:\"\n            json_str_candidate = output[input_match.end() :].strip()\n\n            # Remove markdown code block markers first\n            for marker in [\"```json\", \"```JSON\", \"```\"]:\n                json_str_candidate = json_str_candidate.replace(marker, \"\").strip()\n\n            # Manual JSON extraction with brace counting to handle nested structures\n            brace_count = 0\n            json_end = 0\n            in_string = False\n            escape = False\n            found_start = False\n            start_idx = 0\n\n            for j, char in enumerate(json_str_candidate):\n                if not found_start:\n                    if char == \"{\":\n                        found_start = True\n                        start_idx = j\n                        brace_count = 1\n                    elif not char.isspace():\n                        # Non-whitespace character before '{' means invalid format\n                        break\n                    continue\n\n                if char == '\"' and not escape:\n                    in_string = not in_string\n\n                if char == \"\\\\\" and not escape:\n                    escape = True\n                else:\n                    escape = False\n\n                if not in_string:\n                    if char == \"{\":\n                        brace_count += 1\n                    elif char == \"}\":\n                        brace_count -= 1\n                        if brace_count == 0:\n                            json_end = j + 1\n                            break\n\n            if json_end &gt; 0:\n                raw_input = json_str_candidate[start_idx:json_end]\n\n                try:\n                    action_input = json.loads(raw_input)\n                    actions.append({\"tool_name\": action_name, \"tool_input\": action_input})\n                except json.JSONDecodeError as e:\n                    raise ActionParsingException(\n                        f\"Invalid JSON in Action Input for {action_name}: {str(e)} : {raw_input}\",\n                        recoverable=True,\n                    )\n            else:\n                # Brace counting failed - no valid JSON structure found\n                raise ActionParsingException(\n                    f\"Could not parse JSON structure in Action Input for {action_name}. \"\n                    f\"Expected JSON object starting with '{{' but found: \"\n                    f\"{json_str_candidate[:100]}{'...' if len(json_str_candidate) &gt; 100 else ''}\",\n                    recoverable=True,\n                )\n\n        if not actions:\n            logger.info(\"No valid Action and Action Input pairs found in the output \")\n            raise ActionParsingException(\n                \"No valid Action and Action Input pairs found in the output.\",\n                recoverable=True,\n            )\n\n        action = actions[0][\"tool_name\"]\n        action_input = actions[0][\"tool_input\"]\n        return thought, action, action_input\n\n    except Exception as e:\n        logger.error(f\"Error: {e}\")\n        if isinstance(e, ActionParsingException):\n            raise\n        raise ActionParsingException(\n            f\"Error parsing action(s): {str(e)}. \"\n            f\"Please ensure the output follows the format 'Thought: &lt;text&gt; \"\n            f\"Action: &lt;action&gt; Action Input: &lt;valid JSON&gt;' \",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/parser/#dynamiq.nodes.agents.components.parser.parse_default_thought","title":"<code>parse_default_thought(output)</code>","text":"<p>Extracts thought from the output string in default inference mode format.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The LLM output string</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted thought or empty string if not found</p> Source code in <code>dynamiq/nodes/agents/components/parser.py</code> <pre><code>def parse_default_thought(output: str) -&gt; str:\n    \"\"\"\n    Extracts thought from the output string in default inference mode format.\n\n    Args:\n        output: The LLM output string\n\n    Returns:\n        The extracted thought or empty string if not found\n    \"\"\"\n    thought_match = re.search(\n        r\"Thought:\\s*(.*?)Action\",\n        output,\n        re.DOTALL,\n    )\n\n    if thought_match:\n        return thought_match.group(1).strip()\n\n    return \"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/schema_generator/","title":"Schema generator","text":"<p>Schema generation for Agent function calling and structured output modes.</p>"},{"location":"dynamiq/nodes/agents/components/schema_generator/#dynamiq.nodes.agents.components.schema_generator.filter_format_type","title":"<code>filter_format_type(param_annotation)</code>","text":"<p>Filters proper type for a function calling schema.</p> <p>Parameters:</p> Name Type Description Default <code>param_annotation</code> <code>Any</code> <p>Parameter annotation</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of parameter types that describe provided annotation</p> Source code in <code>dynamiq/nodes/agents/components/schema_generator.py</code> <pre><code>def filter_format_type(param_annotation: Any) -&gt; list[str]:\n    \"\"\"\n    Filters proper type for a function calling schema.\n\n    Args:\n        param_annotation: Parameter annotation\n\n    Returns:\n        List of parameter types that describe provided annotation\n    \"\"\"\n    if get_origin(param_annotation) in (Union, types.UnionType):\n        return get_args(param_annotation)\n\n    return [param_annotation]\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/schema_generator/#dynamiq.nodes.agents.components.schema_generator.generate_function_calling_schemas","title":"<code>generate_function_calling_schemas(tools, delegation_allowed, sanitize_tool_name, llm)</code>","text":"<p>Generate schemas for function calling mode.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Node]</code> <p>List of tools to generate schemas for</p> required <code>delegation_allowed</code> <code>bool</code> <p>Whether delegation is allowed</p> required <code>sanitize_tool_name</code> <code>Callable[[str], str]</code> <p>Function to sanitize tool names</p> required <code>llm</code> <code>Any</code> <p>The LLM instance</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of function calling schemas for all tools</p> Source code in <code>dynamiq/nodes/agents/components/schema_generator.py</code> <pre><code>def generate_function_calling_schemas(\n    tools: list[Node], delegation_allowed: bool, sanitize_tool_name: Callable[[str], str], llm: Any\n) -&gt; list[dict]:\n    \"\"\"\n    Generate schemas for function calling mode.\n\n    Args:\n        tools: List of tools to generate schemas for\n        delegation_allowed: Whether delegation is allowed\n        sanitize_tool_name: Function to sanitize tool names\n        llm: The LLM instance\n\n    Returns:\n        List of function calling schemas for all tools\n    \"\"\"\n    schemas = [FINAL_ANSWER_FUNCTION_SCHEMA]\n\n    for tool in tools:\n        # Agent tools: accept action_input as a JSON string to avoid nested schema issues.\n        if isinstance(tool, BaseAgent):\n            agent_action_input_description = \"JSON string containing the agent's inputs \"\n            if delegation_allowed:\n                agent_action_input_description += '(e.g., {\"input\": \"&lt;subtask&gt;\", \"delegate_final\": true}).'\n            else:\n                agent_action_input_description += '(e.g., {\"input\": \"&lt;subtask&gt;\"}).'\n\n            schema = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": sanitize_tool_name(tool.name),\n                    \"description\": tool.description[:1024],\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"thought\": {\n                                \"type\": \"string\",\n                                \"description\": \"Your reasoning about using this tool.\",\n                            },\n                            \"action_input\": {\n                                \"type\": \"string\",\n                                \"description\": agent_action_input_description,\n                            },\n                        },\n                        \"additionalProperties\": False,\n                        \"required\": [\"thought\", \"action_input\"],\n                    },\n                    \"strict\": True,\n                },\n            }\n\n            schemas.append(schema)\n            continue\n\n        properties = {}\n        input_params = tool.input_schema.model_fields.items()\n        if list(input_params) and not isinstance(llm, Gemini):\n            for name, field in tool.input_schema.model_fields.items():\n                generate_property_schema(properties, name, field)\n\n            schema = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": sanitize_tool_name(tool.name),\n                    \"description\": tool.description[:1024],\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"thought\": {\n                                \"type\": \"string\",\n                                \"description\": \"Your reasoning about using this tool.\",\n                            },\n                            \"action_input\": {\n                                \"type\": \"object\",\n                                \"description\": \"Input for the selected tool\",\n                                \"properties\": properties,\n                                \"required\": list(properties.keys()),\n                                \"additionalProperties\": False,\n                            },\n                        },\n                        \"additionalProperties\": False,\n                        \"required\": [\"thought\", \"action_input\"],\n                    },\n                    \"strict\": True,\n                },\n            }\n\n            schemas.append(schema)\n\n        else:\n            schema = {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": sanitize_tool_name(tool.name),\n                    \"description\": tool.description[:1024],\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"thought\": {\n                                \"type\": \"string\",\n                                \"description\": \"Your reasoning about using this tool.\",\n                            },\n                            \"action_input\": {\n                                \"type\": \"string\",\n                                \"description\": \"Input for the selected tool in JSON string format.\",\n                            },\n                        },\n                        \"additionalProperties\": False,\n                        \"required\": [\"thought\", \"action_input\"],\n                    },\n                    \"strict\": True,\n                },\n            }\n\n            schemas.append(schema)\n\n    return schemas\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/schema_generator/#dynamiq.nodes.agents.components.schema_generator.generate_input_formats","title":"<code>generate_input_formats(tools, sanitize_tool_name)</code>","text":"<p>Generate formatted input descriptions for each tool.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Node]</code> <p>List of tools to generate input formats for</p> required <code>sanitize_tool_name</code> <code>Callable[[str], str]</code> <p>Function to sanitize tool names</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string describing input parameters for each tool</p> Source code in <code>dynamiq/nodes/agents/components/schema_generator.py</code> <pre><code>def generate_input_formats(tools: list[Node], sanitize_tool_name: Callable[[str], str]) -&gt; str:\n    \"\"\"\n    Generate formatted input descriptions for each tool.\n\n    Args:\n        tools: List of tools to generate input formats for\n        sanitize_tool_name: Function to sanitize tool names\n\n    Returns:\n        Formatted string describing input parameters for each tool\n    \"\"\"\n    input_formats = []\n    for tool in tools:\n        params = []\n        for name, field in tool.input_schema.model_fields.items():\n            if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n                args = get_args(field.annotation)\n                if get_origin(field.annotation) in (Union, types.UnionType):\n                    type_str = str(field.annotation)\n                elif field.json_schema_extra and field.json_schema_extra.get(\"map_from_storage\", False):\n                    type_str = \"tuple[str, ...]\"\n                elif args and hasattr(args[0], \"model_fields\") and get_origin(field.annotation) is list:\n                    nested_fields = [\n                        f\"{fn}: {getattr(fi.annotation, '__name__', str(fi.annotation))} - {fi.description or ''}\"\n                        for fn, fi in args[0].model_fields.items()\n                    ]\n                    type_str = f\"list[{{{', '.join(nested_fields)}}}, ...]\"\n                else:\n                    type_str = getattr(field.annotation, \"__name__\", str(field.annotation))\n\n                description = field.description or \"No description\"\n                params.append(f\"{name} ({type_str}): {description}\")\n        if params:\n            input_formats.append(f\" - {sanitize_tool_name(tool.name)}\\n \\t* \" + \"\\n\\t* \".join(params))\n    return \"\\n\".join(input_formats)\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/schema_generator/#dynamiq.nodes.agents.components.schema_generator.generate_property_schema","title":"<code>generate_property_schema(properties, name, field)</code>","text":"<p>Generate property schema for a field in function calling mode.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>dict</code> <p>Dictionary to store the generated property schema</p> required <code>name</code> <code>str</code> <p>Name of the property</p> required <code>field</code> <code>Any</code> <p>Field object containing metadata</p> required Source code in <code>dynamiq/nodes/agents/components/schema_generator.py</code> <pre><code>def generate_property_schema(properties: dict, name: str, field: Any) -&gt; None:\n    \"\"\"\n    Generate property schema for a field in function calling mode.\n\n    Args:\n        properties: Dictionary to store the generated property schema\n        name: Name of the property\n        field: Field object containing metadata\n    \"\"\"\n    if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n        description = field.description or \"No description.\"\n\n        description += f\" Defaults to: {field.default}.\" if field.default and not field.is_required() else \"\"\n        params = filter_format_type(field.annotation)\n\n        properties[name] = {\"description\": description}\n        types = []\n\n        for param in params:\n            if param is type(None):\n                types.append(\"null\")\n\n            elif param_type := TYPE_MAPPING.get(param):\n                types.append(param_type)\n\n            elif isinstance(param, type) and issubclass(param, Enum):\n                element_type = TYPE_MAPPING.get(filter_format_type(type(list(param.__members__.values())[0].value))[0])\n                types.append(element_type)\n                properties[name][\"enum\"] = [field.value for field in param.__members__.values()]\n\n            elif getattr(param, \"__origin__\", None) is list:\n                types.append(\"array\")\n                properties[name][\"items\"] = {\"type\": TYPE_MAPPING.get(param.__args__[0])}\n\n            elif getattr(param, \"__origin__\", None) is dict:\n                types.append(\"object\")\n\n        if len(types) == 1:\n            properties[name][\"type\"] = types[0]\n        elif len(types) &gt; 1:\n            properties[name][\"type\"] = types\n        else:\n            properties[name][\"type\"] = \"string\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/components/schema_generator/#dynamiq.nodes.agents.components.schema_generator.generate_structured_output_schemas","title":"<code>generate_structured_output_schemas(tools, sanitize_tool_name, delegation_allowed)</code>","text":"<p>Generate schema for structured output mode.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Node]</code> <p>List of tools to generate schema for</p> required <code>sanitize_tool_name</code> <code>Callable[[str], str]</code> <p>Function to sanitize tool names</p> required <code>delegation_allowed</code> <code>bool</code> <p>Whether delegation is allowed</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the structured output schema</p> Source code in <code>dynamiq/nodes/agents/components/schema_generator.py</code> <pre><code>def generate_structured_output_schemas(\n    tools: list[Node], sanitize_tool_name: Callable[[str], str], delegation_allowed: bool\n) -&gt; dict:\n    \"\"\"\n    Generate schema for structured output mode.\n\n    Args:\n        tools: List of tools to generate schema for\n        sanitize_tool_name: Function to sanitize tool names\n        delegation_allowed: Whether delegation is allowed\n\n    Returns:\n        Dictionary containing the structured output schema\n    \"\"\"\n    tool_names = [sanitize_tool_name(tool.name) for tool in tools]\n\n    action_input_description = \"Input for chosen action.\"\n\n    if delegation_allowed and any(isinstance(tool, BaseAgent) for tool in tools):\n        action_input_description += (\n            ' For agent tools, include {\"input\": \"&lt;subtask&gt;\", \"delegate_final\": true} '\n            \"to return that agent's response directly as the final answer.\"\n        )\n\n    schema = {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"plan_next_action\",\n            \"strict\": True,\n            \"schema\": {\n                \"type\": \"object\",\n                \"required\": [\"thought\", \"action\", \"action_input\", \"output_files\"],\n                \"properties\": {\n                    \"thought\": {\n                        \"type\": \"string\",\n                        \"description\": \"Your reasoning about the next step.\",\n                    },\n                    \"action\": {\n                        \"type\": \"string\",\n                        \"description\": f\"Next action to make (choose from [{tool_names}, finish]).\",\n                    },\n                    \"action_input\": {\n                        \"type\": \"string\",\n                        \"description\": action_input_description,\n                    },\n                    \"output_files\": {\n                        \"type\": \"string\",\n                        \"description\": (\n                            \"Comma-separated file paths to return when action is finish.\" \" Empty string otherwise.\"\n                        ),\n                    },\n                },\n                \"additionalProperties\": False,\n            },\n        },\n    }\n\n    return schema\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/","title":"Adaptive","text":""},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator","title":"<code>AdaptiveOrchestrator</code>","text":"<p>               Bases: <code>Orchestrator</code></p> <p>Orchestrates the execution of complex tasks using multiple specialized agents.</p> <p>This class manages the breakdown of a main objective into subtasks, delegates these subtasks to appropriate agents, and synthesizes the results into a final answer.</p> <p>Attributes:</p> Name Type Description <code>manager</code> <code>ManagerAgent</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>agents</code> <code>List[Agent]</code> <p>List of specialized agents available for task execution.</p> <code>objective</code> <code>Optional[str]</code> <p>The main objective of the orchestration.</p> <code>max_loops</code> <code>Optional[int]</code> <p>Maximum number of actions.</p> <code>reflection_enabled</code> <code>Optional[bool]</code> <p>Enable reflection mode</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class AdaptiveOrchestrator(Orchestrator):\n    \"\"\"\n    Orchestrates the execution of complex tasks using multiple specialized agents.\n\n    This class manages the breakdown of a main objective into subtasks,\n    delegates these subtasks to appropriate agents, and synthesizes the results\n    into a final answer.\n\n    Attributes:\n        manager (ManagerAgent): The managing agent responsible for overseeing the orchestration process.\n        agents (List[BaseAgent]): List of specialized agents available for task execution.\n        objective (Optional[str]): The main objective of the orchestration.\n        max_loops (Optional[int]): Maximum number of actions.\n        reflection_enabled (Optional[bool]): Enable reflection mode\n    \"\"\"\n\n    name: str | None = \"AdaptiveOrchestrator\"\n    group: NodeGroup = NodeGroup.AGENTS\n    manager: AdaptiveAgentManager\n    agents: list[Agent] = []\n    max_loops: int = 15\n    reflection_enabled: bool = False\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"agents\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n        data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n        return data\n\n    def reset_run_state(self):\n        super().reset_run_state()\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize components of the orchestrator.\n\n        Args:\n            connection_manager (ConnectionManager | None): The connection manager. Defaults to None.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for agent in self.agents:\n            if agent.is_postponed_component_init:\n                agent.init_components(connection_manager)\n\n    @property\n    def agents_descriptions(self) -&gt; str:\n        \"\"\"Get a formatted string of agent descriptions.\"\"\"\n        return \"\\n\".join([f\"{i}. {agent.name}\" for i, agent in enumerate(self.agents)]) if self.agents else \"\"\n\n    def _handle_next_action(self, content: str, config: RunnableConfig | None = None, **kwargs) -&gt; Action:\n        \"\"\"\n        Parse XML content to extract action details. Streams action if streaming is enabled.\n\n        Args:\n            content (str): XML formatted content from LLM response\n\n        Returns:\n            Action: Parsed action object\n        \"\"\"\n        action = self._parse_xml_content(content)\n        if self.manager.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n            self.manager.stream_content(\n                content={\"next_action\": action},\n                step=\"manager_planning\",\n                source=self.name,\n                config=config,\n                **kwargs,\n            )\n        return action\n\n    def get_next_action(self, config: RunnableConfig = None, **kwargs) -&gt; Action:\n        \"\"\"\n        Determine the next action based on the current state and LLM output.\n\n        Returns:\n            Action: The next action to be taken.\n\n        Raises:\n            ActionParseError: If there is an error parsing the action from the LLM response.\n        \"\"\"\n\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"plan\",\n                \"agents\": self.agents_descriptions,\n                \"chat_history\": format_chat_history(self._chat_history),\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"Agent '{self.manager.name}' failed: {manager_result.error.message}\"\n            raise ActionParseError(f\"Unable to retrieve the next action from Agent Manager, Error: {error_message}\")\n\n        manager_content = manager_result.output.get(\"content\").get(\"result\")\n\n        if self.reflection_enabled:\n            reflect_result = self.manager.run(\n                input_data={\n                    \"action\": \"reflect\",\n                    \"agents\": self.agents_descriptions,\n                    \"chat_history\": format_chat_history(self._chat_history),\n                    \"plan\": manager_content,\n                    \"agent_output\": \"\",\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n            if reflect_result.status != RunnableStatus.SUCCESS:\n                error_message = f\"Agent '{self.manager.name}' failed on reflection: {reflect_result.error.message}\"\n                logger.error(error_message)\n                return self._handle_next_action(manager_content, config=config, **kwargs)\n            else:\n                reflect_content = reflect_result.output.get(\"content\").get(\"result\")\n                try:\n                    return self._handle_next_action(reflect_content, config=config, **kwargs)\n                except ActionParseError as e:\n                    logger.error(f\"Agent '{self.manager.name}' failed on reflection parsing: {str(e)}\")\n                    return self._handle_next_action(manager_content, config=config, **kwargs)\n\n        return self._handle_next_action(manager_content, config=config, **kwargs)\n\n    def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the given task using the manager agent logic.\n\n        Args:\n            input_task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            dict[str, Any]: The final output generated after processing the task.\n        \"\"\"\n\n        analysis = self._analyze_user_input(input_task, self.agents_descriptions, config=config, **kwargs)\n        decision = analysis.decision\n        message = analysis.message\n\n        if decision == Decision.RESPOND:\n            return {\"content\": message}\n        else:\n            self._chat_history.append({\"role\": \"user\", \"content\": input_task})\n\n            for i in range(self.max_loops):\n                action = self.get_next_action(config=config, **kwargs)\n                logger.info(f\"Orchestrator {self.name} - {self.id}: Loop {i + 1} - Action: {action.dict()}\")\n                if action.command == ActionCommand.DELEGATE:\n                    self._handle_delegation(action=action, config=config, **kwargs)\n\n                elif action.command == ActionCommand.RESPOND:\n                    respond_result = self._handle_respond(action=action)\n                    respond_final_result = self.parse_xml_final_answer(respond_result)\n                    return {\"content\": respond_final_result}\n\n                elif action.command == ActionCommand.FINAL_ANSWER:\n                    manager_final_result = self.get_final_result(\n                        {\n                            \"input_task\": input_task,\n                            \"chat_history\": format_chat_history(self._chat_history),\n                            \"preliminary_answer\": action.answer,\n                        },\n                        config=config,\n                        **kwargs,\n                    )\n                    final_result = self.parse_xml_final_answer(manager_final_result)\n                    return {\"content\": final_result}\n\n    def _handle_delegation(self, action: Action, config: RunnableConfig = None, **kwargs) -&gt; None:\n        \"\"\"\n        Handle task delegation to a specialized agent.\n\n        Args:\n            action (Action): The action containing the delegation command and details.\n        \"\"\"\n        agent = next((a for a in self.agents if a.name == action.agent), None)\n        if agent:\n            result = agent.run(\n                input_data={\"input\": action.task},\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=agent).to_dict(for_tracing=True)]\n            if result.status != RunnableStatus.SUCCESS:\n                error_message = f\"Agent '{agent.name}' failed: {result.error.message}\"\n                raise OrchestratorError(f\"Failed to execute Agent {agent.name}, due to error: {error_message}\")\n\n            self._chat_history.append(\n                {\n                    \"role\": \"system\",\n                    \"content\": f\"Agent {action.agent} result: {result.output.get('content')}\",\n                }\n            )\n        else:\n            result = self.manager.run(\n                input_data={\n                    \"action\": \"respond\",\n                    \"task\": action.task,\n                    \"agents\": self.agents_descriptions,\n                    \"chat_history\": format_chat_history(self._chat_history),\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n            if result.status != RunnableStatus.SUCCESS:\n                content = result.error.message\n                logger.error(\n                    f\"Orchestrator {self.name} - {self.id}: \" f\"Error executing {self.manager.name}:\" f\"{content}\"\n                )\n            else:\n                content = result.output.get(\"content\")\n\n            self._chat_history.append(\n                {\n                    \"role\": \"system\",\n                    \"content\": f\"LLM result: {content}\",\n                }\n            )\n\n    def _handle_respond(self, action: Action, config: RunnableConfig = None, **kwargs) -&gt; str:\n        \"\"\"\n        Handle a direct response from the Manager.\n\n        Args:\n            action (Action): The action to handle.\n            config (RunnableConfig | None): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The manager's response content.\n\n        Raises:\n            OrchestratorError: If the manager fails to execute the respond action.\n        \"\"\"\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"respond\",\n                \"task\": action.task,\n                \"agents\": self.agents_descriptions,\n                \"chat_history\": format_chat_history(self._chat_history),\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"Manager agent '{self.manager.name}' failed: {manager_result.error.message}\"\n            raise OrchestratorError(f\"Failed to execute respond action with manager, due to error: {error_message}\")\n\n        manager_result_content = manager_result.output.get(\"content\").get(\"result\")\n\n        self._chat_history.append(\n            {\n                \"role\": \"system\",\n                \"content\": f\"[Manager agent '{self.manager.name} - quick response]: {manager_result_content}\",\n            }\n        )\n        return manager_result_content\n\n    def setup_streaming(self) -&gt; None:\n        \"\"\"Setups streaming for orchestrator.\"\"\"\n        self.manager.streaming = self.streaming\n        for agent in self.agents:\n            agent.streaming = self.streaming\n\n    def _parse_xml_content(self, content: str) -&gt; Action:\n        \"\"\"\n        Parse XML content to extract action details.\n\n        Args:\n            content (str): XML formatted content from LLM response\n\n        Returns:\n            Action: Parsed action object\n\n        Raises:\n            ActionParseError: If XML parsing fails or required tags are missing\n        \"\"\"\n\n        if \"&lt;action&gt;\" not in content.lower():\n            logger.info(\"No &lt;action&gt; tag found in content, applying fallback wrapping for XML parsing.\")\n            content = f\"&lt;root&gt;&lt;action&gt;respond&lt;/action&gt;&lt;task&gt;{content.strip()}&lt;/task&gt;&lt;/root&gt;\"\n        try:\n            root = self._clean_content(content=content)\n        except Exception as e:\n            error_message = f\"XML parsing error: {str(e)} in content: {content}\"\n            logger.error(error_message)\n            raise ActionParseError(error_message)\n\n        try:\n            action_elem = root.find(\".//action\")\n            if action_elem is None or not action_elem.text:\n                error_message = (\n                    f\"Orchestrator {self.name} - {self.id}: XML parsing error: No &lt;action&gt; tag found in the response\"\n                )\n                raise ActionParseError(error_message)\n\n            action_type = action_elem.text.strip().lower()\n\n            if action_type == \"delegate\":\n                agent_elem = root.find(\".//agent\")\n                task_elem = root.find(\".//task\")\n                task_data_elem = root.find(\".//task_data\")\n\n                if agent_elem is None or task_elem is None:\n                    error_message = (\n                        f\"Orchestrator {self.name} - {self.id}: XML parsing error: \"\n                        f\"Delegate action missing required &lt;agent&gt; or &lt;task&gt; tags\"\n                    )\n                    raise ActionParseError(error_message)\n\n                return Action(\n                    command=ActionCommand.DELEGATE,\n                    agent=agent_elem.text.strip(),\n                    task=task_elem.text.strip()\n                    + (f\" {task_data_elem.text.strip()}\" if task_data_elem is not None and task_data_elem.text else \"\"),\n                )\n\n            elif action_type == \"final_answer\":\n                answer_elem = root.find(\".//final_answer\")\n                if answer_elem is None or not answer_elem.text:\n                    error_message = (\n                        f\"Orchestrator {self.name} - {self.id}: XML parsing error: \"\n                        f\"Final answer action missing &lt;final_answer&gt; tag\"\n                    )\n                    raise ActionParseError(error_message)\n                return Action(command=ActionCommand.FINAL_ANSWER, answer=answer_elem.text.strip())\n\n            elif action_type == \"respond\":\n                task_elem = root.find(\".//task\")\n                if task_elem is None or not task_elem.text:\n                    error_message = (\n                        f\"Orchestrator {self.name} - {self.id}: XML parsing error: Respond action missing &lt;task&gt; tag\"\n                    )\n                    raise ActionParseError(error_message)\n                return Action(\n                    command=ActionCommand.RESPOND,\n                    task=task_elem.text.strip(),\n                )\n            else:\n                raise ActionParseError(f\"Unknown action type: {action_type}\")\n\n        except LET.ParseError as e:\n            error_message = f\"Orchestrator {self.name} - {self.id}: XML parsing error: {str(e)}\"\n            raise ActionParseError(error_message)\n        except Exception as e:\n            error_message = f\"Orchestrator {self.name} - {self.id}: Error parsing action: {str(e)}\"\n            raise ActionParseError(error_message)\n\n    def parse_xml_final_answer(self, content: str) -&gt; str:\n        \"\"\"\n        Parses XML content to extract the final answer from either 'output' or 'final_answer' tags.\n\n        This method attempts to extract content using XML parsing first, and if that fails,\n        falls back to regex pattern matching. If both methods fail, it returns the original content.\n\n        Args:\n            content (str): The XML-formatted string containing the answer.\n\n        Returns:\n            str: The extracted answer from either the 'output' or 'final_answer' tags.\n                If parsing fails, returns the original content.\n\n        Raises:\n            ActionParseError: When XML parsing fails and neither 'output' nor 'final_answer' tags\n                contain valid content.\n        \"\"\"\n        try:\n            root = self._clean_content(content=content)\n            for tag in [\"output\", \"final_answer\"]:\n                elem = root.find(f\".//{tag}\")\n                if elem is not None and elem.text and elem.text.strip():\n                    return elem.text.strip()\n            error_message = (\n                f\"Error parsing final answer: {str(content)[:100]}...\"\n                f\" Neither &lt;output&gt; nor &lt;final_answer&gt; tag found with valid content.\"\n            )\n            raise ActionParseError(error_message)\n        except Exception as e:\n            logger.info(\"Error parsing final answer using XML: %s. Falling back to regex extraction.\", e)\n            for tag in [\"output\", \"final_answer\"]:\n                pattern = rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n                match = re.search(pattern, content, re.DOTALL)\n                if match:\n                    extracted = match.group(1).strip()\n                    if extracted:\n                        return extracted\n            logger.info(\"Regex extraction failed. Returning original content as fallback.\")\n            return content\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.agents_descriptions","title":"<code>agents_descriptions: str</code>  <code>property</code>","text":"<p>Get a formatted string of agent descriptions.</p>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.get_next_action","title":"<code>get_next_action(config=None, **kwargs)</code>","text":"<p>Determine the next action based on the current state and LLM output.</p> <p>Returns:</p> Name Type Description <code>Action</code> <code>Action</code> <p>The next action to be taken.</p> <p>Raises:</p> Type Description <code>ActionParseError</code> <p>If there is an error parsing the action from the LLM response.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def get_next_action(self, config: RunnableConfig = None, **kwargs) -&gt; Action:\n    \"\"\"\n    Determine the next action based on the current state and LLM output.\n\n    Returns:\n        Action: The next action to be taken.\n\n    Raises:\n        ActionParseError: If there is an error parsing the action from the LLM response.\n    \"\"\"\n\n    manager_result = self.manager.run(\n        input_data={\n            \"action\": \"plan\",\n            \"agents\": self.agents_descriptions,\n            \"chat_history\": format_chat_history(self._chat_history),\n        },\n        config=config,\n        run_depends=self._run_depends,\n        **kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        error_message = f\"Agent '{self.manager.name}' failed: {manager_result.error.message}\"\n        raise ActionParseError(f\"Unable to retrieve the next action from Agent Manager, Error: {error_message}\")\n\n    manager_content = manager_result.output.get(\"content\").get(\"result\")\n\n    if self.reflection_enabled:\n        reflect_result = self.manager.run(\n            input_data={\n                \"action\": \"reflect\",\n                \"agents\": self.agents_descriptions,\n                \"chat_history\": format_chat_history(self._chat_history),\n                \"plan\": manager_content,\n                \"agent_output\": \"\",\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n        if reflect_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"Agent '{self.manager.name}' failed on reflection: {reflect_result.error.message}\"\n            logger.error(error_message)\n            return self._handle_next_action(manager_content, config=config, **kwargs)\n        else:\n            reflect_content = reflect_result.output.get(\"content\").get(\"result\")\n            try:\n                return self._handle_next_action(reflect_content, config=config, **kwargs)\n            except ActionParseError as e:\n                logger.error(f\"Agent '{self.manager.name}' failed on reflection parsing: {str(e)}\")\n                return self._handle_next_action(manager_content, config=config, **kwargs)\n\n    return self._handle_next_action(manager_content, config=config, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components of the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager | None</code> <p>The connection manager. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize components of the orchestrator.\n\n    Args:\n        connection_manager (ConnectionManager | None): The connection manager. Defaults to None.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for agent in self.agents:\n        if agent.is_postponed_component_init:\n            agent.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.parse_xml_final_answer","title":"<code>parse_xml_final_answer(content)</code>","text":"<p>Parses XML content to extract the final answer from either 'output' or 'final_answer' tags.</p> <p>This method attempts to extract content using XML parsing first, and if that fails, falls back to regex pattern matching. If both methods fail, it returns the original content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The XML-formatted string containing the answer.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted answer from either the 'output' or 'final_answer' tags. If parsing fails, returns the original content.</p> <p>Raises:</p> Type Description <code>ActionParseError</code> <p>When XML parsing fails and neither 'output' nor 'final_answer' tags contain valid content.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def parse_xml_final_answer(self, content: str) -&gt; str:\n    \"\"\"\n    Parses XML content to extract the final answer from either 'output' or 'final_answer' tags.\n\n    This method attempts to extract content using XML parsing first, and if that fails,\n    falls back to regex pattern matching. If both methods fail, it returns the original content.\n\n    Args:\n        content (str): The XML-formatted string containing the answer.\n\n    Returns:\n        str: The extracted answer from either the 'output' or 'final_answer' tags.\n            If parsing fails, returns the original content.\n\n    Raises:\n        ActionParseError: When XML parsing fails and neither 'output' nor 'final_answer' tags\n            contain valid content.\n    \"\"\"\n    try:\n        root = self._clean_content(content=content)\n        for tag in [\"output\", \"final_answer\"]:\n            elem = root.find(f\".//{tag}\")\n            if elem is not None and elem.text and elem.text.strip():\n                return elem.text.strip()\n        error_message = (\n            f\"Error parsing final answer: {str(content)[:100]}...\"\n            f\" Neither &lt;output&gt; nor &lt;final_answer&gt; tag found with valid content.\"\n        )\n        raise ActionParseError(error_message)\n    except Exception as e:\n        logger.info(\"Error parsing final answer using XML: %s. Falling back to regex extraction.\", e)\n        for tag in [\"output\", \"final_answer\"]:\n            pattern = rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n            match = re.search(pattern, content, re.DOTALL)\n            if match:\n                extracted = match.group(1).strip()\n                if extracted:\n                    return extracted\n        logger.info(\"Regex extraction failed. Returning original content as fallback.\")\n        return content\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.run_flow","title":"<code>run_flow(input_task, config=None, **kwargs)</code>","text":"<p>Process the given task using the manager agent logic.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The final output generated after processing the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the given task using the manager agent logic.\n\n    Args:\n        input_task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        dict[str, Any]: The final output generated after processing the task.\n    \"\"\"\n\n    analysis = self._analyze_user_input(input_task, self.agents_descriptions, config=config, **kwargs)\n    decision = analysis.decision\n    message = analysis.message\n\n    if decision == Decision.RESPOND:\n        return {\"content\": message}\n    else:\n        self._chat_history.append({\"role\": \"user\", \"content\": input_task})\n\n        for i in range(self.max_loops):\n            action = self.get_next_action(config=config, **kwargs)\n            logger.info(f\"Orchestrator {self.name} - {self.id}: Loop {i + 1} - Action: {action.dict()}\")\n            if action.command == ActionCommand.DELEGATE:\n                self._handle_delegation(action=action, config=config, **kwargs)\n\n            elif action.command == ActionCommand.RESPOND:\n                respond_result = self._handle_respond(action=action)\n                respond_final_result = self.parse_xml_final_answer(respond_result)\n                return {\"content\": respond_final_result}\n\n            elif action.command == ActionCommand.FINAL_ANSWER:\n                manager_final_result = self.get_final_result(\n                    {\n                        \"input_task\": input_task,\n                        \"chat_history\": format_chat_history(self._chat_history),\n                        \"preliminary_answer\": action.answer,\n                    },\n                    config=config,\n                    **kwargs,\n                )\n                final_result = self.parse_xml_final_answer(manager_final_result)\n                return {\"content\": final_result}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.setup_streaming","title":"<code>setup_streaming()</code>","text":"<p>Setups streaming for orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def setup_streaming(self) -&gt; None:\n    \"\"\"Setups streaming for orchestrator.\"\"\"\n    self.manager.streaming = self.streaming\n    for agent in self.agents:\n        agent.streaming = self.streaming\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"manager\"] = self.manager.to_dict(**kwargs)\n    data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AgentNotFoundError","title":"<code>AgentNotFoundError</code>","text":"<p>               Bases: <code>OrchestratorError</code></p> <p>Raised when a specified agent is not found.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class AgentNotFoundError(OrchestratorError):\n    \"\"\"Raised when a specified agent is not found.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/","title":"Adaptive manager","text":""},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/#dynamiq.nodes.agents.orchestrators.adaptive_manager.AdaptiveAgentManager","title":"<code>AdaptiveAgentManager</code>","text":"<p>               Bases: <code>AgentManager</code></p> <p>An adaptive agent manager that coordinates specialized agents to complete complex tasks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive_manager.py</code> <pre><code>class AdaptiveAgentManager(AgentManager):\n    \"\"\"An adaptive agent manager that coordinates specialized agents to complete complex tasks.\"\"\"\n\n    name: str = \"Adaptive Manager\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AdaptiveAgentManager and set up prompt templates.\"\"\"\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_actions(self):\n        \"\"\"Extend the default actions with 'respond'.\"\"\"\n        super()._init_actions()  #\n        self._actions[\"respond\"] = self._respond\n        self._actions[\"reflect\"] = self._reflect\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initialize the prompt blocks with adaptive plan and final prompts.\"\"\"\n        super()._init_prompt_blocks()\n        self.system_prompt_manager.update_blocks(\n            {\n                \"plan\": self._get_adaptive_plan_prompt(),\n                \"final\": self._get_adaptive_final_prompt(),\n                \"respond\": self._get_adaptive_respond_prompt(),\n                \"reflect\": self._get_adaptive_reflect_prompt(),\n                \"handle_input\": self._get_adaptive_handle_input_prompt(),\n            }\n        )\n\n    @staticmethod\n    def _get_adaptive_plan_prompt() -&gt; str:\n        \"\"\"Return the adaptive plan prompt template.\"\"\"\n        return PROMPT_TEMPLATE_ADAPTIVE_PLAN\n\n    @staticmethod\n    def _get_adaptive_handle_input_prompt() -&gt; str:\n        \"\"\"Determines how to handle input, either by continuing the flow or providing a direct response.\"\"\"\n        return PROMPT_TEMPLATE_BASE_HANDLE_INPUT\n\n    @staticmethod\n    def _get_adaptive_final_prompt() -&gt; str:\n        \"\"\"Return the adaptive final answer prompt template.\"\"\"\n        return PROMPT_TEMPLATE_ADAPTIVE_FINAL\n\n    @staticmethod\n    def _get_adaptive_respond_prompt() -&gt; str:\n        \"\"\"Return the adaptive clarify prompt template.\"\"\"\n        return PROMPT_TEMPLATE_ADAPTIVE_RESPOND\n\n    @staticmethod\n    def _get_adaptive_reflect_prompt() -&gt; str:\n        \"\"\"Return the adaptive reflect prompt template.\"\"\"\n        return PROMPT_TEMPLATE_ADAPTIVE_REFLECT\n\n    def _reflect(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'reflect' action.\"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"reflect\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n            return self.stream_content(\n                content=llm_result,\n                step=\"manager_reflection\",\n                source=self.name,\n                config=config,\n                **kwargs\n            )\n        return llm_result\n\n    def _respond(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'respond' action.\"\"\"\n        prompt = self.system_prompt_manager.render_block(\n            \"respond\", **(self.system_prompt_manager._prompt_variables | kwargs)\n        )\n        llm_result = self._run_llm([Message(role=MessageRole.USER, content=prompt)], config, **kwargs).output[\"content\"]\n        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n            return self.stream_content(\n                content=llm_result, step=\"manager_response\", source=self.name, config=config, **kwargs\n            )\n        return llm_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/#dynamiq.nodes.agents.orchestrators.adaptive_manager.AdaptiveAgentManager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AdaptiveAgentManager and set up prompt templates.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive_manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AdaptiveAgentManager and set up prompt templates.\"\"\"\n    super().__init__(**kwargs)\n    self._init_prompt_blocks()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/","title":"Graph","text":""},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator","title":"<code>GraphOrchestrator</code>","text":"<p>               Bases: <code>Orchestrator</code></p> <p>Orchestrates the execution of complex tasks, interconnected within the graph structure.</p> <p>This class manages the execution by following structure of directed graph. When finished synthesizes the results into a final answer.</p> <p>Attributes:</p> Name Type Description <code>manager</code> <code>ManagerAgent</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>context</code> <code>Dict[str, Any]</code> <p>Context of the orchestrator.</p> <code>states</code> <code>List[GraphState]</code> <p>List of states within orchestrator.</p> <code>initial_state</code> <code>str</code> <p>State to start from.</p> <code>objective</code> <code>Optional[str]</code> <p>The main objective of the orchestration.</p> <code>max_loops</code> <code>Optional[int]</code> <p>Maximum number of transition between states.</p> <code>input_analysis_enabled</code> <code>bool</code> <p>Enables initial input analysis.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>class GraphOrchestrator(Orchestrator):\n    \"\"\"\n    Orchestrates the execution of complex tasks, interconnected within the graph structure.\n\n    This class manages the execution by following structure of directed graph. When finished synthesizes the results\n    into a final answer.\n\n    Attributes:\n        manager (ManagerAgent): The managing agent responsible for overseeing the orchestration process.\n        context (Dict[str, Any]): Context of the orchestrator.\n        states (List[GraphState]): List of states within orchestrator.\n        initial_state (str): State to start from.\n        objective (Optional[str]): The main objective of the orchestration.\n        max_loops (Optional[int]): Maximum number of transition between states.\n        input_analysis_enabled (bool): Enables initial input analysis.\n    \"\"\"\n\n    name: str | None = \"GraphOrchestrator\"\n    manager: GraphAgentManager\n    initial_state: str = START\n    context: dict[str, Any] = {}\n    states: list[GraphState] = []\n    max_loops: int = 15\n    input_analysis_enabled: bool = False\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize components of the orchestrator.\n\n        Args:\n            connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        super().init_components(connection_manager)\n\n        if self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for state in self.states:\n            if state.is_postponed_component_init:\n                state.init_components(connection_manager)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._state_by_id = {state.id: state for state in self.states}\n\n        if START not in self._state_by_id:\n            start_state = GraphState(id=START, description=\"Initial state\")\n            self._state_by_id[START] = start_state\n            self.states.append(start_state)\n\n        if END not in self._state_by_id:\n            end_state = GraphState(id=END, description=\"Final state\")\n            self._state_by_id[END] = end_state\n            self.states.append(end_state)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"states\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n        data[\"states\"] = [state.to_dict(**kwargs) for state in self.states]\n        return data\n\n    def add_state_by_tasks(\n        self, state_id: str, tasks: list[Node | Callable], callbacks: list[NodeCallbackHandler] = []\n    ) -&gt; None:\n        \"\"\"\n        Adds state to the graph based on tasks.\n\n        Args:\n            state_id (str): Id of the state.\n            tasks (list[Node | Callable]): List of tasks that have to be executed when running this state.\n            callbacks: list[NodeCallbackHandler]: List of callbacks.\n        Raises:\n            ValueError: If state with specified id already exists.\n        \"\"\"\n        if state_id in self._state_by_id:\n            raise ValueError(f\"Error: State with id {state_id} already exists.\")\n\n        filtered_tasks = []\n\n        has_agent = False\n        for task in tasks:\n            if isinstance(task, Node):\n                if isinstance(task, Agent):\n                    has_agent = True\n                filtered_tasks.append(task)\n            elif isinstance(task, Callable):\n                filtered_tasks.append(function_tool(task)())\n            else:\n                raise OrchestratorError(\"Error: Task must be either a Node or a Callable.\")\n\n        state = GraphState(\n            id=state_id,\n            name=state_id,\n            manager=self.manager if has_agent else None,\n            tasks=filtered_tasks,\n            callbacks=callbacks,\n        )\n        self.states.append(state)\n        self._state_by_id[state.id] = state\n\n    def add_state(self, state: GraphState) -&gt; None:\n        \"\"\"\n        Adds state to the graph.\n\n        Args:\n            state (State): State to add to the graph.\n\n        Raises:\n            ValueError: If state with specified id already exists.\n        \"\"\"\n        if state.id in self._state_by_id:\n            raise ValueError(f\"Error: State with id {state.id} already exists.\")\n\n        self.states.append(state)\n        self._state_by_id[state.id] = state\n\n    def add_edge(self, source_id: str, destination_id: str) -&gt; None:\n        \"\"\"\n        Adds edge to the graph. When source state finishes execution, destination state will be executed next.\n\n        Args:\n            source_id (str): Id of source state.\n            destination_id (str): Id of destination state.\n\n        Raises:\n            ValueError: If state with specified id does not exist.\n        \"\"\"\n        self.validate_states([source_id, destination_id])\n        self._state_by_id[source_id].next_states = [destination_id]\n\n    def validate_states(self, ids: list[str]) -&gt; None:\n        \"\"\"\n        Check if the provided state ids are valid.\n\n        Args:\n            ids (list[str]): State ids to validate.\n\n        Raises:\n            ValueError: If state with specified id does not exist.\n        \"\"\"\n        for state_id in ids:\n            if state_id not in self._state_by_id:\n                raise ValueError(f\"State with id {state_id} does not exist\")\n\n    def add_conditional_edge(\n        self,\n        source_id: str,\n        destination_ids: list[str],\n        condition: Callable | Python,\n        callbacks: list[NodeCallbackHandler] = [],\n    ) -&gt; None:\n        \"\"\"\n        Adds conditional edge to the graph.\n        Conditional edge provides opportunity to choose between destination states based on condition.\n\n        Args:\n            source_id (str): Id of the source state.\n            destination_ids (list[str]): Ids of destination states.\n            condition (Callable | Python): Condition that will determine next state.\n            callbacks: list[NodeCallbackHandler]: List of callbacks.\n        Raises:\n            ValueError: If state with specified id is not present.\n        \"\"\"\n        self.validate_states(destination_ids + [source_id])\n\n        if isinstance(condition, Python):\n            condition.callbacks.extend(callbacks)\n            self._state_by_id[source_id].condition = condition\n        elif isinstance(condition, Callable):\n            tool = function_tool(condition)()\n            tool.callbacks = callbacks\n            self._state_by_id[source_id].condition = tool\n        else:\n            raise OrchestratorError(\"Error: Conditional edge must be either a Python Node or a Callable.\")\n\n        self._state_by_id[source_id].next_states = destination_ids\n\n    def get_next_state_by_manager(self, state: GraphState, config: RunnableConfig, **kwargs) -&gt; GraphState:\n        \"\"\"\n        Determine the next state based on the current state and history. Uses GraphAgentManager.\n\n        Args:\n            state (State): Current state.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            State: Next state to execute.\n\n        Raises:\n            OrchestratorError: If there is an error parsing the action from the LLM response.\n            StateNotFoundError: If the state is invalid or not found.\n        \"\"\"\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"plan\",\n                \"states_description\": self.states_descriptions(state.next_states),\n                \"chat_history\": self._chat_history,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            error = manager_result.error.to_dict()\n            logger.error(f\"GraphOrchestrator {self.id}: Error generating final answer: {error}\")\n            raise OrchestratorError(\"Failed to generate final answer\")\n\n        try:\n            next_state = json.loads(\n                manager_result.output.get(\"content\").get(\"result\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n            )[\"state\"]\n\n            if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n                self.manager.stream_content(\n                    content={\"next_state\": next_state},\n                    step=\"manager_planning\",\n                    source=self.name,\n                    config=config,\n                    **kwargs,\n                )\n\n        except Exception as e:\n            logger.error(\"GraphOrchestrator: Error when parsing response about next state.\")\n            raise OrchestratorError(f\"Error when parsing response about next state {e}\")\n\n        if next_state in self._state_by_id:\n            return self._state_by_id[next_state]\n        else:\n            logger.error(f\"GraphOrchestrator: State with id {next_state} was not found.\")\n            raise StateNotFoundError(f\"State with id {next_state} was not found.\")\n\n    def _get_next_state(self, state: GraphState, config: RunnableConfig = None, **kwargs) -&gt; GraphState:\n        \"\"\"\n        Determine the next state based on the current state and chat history.\n\n        Returns:\n            state (State): Current state.\n\n        Raises:\n            OrchestratorError: If there is an error parsing output of conditional edge.\n            StateNotFoundError: If the state is invalid or not found.\n        \"\"\"\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self._chat_history])\n\n        logger.debug(f\"GraphOrchestrator {self.id}: PROMPT {prompt}\")\n\n        if len(state.next_states) &gt; 1:\n            if condition := state.condition:\n                if isinstance(condition, Python):\n                    input_data = {**self.context, \"history\": self._chat_history}\n                else:\n                    input_data = {\"context\": self.context | {\"history\": self._chat_history}}\n\n                next_state = condition.run(\n                    input_data=input_data, config=config, run_depends=self._run_depends, **kwargs\n                ).output.get(\"content\")\n\n                self._run_depends = [NodeDependency(node=condition).to_dict(for_tracing=True)]\n\n                if not isinstance(next_state, str):\n                    raise OrchestratorError(\n                        f\"Error: Condition return invalid type. Expected a string got {type(next_state)} \"\n                    )\n\n                if next_state not in self._state_by_id:\n                    raise StateNotFoundError(f\"State with id {next_state} was not found.\")\n\n                return self._state_by_id[next_state]\n            else:\n                return self.get_next_state_by_manager(state, config)\n        else:\n            return self._state_by_id[state.next_states[0]]\n\n    def states_descriptions(self, states: list[str]) -&gt; str:\n        \"\"\"Get a formatted string of states descriptions.\"\"\"\n        return \"\\n\".join(\n            [f\"'{self._state_by_id[state].name}': {self._state_by_id[state].description}\" for state in states]\n        )\n\n    def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the graph workflow.\n\n        Args:\n            input_task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            dict[str, Any]: The final output generated after processing the task and inner context of orchestrator.\n        \"\"\"\n        if self.input_analysis_enabled:\n            analysis = self._analyze_user_input(\n                input_task, self.states_descriptions(list(self._state_by_id.keys())), config=config, **kwargs\n            )\n            decision = analysis.decision\n            message = analysis.message\n        else:\n            decision = Decision.PLAN\n\n        if decision == Decision.RESPOND:\n            return {\"content\": message}\n        else:\n            self._chat_history.append({\"role\": \"user\", \"content\": input_task})\n            state = self._state_by_id[self.initial_state]\n\n            for _ in range(self.max_loops):\n                logger.info(f\"GraphOrchestrator {self.id}: Next state: {state.id}\")\n\n                if state.id == END:\n                    final_output = self._chat_history[-1][\"content\"] if self._chat_history else \"\"\n                    return {\"content\": final_output, \"context\": self.context | {\"history\": self._chat_history}}\n\n                elif state.id != START:\n\n                    output = state.run(\n                        input_data={\"context\": self.context, \"chat_history\": self._chat_history},\n                        config=config,\n                        run_depends=self._run_depends,\n                        **kwargs,\n                    )\n                    if output.status != RunnableStatus.SUCCESS:\n                        raise OrchestratorError(output.error.message)\n\n                    output = output.output\n                    self.context = self.context | output[\"context\"]\n                    self._run_depends = [NodeDependency(node=state).to_dict(for_tracing=True)]\n                    self._chat_history = self._chat_history + output[\"history_messages\"]\n\n                state = self._get_next_state(state, config=config, **kwargs)\n\n    def setup_streaming(self) -&gt; None:\n        \"\"\"Setups streaming for orchestrator.\"\"\"\n        self.manager.streaming = self.streaming\n        for state in self.states:\n            for task in state.tasks:\n                task.streaming = self.streaming\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.add_conditional_edge","title":"<code>add_conditional_edge(source_id, destination_ids, condition, callbacks=[])</code>","text":"<p>Adds conditional edge to the graph. Conditional edge provides opportunity to choose between destination states based on condition.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Id of the source state.</p> required <code>destination_ids</code> <code>list[str]</code> <p>Ids of destination states.</p> required <code>condition</code> <code>Callable | Python</code> <p>Condition that will determine next state.</p> required <code>callbacks</code> <code>list[NodeCallbackHandler]</code> <p>list[NodeCallbackHandler]: List of callbacks.</p> <code>[]</code> <p>Raises:     ValueError: If state with specified id is not present.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def add_conditional_edge(\n    self,\n    source_id: str,\n    destination_ids: list[str],\n    condition: Callable | Python,\n    callbacks: list[NodeCallbackHandler] = [],\n) -&gt; None:\n    \"\"\"\n    Adds conditional edge to the graph.\n    Conditional edge provides opportunity to choose between destination states based on condition.\n\n    Args:\n        source_id (str): Id of the source state.\n        destination_ids (list[str]): Ids of destination states.\n        condition (Callable | Python): Condition that will determine next state.\n        callbacks: list[NodeCallbackHandler]: List of callbacks.\n    Raises:\n        ValueError: If state with specified id is not present.\n    \"\"\"\n    self.validate_states(destination_ids + [source_id])\n\n    if isinstance(condition, Python):\n        condition.callbacks.extend(callbacks)\n        self._state_by_id[source_id].condition = condition\n    elif isinstance(condition, Callable):\n        tool = function_tool(condition)()\n        tool.callbacks = callbacks\n        self._state_by_id[source_id].condition = tool\n    else:\n        raise OrchestratorError(\"Error: Conditional edge must be either a Python Node or a Callable.\")\n\n    self._state_by_id[source_id].next_states = destination_ids\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.add_edge","title":"<code>add_edge(source_id, destination_id)</code>","text":"<p>Adds edge to the graph. When source state finishes execution, destination state will be executed next.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Id of source state.</p> required <code>destination_id</code> <code>str</code> <p>Id of destination state.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If state with specified id does not exist.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def add_edge(self, source_id: str, destination_id: str) -&gt; None:\n    \"\"\"\n    Adds edge to the graph. When source state finishes execution, destination state will be executed next.\n\n    Args:\n        source_id (str): Id of source state.\n        destination_id (str): Id of destination state.\n\n    Raises:\n        ValueError: If state with specified id does not exist.\n    \"\"\"\n    self.validate_states([source_id, destination_id])\n    self._state_by_id[source_id].next_states = [destination_id]\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.add_state","title":"<code>add_state(state)</code>","text":"<p>Adds state to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State to add to the graph.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If state with specified id already exists.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def add_state(self, state: GraphState) -&gt; None:\n    \"\"\"\n    Adds state to the graph.\n\n    Args:\n        state (State): State to add to the graph.\n\n    Raises:\n        ValueError: If state with specified id already exists.\n    \"\"\"\n    if state.id in self._state_by_id:\n        raise ValueError(f\"Error: State with id {state.id} already exists.\")\n\n    self.states.append(state)\n    self._state_by_id[state.id] = state\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.add_state_by_tasks","title":"<code>add_state_by_tasks(state_id, tasks, callbacks=[])</code>","text":"<p>Adds state to the graph based on tasks.</p> <p>Parameters:</p> Name Type Description Default <code>state_id</code> <code>str</code> <p>Id of the state.</p> required <code>tasks</code> <code>list[Node | Callable]</code> <p>List of tasks that have to be executed when running this state.</p> required <code>callbacks</code> <code>list[NodeCallbackHandler]</code> <p>list[NodeCallbackHandler]: List of callbacks.</p> <code>[]</code> <p>Raises:     ValueError: If state with specified id already exists.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def add_state_by_tasks(\n    self, state_id: str, tasks: list[Node | Callable], callbacks: list[NodeCallbackHandler] = []\n) -&gt; None:\n    \"\"\"\n    Adds state to the graph based on tasks.\n\n    Args:\n        state_id (str): Id of the state.\n        tasks (list[Node | Callable]): List of tasks that have to be executed when running this state.\n        callbacks: list[NodeCallbackHandler]: List of callbacks.\n    Raises:\n        ValueError: If state with specified id already exists.\n    \"\"\"\n    if state_id in self._state_by_id:\n        raise ValueError(f\"Error: State with id {state_id} already exists.\")\n\n    filtered_tasks = []\n\n    has_agent = False\n    for task in tasks:\n        if isinstance(task, Node):\n            if isinstance(task, Agent):\n                has_agent = True\n            filtered_tasks.append(task)\n        elif isinstance(task, Callable):\n            filtered_tasks.append(function_tool(task)())\n        else:\n            raise OrchestratorError(\"Error: Task must be either a Node or a Callable.\")\n\n    state = GraphState(\n        id=state_id,\n        name=state_id,\n        manager=self.manager if has_agent else None,\n        tasks=filtered_tasks,\n        callbacks=callbacks,\n    )\n    self.states.append(state)\n    self._state_by_id[state.id] = state\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.get_next_state_by_manager","title":"<code>get_next_state_by_manager(state, config, **kwargs)</code>","text":"<p>Determine the next state based on the current state and history. Uses GraphAgentManager.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>Current state.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>State</code> <code>GraphState</code> <p>Next state to execute.</p> <p>Raises:</p> Type Description <code>OrchestratorError</code> <p>If there is an error parsing the action from the LLM response.</p> <code>StateNotFoundError</code> <p>If the state is invalid or not found.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def get_next_state_by_manager(self, state: GraphState, config: RunnableConfig, **kwargs) -&gt; GraphState:\n    \"\"\"\n    Determine the next state based on the current state and history. Uses GraphAgentManager.\n\n    Args:\n        state (State): Current state.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        State: Next state to execute.\n\n    Raises:\n        OrchestratorError: If there is an error parsing the action from the LLM response.\n        StateNotFoundError: If the state is invalid or not found.\n    \"\"\"\n    manager_result = self.manager.run(\n        input_data={\n            \"action\": \"plan\",\n            \"states_description\": self.states_descriptions(state.next_states),\n            \"chat_history\": self._chat_history,\n        },\n        config=config,\n        run_depends=self._run_depends,\n        **kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        error = manager_result.error.to_dict()\n        logger.error(f\"GraphOrchestrator {self.id}: Error generating final answer: {error}\")\n        raise OrchestratorError(\"Failed to generate final answer\")\n\n    try:\n        next_state = json.loads(\n            manager_result.output.get(\"content\").get(\"result\").replace(\"json\", \"\").replace(\"```\", \"\").strip()\n        )[\"state\"]\n\n        if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n            self.manager.stream_content(\n                content={\"next_state\": next_state},\n                step=\"manager_planning\",\n                source=self.name,\n                config=config,\n                **kwargs,\n            )\n\n    except Exception as e:\n        logger.error(\"GraphOrchestrator: Error when parsing response about next state.\")\n        raise OrchestratorError(f\"Error when parsing response about next state {e}\")\n\n    if next_state in self._state_by_id:\n        return self._state_by_id[next_state]\n    else:\n        logger.error(f\"GraphOrchestrator: State with id {next_state} was not found.\")\n        raise StateNotFoundError(f\"State with id {next_state} was not found.\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components of the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize components of the orchestrator.\n\n    Args:\n        connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    super().init_components(connection_manager)\n\n    if self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for state in self.states:\n        if state.is_postponed_component_init:\n            state.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.run_flow","title":"<code>run_flow(input_task, config=None, **kwargs)</code>","text":"<p>Process the graph workflow.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The final output generated after processing the task and inner context of orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the graph workflow.\n\n    Args:\n        input_task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        dict[str, Any]: The final output generated after processing the task and inner context of orchestrator.\n    \"\"\"\n    if self.input_analysis_enabled:\n        analysis = self._analyze_user_input(\n            input_task, self.states_descriptions(list(self._state_by_id.keys())), config=config, **kwargs\n        )\n        decision = analysis.decision\n        message = analysis.message\n    else:\n        decision = Decision.PLAN\n\n    if decision == Decision.RESPOND:\n        return {\"content\": message}\n    else:\n        self._chat_history.append({\"role\": \"user\", \"content\": input_task})\n        state = self._state_by_id[self.initial_state]\n\n        for _ in range(self.max_loops):\n            logger.info(f\"GraphOrchestrator {self.id}: Next state: {state.id}\")\n\n            if state.id == END:\n                final_output = self._chat_history[-1][\"content\"] if self._chat_history else \"\"\n                return {\"content\": final_output, \"context\": self.context | {\"history\": self._chat_history}}\n\n            elif state.id != START:\n\n                output = state.run(\n                    input_data={\"context\": self.context, \"chat_history\": self._chat_history},\n                    config=config,\n                    run_depends=self._run_depends,\n                    **kwargs,\n                )\n                if output.status != RunnableStatus.SUCCESS:\n                    raise OrchestratorError(output.error.message)\n\n                output = output.output\n                self.context = self.context | output[\"context\"]\n                self._run_depends = [NodeDependency(node=state).to_dict(for_tracing=True)]\n                self._chat_history = self._chat_history + output[\"history_messages\"]\n\n            state = self._get_next_state(state, config=config, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.setup_streaming","title":"<code>setup_streaming()</code>","text":"<p>Setups streaming for orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def setup_streaming(self) -&gt; None:\n    \"\"\"Setups streaming for orchestrator.\"\"\"\n    self.manager.streaming = self.streaming\n    for state in self.states:\n        for task in state.tasks:\n            task.streaming = self.streaming\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.states_descriptions","title":"<code>states_descriptions(states)</code>","text":"<p>Get a formatted string of states descriptions.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def states_descriptions(self, states: list[str]) -&gt; str:\n    \"\"\"Get a formatted string of states descriptions.\"\"\"\n    return \"\\n\".join(\n        [f\"'{self._state_by_id[state].name}': {self._state_by_id[state].description}\" for state in states]\n    )\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"manager\"] = self.manager.to_dict(**kwargs)\n    data[\"states\"] = [state.to_dict(**kwargs) for state in self.states]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.GraphOrchestrator.validate_states","title":"<code>validate_states(ids)</code>","text":"<p>Check if the provided state ids are valid.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>State ids to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If state with specified id does not exist.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>def validate_states(self, ids: list[str]) -&gt; None:\n    \"\"\"\n    Check if the provided state ids are valid.\n\n    Args:\n        ids (list[str]): State ids to validate.\n\n    Raises:\n        ValueError: If state with specified id does not exist.\n    \"\"\"\n    for state_id in ids:\n        if state_id not in self._state_by_id:\n            raise ValueError(f\"State with id {state_id} does not exist\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph/#dynamiq.nodes.agents.orchestrators.graph.StateNotFoundError","title":"<code>StateNotFoundError</code>","text":"<p>               Bases: <code>OrchestratorError</code></p> <p>Raised when next state was not found.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph.py</code> <pre><code>class StateNotFoundError(OrchestratorError):\n    \"\"\"Raised when next state was not found.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_manager/","title":"Graph manager","text":""},{"location":"dynamiq/nodes/agents/orchestrators/graph_manager/#dynamiq.nodes.agents.orchestrators.graph_manager.GraphAgentManager","title":"<code>GraphAgentManager</code>","text":"<p>               Bases: <code>AgentManager</code></p> <p>A graph agent manager that coordinates graph flow execution.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_manager.py</code> <pre><code>class GraphAgentManager(AgentManager):\n    \"\"\"A graph agent manager that coordinates graph flow execution.\"\"\"\n\n    name: str = \"Graph Manager\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the GraphAgentManager and set up prompt templates.\"\"\"\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initialize the prompt blocks with finding next state, actions and final answer prompts.\"\"\"\n        super()._init_prompt_blocks()\n        self.system_prompt_manager.update_blocks(\n            {\n                \"plan\": self._get_next_state_prompt(),\n                \"assign\": self._get_actions_prompt(),\n                \"handle_input\": self._get_graph_handle_input_prompt(),\n            }\n        )\n\n    @staticmethod\n    def _get_graph_handle_input_prompt() -&gt; str:\n        \"\"\"Determines how to handle input, either by continuing the flow or providing a direct response.\"\"\"\n        return PROMPT_TEMPLATE_GRAPH_HANDLE_INPUT\n\n    @staticmethod\n    def _get_next_state_prompt() -&gt; str:\n        \"\"\"Return next step prompt template.\"\"\"\n        return PROMPT_TEMPLATE_GRAPH_PLAN\n\n    @staticmethod\n    def _get_actions_prompt() -&gt; str:\n        \"\"\"Return actions prompt template.\"\"\"\n        return PROMPT_TEMPLATE_GRAPH_ASSIGN\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_manager/#dynamiq.nodes.agents.orchestrators.graph_manager.GraphAgentManager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the GraphAgentManager and set up prompt templates.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the GraphAgentManager and set up prompt templates.\"\"\"\n    super().__init__(**kwargs)\n    self._init_prompt_blocks()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/","title":"Graph state","text":""},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState","title":"<code>GraphState</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents single state of graph flow</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the state.</p> <code>name</code> <code>str</code> <p>Name of the state</p> <code>description</code> <code>str</code> <p>Description of the state.</p> <code>next_states</code> <code>list[str]</code> <p>List of adjacent node</p> <code>tasks</code> <code>list[Node]</code> <p>List of tasks that have to be executed in this state.</p> <code>condition</code> <code>Python | FunctionTool</code> <p>Condition that determines next state to execute.</p> <code>manager</code> <code>GraphAgentManager</code> <p>The managing agent responsible for overseeing state execution.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>class GraphState(Node):\n    \"\"\"Represents single state of graph flow\n\n    Attributes:\n        id (str): Unique identifier for the state.\n        name (str): Name of the state\n        description (str): Description of the state.\n        next_states (list[str]): List of adjacent node\n        tasks (list[Node]): List of tasks that have to be executed in this state.\n        condition (Python | FunctionTool): Condition that determines next state to execute.\n        manager (GraphAgentManager): The managing agent responsible for overseeing state execution.\n    \"\"\"\n\n    id: str\n    name: str = \"State\"\n    group: NodeGroup = NodeGroup.UTILS\n    input_schema: ClassVar[type[StateInputSchema]] = StateInputSchema\n    description: str = \"\"\n    next_states: list[str] = []\n    tasks: list[Node] = []\n    condition: Python | FunctionTool | None = None\n    manager: GraphAgentManager | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_manager(self):\n        for task in self.tasks:\n            if isinstance(task, Agent) and not self.manager:\n                raise ValueError(\"Error: Provide manager to state to execute agent tasks.\")\n\n        return self\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"tasks\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        if self.manager:\n            data[\"manager\"] = self.manager.to_dict(**kwargs)\n        else:\n            data[\"manager\"] = None\n        data[\"tasks\"] = [task.to_dict(**kwargs) for task in self.tasks]\n        return data\n\n    def merge_contexts(self, context_list: list[dict[str, Any]]) -&gt; dict:\n        \"\"\"\n        Merges contexts. Raises error when lossless merging is not possible.\n\n        Args:\n            context_list (list[dict[str, Any]]): List of contexts to merge.\n        Raises:\n            OrchestratorError: If multiple changes of the same context variable are detected.\n        \"\"\"\n        merged_dict = {}\n\n        for d in context_list:\n            for key, value in d.items():\n                if key in merged_dict:\n                    if merged_dict[key] != value:\n                        raise OrchestratorError(f\"Error: multiple changes of context variable {key} are detected.\")\n                merged_dict[key] = value\n\n        return merged_dict\n\n    def agent_description(self, agent: Agent) -&gt; str:\n        \"\"\"\n        Creates agent description.\n\n        Args:\n            agent (Agent): Agent for which to provide a description.\n\n        Return:\n            str: Description of the agent.\n        \"\"\"\n        return f\"Name: {agent.name}. Role: {agent.role}\"\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize components of the orchestrator.\n\n        Args:\n            connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.manager and self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for task in self.tasks:\n            if task.is_postponed_component_init:\n                task.init_components(connection_manager)\n\n    def validate_input_transformer(self, task: Node, input_data: dict[str, Any], **kwargs) -&gt; bool:\n        \"\"\"\n        Validates whether input data after transformation is a correct input for the task.\n\n        Args:\n            task (Node): Task that have to be executed.\n            input_data (dict[str, Any]): Original input to the task.\n\n        Return:\n            bool: Whether input data is correct.\n        \"\"\"\n\n        try:\n            if task.input_transformer and (task.input_transformer.path or task.input_transformer.selector):\n                output = task.transform(input_data, task.input_transformer)\n                task.validate_input_schema(output, **kwargs)\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error occurred while applying the InputTransformer to the {task.name} Node. {e}\")\n            return False\n\n    def _submit_task(\n        self,\n        task: Node,\n        global_context: dict[str, Any],\n        chat_history: list[dict[str, str]],\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; tuple[str, dict[str, Any]]:\n        \"\"\"Executes single task.\n\n        Args:\n            task (Node): Task to be executed.\n            global_context (dict[str, Any]): Current context of the execution.\n            chat_history (list[dict[str, str]]): List of history messages.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The result of the task execution.\n            dict[str, Any]: Updates to the context.\n\n        Raises:\n            OrchestratorError: If an error occurs during the execution process.\n\n        \"\"\"\n\n        run_depends = []\n\n        if isinstance(task, Agent):\n            agent_input = {\"context\": global_context | {\"history\": chat_history}}\n            is_input_correct = self.validate_input_transformer(task, agent_input, **kwargs)\n            if not is_input_correct:\n                manager_result = self.manager.run(\n                    input_data={\n                        \"action\": \"assign\",\n                        \"task\": self.agent_description(task),\n                        \"chat_history\": chat_history,\n                    },\n                    run_depends=run_depends,\n                    config=config,\n                    **kwargs,\n                )\n\n                run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n                if manager_result.status != RunnableStatus.SUCCESS:\n                    result = manager_result.to_dict()\n                    logger.error(f\"GraphOrchestrator: Error generating actions for state: {result}\")\n                    raise OrchestratorError(f\"GraphOrchestrator: Error generating actions for state: {result}\")\n\n                try:\n                    agent_input = {\n                        \"input\": json.loads(\n                            manager_result.output.get(\"content\")\n                            .get(\"result\")\n                            .replace(\"json\", \"\")\n                            .replace(\"```\", \"\")\n                            .strip()\n                        )[\"input\"]\n                    }\n                except Exception as e:\n                    logger.error(f\"GraphOrchestrator: Error when parsing response about next state {e}\")\n                    raise OrchestratorError(f\"Error when parsing response about next state {e}\")\n\n            response = task.run(\n                input_data=agent_input,\n                config=config,\n                run_depends=run_depends,\n                use_input_transformer=is_input_correct,\n                **kwargs,\n            )\n\n            if response.status != RunnableStatus.SUCCESS:\n                error_msg = response.error.message\n                logger.error(f\"GraphOrchestrator: Failed to execute Agent {task.name} with Error: {error_msg}\")\n                raise OrchestratorError(f\"Failed to execute Agent {task.name} with Error: {error_msg}\")\n\n            result = response.output.get(\"content\")\n            return result, {}\n\n        elif isinstance(task, FunctionTool):\n            input_data = {\"context\": global_context | {\"history\": chat_history}}\n        else:\n            input_data = {**global_context, \"history\": chat_history}\n\n        response = task.run(input_data=input_data, config=config, run_depends=run_depends, **kwargs)\n\n        if response.status != RunnableStatus.SUCCESS:\n            error_msg = response.error.message\n            logger.error(f\"GraphOrchestrator: Failed to execute {task.name} with Error: {error_msg}\")\n            raise OrchestratorError(f\"Failed to execute {task.name} with Error: {error_msg}\")\n\n        context = response.output.get(\"content\")\n\n        if isinstance(task, FunctionTool) or isinstance(task, Python):\n            if not isinstance(context, dict):\n                raise OrchestratorError(\n                    f\"Error: Task returned invalid data format. Expected a dictionary got {type(context)}\"\n                )\n\n            if \"result\" not in context:\n                raise OrchestratorError(\"Error: Task returned dictionary with no 'result' key in it.\")\n        else:\n            return context, {}\n\n        context.pop(\"history\", None)\n\n        result = context.pop(\"result\")\n\n        return result, context\n\n    def execute(self, input_data: StateInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict:\n        \"\"\"\n        Execute the State.\n\n        Args:\n            input_data (StateInputSchema): The input data containing context and chat history.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the state execution (updated context and chat history).\n        \"\"\"\n        logger.debug(f\"State {self.id}: starting the flow with input_task:\\n```{input_data}```\")\n        kwargs.pop(\"run_depends\", None)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n\n        global_context = input_data.context\n        chat_history = input_data.chat_history\n\n        history_messages = []\n\n        if len(self.tasks) == 1:\n            result, context = self._submit_task(\n                self.tasks[0],\n                global_context,\n                chat_history,\n                config=config,\n                **kwargs,\n            )\n\n            history_messages.append(\n                {\n                    \"role\": MessageRole.ASSISTANT,\n                    \"content\": result,\n                }\n            )\n\n            global_context = global_context | context\n\n        elif len(self.tasks) &gt; 1:\n\n            contexts = []\n\n            for task in self.tasks:\n\n                result, context = self._submit_task(\n                    task, copy.deepcopy(global_context), copy.deepcopy(chat_history), config=config, **kwargs\n                )\n\n                history_messages.append(\n                    {\n                        \"role\": MessageRole.ASSISTANT,\n                        \"content\": result,\n                    }\n                )\n\n                contexts.append(context)\n            global_context = global_context | self.merge_contexts(contexts)\n\n        return {\"context\": global_context, \"history_messages\": history_messages}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.agent_description","title":"<code>agent_description(agent)</code>","text":"<p>Creates agent description.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>Agent for which to provide a description.</p> required Return <p>str: Description of the agent.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def agent_description(self, agent: Agent) -&gt; str:\n    \"\"\"\n    Creates agent description.\n\n    Args:\n        agent (Agent): Agent for which to provide a description.\n\n    Return:\n        str: Description of the agent.\n    \"\"\"\n    return f\"Name: {agent.name}. Role: {agent.role}\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the State.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>StateInputSchema</code> <p>The input data containing context and chat history.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, Any]: The result of the state execution (updated context and chat history).</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def execute(self, input_data: StateInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict:\n    \"\"\"\n    Execute the State.\n\n    Args:\n        input_data (StateInputSchema): The input data containing context and chat history.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the state execution (updated context and chat history).\n    \"\"\"\n    logger.debug(f\"State {self.id}: starting the flow with input_task:\\n```{input_data}```\")\n    kwargs.pop(\"run_depends\", None)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n\n    global_context = input_data.context\n    chat_history = input_data.chat_history\n\n    history_messages = []\n\n    if len(self.tasks) == 1:\n        result, context = self._submit_task(\n            self.tasks[0],\n            global_context,\n            chat_history,\n            config=config,\n            **kwargs,\n        )\n\n        history_messages.append(\n            {\n                \"role\": MessageRole.ASSISTANT,\n                \"content\": result,\n            }\n        )\n\n        global_context = global_context | context\n\n    elif len(self.tasks) &gt; 1:\n\n        contexts = []\n\n        for task in self.tasks:\n\n            result, context = self._submit_task(\n                task, copy.deepcopy(global_context), copy.deepcopy(chat_history), config=config, **kwargs\n            )\n\n            history_messages.append(\n                {\n                    \"role\": MessageRole.ASSISTANT,\n                    \"content\": result,\n                }\n            )\n\n            contexts.append(context)\n        global_context = global_context | self.merge_contexts(contexts)\n\n    return {\"context\": global_context, \"history_messages\": history_messages}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components of the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize components of the orchestrator.\n\n    Args:\n        connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.manager and self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for task in self.tasks:\n        if task.is_postponed_component_init:\n            task.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.merge_contexts","title":"<code>merge_contexts(context_list)</code>","text":"<p>Merges contexts. Raises error when lossless merging is not possible.</p> <p>Parameters:</p> Name Type Description Default <code>context_list</code> <code>list[dict[str, Any]]</code> <p>List of contexts to merge.</p> required <p>Raises:     OrchestratorError: If multiple changes of the same context variable are detected.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def merge_contexts(self, context_list: list[dict[str, Any]]) -&gt; dict:\n    \"\"\"\n    Merges contexts. Raises error when lossless merging is not possible.\n\n    Args:\n        context_list (list[dict[str, Any]]): List of contexts to merge.\n    Raises:\n        OrchestratorError: If multiple changes of the same context variable are detected.\n    \"\"\"\n    merged_dict = {}\n\n    for d in context_list:\n        for key, value in d.items():\n            if key in merged_dict:\n                if merged_dict[key] != value:\n                    raise OrchestratorError(f\"Error: multiple changes of context variable {key} are detected.\")\n            merged_dict[key] = value\n\n    return merged_dict\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    if self.manager:\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n    else:\n        data[\"manager\"] = None\n    data[\"tasks\"] = [task.to_dict(**kwargs) for task in self.tasks]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/graph_state/#dynamiq.nodes.agents.orchestrators.graph_state.GraphState.validate_input_transformer","title":"<code>validate_input_transformer(task, input_data, **kwargs)</code>","text":"<p>Validates whether input data after transformation is a correct input for the task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Node</code> <p>Task that have to be executed.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Original input to the task.</p> required Return <p>bool: Whether input data is correct.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/graph_state.py</code> <pre><code>def validate_input_transformer(self, task: Node, input_data: dict[str, Any], **kwargs) -&gt; bool:\n    \"\"\"\n    Validates whether input data after transformation is a correct input for the task.\n\n    Args:\n        task (Node): Task that have to be executed.\n        input_data (dict[str, Any]): Original input to the task.\n\n    Return:\n        bool: Whether input data is correct.\n    \"\"\"\n\n    try:\n        if task.input_transformer and (task.input_transformer.path or task.input_transformer.selector):\n            output = task.transform(input_data, task.input_transformer)\n            task.validate_input_schema(output, **kwargs)\n            return True\n        else:\n            return False\n\n    except Exception as e:\n        logger.error(f\"Error occurred while applying the InputTransformer to the {task.name} Node. {e}\")\n        return False\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/","title":"Linear","text":""},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator","title":"<code>LinearOrchestrator</code>","text":"<p>               Bases: <code>Orchestrator</code></p> <p>Manages the execution of tasks by coordinating multiple agents and leveraging LLM (Large Language Model).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str | None</code> <p>Name of the orchestrator.</p> <code>group</code> <code>NodeGroup</code> <p>The group this node belongs to.</p> <code>manager</code> <code>LinearAgentManager</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>agents</code> <code>list[Agent]</code> <p>List of specialized agents available for task execution.</p> <code>use_summarizer</code> <code>bool</code> <p>Indicates if a final summarizer is used.</p> <code>summarize_all_answers</code> <code>bool</code> <p>Indicates whether to summarize answers to all tasks or use only last one. Will only be applied if use_summarizer is set to True.</p> <code>max_plan_retries</code> <code>int</code> <p>Maximum number of plan generation retries.</p> <code>plan_approval</code> <code>PlanApprovalConfig</code> <p>Configuration for plan approval.</p> <code>max_user_analyze_retries</code> <code>int</code> <p>Maximum number of retries for analyzing user input.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>class LinearOrchestrator(Orchestrator):\n    \"\"\"\n    Manages the execution of tasks by coordinating multiple agents and leveraging LLM (Large Language Model).\n\n    Attributes:\n        name (str | None): Name of the orchestrator.\n        group (NodeGroup): The group this node belongs to.\n        manager (LinearAgentManager): The managing agent responsible for overseeing the orchestration process.\n        agents (list[Agent]): List of specialized agents available for task execution.\n        use_summarizer (bool): Indicates if a final summarizer is used.\n        summarize_all_answers (bool): Indicates whether to summarize answers to all tasks\n            or use only last one. Will only be applied if use_summarizer is set to True.\n        max_plan_retries (int): Maximum number of plan generation retries.\n        plan_approval (PlanApprovalConfig): Configuration for plan approval.\n        max_user_analyze_retries (int): Maximum number of retries for analyzing user input.\n    \"\"\"\n\n    name: str | None = \"LinearOrchestrator\"\n    group: NodeGroup = NodeGroup.AGENTS\n    manager: LinearAgentManager\n    agents: list[Agent] = []\n    use_summarizer: bool = True\n    summarize_all_answers: bool = False\n    max_plan_retries: int = 5\n    plan_approval: PlanApprovalConfig = Field(default_factory=PlanApprovalConfig)\n    max_user_analyze_retries: int = 3\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._results = {}\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"agents\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n        data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n        return data\n\n    def reset_run_state(self):\n        super().reset_run_state()\n        self._results = {}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize components for the manager and agents.\n\n        Args:\n            connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for agent in self.agents:\n            if agent.is_postponed_component_init:\n                agent.init_components(connection_manager)\n\n    @cached_property\n    def agents_descriptions(self) -&gt; str:\n        \"\"\"Generate a string description of all agents.\"\"\"\n        return \"\\n\".join([f\"{i}. {_agent.name}\" for i, _agent in enumerate(self.agents)]) if self.agents else \"\"\n\n    def get_tasks(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; list[Task]:\n        \"\"\"\n        Generate tasks using the manager agent.\n\n        Args:\n            input_task (str): The input task to generate subtasks from\n            config (RunnableConfig, optional): Configuration for the runnable\n            **kwargs: Additional keyword arguments passed to the manager's run method\n\n        Returns:\n            list[Task]: List of generated tasks\n\n        Raises:\n            ValueError: If task generation fails\n            OrchestratorError: If maximum number of retries is reached\n        \"\"\"\n        manager_result_content = \"\"\n        feedback = \"\"\n\n        for _ in range(self.max_plan_retries):\n            manager_result = self.manager.run(\n                input_data={\n                    \"action\": \"plan\",\n                    \"input_task\": input_task,\n                    \"agents\": self.agents_descriptions,\n                    \"feedback\": feedback,\n                    \"previous_plan\": manager_result_content,\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n            if manager_result.status != RunnableStatus.SUCCESS:\n                error_message = f\"LLM '{self.manager.name}' failed: {manager_result.error.message}\"\n                raise ValueError(f\"Failed to generate tasks: {error_message}\")\n\n            manager_result_content = manager_result.output.get(\"content\").get(\"result\")\n            logger.info(\n                f\"Orchestrator {self.name} - {self.id}: Tasks generated by {self.manager.name} - {self.manager.id}:\"\n                f\"\\n{manager_result_content}\"\n            )\n            try:\n                tasks = self.parse_tasks_from_output(manager_result_content)\n                if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n                    self.manager.stream_content(\n                        content={\"tasks\": tasks},\n                        step=\"manager_planning\",\n                        source=self.manager.name,\n                        config=config,\n                        **kwargs,\n                    )\n\n            except ActionParseError as e:\n                feedback = str(e)\n                continue\n\n            if not self.plan_approval.enabled:\n                return tasks\n            else:\n                approval_result = self.send_approval_message(\n                    self.plan_approval, {\"tasks\": tasks}, config=config, **kwargs\n                )\n\n                feedback = approval_result.feedback\n                if approval_result.is_approved:\n                    return approval_result.data.get(\"tasks\")\n\n        raise OrchestratorError(\"Maximum number of loops reached for generating plan.\")\n\n    def parse_tasks_from_output(self, output: str) -&gt; list[Task]:\n        \"\"\"Parse tasks from the manager's output string.\"\"\"\n\n        output_match = re.search(r\"&lt;output&gt;(.*?)&lt;/output&gt;\", output, re.DOTALL)\n        if not output_match:\n            error_response = f\"Error parsing final answer: No &lt;output&gt; tags found in the response {output}\"\n            raise ActionParseError(f\"Error: {error_response}\")\n\n        output_content = output_match.group(1).strip()\n\n        try:\n            output_content = self._clean_output(output_content)\n        except AttributeError as e:\n            logger.warning(\n                f\"Orchestrator {self.name} - {self.id}: \"\n                f\"Failed to remove code block markers and 'json' keyword \"\n                f\"from output {output_content} due to error: {e}\"\n            )\n\n        try:\n            task_list_json = output_content.strip()\n        except AttributeError as e:\n            logger.warning(\n                f\"Orchestrator {self.name} - {self.id}: Failed to strip the output {output_content} due to error: {e}\"\n            )\n            task_list_json = output_content\n        return TypeAdapter(list[Task]).validate_json(task_list_json)\n\n    def get_dependency_outputs(self, dependencies: list[int]) -&gt; str:\n        \"\"\"Format the outputs of dependent tasks.\"\"\"\n        if not dependencies:\n            return \"\"\n\n        dependencies_formatted = \"**Here is the previously collected information:**\\n\"\n        for dep in dependencies:\n            if dep in self._results:\n                task_name = self._results[dep][\"name\"]\n                task_result = str(self._results[dep][\"result\"])\n                dependencies_formatted += f\"**Task:** {task_name}\\n**Result:** {task_result}\\n\\n\"\n\n        return dependencies_formatted.strip()\n\n    def run_tasks(self, tasks: list[Task], input_task: str, config: RunnableConfig = None, **kwargs) -&gt; None:\n        \"\"\"Execute the tasks using appropriate agents.\"\"\"\n\n        for count, task in enumerate(tasks, start=1):\n            task_per_llm = f\"**{task.description}**\\n**Required information for output**: {task.output}\"\n\n            dependency_outputs = self.get_dependency_outputs(task.dependencies)\n            if dependency_outputs:\n                task_per_llm += f\"\\n{dependency_outputs}\"\n\n            success_flag = False\n            for _ in range(self.manager.max_loops):\n                manager_result = self.manager.run(\n                    input_data={\n                        \"action\": \"assign\",\n                        \"input_task\": input_task,\n                        \"task\": task_per_llm,\n                        \"agents\": self.agents_descriptions,\n                    },\n                    config=config,\n                    run_depends=self._run_depends,\n                    **kwargs,\n                )\n                self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n                if manager_result.status == RunnableStatus.SUCCESS:\n                    assigned_agent_index = self._extract_agent_index(manager_result.output.get(\"content\", {}))\n\n                    if 0 &lt;= assigned_agent_index &lt; len(self.agents):\n                        assigned_agent = self.agents[assigned_agent_index]\n\n                        if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n                            self.manager.stream_content(\n                                content={\"agent\": assigned_agent, \"task\": task},\n                                step=\"manager_assigning\",\n                                source=self.name,\n                                config=config,\n                                **kwargs,\n                            )\n\n                        logger.info(\n                            f\"Orchestrator {self.name} - {self.id}: Loop {count} - \"\n                            f\"Assigned agent: {assigned_agent.name} - {assigned_agent.id}\"\n                        )\n                        result = assigned_agent.run(\n                            input_data={\"input\": task_per_llm},\n                            config=config,\n                            run_depends=self._run_depends,\n                            **kwargs,\n                        )\n                        self._run_depends = [NodeDependency(node=assigned_agent).to_dict(for_tracing=True)]\n                        if result.status != RunnableStatus.SUCCESS:\n                            raise ValueError(\n                                f\"Failed to execute task {task.id}.{task.name} \"\n                                f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                                f\"due to error: {result.error.message}\"\n                            )\n\n                        self._results[task.id] = {\n                            \"name\": task.name,\n                            \"result\": result.output[\"content\"],\n                        }\n\n                        success_flag = True\n                        break\n                task_per_llm += f\"Error occurred:{manager_result.error.to_dict()}\"\n\n            if success_flag:\n                continue\n\n            else:\n                raise ValueError(\n                    f\"Orchestrator {self.name} - {self.id}: \"\n                    f\"Failed to assign task {task.id}.{task.name} \"\n                    f\"by Manager Agent due to error: \"\n                    f\"{manager_result.error.to_dict() if manager_result.error else manager_result.output}\"\n                )\n\n    def generate_final_answer(self, task: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Generates final answer using the manager agent logic.\n\n        Args:\n            task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            str: The final answer generated after processing the task.\n        \"\"\"\n        tasks_outputs = \"\\n\\n\".join(\n            f\"**Task:** {result['name']}\\n**Result:** {result['result']}\" for result in self._results.values() if result\n        )\n\n        if self.use_summarizer:\n            if not self.summarize_all_answers:\n                final_task_id = max(self._results.keys(), default=None)\n\n                if final_task_id is not None:\n                    final_task_output = self._results[final_task_id].get(\"result\", \"\")\n                    logger.debug(f\"Orchestrator {self.name} - {self.id}: Final task output: {final_task_output}\")\n                    return final_task_output\n\n            final_result_content = self.get_final_result(\n                {\"input_task\": task, \"chat_history\": self._chat_history, \"tasks_outputs\": tasks_outputs},\n                config=config,\n                **kwargs,\n            )\n\n            try:\n                final_result = re.search(r\"&lt;final_answer&gt;(.*?)&lt;/final_answer&gt;\", final_result_content, re.DOTALL)\n                final_result_answer = final_result.group(1).strip()\n                return final_result_answer\n            except Exception as e:\n                error_response = f\"Orchestrator {self.name} - {self.id}: Error parsing final answer: {e}\"\n                logger.error(error_response)\n                if \"final_answer\" in final_result_content:\n                    logger.info(f\"Orchestrator {self.name} - {self.id}: Return raw answer\")\n                    return final_result_content\n                else:\n                    raise ActionParseError(f\"{error_response}\")\n        return tasks_outputs\n\n    def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the given task using the manager agent logic.\n\n        Args:\n            input_task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            dict[str, Any]: The final output generated after processing the task.\n        \"\"\"\n        analysis = self._analyze_user_input(input_task, self.agents_descriptions, config=config, **kwargs)\n        decision = analysis.decision\n        message = analysis.message\n\n        if decision == Decision.RESPOND:\n            return {\"content\": message}\n        else:\n            tasks = self.get_tasks(input_task, config=config, **kwargs)\n            self.run_tasks(tasks=tasks, input_task=input_task, config=config, **kwargs)\n            return {\"content\": self.generate_final_answer(input_task, config, **kwargs)}\n\n    def setup_streaming(self) -&gt; None:\n        \"\"\"Setups streaming for orchestrator.\"\"\"\n        self.manager.streaming = self.streaming\n        for agent in self.agents:\n            agent.streaming = self.streaming\n\n    def extract_json_from_output(self, result_text: str) -&gt; tuple[str, dict] | None:\n        \"\"\"\n        Extracts JSON data from the given text by looking for content within\n        &lt;output&gt;...&lt;/output&gt; and &lt;analysis&gt;...&lt;/analysis&gt; tags. Strips any Markdown code block fences.\n\n        Args:\n            result_text (str): The text from which to extract JSON data.\n\n        Returns:\n            dict | None: The extracted JSON dictionary if successful, otherwise None.\n        \"\"\"\n        analysis, output_content = self._extract_output_content(result_text)\n        output_content = self._clean_output(output_content)\n\n        try:\n            data = json.loads(output_content)\n            return analysis, data\n        except json.JSONDecodeError as e:\n            error_message = f\"Orchestrator {self.name} - {self.id}: JSON decoding error: {e}\"\n            logger.error(error_message)\n            return None\n\n    def _clean_output(self, text: str) -&gt; str:\n        \"\"\"Remove Markdown code fences and extra whitespace from a text.\"\"\"\n        cleaned = re.sub(r\"^```(?:json)?\\s*|```$\", \"\", text).strip()\n        return cleaned\n\n    def _extract_agent_index(self, result_content: dict[str, Any]) -&gt; int:\n        \"\"\"\n        Extracts and validates the agent index from the result content.\n\n        Args:\n            result_content (dict[str, Any]): The content containing the agent index\n\n        Returns:\n            int: The extracted agent index, or -1 if extraction fails\n        \"\"\"\n        raw = result_content.get(\"result\", -1)\n        try:\n            return int(raw)\n        except ValueError:\n            logger.warning(f\"Invalid agent index: {raw}\")\n            match = re.match(r\"^\\d+\", str(raw))\n            if match:\n                return int(match.group())\n            else:\n                logger.error(f\"Failed to extract agent index from: {raw}\")\n                return -1\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.agents_descriptions","title":"<code>agents_descriptions: str</code>  <code>cached</code> <code>property</code>","text":"<p>Generate a string description of all agents.</p>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.extract_json_from_output","title":"<code>extract_json_from_output(result_text)</code>","text":"<p>Extracts JSON data from the given text by looking for content within</p> ... <p>and ... tags. Strips any Markdown code block fences.</p> <p>Parameters:</p> Name Type Description Default <code>result_text</code> <code>str</code> <p>The text from which to extract JSON data.</p> required <p>Returns:</p> Type Description <code>tuple[str, dict] | None</code> <p>dict | None: The extracted JSON dictionary if successful, otherwise None.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def extract_json_from_output(self, result_text: str) -&gt; tuple[str, dict] | None:\n    \"\"\"\n    Extracts JSON data from the given text by looking for content within\n    &lt;output&gt;...&lt;/output&gt; and &lt;analysis&gt;...&lt;/analysis&gt; tags. Strips any Markdown code block fences.\n\n    Args:\n        result_text (str): The text from which to extract JSON data.\n\n    Returns:\n        dict | None: The extracted JSON dictionary if successful, otherwise None.\n    \"\"\"\n    analysis, output_content = self._extract_output_content(result_text)\n    output_content = self._clean_output(output_content)\n\n    try:\n        data = json.loads(output_content)\n        return analysis, data\n    except json.JSONDecodeError as e:\n        error_message = f\"Orchestrator {self.name} - {self.id}: JSON decoding error: {e}\"\n        logger.error(error_message)\n        return None\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.generate_final_answer","title":"<code>generate_final_answer(task, config, **kwargs)</code>","text":"<p>Generates final answer using the manager agent logic.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final answer generated after processing the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def generate_final_answer(self, task: str, config: RunnableConfig, **kwargs) -&gt; str:\n    \"\"\"\n    Generates final answer using the manager agent logic.\n\n    Args:\n        task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        str: The final answer generated after processing the task.\n    \"\"\"\n    tasks_outputs = \"\\n\\n\".join(\n        f\"**Task:** {result['name']}\\n**Result:** {result['result']}\" for result in self._results.values() if result\n    )\n\n    if self.use_summarizer:\n        if not self.summarize_all_answers:\n            final_task_id = max(self._results.keys(), default=None)\n\n            if final_task_id is not None:\n                final_task_output = self._results[final_task_id].get(\"result\", \"\")\n                logger.debug(f\"Orchestrator {self.name} - {self.id}: Final task output: {final_task_output}\")\n                return final_task_output\n\n        final_result_content = self.get_final_result(\n            {\"input_task\": task, \"chat_history\": self._chat_history, \"tasks_outputs\": tasks_outputs},\n            config=config,\n            **kwargs,\n        )\n\n        try:\n            final_result = re.search(r\"&lt;final_answer&gt;(.*?)&lt;/final_answer&gt;\", final_result_content, re.DOTALL)\n            final_result_answer = final_result.group(1).strip()\n            return final_result_answer\n        except Exception as e:\n            error_response = f\"Orchestrator {self.name} - {self.id}: Error parsing final answer: {e}\"\n            logger.error(error_response)\n            if \"final_answer\" in final_result_content:\n                logger.info(f\"Orchestrator {self.name} - {self.id}: Return raw answer\")\n                return final_result_content\n            else:\n                raise ActionParseError(f\"{error_response}\")\n    return tasks_outputs\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.get_dependency_outputs","title":"<code>get_dependency_outputs(dependencies)</code>","text":"<p>Format the outputs of dependent tasks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def get_dependency_outputs(self, dependencies: list[int]) -&gt; str:\n    \"\"\"Format the outputs of dependent tasks.\"\"\"\n    if not dependencies:\n        return \"\"\n\n    dependencies_formatted = \"**Here is the previously collected information:**\\n\"\n    for dep in dependencies:\n        if dep in self._results:\n            task_name = self._results[dep][\"name\"]\n            task_result = str(self._results[dep][\"result\"])\n            dependencies_formatted += f\"**Task:** {task_name}\\n**Result:** {task_result}\\n\\n\"\n\n    return dependencies_formatted.strip()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.get_tasks","title":"<code>get_tasks(input_task, config=None, **kwargs)</code>","text":"<p>Generate tasks using the manager agent.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The input task to generate subtasks from</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the manager's run method</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Task]</code> <p>list[Task]: List of generated tasks</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task generation fails</p> <code>OrchestratorError</code> <p>If maximum number of retries is reached</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def get_tasks(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; list[Task]:\n    \"\"\"\n    Generate tasks using the manager agent.\n\n    Args:\n        input_task (str): The input task to generate subtasks from\n        config (RunnableConfig, optional): Configuration for the runnable\n        **kwargs: Additional keyword arguments passed to the manager's run method\n\n    Returns:\n        list[Task]: List of generated tasks\n\n    Raises:\n        ValueError: If task generation fails\n        OrchestratorError: If maximum number of retries is reached\n    \"\"\"\n    manager_result_content = \"\"\n    feedback = \"\"\n\n    for _ in range(self.max_plan_retries):\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"plan\",\n                \"input_task\": input_task,\n                \"agents\": self.agents_descriptions,\n                \"feedback\": feedback,\n                \"previous_plan\": manager_result_content,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"LLM '{self.manager.name}' failed: {manager_result.error.message}\"\n            raise ValueError(f\"Failed to generate tasks: {error_message}\")\n\n        manager_result_content = manager_result.output.get(\"content\").get(\"result\")\n        logger.info(\n            f\"Orchestrator {self.name} - {self.id}: Tasks generated by {self.manager.name} - {self.manager.id}:\"\n            f\"\\n{manager_result_content}\"\n        )\n        try:\n            tasks = self.parse_tasks_from_output(manager_result_content)\n            if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n                self.manager.stream_content(\n                    content={\"tasks\": tasks},\n                    step=\"manager_planning\",\n                    source=self.manager.name,\n                    config=config,\n                    **kwargs,\n                )\n\n        except ActionParseError as e:\n            feedback = str(e)\n            continue\n\n        if not self.plan_approval.enabled:\n            return tasks\n        else:\n            approval_result = self.send_approval_message(\n                self.plan_approval, {\"tasks\": tasks}, config=config, **kwargs\n            )\n\n            feedback = approval_result.feedback\n            if approval_result.is_approved:\n                return approval_result.data.get(\"tasks\")\n\n    raise OrchestratorError(\"Maximum number of loops reached for generating plan.\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components for the manager and agents.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize components for the manager and agents.\n\n    Args:\n        connection_manager (Optional[ConnectionManager]): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for agent in self.agents:\n        if agent.is_postponed_component_init:\n            agent.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.parse_tasks_from_output","title":"<code>parse_tasks_from_output(output)</code>","text":"<p>Parse tasks from the manager's output string.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def parse_tasks_from_output(self, output: str) -&gt; list[Task]:\n    \"\"\"Parse tasks from the manager's output string.\"\"\"\n\n    output_match = re.search(r\"&lt;output&gt;(.*?)&lt;/output&gt;\", output, re.DOTALL)\n    if not output_match:\n        error_response = f\"Error parsing final answer: No &lt;output&gt; tags found in the response {output}\"\n        raise ActionParseError(f\"Error: {error_response}\")\n\n    output_content = output_match.group(1).strip()\n\n    try:\n        output_content = self._clean_output(output_content)\n    except AttributeError as e:\n        logger.warning(\n            f\"Orchestrator {self.name} - {self.id}: \"\n            f\"Failed to remove code block markers and 'json' keyword \"\n            f\"from output {output_content} due to error: {e}\"\n        )\n\n    try:\n        task_list_json = output_content.strip()\n    except AttributeError as e:\n        logger.warning(\n            f\"Orchestrator {self.name} - {self.id}: Failed to strip the output {output_content} due to error: {e}\"\n        )\n        task_list_json = output_content\n    return TypeAdapter(list[Task]).validate_json(task_list_json)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.run_flow","title":"<code>run_flow(input_task, config=None, **kwargs)</code>","text":"<p>Process the given task using the manager agent logic.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The final output generated after processing the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the given task using the manager agent logic.\n\n    Args:\n        input_task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        dict[str, Any]: The final output generated after processing the task.\n    \"\"\"\n    analysis = self._analyze_user_input(input_task, self.agents_descriptions, config=config, **kwargs)\n    decision = analysis.decision\n    message = analysis.message\n\n    if decision == Decision.RESPOND:\n        return {\"content\": message}\n    else:\n        tasks = self.get_tasks(input_task, config=config, **kwargs)\n        self.run_tasks(tasks=tasks, input_task=input_task, config=config, **kwargs)\n        return {\"content\": self.generate_final_answer(input_task, config, **kwargs)}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.run_tasks","title":"<code>run_tasks(tasks, input_task, config=None, **kwargs)</code>","text":"<p>Execute the tasks using appropriate agents.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def run_tasks(self, tasks: list[Task], input_task: str, config: RunnableConfig = None, **kwargs) -&gt; None:\n    \"\"\"Execute the tasks using appropriate agents.\"\"\"\n\n    for count, task in enumerate(tasks, start=1):\n        task_per_llm = f\"**{task.description}**\\n**Required information for output**: {task.output}\"\n\n        dependency_outputs = self.get_dependency_outputs(task.dependencies)\n        if dependency_outputs:\n            task_per_llm += f\"\\n{dependency_outputs}\"\n\n        success_flag = False\n        for _ in range(self.manager.max_loops):\n            manager_result = self.manager.run(\n                input_data={\n                    \"action\": \"assign\",\n                    \"input_task\": input_task,\n                    \"task\": task_per_llm,\n                    \"agents\": self.agents_descriptions,\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n            if manager_result.status == RunnableStatus.SUCCESS:\n                assigned_agent_index = self._extract_agent_index(manager_result.output.get(\"content\", {}))\n\n                if 0 &lt;= assigned_agent_index &lt; len(self.agents):\n                    assigned_agent = self.agents[assigned_agent_index]\n\n                    if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n                        self.manager.stream_content(\n                            content={\"agent\": assigned_agent, \"task\": task},\n                            step=\"manager_assigning\",\n                            source=self.name,\n                            config=config,\n                            **kwargs,\n                        )\n\n                    logger.info(\n                        f\"Orchestrator {self.name} - {self.id}: Loop {count} - \"\n                        f\"Assigned agent: {assigned_agent.name} - {assigned_agent.id}\"\n                    )\n                    result = assigned_agent.run(\n                        input_data={\"input\": task_per_llm},\n                        config=config,\n                        run_depends=self._run_depends,\n                        **kwargs,\n                    )\n                    self._run_depends = [NodeDependency(node=assigned_agent).to_dict(for_tracing=True)]\n                    if result.status != RunnableStatus.SUCCESS:\n                        raise ValueError(\n                            f\"Failed to execute task {task.id}.{task.name} \"\n                            f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                            f\"due to error: {result.error.message}\"\n                        )\n\n                    self._results[task.id] = {\n                        \"name\": task.name,\n                        \"result\": result.output[\"content\"],\n                    }\n\n                    success_flag = True\n                    break\n            task_per_llm += f\"Error occurred:{manager_result.error.to_dict()}\"\n\n        if success_flag:\n            continue\n\n        else:\n            raise ValueError(\n                f\"Orchestrator {self.name} - {self.id}: \"\n                f\"Failed to assign task {task.id}.{task.name} \"\n                f\"by Manager Agent due to error: \"\n                f\"{manager_result.error.to_dict() if manager_result.error else manager_result.output}\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.setup_streaming","title":"<code>setup_streaming()</code>","text":"<p>Setups streaming for orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def setup_streaming(self) -&gt; None:\n    \"\"\"Setups streaming for orchestrator.\"\"\"\n    self.manager.streaming = self.streaming\n    for agent in self.agents:\n        agent.streaming = self.streaming\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"manager\"] = self.manager.to_dict(**kwargs)\n    data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single task in the LinearOrchestrator system.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the task</p> <code>name</code> <code>str</code> <p>Name of the task</p> <code>description</code> <code>str</code> <p>Detailed description of the task</p> <code>dependencies</code> <code>list[int]</code> <p>List of task IDs that this task depends on</p> <code>output</code> <code>Union[dict[str, Any], str]</code> <p>Expected output of the task, either as a structured dictionary or string</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"\n    Represents a single task in the LinearOrchestrator system.\n\n    Attributes:\n        id (int): Unique identifier for the task\n        name (str): Name of the task\n        description (str): Detailed description of the task\n        dependencies (list[int]): List of task IDs that this task depends on\n        output (Union[dict[str, Any], str]): Expected output of the task,\n            either as a structured dictionary or string\n    \"\"\"\n\n    id: int\n    name: str\n    description: str\n    dependencies: list[int]\n    output: dict[str, Any] | str\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/","title":"Linear manager","text":""},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/#dynamiq.nodes.agents.orchestrators.linear_manager.LinearAgentManager","title":"<code>LinearAgentManager</code>","text":"<p>               Bases: <code>AgentManager</code></p> <p>A specialized AgentManager that manages tasks in a linear, sequential order. It uses predefined prompts to plan tasks, assign them to the appropriate agents, and compile the final result.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear_manager.py</code> <pre><code>class LinearAgentManager(AgentManager):\n    \"\"\"\n    A specialized AgentManager that manages tasks in a linear, sequential order.\n    It uses predefined prompts to plan tasks, assign them to the appropriate agents,\n    and compile the final result.\n    \"\"\"\n\n    name: str = \"Linear Manager\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LinearAgentManager and sets up the prompt blocks.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_actions(self):\n        \"\"\"\n        Extend default actions with 'respond'.\n        \"\"\"\n        super()._init_actions()\n        self._actions[\"handle_input\"] = self._handle_input\n\n    def _init_prompt_blocks(self):\n        \"\"\"\n        Initializes the prompt blocks used in the task planning, assigning,\n        and final answer generation processes.\n        \"\"\"\n        super()._init_prompt_blocks()\n        self.system_prompt_manager.update_blocks(\n            {\n                \"plan\": self._get_linear_plan_prompt(),\n                \"assign\": self._get_linear_assign_prompt(),\n                \"final\": self._get_linear_final_prompt(),\n                \"handle_input\": self._get_linear_handle_input_prompt(),\n            }\n        )\n\n    @staticmethod\n    def _get_linear_plan_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for planning tasks.\n        \"\"\"\n        return PROMPT_TEMPLATE_LINEAR_PLAN\n\n    @staticmethod\n    def _get_linear_assign_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for assigning tasks to agents.\n        \"\"\"\n        return PROMPT_TEMPLATE_LINEAR_ASSIGN\n\n    @staticmethod\n    def _get_linear_final_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for generating the final answer.\n        \"\"\"\n        return PROMPT_TEMPLATE_LINEAR_FINAL\n\n    @staticmethod\n    def _get_linear_handle_input_prompt() -&gt; str:\n        \"\"\"Determines how to handle input, either by continuing the flow or providing a direct response.\"\"\"\n        return PROMPT_TEMPLATE_BASE_HANDLE_INPUT\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/#dynamiq.nodes.agents.orchestrators.linear_manager.LinearAgentManager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LinearAgentManager and sets up the prompt blocks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear_manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LinearAgentManager and sets up the prompt blocks.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._init_prompt_blocks()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/","title":"Orchestrator","text":""},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.ActionParseError","title":"<code>ActionParseError</code>","text":"<p>               Bases: <code>OrchestratorError</code></p> <p>Exception raised when an error occurs during action parsing.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>class ActionParseError(OrchestratorError):\n    \"\"\"Exception raised when an error occurs during action parsing.\"\"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Decision","title":"<code>Decision</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for possible decisions after analyzing the user input.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>class Decision(str, Enum):\n    \"\"\"\n    Enumeration for possible decisions after analyzing the user input.\n    \"\"\"\n\n    RESPOND = \"respond\"\n    PLAN = \"plan\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.DecisionResult","title":"<code>DecisionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Holds the result of analyzing the user input.</p> <p>Attributes:</p> Name Type Description <code>decision</code> <code>Decision</code> <p>The decision on how to handle the input.</p> <code>message</code> <code>str</code> <p>The message or response associated with the decision.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>class DecisionResult(BaseModel):\n    \"\"\"\n    Holds the result of analyzing the user input.\n\n    Attributes:\n        decision (Decision): The decision on how to handle the input.\n        message (str): The message or response associated with the decision.\n    \"\"\"\n\n    decision: Decision\n    message: str\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator","title":"<code>Orchestrator</code>","text":"<p>               Bases: <code>Node</code>, <code>ABC</code></p> <p>Orchestrates the execution of complex tasks using multiple specialized agents.</p> <p>This abstract base class provides the framework for orchestrating complex tasks through multiple agents. It manages the execution flow and communication between different specialized agents.</p> <p>Attributes:</p> Name Type Description <code>manager</code> <code>ManagerAgent</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>objective</code> <code>Optional[str]</code> <p>The main objective of the orchestration.</p> Abstract Methods <p>run_flow: Processes the given task and returns the result. setup_streaming: Configures streaming functionality for the orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>class Orchestrator(Node, ABC):\n    \"\"\"\n    Orchestrates the execution of complex tasks using multiple specialized agents.\n\n    This abstract base class provides the framework for orchestrating complex tasks\n    through multiple agents. It manages the execution flow and communication between\n    different specialized agents.\n\n    Attributes:\n        manager (ManagerAgent): The managing agent responsible for overseeing the orchestration process.\n        objective (Optional[str]): The main objective of the orchestration.\n\n    Abstract Methods:\n        run_flow: Processes the given task and returns the result.\n        setup_streaming: Configures streaming functionality for the orchestrator.\n    \"\"\"\n\n    name: str | None = \"Orchestrator\"\n    group: NodeGroup = NodeGroup.AGENTS\n    input_schema: ClassVar[type[OrchestratorInputSchema]] = OrchestratorInputSchema\n    manager: AgentManager\n    objective: str = \"\"\n    enable_handle_input: bool = True\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the orchestrator with given parameters.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n        self._chat_history = []\n\n    def _clean_output(self, text: str) -&gt; str:\n        \"\"\"Remove Markdown code fences and extra whitespace from a text.\"\"\"\n        cleaned = re.sub(r\"^```(?:json)?\\s*|```$\", \"\", text).strip()\n        return cleaned\n\n    def extract_json_from_output(self, result_text: str) -&gt; tuple[str, dict] | None:\n        \"\"\"\n        Extracts JSON data from the given text by looking for content within\n        &lt;output&gt;...&lt;/output&gt; and &lt;analysis&gt;...&lt;/analysis&gt; tags. Strips any Markdown code block fences.\n\n        Args:\n            result_text (str): The text from which to extract JSON data.\n\n        Returns:\n            dict | None: The extracted JSON dictionary if successful, otherwise None.\n        \"\"\"\n        analysis, output_content = self._extract_output_content(result_text)\n        output_content = self._clean_output(output_content)\n\n        try:\n            data = json.loads(output_content)\n            return analysis, data\n        except json.JSONDecodeError as e:\n            error_message = f\"Orchestrator {self.name} - {self.id}: JSON decoding error: {e}\"\n            logger.error(error_message)\n            return None\n\n    def _analyze_user_input(\n        self, input_task: str, description: str, config: RunnableConfig = None, attempt: int = 1, **kwargs\n    ) -&gt; DecisionResult:\n        \"\"\"\n        Calls the manager's 'handle_input' action to decide if we should respond\n        immediately or proceed with a plan.\n\n        Args:\n            input_task (str): The user's input or task description.\n            description (str): Description of the orchestrator capabilities.\n            config: Optional configuration object passed to the manager's run method.\n            attempt (int): The current attempt number for analyzing user input.\n            **kwargs: Additional keyword arguments passed to the manager's run method.\n\n        Returns:\n            DecisionResult: An object containing the decision (as an Enum) and a message.\n        \"\"\"\n        if not self.enable_handle_input:\n            return DecisionResult(decision=Decision.PLAN, message=\"\")\n\n        handle_result = self.manager.run(\n            input_data={\n                \"action\": \"handle_input\",\n                \"task\": input_task,\n                \"description\": description,\n            },\n            config=config,\n            run_depends=[],\n            **kwargs,\n        )\n\n        if handle_result.status != RunnableStatus.SUCCESS:\n            error = handle_result.error.to_dict()\n            error_message = f\"Orchestrator {self.name} - {self.id}: Manager failed to analyze input: {error}\"\n            logger.error(error_message)\n            return DecisionResult(decision=Decision.RESPOND, message=f\"Error analyzing request: {error}\")\n\n        content = handle_result.output.get(\"content\", {})\n        raw_text = content.get(\"result\", \"\")\n        if not raw_text:\n            error_message = f\"Orchestrator {self.name} - {self.id}: No 'result' field in manager output.\"\n            logger.error(error_message)\n            return DecisionResult(decision=Decision.RESPOND, message=\"Manager did not return any result.\")\n\n        analysis, data = self.extract_json_from_output(result_text=raw_text)\n        if self.manager.streaming.enabled and self.manager.streaming.mode == StreamingMode.ALL:\n            self.manager.stream_content(\n                content={\"analysis\": analysis, \"data\": data},\n                step=\"manager_input_handling\",\n                source=self.name,\n                config=config,\n                **kwargs,\n            )\n\n        if not data:\n            error_message = f\"Orchestrator {self.name} - {self.id}: Failed to extract JSON from manager output.\"\n            logger.error(error_message)\n            if attempt &gt;= self.max_user_analyze_retries:\n                return DecisionResult(\n                    decision=Decision.RESPOND,\n                    message=\"Unable to extract valid JSON from manager output after multiple attempts.\",\n                )\n            _json_prompt_fix = \" Please provide a valid JSON response, inside &lt;output&gt;...&lt;/output&gt; tags.\"\n            return self._analyze_user_input(input_task + _json_prompt_fix, config=config, attempt=attempt + 1, **kwargs)\n\n        decision_str = data.get(\"decision\", Decision.RESPOND.value)\n        message = data.get(\"message\", \"\")\n\n        try:\n            decision = Decision(decision_str)\n        except ValueError:\n            warning_message = (\n                f\"Orchestrator {self.name} - {self.id}: Unrecognized decision '{decision_str}', defaulting to RESPOND.\"\n            )\n            logger.warning(warning_message)\n            decision = Decision.RESPOND\n\n        return DecisionResult(decision=decision, message=message)\n\n    def get_final_result(\n        self,\n        input_data: dict[str, str],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Generate a comprehensive final result based on the provided data.\n\n        Args:\n            input_data (dict[str, str]): Input data for the manager.\n            config (RunnableConfig): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The final comprehensive result.\n\n        Raises:\n            OrchestratorError: If an error occurs while generating the final answer.\n        \"\"\"\n        logger.debug(f\"Orchestrator {self.name} - {self.id}: Running final summarizer\")\n        manager_result = self.manager.run(\n            input_data={\"action\": \"final\", **input_data}, config=config, run_depends=self._run_depends, **kwargs\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            error_message = f\"Manager '{self.manager.name}' failed: {manager_result.error.message}\"\n            logger.error(f\"Orchestrator {self.name} - {self.id}: Error generating final, due to error: {error_message}\")\n            raise OrchestratorError(\n                f\"Orchestrator {self.name} - {self.id}: Error generating final, due to error: {error_message}\"\n            )\n\n        return manager_result.output.get(\"content\").get(\"result\")\n\n    def reset_run_state(self):\n        self._run_depends = []\n        self._chat_history = []\n        if self.manager:\n            self.manager.reset_run_state()\n\n    @abstractmethod\n    def run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the given task.\n\n        Args:\n            input_task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            dict[str, Any]: The final output generated after processing the task.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def setup_streaming(self) -&gt; None:\n        \"\"\"Setups streaming for orchestrator.\"\"\"\n        pass\n\n    def execute(self, input_data: OrchestratorInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict:\n        \"\"\"\n        Execute orchestrator flow.\n\n        Args:\n            input_data (OrchestratorInputSchema): The input data containing the objective.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the orchestration process.\n\n        Raises:\n            OrchestratorError: If the orchestration process fails.\n        \"\"\"\n        logger.info(f\"Orchestrator {self.name} - {self.id}: started with INPUT DATA:\\n{input_data}\")\n        self.reset_run_state()\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_task = input_data.input or self.objective\n\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n\n        if self.streaming.enabled:\n            self.setup_streaming()\n\n        output = self.run_flow(\n            input_task=input_task,\n            config=config,\n            **kwargs,\n        )\n\n        logger.info(f\"Orchestrator {self.name} - {self.id}: finished with RESULT:\\n{str(output)[:200]}...\")\n        return output\n\n    def parse_xml_content(self, text: str, tag: str) -&gt; str:\n        \"\"\"Extract content from XML-like tags.\"\"\"\n        match = re.search(f\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL)\n        return match.group(1).strip() if match else \"\"\n\n    def _extract_output_content(self, text: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Extracts the content of the &lt;analysis&gt; and &lt;output&gt; tags. If a properly closed tag is not found,\n        fall back to extracting everything after the first occurrence of &lt;output&gt;.\n        \"\"\"\n\n        output = self.parse_xml_content(text, \"output\")\n        analysis = self.parse_xml_content(text, \"analysis\")\n\n        if output:\n            return analysis, output\n\n        start = text.find(\"&lt;output&gt;\")\n        if start != -1:\n            fallback_content = text[start + len(\"&lt;output&gt;\") :].strip()\n            if fallback_content:\n                return analysis, fallback_content\n        raise ActionParseError(\"No &lt;output&gt; tags found in the response.\")\n\n    def _clean_content(self, content: str) -&gt; LET.Element:\n        \"\"\"\n        Clean and parse XML content by removing code block markers and wrapping in a root element.\n\n        Args:\n            content (str): The input string containing XML content, possibly with code block markers.\n\n        Returns:\n            LET.Element: A parsed XML element tree with the cleaned content wrapped in a root element.\n\n        Note:\n            - Removes triple backticks (```) from the content\n            - Wraps the content in a &lt;root&gt; element for proper XML parsing\n            - Uses a lenient XML parser that attempts to recover from malformed XML\n        \"\"\"\n        cleaned_content = content.replace(\"```\", \"\").strip()\n        wrapped_content = f\"&lt;root&gt;{cleaned_content}&lt;/root&gt;\"\n        parser = LET.XMLParser(recover=True)  # nosec B320\n        return LET.fromstring(wrapped_content, parser=parser)  # nosec B320\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the orchestrator with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the orchestrator with given parameters.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n    self._chat_history = []\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute orchestrator flow.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>OrchestratorInputSchema</code> <p>The input data containing the objective.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, Any]: The result of the orchestration process.</p> <p>Raises:</p> Type Description <code>OrchestratorError</code> <p>If the orchestration process fails.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>def execute(self, input_data: OrchestratorInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict:\n    \"\"\"\n    Execute orchestrator flow.\n\n    Args:\n        input_data (OrchestratorInputSchema): The input data containing the objective.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the orchestration process.\n\n    Raises:\n        OrchestratorError: If the orchestration process fails.\n    \"\"\"\n    logger.info(f\"Orchestrator {self.name} - {self.id}: started with INPUT DATA:\\n{input_data}\")\n    self.reset_run_state()\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_task = input_data.input or self.objective\n\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    kwargs.pop(\"run_depends\", None)\n\n    if self.streaming.enabled:\n        self.setup_streaming()\n\n    output = self.run_flow(\n        input_task=input_task,\n        config=config,\n        **kwargs,\n    )\n\n    logger.info(f\"Orchestrator {self.name} - {self.id}: finished with RESULT:\\n{str(output)[:200]}...\")\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.extract_json_from_output","title":"<code>extract_json_from_output(result_text)</code>","text":"<p>Extracts JSON data from the given text by looking for content within</p> ... <p>and ... tags. Strips any Markdown code block fences.</p> <p>Parameters:</p> Name Type Description Default <code>result_text</code> <code>str</code> <p>The text from which to extract JSON data.</p> required <p>Returns:</p> Type Description <code>tuple[str, dict] | None</code> <p>dict | None: The extracted JSON dictionary if successful, otherwise None.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>def extract_json_from_output(self, result_text: str) -&gt; tuple[str, dict] | None:\n    \"\"\"\n    Extracts JSON data from the given text by looking for content within\n    &lt;output&gt;...&lt;/output&gt; and &lt;analysis&gt;...&lt;/analysis&gt; tags. Strips any Markdown code block fences.\n\n    Args:\n        result_text (str): The text from which to extract JSON data.\n\n    Returns:\n        dict | None: The extracted JSON dictionary if successful, otherwise None.\n    \"\"\"\n    analysis, output_content = self._extract_output_content(result_text)\n    output_content = self._clean_output(output_content)\n\n    try:\n        data = json.loads(output_content)\n        return analysis, data\n    except json.JSONDecodeError as e:\n        error_message = f\"Orchestrator {self.name} - {self.id}: JSON decoding error: {e}\"\n        logger.error(error_message)\n        return None\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.get_final_result","title":"<code>get_final_result(input_data, config=None, **kwargs)</code>","text":"<p>Generate a comprehensive final result based on the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, str]</code> <p>Input data for the manager.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final comprehensive result.</p> <p>Raises:</p> Type Description <code>OrchestratorError</code> <p>If an error occurs while generating the final answer.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>def get_final_result(\n    self,\n    input_data: dict[str, str],\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Generate a comprehensive final result based on the provided data.\n\n    Args:\n        input_data (dict[str, str]): Input data for the manager.\n        config (RunnableConfig): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: The final comprehensive result.\n\n    Raises:\n        OrchestratorError: If an error occurs while generating the final answer.\n    \"\"\"\n    logger.debug(f\"Orchestrator {self.name} - {self.id}: Running final summarizer\")\n    manager_result = self.manager.run(\n        input_data={\"action\": \"final\", **input_data}, config=config, run_depends=self._run_depends, **kwargs\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict(for_tracing=True)]\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        error_message = f\"Manager '{self.manager.name}' failed: {manager_result.error.message}\"\n        logger.error(f\"Orchestrator {self.name} - {self.id}: Error generating final, due to error: {error_message}\")\n        raise OrchestratorError(\n            f\"Orchestrator {self.name} - {self.id}: Error generating final, due to error: {error_message}\"\n        )\n\n    return manager_result.output.get(\"content\").get(\"result\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.parse_xml_content","title":"<code>parse_xml_content(text, tag)</code>","text":"<p>Extract content from XML-like tags.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>def parse_xml_content(self, text: str, tag: str) -&gt; str:\n    \"\"\"Extract content from XML-like tags.\"\"\"\n    match = re.search(f\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL)\n    return match.group(1).strip() if match else \"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.run_flow","title":"<code>run_flow(input_task, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Process the given task.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The final output generated after processing the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>@abstractmethod\ndef run_flow(self, input_task: str, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the given task.\n\n    Args:\n        input_task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        dict[str, Any]: The final output generated after processing the task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.Orchestrator.setup_streaming","title":"<code>setup_streaming()</code>  <code>abstractmethod</code>","text":"<p>Setups streaming for orchestrator.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>@abstractmethod\ndef setup_streaming(self) -&gt; None:\n    \"\"\"Setups streaming for orchestrator.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/orchestrator/#dynamiq.nodes.agents.orchestrators.orchestrator.OrchestratorError","title":"<code>OrchestratorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Orchestrator errors.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/orchestrator.py</code> <pre><code>class OrchestratorError(Exception):\n    \"\"\"Base exception for Orchestrator errors.\"\"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/","title":"Manager","text":"<p>Helper functions and managers for model-specific prompts.</p>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager","title":"<code>AgentPromptManager</code>","text":"<p>Manager for holding and managing model-specific prompts for an agent.</p> <p>Each agent instance has its own prompt manager that stores all prompts needed during the agent's lifetime, including prompt blocks, variables, and generation logic.</p> Usage <p>agent.prompt_manager.history_prompt agent.prompt_manager.max_loops_prompt agent.prompt_manager.generate_prompt()</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>class AgentPromptManager:\n    \"\"\"\n    Manager for holding and managing model-specific prompts for an agent.\n\n    Each agent instance has its own prompt manager that stores all\n    prompts needed during the agent's lifetime, including prompt blocks,\n    variables, and generation logic.\n\n    Usage:\n        agent.prompt_manager.history_prompt\n        agent.prompt_manager.max_loops_prompt\n        agent.prompt_manager.generate_prompt()\n    \"\"\"\n\n    def __init__(self, model_name: str = None, tool_description: str = \"\"):\n        \"\"\"\n        Initialize the prompt manager.\n\n        Args:\n            model_name: The LLM model name for model-specific prompts\n            tool_description: Description of available tools\n        \"\"\"\n        self.model_name = model_name\n\n        # Runtime prompts (used during agent execution)\n        self.max_loops_prompt: str = REACT_MAX_LOOPS_PROMPT\n\n        # Template\n        self.agent_template: str = AGENT_PROMPT_TEMPLATE\n\n        # Prompt blocks and variables\n        # Store initial blocks for reset\n        self._prompt_blocks: dict[str, str] = {\n            \"date\": \"{{ date }}\",\n            \"tools\": \"{{ tool_description }}\",\n            \"instructions\": \"\",\n            \"context\": \"{{ context }}\",\n        }\n\n        # Store initial variables for reset\n        self._initial_variables: dict[str, Any] = {\n            \"tool_description\": tool_description,\n            \"date\": datetime.now().strftime(\"%d %B %Y\"),\n        }\n        self._prompt_variables: dict[str, Any] = self._initial_variables.copy()\n\n    def set_block(self, block_name: str, content: str):\n        \"\"\"Sets or updates a specific prompt block.\"\"\"\n        self._prompt_blocks[block_name] = content\n\n    def update_blocks(self, blocks: dict[str, str]):\n        \"\"\"Updates multiple prompt blocks at once.\"\"\"\n        self._prompt_blocks.update(blocks)\n\n    def set_variable(self, var_name: str, value: Any):\n        \"\"\"Sets or updates a specific prompt variable.\"\"\"\n        self._prompt_variables[var_name] = value\n\n    def update_variables(self, variables: dict[str, Any], merge: bool = True):\n        \"\"\"\n        Updates multiple prompt variables at once.\n\n        Args:\n            variables: Dictionary of variables to update\n            merge: If True, merge with existing variables. If False, replace existing variables.\n        \"\"\"\n        if merge:\n            self._prompt_variables.update(variables)\n        else:\n            self._prompt_variables = variables.copy()\n\n    def set_initial_variable(self, var_name: str, value: Any):\n        \"\"\"\n        Sets or updates a specific initial variable that persists across resets.\n\n        This method should be used when you need to update the initial state\n        that the manager returns to when reset() is called.\n\n        Args:\n            var_name: Name of the variable to set\n            value: Value to set for the variable\n        \"\"\"\n        self._initial_variables[var_name] = value\n        self._prompt_variables[var_name] = value\n\n    def reset(self):\n        \"\"\"\n        Resets prompt manager to its initial state.\n\n        This should be called between runs to prevent variable accumulation.\n        Prompt variables are reset to their initial state.\n        The date is refreshed on each reset to ensure it's always current.\n        \"\"\"\n        self._prompt_variables = self._initial_variables.copy()\n        self._prompt_variables[\"date\"] = datetime.now().strftime(\"%d %B %Y\")\n\n    def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n        \"\"\"\n        Generates the prompt using specified blocks and variables.\n\n        Args:\n            block_names: Optional list of block names to include. If None, includes all blocks.\n            **kwargs: Additional variables to use for rendering\n\n        Returns:\n            The generated prompt string\n        \"\"\"\n        temp_variables = self._prompt_variables.copy()\n        temp_variables.update(kwargs)\n\n        formatted_prompt_blocks = {}\n        for block, content in self._prompt_blocks.items():\n            if block_names is None or block in block_names:\n                formatted_content = Template(content).render(temp_variables)\n                if content:\n                    formatted_prompt_blocks[block] = formatted_content\n\n        render_context = {**temp_variables, **formatted_prompt_blocks}\n        prompt = Template(self.agent_template).render(render_context).strip()\n        prompt = self._clean_prompt(prompt)\n        return textwrap.dedent(prompt)\n\n    def render_block(self, block_name: str, **kwargs) -&gt; str:\n        \"\"\"\n        Renders a specific prompt block with variables.\n\n        This is useful for rendering individual blocks like 'plan', 'assign', 'final',\n        or 'handle_input' in AgentManager workflows.\n\n        Args:\n            block_name: Name of the block to render\n            **kwargs: Additional variables to merge with existing prompt variables\n\n        Returns:\n            Rendered block content as string\n        \"\"\"\n        template_content = self._prompt_blocks.get(block_name)\n        if not template_content:\n            logger.warning(f\"Block '{block_name}' not found in prompt blocks\")\n            return \"\"\n\n        # Merge existing variables with provided kwargs\n        variables = self._prompt_variables.copy()\n        variables.update(kwargs)\n\n        # Pass dict directly to avoid crash on non-identifier keys\n        return Template(template_content).render(variables)\n\n    @staticmethod\n    def _clean_prompt(prompt: str) -&gt; str:\n        \"\"\"\n        Cleans the generated prompt by removing excess blank lines.\n\n        Args:\n            prompt: The prompt to clean\n\n        Returns:\n            Cleaned prompt string\n        \"\"\"\n        prompt = re.sub(r\"\\n{3,}\", \"\\n\\n\", prompt)\n        return prompt.strip()\n\n    def setup_for_base_agent(self) -&gt; None:\n        \"\"\"Setup prompts for base Agent class.\"\"\"\n        if not self.model_name:\n            return\n\n        # Get model-specific template\n        self.agent_template = get_prompt_constant(self.model_name, \"AGENT_PROMPT_TEMPLATE\", AGENT_PROMPT_TEMPLATE)\n\n        if self.agent_template != AGENT_PROMPT_TEMPLATE:\n            logger.info(f\"Applied model-specific AGENT_PROMPT_TEMPLATE for model '{self.model_name}'\")\n\n    def setup_for_react_agent(\n        self,\n        inference_mode: InferenceMode,\n        parallel_tool_calls_enabled: bool,\n        has_tools: bool,\n        delegation_allowed: bool = False,\n        context_compaction_enabled: bool = False,\n        todo_management_enabled: bool = False,\n        sandbox_base_path: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Setup prompts for ReAct-style Agent.\n\n        Updates the prompt blocks with ReAct-specific prompts.\n        \"\"\"\n        # Get all prompt blocks for this configuration\n        prompt_blocks, agent_template = get_model_specific_prompts(\n            model_name=self.model_name,\n            inference_mode=inference_mode,\n            parallel_tool_calls_enabled=parallel_tool_calls_enabled,\n            has_tools=has_tools,\n            delegation_allowed=delegation_allowed,\n            context_compaction_enabled=context_compaction_enabled,\n            todo_management_enabled=todo_management_enabled,\n            sandbox_base_path=sandbox_base_path,\n        )\n\n        # Update prompt blocks\n        self._prompt_blocks.update(prompt_blocks)\n\n        # Store template\n        if agent_template:\n            self.agent_template = agent_template\n\n        # Store runtime prompts\n        self.max_loops_prompt = get_prompt_constant(self.model_name, \"REACT_MAX_LOOPS_PROMPT\", REACT_MAX_LOOPS_PROMPT)\n\n        # Log only if model-specific prompts were actually applied\n        if agent_template and agent_template != AGENT_PROMPT_TEMPLATE:\n            logger.debug(f\"Applied model-specific prompts and template for model '{self.model_name}'\")\n        else:\n            logger.debug(f\"Using default prompts for model '{self.model_name}'\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.__init__","title":"<code>__init__(model_name=None, tool_description='')</code>","text":"<p>Initialize the prompt manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The LLM model name for model-specific prompts</p> <code>None</code> <code>tool_description</code> <code>str</code> <p>Description of available tools</p> <code>''</code> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def __init__(self, model_name: str = None, tool_description: str = \"\"):\n    \"\"\"\n    Initialize the prompt manager.\n\n    Args:\n        model_name: The LLM model name for model-specific prompts\n        tool_description: Description of available tools\n    \"\"\"\n    self.model_name = model_name\n\n    # Runtime prompts (used during agent execution)\n    self.max_loops_prompt: str = REACT_MAX_LOOPS_PROMPT\n\n    # Template\n    self.agent_template: str = AGENT_PROMPT_TEMPLATE\n\n    # Prompt blocks and variables\n    # Store initial blocks for reset\n    self._prompt_blocks: dict[str, str] = {\n        \"date\": \"{{ date }}\",\n        \"tools\": \"{{ tool_description }}\",\n        \"instructions\": \"\",\n        \"context\": \"{{ context }}\",\n    }\n\n    # Store initial variables for reset\n    self._initial_variables: dict[str, Any] = {\n        \"tool_description\": tool_description,\n        \"date\": datetime.now().strftime(\"%d %B %Y\"),\n    }\n    self._prompt_variables: dict[str, Any] = self._initial_variables.copy()\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.generate_prompt","title":"<code>generate_prompt(block_names=None, **kwargs)</code>","text":"<p>Generates the prompt using specified blocks and variables.</p> <p>Parameters:</p> Name Type Description Default <code>block_names</code> <code>list[str] | None</code> <p>Optional list of block names to include. If None, includes all blocks.</p> <code>None</code> <code>**kwargs</code> <p>Additional variables to use for rendering</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated prompt string</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n    \"\"\"\n    Generates the prompt using specified blocks and variables.\n\n    Args:\n        block_names: Optional list of block names to include. If None, includes all blocks.\n        **kwargs: Additional variables to use for rendering\n\n    Returns:\n        The generated prompt string\n    \"\"\"\n    temp_variables = self._prompt_variables.copy()\n    temp_variables.update(kwargs)\n\n    formatted_prompt_blocks = {}\n    for block, content in self._prompt_blocks.items():\n        if block_names is None or block in block_names:\n            formatted_content = Template(content).render(temp_variables)\n            if content:\n                formatted_prompt_blocks[block] = formatted_content\n\n    render_context = {**temp_variables, **formatted_prompt_blocks}\n    prompt = Template(self.agent_template).render(render_context).strip()\n    prompt = self._clean_prompt(prompt)\n    return textwrap.dedent(prompt)\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.render_block","title":"<code>render_block(block_name, **kwargs)</code>","text":"<p>Renders a specific prompt block with variables.</p> <p>This is useful for rendering individual blocks like 'plan', 'assign', 'final', or 'handle_input' in AgentManager workflows.</p> <p>Parameters:</p> Name Type Description Default <code>block_name</code> <code>str</code> <p>Name of the block to render</p> required <code>**kwargs</code> <p>Additional variables to merge with existing prompt variables</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Rendered block content as string</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def render_block(self, block_name: str, **kwargs) -&gt; str:\n    \"\"\"\n    Renders a specific prompt block with variables.\n\n    This is useful for rendering individual blocks like 'plan', 'assign', 'final',\n    or 'handle_input' in AgentManager workflows.\n\n    Args:\n        block_name: Name of the block to render\n        **kwargs: Additional variables to merge with existing prompt variables\n\n    Returns:\n        Rendered block content as string\n    \"\"\"\n    template_content = self._prompt_blocks.get(block_name)\n    if not template_content:\n        logger.warning(f\"Block '{block_name}' not found in prompt blocks\")\n        return \"\"\n\n    # Merge existing variables with provided kwargs\n    variables = self._prompt_variables.copy()\n    variables.update(kwargs)\n\n    # Pass dict directly to avoid crash on non-identifier keys\n    return Template(template_content).render(variables)\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.reset","title":"<code>reset()</code>","text":"<p>Resets prompt manager to its initial state.</p> <p>This should be called between runs to prevent variable accumulation. Prompt variables are reset to their initial state. The date is refreshed on each reset to ensure it's always current.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets prompt manager to its initial state.\n\n    This should be called between runs to prevent variable accumulation.\n    Prompt variables are reset to their initial state.\n    The date is refreshed on each reset to ensure it's always current.\n    \"\"\"\n    self._prompt_variables = self._initial_variables.copy()\n    self._prompt_variables[\"date\"] = datetime.now().strftime(\"%d %B %Y\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.set_block","title":"<code>set_block(block_name, content)</code>","text":"<p>Sets or updates a specific prompt block.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def set_block(self, block_name: str, content: str):\n    \"\"\"Sets or updates a specific prompt block.\"\"\"\n    self._prompt_blocks[block_name] = content\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.set_initial_variable","title":"<code>set_initial_variable(var_name, value)</code>","text":"<p>Sets or updates a specific initial variable that persists across resets.</p> <p>This method should be used when you need to update the initial state that the manager returns to when reset() is called.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the variable to set</p> required <code>value</code> <code>Any</code> <p>Value to set for the variable</p> required Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def set_initial_variable(self, var_name: str, value: Any):\n    \"\"\"\n    Sets or updates a specific initial variable that persists across resets.\n\n    This method should be used when you need to update the initial state\n    that the manager returns to when reset() is called.\n\n    Args:\n        var_name: Name of the variable to set\n        value: Value to set for the variable\n    \"\"\"\n    self._initial_variables[var_name] = value\n    self._prompt_variables[var_name] = value\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.set_variable","title":"<code>set_variable(var_name, value)</code>","text":"<p>Sets or updates a specific prompt variable.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def set_variable(self, var_name: str, value: Any):\n    \"\"\"Sets or updates a specific prompt variable.\"\"\"\n    self._prompt_variables[var_name] = value\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.setup_for_base_agent","title":"<code>setup_for_base_agent()</code>","text":"<p>Setup prompts for base Agent class.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def setup_for_base_agent(self) -&gt; None:\n    \"\"\"Setup prompts for base Agent class.\"\"\"\n    if not self.model_name:\n        return\n\n    # Get model-specific template\n    self.agent_template = get_prompt_constant(self.model_name, \"AGENT_PROMPT_TEMPLATE\", AGENT_PROMPT_TEMPLATE)\n\n    if self.agent_template != AGENT_PROMPT_TEMPLATE:\n        logger.info(f\"Applied model-specific AGENT_PROMPT_TEMPLATE for model '{self.model_name}'\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.setup_for_react_agent","title":"<code>setup_for_react_agent(inference_mode, parallel_tool_calls_enabled, has_tools, delegation_allowed=False, context_compaction_enabled=False, todo_management_enabled=False, sandbox_base_path=None)</code>","text":"<p>Setup prompts for ReAct-style Agent.</p> <p>Updates the prompt blocks with ReAct-specific prompts.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def setup_for_react_agent(\n    self,\n    inference_mode: InferenceMode,\n    parallel_tool_calls_enabled: bool,\n    has_tools: bool,\n    delegation_allowed: bool = False,\n    context_compaction_enabled: bool = False,\n    todo_management_enabled: bool = False,\n    sandbox_base_path: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Setup prompts for ReAct-style Agent.\n\n    Updates the prompt blocks with ReAct-specific prompts.\n    \"\"\"\n    # Get all prompt blocks for this configuration\n    prompt_blocks, agent_template = get_model_specific_prompts(\n        model_name=self.model_name,\n        inference_mode=inference_mode,\n        parallel_tool_calls_enabled=parallel_tool_calls_enabled,\n        has_tools=has_tools,\n        delegation_allowed=delegation_allowed,\n        context_compaction_enabled=context_compaction_enabled,\n        todo_management_enabled=todo_management_enabled,\n        sandbox_base_path=sandbox_base_path,\n    )\n\n    # Update prompt blocks\n    self._prompt_blocks.update(prompt_blocks)\n\n    # Store template\n    if agent_template:\n        self.agent_template = agent_template\n\n    # Store runtime prompts\n    self.max_loops_prompt = get_prompt_constant(self.model_name, \"REACT_MAX_LOOPS_PROMPT\", REACT_MAX_LOOPS_PROMPT)\n\n    # Log only if model-specific prompts were actually applied\n    if agent_template and agent_template != AGENT_PROMPT_TEMPLATE:\n        logger.debug(f\"Applied model-specific prompts and template for model '{self.model_name}'\")\n    else:\n        logger.debug(f\"Using default prompts for model '{self.model_name}'\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.update_blocks","title":"<code>update_blocks(blocks)</code>","text":"<p>Updates multiple prompt blocks at once.</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def update_blocks(self, blocks: dict[str, str]):\n    \"\"\"Updates multiple prompt blocks at once.\"\"\"\n    self._prompt_blocks.update(blocks)\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.AgentPromptManager.update_variables","title":"<code>update_variables(variables, merge=True)</code>","text":"<p>Updates multiple prompt variables at once.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>dict[str, Any]</code> <p>Dictionary of variables to update</p> required <code>merge</code> <code>bool</code> <p>If True, merge with existing variables. If False, replace existing variables.</p> <code>True</code> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def update_variables(self, variables: dict[str, Any], merge: bool = True):\n    \"\"\"\n    Updates multiple prompt variables at once.\n\n    Args:\n        variables: Dictionary of variables to update\n        merge: If True, merge with existing variables. If False, replace existing variables.\n    \"\"\"\n    if merge:\n        self._prompt_variables.update(variables)\n    else:\n        self._prompt_variables = variables.copy()\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/manager/#dynamiq.nodes.agents.prompts.manager.get_model_specific_prompts","title":"<code>get_model_specific_prompts(model_name, inference_mode, parallel_tool_calls_enabled, has_tools, delegation_allowed=False, context_compaction_enabled=False, todo_management_enabled=False, sandbox_base_path=None)</code>","text":"<p>Get model-specific prompts based on the model name and agent configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The LLM model name</p> required <code>inference_mode</code> <code>InferenceMode</code> <p>The inference mode being used</p> required <code>parallel_tool_calls_enabled</code> <code>bool</code> <p>Whether parallel tool calls are enabled</p> required <code>has_tools</code> <code>bool</code> <p>Whether the agent has tools</p> required <code>delegation_allowed</code> <code>bool</code> <p>Whether delegation is allowed</p> <code>False</code> <code>context_compaction_enabled</code> <code>bool</code> <p>Whether context compaction/summarization is enabled</p> <code>False</code> <code>todo_management_enabled</code> <code>bool</code> <p>Whether todo management is enabled</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[dict[str, str], str]</code> <p>Tuple of (prompt_blocks dict, agent_prompt_template string)</p> Source code in <code>dynamiq/nodes/agents/prompts/manager.py</code> <pre><code>def get_model_specific_prompts(\n    model_name: str,\n    inference_mode: InferenceMode,\n    parallel_tool_calls_enabled: bool,\n    has_tools: bool,\n    delegation_allowed: bool = False,\n    context_compaction_enabled: bool = False,\n    todo_management_enabled: bool = False,\n    sandbox_base_path: str | None = None,\n) -&gt; tuple[dict[str, str], str]:\n    \"\"\"\n    Get model-specific prompts based on the model name and agent configuration.\n\n    Args:\n        model_name: The LLM model name\n        inference_mode: The inference mode being used\n        parallel_tool_calls_enabled: Whether parallel tool calls are enabled\n        has_tools: Whether the agent has tools\n        delegation_allowed: Whether delegation is allowed\n        context_compaction_enabled: Whether context compaction/summarization is enabled\n        todo_management_enabled: Whether todo management is enabled\n\n    Returns:\n        Tuple of (prompt_blocks dict, agent_prompt_template string)\n    \"\"\"\n    # Get model-specific agent template\n    agent_template = get_prompt_constant(model_name, \"AGENT_PROMPT_TEMPLATE\", AGENT_PROMPT_TEMPLATE)\n    if agent_template != AGENT_PROMPT_TEMPLATE:\n        logger.debug(f\"Using model-specific AGENT_PROMPT_TEMPLATE for '{model_name}'\")\n\n    instructions_default = get_prompt_constant(\n        model_name, \"REACT_BLOCK_INSTRUCTIONS_SINGLE\", REACT_BLOCK_INSTRUCTIONS_SINGLE\n    )\n    if instructions_default != REACT_BLOCK_INSTRUCTIONS_SINGLE:\n        logger.debug(f\"Using model-specific REACT_BLOCK_INSTRUCTIONS_SINGLE for '{model_name}'\")\n\n    # Get other model-specific prompts\n    react_block_tools = get_prompt_constant(model_name, \"REACT_BLOCK_TOOLS\", REACT_BLOCK_TOOLS)\n    if react_block_tools != REACT_BLOCK_TOOLS:\n        logger.debug(f\"Using model-specific REACT_BLOCK_TOOLS for '{model_name}'\")\n\n    react_block_instructions_no_tools = get_prompt_constant(\n        model_name, \"REACT_BLOCK_INSTRUCTIONS_NO_TOOLS\", REACT_BLOCK_INSTRUCTIONS_NO_TOOLS\n    )\n    if react_block_instructions_no_tools != REACT_BLOCK_INSTRUCTIONS_NO_TOOLS:\n        logger.debug(f\"Using model-specific REACT_BLOCK_INSTRUCTIONS_NO_TOOLS for '{model_name}'\")\n\n    react_block_output_format = get_prompt_constant(model_name, \"REACT_BLOCK_OUTPUT_FORMAT\", REACT_BLOCK_OUTPUT_FORMAT)\n    if react_block_output_format != REACT_BLOCK_OUTPUT_FORMAT:\n        logger.debug(f\"Using model-specific REACT_BLOCK_OUTPUT_FORMAT for '{model_name}'\")\n\n    # Build initial prompt blocks\n    prompt_blocks = {\n        \"tools\": \"\" if not has_tools else react_block_tools,\n        \"instructions\": react_block_instructions_no_tools if not has_tools else instructions_default,\n        \"output_format\": react_block_output_format,\n    }\n\n    # Override based on inference mode\n    match inference_mode:\n        case InferenceMode.FUNCTION_CALLING:\n            prompt_blocks[\"instructions\"] = get_prompt_constant(\n                model_name, \"REACT_BLOCK_INSTRUCTIONS_FUNCTION_CALLING\", REACT_BLOCK_INSTRUCTIONS_FUNCTION_CALLING\n            )\n            if has_tools:\n                prompt_blocks[\"tools\"] = get_prompt_constant(\n                    model_name, \"REACT_BLOCK_TOOLS_NO_FORMATS\", REACT_BLOCK_TOOLS_NO_FORMATS\n                )\n\n        case InferenceMode.STRUCTURED_OUTPUT:\n            prompt_blocks[\"instructions\"] = get_prompt_constant(\n                model_name, \"REACT_BLOCK_INSTRUCTIONS_STRUCTURED_OUTPUT\", REACT_BLOCK_INSTRUCTIONS_STRUCTURED_OUTPUT\n            )\n\n        case InferenceMode.XML:\n            xml_instructions_no_tools = get_prompt_constant(\n                model_name, \"REACT_BLOCK_XML_INSTRUCTIONS_NO_TOOLS\", REACT_BLOCK_XML_INSTRUCTIONS_NO_TOOLS\n            )\n            # XML mode instructions\n            instructions_xml = get_prompt_constant(\n                model_name, \"REACT_BLOCK_XML_INSTRUCTIONS_SINGLE\", REACT_BLOCK_XML_INSTRUCTIONS_SINGLE\n            )\n            if instructions_xml != REACT_BLOCK_XML_INSTRUCTIONS_SINGLE:\n                logger.debug(f\"Using model-specific REACT_BLOCK_XML_INSTRUCTIONS_SINGLE for '{model_name}'\")\n            prompt_blocks[\"instructions\"] = xml_instructions_no_tools if not has_tools else instructions_xml\n\n    # Build secondary_instructions from enabled features\n    secondary_parts = []\n    if parallel_tool_calls_enabled:\n        secondary_parts.append(REACT_BLOCK_MULTI_TOOL_PLANNING)\n    if delegation_allowed:\n        if inference_mode == InferenceMode.XML:\n            secondary_parts.append(DELEGATION_INSTRUCTIONS_XML)\n        else:\n            secondary_parts.append(DELEGATION_INSTRUCTIONS)\n    if context_compaction_enabled:\n        secondary_parts.append(CONTEXT_MANAGER_INSTRUCTIONS)\n    if todo_management_enabled:\n        secondary_parts.append(TODO_TOOLS_INSTRUCTIONS)\n    if sandbox_base_path:\n        secondary_parts.append(\n            SANDBOX_INSTRUCTIONS_TEMPLATE.format(\n                base_path=sandbox_base_path,\n            )\n        )\n\n    if secondary_parts:\n        prompt_blocks[\"secondary_instructions\"] = \"\\n\\n\".join(secondary_parts)\n\n    return prompt_blocks, agent_template\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/","title":"Registry","text":"<p>Registry for managing model-specific prompts.</p>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.ModelPromptsRegistry","title":"<code>ModelPromptsRegistry</code>","text":"<p>Registry for managing model-specific prompts.</p> Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>class ModelPromptsRegistry:\n    \"\"\"Registry for managing model-specific prompts.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the prompts registry.\"\"\"\n        self._registry: dict[str, dict[str, str]] = {}\n\n    def register(self, model_name: str, prompts: dict[str, str]) -&gt; None:\n        \"\"\"\n        Register prompts for a specific model.\n\n        Args:\n            model_name: The model identifier (e.g., \"gpt-5.1\", \"claude-sonnet-4.5\")\n            prompts: Dictionary of prompt constant names and their values\n        \"\"\"\n        if model_name in self._registry:\n            logger.warning(f\"Overwriting existing prompts for model '{model_name}'\")\n        self._registry[model_name] = prompts\n        logger.debug(f\"Registered {len(prompts)} custom prompts for model '{model_name}'\")\n\n    def get(self, model_name: str, prompt_name: str, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get a specific prompt for a model.\n\n        Args:\n            model_name: The model identifier\n            prompt_name: The name of the prompt constant\n            default: Default value if prompt not found\n\n        Returns:\n            The prompt value or default\n        \"\"\"\n\n        if not model_name:\n            return default\n\n        # Try exact match first\n        if model_name in self._registry:\n            if prompt_name in self._registry[model_name]:\n                logger.debug(f\"Using model-specific prompt '{prompt_name}' for model '{model_name}'\")\n                return self._registry[model_name][prompt_name]\n\n        # Try normalized variations\n        normalized_names = self._normalize_model_name(model_name)\n        for name in normalized_names:\n            if name in self._registry:\n                if prompt_name in self._registry[name]:\n                    logger.debug(\n                        f\"Using model-specific prompt '{prompt_name}' for model '{name}' (matched from '{model_name}')\"\n                    )\n                    return self._registry[name][prompt_name]\n\n        return default\n\n    def _normalize_model_name(self, model_name: str) -&gt; list[str]:\n        \"\"\"\n        Generate possible variations of a model name for matching.\n\n        Args:\n            model_name: Original model name\n\n        Returns:\n            List of possible name variations\n        \"\"\"\n        variations = [model_name]\n\n        # Add lowercase version\n        lower = model_name.lower()\n        if lower != model_name:\n            variations.append(lower)\n\n        # Add version with underscores instead of hyphens\n        with_underscores = lower.replace(\"-\", \"_\")\n        if with_underscores != lower:\n            variations.append(with_underscores)\n\n        # Add version without provider prefix (e.g., \"openai/gpt-4\" -&gt; \"gpt-4\")\n        if \"/\" in model_name:\n            without_provider = model_name.split(\"/\", 1)[1]\n            variations.append(without_provider)\n            variations.append(without_provider.lower())\n            variations.append(without_provider.lower().replace(\"-\", \"_\"))\n\n        return variations\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.ModelPromptsRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the prompts registry.</p> Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the prompts registry.\"\"\"\n    self._registry: dict[str, dict[str, str]] = {}\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.ModelPromptsRegistry.get","title":"<code>get(model_name, prompt_name, default=None)</code>","text":"<p>Get a specific prompt for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model identifier</p> required <code>prompt_name</code> <code>str</code> <p>The name of the prompt constant</p> required <code>default</code> <code>Any</code> <p>Default value if prompt not found</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The prompt value or default</p> Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def get(self, model_name: str, prompt_name: str, default: Any = None) -&gt; Any:\n    \"\"\"\n    Get a specific prompt for a model.\n\n    Args:\n        model_name: The model identifier\n        prompt_name: The name of the prompt constant\n        default: Default value if prompt not found\n\n    Returns:\n        The prompt value or default\n    \"\"\"\n\n    if not model_name:\n        return default\n\n    # Try exact match first\n    if model_name in self._registry:\n        if prompt_name in self._registry[model_name]:\n            logger.debug(f\"Using model-specific prompt '{prompt_name}' for model '{model_name}'\")\n            return self._registry[model_name][prompt_name]\n\n    # Try normalized variations\n    normalized_names = self._normalize_model_name(model_name)\n    for name in normalized_names:\n        if name in self._registry:\n            if prompt_name in self._registry[name]:\n                logger.debug(\n                    f\"Using model-specific prompt '{prompt_name}' for model '{name}' (matched from '{model_name}')\"\n                )\n                return self._registry[name][prompt_name]\n\n    return default\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.ModelPromptsRegistry.register","title":"<code>register(model_name, prompts)</code>","text":"<p>Register prompts for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model identifier (e.g., \"gpt-5.1\", \"claude-sonnet-4.5\")</p> required <code>prompts</code> <code>dict[str, str]</code> <p>Dictionary of prompt constant names and their values</p> required Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def register(self, model_name: str, prompts: dict[str, str]) -&gt; None:\n    \"\"\"\n    Register prompts for a specific model.\n\n    Args:\n        model_name: The model identifier (e.g., \"gpt-5.1\", \"claude-sonnet-4.5\")\n        prompts: Dictionary of prompt constant names and their values\n    \"\"\"\n    if model_name in self._registry:\n        logger.warning(f\"Overwriting existing prompts for model '{model_name}'\")\n    self._registry[model_name] = prompts\n    logger.debug(f\"Registered {len(prompts)} custom prompts for model '{model_name}'\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.get_prompt_constant","title":"<code>get_prompt_constant(model_name, constant_name, default_value)</code>","text":"<p>Get a prompt constant for a model or fall back to default.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model name to look up</p> required <code>constant_name</code> <code>str</code> <p>The name of the constant to retrieve</p> required <code>default_value</code> <code>Any</code> <p>The default value if model-specific prompt is not found</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The model-specific prompt or the default value</p> Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def get_prompt_constant(model_name: str, constant_name: str, default_value: Any) -&gt; Any:\n    \"\"\"\n    Get a prompt constant for a model or fall back to default.\n\n    Args:\n        model_name: The model name to look up\n        constant_name: The name of the constant to retrieve\n        default_value: The default value if model-specific prompt is not found\n\n    Returns:\n        The model-specific prompt or the default value\n    \"\"\"\n    return _prompts_registry.get(model_name, constant_name, default_value)\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.get_registry","title":"<code>get_registry()</code>","text":"<p>Get the global prompts registry.</p> Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def get_registry() -&gt; ModelPromptsRegistry:\n    \"\"\"Get the global prompts registry.\"\"\"\n    return _prompts_registry\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/registry/#dynamiq.nodes.agents.prompts.registry.register_model_prompts","title":"<code>register_model_prompts(model_name, prompts)</code>","text":"<p>Register prompts for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model identifier</p> required <code>prompts</code> <code>dict[str, str]</code> <p>Dictionary of prompt constant names and their values</p> required Source code in <code>dynamiq/nodes/agents/prompts/registry.py</code> <pre><code>def register_model_prompts(model_name: str, prompts: dict[str, str]) -&gt; None:\n    \"\"\"\n    Register prompts for a specific model.\n\n    Args:\n        model_name: The model identifier\n        prompts: Dictionary of prompt constant names and their values\n    \"\"\"\n    _prompts_registry.register(model_name, prompts)\n</code></pre>"},{"location":"dynamiq/nodes/agents/prompts/secondary_instructions/","title":"Secondary instructions","text":"<p>Secondary instructions for agent prompts.</p>"},{"location":"dynamiq/nodes/agents/prompts/templates/","title":"Templates","text":""},{"location":"dynamiq/nodes/agents/prompts/orchestrators/adaptive/","title":"Adaptive","text":""},{"location":"dynamiq/nodes/agents/prompts/orchestrators/base/","title":"Base","text":""},{"location":"dynamiq/nodes/agents/prompts/orchestrators/graph/","title":"Graph","text":""},{"location":"dynamiq/nodes/agents/prompts/orchestrators/linear/","title":"Linear","text":""},{"location":"dynamiq/nodes/agents/prompts/overrides/gemini/","title":"Gemini","text":"<p>Gemini specific prompts</p>"},{"location":"dynamiq/nodes/agents/prompts/overrides/gpt/","title":"Gpt","text":"<p>Model-specific prompts for OpenAI GPT models.</p>"},{"location":"dynamiq/nodes/agents/prompts/react/instructions/","title":"Instructions","text":""},{"location":"dynamiq/nodes/audio/elevenlabs/","title":"Elevenlabs","text":""},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS","title":"<code>ElevenLabsSTS</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for vocalizing text using the ElevenLabs API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to. name (str): The name of the node.</p> <code>connection</code> <code>ElevenLabs | None</code> <p>The connection to the ElevenLabs API. A new connection is created if none is provided.</p> <code>voice_id</code> <code>Voices | str | None</code> <p>The voice identifier, that should be used for vocalizing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>model(str)</code> <code>ErrorHandling</code> <p>The model for vocalizing, defaults to \"eleven_english_sts_v2\".</p> <code>stability(float)</code> <code>ErrorHandling</code> <p>The slider determines how stable the voice is and the randomness between each generation.</p> <code>similarity_boost(float)</code> <code>ErrorHandling</code> <p>The slider dictates how closely the AI should adhere to the original voice when attempting to replicate it.</p> <code>style(float)</code> <code>ErrorHandling</code> <p>The setting that attempts to amplify the style of the original speaker.</p> <code>use_speaker_boost(bool)</code> <code>ErrorHandling</code> <p>The setting for boosting the similarity to the original speaker</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>class ElevenLabsSTS(ConnectionNode):\n    \"\"\"\n    A component for vocalizing text using the ElevenLabs API.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to. name (str): The name of the node.\n        connection (ElevenLabsConnection | None): The connection to the ElevenLabs API. A new connection is created if\n            none is provided.\n        voice_id: The voice identifier, that should be used for vocalizing.\n        error_handling (ErrorHandling): Error handling configuration.\n        model(str): The model for vocalizing, defaults to \"eleven_english_sts_v2\".\n        stability(float): The slider determines how stable the voice is and the randomness\n            between each generation.\n        similarity_boost(float): The slider dictates how closely the AI should adhere to the original voice when\n            attempting to replicate it.\n        style(float): The setting that attempts to amplify the style of the original speaker.\n        use_speaker_boost(bool):The setting for boosting the similarity to the original speaker\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"ElevenLabsSTS\"\n    voice_id: Voices | str | None = Voices.Rachel\n    connection: ElevenLabsConnection | None = None\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    model: str = \"eleven_english_sts_v2\"\n    stability: float = 0.5\n    similarity_boost: float = 0.5\n    style: float = 0\n    use_speaker_boost: bool = True\n    input_schema: ClassVar[type[ElevenLabsSTSInputSchema]] = ElevenLabsSTSInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ElevenLabs audio generation.\n\n        If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ElevenLabsConnection()\n        super().__init__(**kwargs)\n\n    def execute(\n        self, input_data: ElevenLabsSTSInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, bytes | list[io.BytesIO]]:\n        \"\"\"Execute the audio generation process.\n\n        This method takes input data and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing audio that should be vocalized. Audio\n                can be BytesIO or bytes format only.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following keys:\n                - \"content\" (bytes): Raw bytes containing the audio generation result.\n                - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.\n        \"\"\"\n        input_dict = {\n            \"model_id\": self.model,\n            \"voice_settings\": json.dumps(\n                {\n                    \"stability\": self.stability,\n                    \"similarity_boost\": self.similarity_boost,\n                    \"style\": self.style,\n                    \"use_speaker_boost\": self.use_speaker_boost,\n                }\n            ),\n        }\n        audio = input_data.audio\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        response = self.client.request(\n            method=self.connection.method,\n            url=format_url(\"speech-to-speech/\", self.connection.url, self.voice_id),\n            headers=self.connection.headers,\n            data=input_dict | self.connection.data,\n            files={\"audio\": audio},\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n\n        audio_bytes = response.content\n        audio_file = io.BytesIO(audio_bytes)\n        audio_file.name = \"audio.mp3\"\n        audio_file.content_type = \"audio/mpeg\"\n\n        return {\n            \"content\": audio_bytes,\n            \"files\": [audio_file],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElevenLabs audio generation.</p> <p>If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ElevenLabs audio generation.\n\n    If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ElevenLabsConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio generation process.</p> <p>This method takes input data and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing audio that should be vocalized. Audio can be BytesIO or bytes format only.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bytes | list[BytesIO]]</code> <p>A dictionary with the following keys: - \"content\" (bytes): Raw bytes containing the audio generation result. - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def execute(\n    self, input_data: ElevenLabsSTSInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, bytes | list[io.BytesIO]]:\n    \"\"\"Execute the audio generation process.\n\n    This method takes input data and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing audio that should be vocalized. Audio\n            can be BytesIO or bytes format only.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following keys:\n            - \"content\" (bytes): Raw bytes containing the audio generation result.\n            - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.\n    \"\"\"\n    input_dict = {\n        \"model_id\": self.model,\n        \"voice_settings\": json.dumps(\n            {\n                \"stability\": self.stability,\n                \"similarity_boost\": self.similarity_boost,\n                \"style\": self.style,\n                \"use_speaker_boost\": self.use_speaker_boost,\n            }\n        ),\n    }\n    audio = input_data.audio\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    response = self.client.request(\n        method=self.connection.method,\n        url=format_url(\"speech-to-speech/\", self.connection.url, self.voice_id),\n        headers=self.connection.headers,\n        data=input_dict | self.connection.data,\n        files={\"audio\": audio},\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n\n    audio_bytes = response.content\n    audio_file = io.BytesIO(audio_bytes)\n    audio_file.name = \"audio.mp3\"\n    audio_file.content_type = \"audio/mpeg\"\n\n    return {\n        \"content\": audio_bytes,\n        \"files\": [audio_file],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS","title":"<code>ElevenLabsTTS</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for vocalizing text using the ElevenLabs API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>ElevenLabs | None</code> <p>The connection to the ElevenLabs API. A new connection is created if none is provided.</p> <code>voice_id</code> <code>Voices | str | None</code> <p>The voice identifier, that should be used for vocalizing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>model(str)</code> <code>ErrorHandling</code> <p>The model for vocalizing, defaults to \"eleven_monolingual_v1\"</p> <code>stability(float)</code> <code>ErrorHandling</code> <p>The slider determines how stable the voice is and the randomness</p> <code>similarity_boost(float)</code> <code>ErrorHandling</code> <p>The slider dictates how closely the AI should adhere to the original voice when</p> <code>style(float)</code> <code>ErrorHandling</code> <p>The setting that attempts to amplify the style of the original speaker.</p> <code>use_speaker_boost(bool)</code> <code>ErrorHandling</code> <p>The setting for boosting the similarity to the original speaker</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>class ElevenLabsTTS(ConnectionNode):\n    \"\"\"\n    A component for vocalizing text using the ElevenLabs API.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (ElevenLabsConnection | None): The connection to the ElevenLabs API. A new connection\n            is created if none is provided.\n        voice_id: The voice identifier, that should be used for vocalizing.\n        error_handling (ErrorHandling): Error handling configuration.\n        model(str): The model for vocalizing, defaults to \"eleven_monolingual_v1\"\n        stability(float): The slider determines how stable the voice is and the randomness\n        between each generation.\n        similarity_boost(float): The slider dictates how closely the AI should adhere to the original voice when\n        attempting to replicate it.\n        style(float): The setting that attempts to amplify the style of the original speaker.\n        use_speaker_boost(bool):The setting for boosting the similarity to the original speaker\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"ElevenLabsTTS\"\n    voice_id: Voices | str | None = Voices.Rachel\n    connection: ElevenLabsConnection | None = None\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    model: str = \"eleven_monolingual_v1\"\n    stability: float = 0.5\n    similarity_boost: float = 0.5\n    style: float = 0\n    use_speaker_boost: bool = True\n    input_schema: ClassVar[type[ElevenLabsTTSInputSchema]] = ElevenLabsTTSInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ElevenLabs audio generation.\n\n        If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ElevenLabsConnection()\n        super().__init__(**kwargs)\n\n    def execute(\n        self, input_data: ElevenLabsTTSInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, bytes | list[io.BytesIO]]:\n        \"\"\"Execute the audio generation process.\n\n        This method takes input data and returns the result.\n\n        Args:\n            input_data (ElevenLabsTTSInputSchema): The input data containing the text.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following keys:\n                - \"content\" (bytes): Raw bytes containing the audio generation result.\n                - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.\n        \"\"\"\n        input_dict = {\n            \"model_id\": self.model,\n            \"text\": input_data.text,\n            \"voice_settings\": {\n                \"stability\": self.stability,\n                \"similarity_boost\": self.similarity_boost,\n                \"style\": self.style,\n                \"use_speaker_boost\": self.use_speaker_boost,\n            },\n        }\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        response = self.client.request(\n            method=self.connection.method,\n            url=format_url(\"text-to-speech/\", self.connection.url, self.voice_id),\n            json={**input_dict, **self.connection.data},\n            headers=self.connection.headers,\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n\n        audio_bytes = response.content\n        audio_file = io.BytesIO(audio_bytes)\n        audio_file.name = \"audio.mp3\"\n        audio_file.content_type = \"audio/mpeg\"\n\n        return {\n            \"content\": audio_bytes,\n            \"files\": [audio_file],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElevenLabs audio generation.</p> <p>If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ElevenLabs audio generation.\n\n    If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ElevenLabsConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio generation process.</p> <p>This method takes input data and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ElevenLabsTTSInputSchema</code> <p>The input data containing the text.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bytes | list[BytesIO]]</code> <p>A dictionary with the following keys: - \"content\" (bytes): Raw bytes containing the audio generation result. - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def execute(\n    self, input_data: ElevenLabsTTSInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, bytes | list[io.BytesIO]]:\n    \"\"\"Execute the audio generation process.\n\n    This method takes input data and returns the result.\n\n    Args:\n        input_data (ElevenLabsTTSInputSchema): The input data containing the text.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following keys:\n            - \"content\" (bytes): Raw bytes containing the audio generation result.\n            - \"files\" (list[io.BytesIO]): List containing a single BytesIO file object.\n    \"\"\"\n    input_dict = {\n        \"model_id\": self.model,\n        \"text\": input_data.text,\n        \"voice_settings\": {\n            \"stability\": self.stability,\n            \"similarity_boost\": self.similarity_boost,\n            \"style\": self.style,\n            \"use_speaker_boost\": self.use_speaker_boost,\n        },\n    }\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    response = self.client.request(\n        method=self.connection.method,\n        url=format_url(\"text-to-speech/\", self.connection.url, self.voice_id),\n        json={**input_dict, **self.connection.data},\n        headers=self.connection.headers,\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n\n    audio_bytes = response.content\n    audio_file = io.BytesIO(audio_bytes)\n    audio_file.name = \"audio.mp3\"\n    audio_file.content_type = \"audio/mpeg\"\n\n    return {\n        \"content\": audio_bytes,\n        \"files\": [audio_file],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.format_url","title":"<code>format_url(method, url, voice_id)</code>","text":"<p>Formats the given URL by including the <code>voice_id</code> if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>type of request for vocalizer</p> required <code>voice_id</code> <code>str</code> <p>voice id for vocalizer.</p> required <code>url</code> <code>str</code> <p>The URL to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The modified URL.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def format_url(method: str, url: str, voice_id: str) -&gt; str:\n    \"\"\"Formats the given URL by including the `voice_id` if necessary.\n\n    Args:\n        method: type of request for vocalizer\n        voice_id: voice id for vocalizer.\n        url (str): The URL to format.\n\n    Returns:\n        str: The modified URL.\n    \"\"\"\n    if url.rstrip(\"/\").endswith(\"v1\"):\n        url = urljoin(url, method)\n    if \"{voice_id}\" in url:\n        url = url.format(voice_id=voice_id)\n    elif url.rstrip(\"/\").endswith(\"text-to-speech\") or url.rstrip(\"/\").endswith(\n        \"speech-to-speech\"\n    ):\n        url = urljoin(url, voice_id)\n    return url\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/","title":"Whisper","text":""},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT","title":"<code>WhisperSTT</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for transcribing audio files using the Whisper speech recognition system.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Whisper | None</code> <p>The connection to the Whisper API.A new connection is created if none is provided.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The model name to use for transcribing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>class WhisperSTT(ConnectionNode):\n    \"\"\"\n    A component for transcribing audio files using the Whisper speech recognition system.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WhisperConnection | None): The connection to the Whisper API.A new connection\n            is created if none is provided.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The model name to use for transcribing.\n        error_handling (ErrorHandling): Error handling configuration.\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"Whisper\"\n    model: str\n    connection: WhisperConnection | OpenAIConnection | None = None\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    default_file_name: str = DEFAULT_FILE_NAME\n    default_content_type: str = DEFAULT_CONTENT_TYPE\n    input_schema: ClassVar[type[WhisperSTTInputSchema]] = WhisperSTTInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Whisper transcriber.\n\n        If neither client nor connection is provided in kwargs, a new Whisper connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WhisperConnection()\n        super().__init__(**kwargs)\n\n    def execute(self, input_data: WhisperSTTInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"Execute the audio transcribing process.\n\n        This method takes input data, modifies it(if necessary), and returns the result.\n\n        Args:\n            input_data (WhisperSTTInputSchema): The input data containing the audio.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: A string containing the transcribe result.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        audio = input_data.audio\n        if isinstance(audio, bytes):\n            audio = io.BytesIO(audio)\n\n        if not isinstance(audio, io.BytesIO):\n            raise ValueError(\"Audio must be a BytesIO object or bytes.\")\n\n        audio.name = getattr(audio, \"name\", self.default_file_name)\n        audio.content_type = getattr(audio, \"content_type\", self.default_content_type)\n\n        if isinstance(self.connection, WhisperConnection):\n            transcription = self.get_transcription_with_http_request(model=self.model, audio=audio)\n        elif isinstance(self.connection, OpenAIConnection):\n            transcription = self.get_transcription_with_openai_client(model=self.model, audio=audio)\n        else:\n            raise ValueError(f\"Connection type {type(self.connection)} does not fit required ones.\")\n\n        return {\"content\": transcription.get(\"text\", \"\")}\n\n    def get_transcription_with_http_request(self, model: str, audio: io.BytesIO):\n        \"\"\"Get the audio transcription by request.\n\n        This method takes whisper model and audio file, sends request with defined params, and returns the\n        transcription.\n\n        Args:\n            model(str): The model used for transcribing.\n            audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n        Returns:\n            dict: transcription result.\n        \"\"\"\n        connection_url = urljoin(self.connection.url, \"audio/transcriptions\")\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            headers=self.connection.headers,\n            params=self.connection.params,\n            data=self.connection.data | {\"model\": model},\n            files={\"file\": (audio.name, audio, audio.content_type)},\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n\n        return response.json()\n\n    def get_transcription_with_openai_client(self, model: str, audio: io.BytesIO):\n        \"\"\"Get the audio transcription by request.\n\n        This method takes whisper model and audio file, sends request with defined params, and returns the\n        transcription.\n\n        Args:\n            model(str): The model used for transcribing.\n            audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n        Returns:\n            dict: transcription result.\n        \"\"\"\n        response = self.client.audio.transcriptions.create(model=model, file=audio)\n\n        return {\"text\": response.text}\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Whisper transcriber.</p> <p>If neither client nor connection is provided in kwargs, a new Whisper connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Whisper transcriber.\n\n    If neither client nor connection is provided in kwargs, a new Whisper connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WhisperConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio transcribing process.</p> <p>This method takes input data, modifies it(if necessary), and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WhisperSTTInputSchema</code> <p>The input data containing the audio.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A string containing the transcribe result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def execute(self, input_data: WhisperSTTInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"Execute the audio transcribing process.\n\n    This method takes input data, modifies it(if necessary), and returns the result.\n\n    Args:\n        input_data (WhisperSTTInputSchema): The input data containing the audio.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: A string containing the transcribe result.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    audio = input_data.audio\n    if isinstance(audio, bytes):\n        audio = io.BytesIO(audio)\n\n    if not isinstance(audio, io.BytesIO):\n        raise ValueError(\"Audio must be a BytesIO object or bytes.\")\n\n    audio.name = getattr(audio, \"name\", self.default_file_name)\n    audio.content_type = getattr(audio, \"content_type\", self.default_content_type)\n\n    if isinstance(self.connection, WhisperConnection):\n        transcription = self.get_transcription_with_http_request(model=self.model, audio=audio)\n    elif isinstance(self.connection, OpenAIConnection):\n        transcription = self.get_transcription_with_openai_client(model=self.model, audio=audio)\n    else:\n        raise ValueError(f\"Connection type {type(self.connection)} does not fit required ones.\")\n\n    return {\"content\": transcription.get(\"text\", \"\")}\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.get_transcription_with_http_request","title":"<code>get_transcription_with_http_request(model, audio)</code>","text":"<p>Get the audio transcription by request.</p> <p>This method takes whisper model and audio file, sends request with defined params, and returns the transcription.</p> <p>Parameters:</p> Name Type Description Default <code>model(str)</code> <p>The model used for transcribing.</p> required <code>audio(io.BytesIO)</code> <p>The audio file in BytesIO that should be transcribed</p> required <p>Returns:     dict: transcription result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def get_transcription_with_http_request(self, model: str, audio: io.BytesIO):\n    \"\"\"Get the audio transcription by request.\n\n    This method takes whisper model and audio file, sends request with defined params, and returns the\n    transcription.\n\n    Args:\n        model(str): The model used for transcribing.\n        audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n    Returns:\n        dict: transcription result.\n    \"\"\"\n    connection_url = urljoin(self.connection.url, \"audio/transcriptions\")\n    response = self.client.request(\n        method=self.connection.method,\n        url=connection_url,\n        headers=self.connection.headers,\n        params=self.connection.params,\n        data=self.connection.data | {\"model\": model},\n        files={\"file\": (audio.name, audio, audio.content_type)},\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n\n    return response.json()\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.get_transcription_with_openai_client","title":"<code>get_transcription_with_openai_client(model, audio)</code>","text":"<p>Get the audio transcription by request.</p> <p>This method takes whisper model and audio file, sends request with defined params, and returns the transcription.</p> <p>Parameters:</p> Name Type Description Default <code>model(str)</code> <p>The model used for transcribing.</p> required <code>audio(io.BytesIO)</code> <p>The audio file in BytesIO that should be transcribed</p> required <p>Returns:     dict: transcription result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def get_transcription_with_openai_client(self, model: str, audio: io.BytesIO):\n    \"\"\"Get the audio transcription by request.\n\n    This method takes whisper model and audio file, sends request with defined params, and returns the\n    transcription.\n\n    Args:\n        model(str): The model used for transcribing.\n        audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n    Returns:\n        dict: transcription result.\n    \"\"\"\n    response = self.client.audio.transcriptions.create(model=model, file=audio)\n\n    return {\"text\": response.text}\n</code></pre>"},{"location":"dynamiq/nodes/converters/csv/","title":"Csv","text":""},{"location":"dynamiq/nodes/converters/csv/#dynamiq.nodes.converters.csv.CSVConverter","title":"<code>CSVConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node that converts CSV files into a standardized document format.</p> <p>This converter processes CSV files from either file paths or file objects, extracting specified content and metadata columns into a structured document format. Each row in the CSV becomes a document with content from the specified content column and metadata from the specified metadata columns.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Display name of the node.</p> <code>group</code> <code>CONVERTERS</code> <p>Node group classification.</p> <code>delimiter</code> <code>str | None</code> <p>Character used to separate fields in the CSV files. Defaults to comma.</p> <code>content_column</code> <code>str | None</code> <p>Name of the column to use as the main document content.</p> <code>metadata_columns</code> <code>list[str] | None</code> <p>Column names to extract as metadata for each document.</p> <code>input_schema</code> <code>type[CSVConverterInputSchema]</code> <p>Schema for validating input parameters.</p> Source code in <code>dynamiq/nodes/converters/csv.py</code> <pre><code>class CSVConverter(Node):\n    \"\"\"\n    A Node that converts CSV files into a standardized document format.\n\n    This converter processes CSV files from either file paths or file objects,\n    extracting specified content and metadata columns into a structured document format.\n    Each row in the CSV becomes a document with content from the specified content column\n    and metadata from the specified metadata columns.\n\n    Attributes:\n        name (str): Display name of the node.\n        group (NodeGroup.CONVERTERS): Node group classification.\n        delimiter (str | None): Character used to separate fields in the CSV files. Defaults to comma.\n        content_column (str | None): Name of the column to use as the main document content.\n        metadata_columns (list[str] | None): Column names to extract as metadata for each document.\n        input_schema (type[CSVConverterInputSchema]): Schema for validating input parameters.\n    \"\"\"\n\n    name: str = \"CSV File Converter\"\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    delimiter: str | None = Field(default=None, description=\"Delimiter used in the CSV files.\")\n    content_column: str | None = Field(\n        ..., description=\"Name of the column that will be used as the document's main content.\"\n    )\n    metadata_columns: list[str] | None = Field(\n        default=None,\n        description=\"Optional list of column names to extract as metadata for each document. Can be None.\",\n    )\n    input_schema: ClassVar[type[CSVConverterInputSchema]] = CSVConverterInputSchema\n\n    def execute(\n        self, input_data: CSVConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Executes the CSV conversion process.\n\n        Processes one or more CSV files according to the input configuration,\n        converting each row into a document with specified content and metadata.\n        If some files fail to process but at least one succeeds, logs errors and continues.\n        If all files fail, raises the last encountered error.\n\n        Args:\n            input_data (CSVConverterInputSchema): Validated input parameters for the conversion.\n            config (RunnableConfig | None): Optional runtime configuration.\n            **kwargs: Additional keyword arguments passed to execution callbacks.\n\n        Returns:\n            dict[str, list[Any]]: Dictionary containing the list of processed documents\n                under the 'documents' key.\n\n        Raises:\n            Exception: If there are errors reading or processing the CSV files.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        all_documents = []\n        last_error = None\n        success_count = 0\n        total_files = len(input_data.file_paths or []) + len(input_data.files or [])\n        delimiter = input_data.delimiter or self.delimiter\n        content_column = input_data.content_column or self.content_column\n        metadata_columns = input_data.metadata_columns or self.metadata_columns\n        external_metadata = input_data.metadata\n\n        if input_data.file_paths:\n            for path in input_data.file_paths:\n                try:\n                    with open(path, encoding=\"utf-8\") as csv_file:\n                        reader = csv.DictReader(csv_file, delimiter=delimiter)\n                        for doc in self._process_rows_generator(\n                            reader,\n                            source=path,\n                            content_column=content_column,\n                            metadata_columns=metadata_columns,\n                            external_metadata=external_metadata,\n\n                        ):\n                            all_documents.append(doc)\n                        success_count += 1\n                except Exception as e:\n                    logger.error(f\"Error processing file {path}: {str(e)}\")\n                    last_error = e\n\n        if input_data.files:\n            for file_obj in input_data.files:\n                source_name = getattr(file_obj, \"name\", \"in-memory file\")\n                try:\n                    if isinstance(file_obj, bytes):\n                        file_obj = BytesIO(file_obj)\n\n                    file_text = TextIOWrapper(file_obj, encoding=\"utf-8\")\n                    reader = csv.DictReader(file_text, delimiter=delimiter)\n\n                    for doc in self._process_rows_generator(\n                        reader,\n                        source=source_name,\n                        content_column=content_column,\n                        metadata_columns=metadata_columns,\n                        external_metadata=external_metadata,\n\n                    ):\n                        all_documents.append(doc)\n                    success_count += 1\n                except Exception as e:\n                    logger.error(f\"Error processing file {source_name}: {str(e)}\")\n                    last_error = e\n\n        if success_count == 0 and last_error is not None:\n            raise last_error\n\n        if success_count &lt; total_files:\n            logger.warning(f\"Processed {success_count} out of {total_files} files successfully\")\n\n        return {\"documents\": all_documents}\n\n    def _process_rows_generator(\n        self,\n        reader: csv.DictReader,\n        source: str,\n        content_column: str,\n        metadata_columns: list[str] | None,\n        external_metadata: dict | list | None,\n\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Processes CSV rows into structured document dictionaries in a streaming fashion.\n\n        This method reads CSV rows one by one from the given DictReader, extracts the\n        specified content and metadata columns, and yields each row as a dictionary\n        with 'content' and 'metadata' fields.\n\n        Args:\n            reader (csv.DictReader): The CSV DictReader from which rows are read.\n            source (str): The source identifier for the CSV data, e.g., a file path\n                or name for in-memory data.\n            content_column (str): The name of the column containing the main document content.\n            metadata_columns (list[str]|None): Column names to include as metadata in the\n                resulting document dictionary.\n            external_metadata (dict | list | None): External metadata to merge with CSV metadata.\n                If a key exists in both, the CSV metadata will override the external metadata.\n\n\n        Yields:\n            dict: A document dictionary with two keys:\n                - 'content': The content extracted from the specified `content_column`.\n                - 'metadata': A dict containing merged metadata from CSV and external sources,\n                  plus a 'source' key identifying the file or in-memory data source.\n\n        Raises:\n            KeyError: If the specified `content_column` is not present in a row of the CSV.\n        \"\"\"\n        metadata_columns = metadata_columns or []\n        for index, row in enumerate(reader):\n            if content_column not in row:\n                raise KeyError(\n                    f\"Content column '{content_column}' not found in CSV \" f\"(source: {source}) at row {index}\"\n                )\n\n            csv_metadata = {col: row[col] for col in metadata_columns if col in row}\n            csv_metadata[\"source\"] = source\n\n            if external_metadata is not None:\n                if isinstance(external_metadata, dict):\n                    merged_metadata = external_metadata.copy()  # create an independent copy\n                    merged_metadata.update(csv_metadata)  # CSV metadata takes precedence on key conflicts\n                else:\n                    # If external_metadata is not a dict (e.g. a list), store it under its own key.\n                    merged_metadata = csv_metadata.copy()\n                    merged_metadata[\"external\"] = external_metadata\n            else:\n                merged_metadata = csv_metadata\n\n            yield {\n                \"content\": row[content_column],\n                \"metadata\": merged_metadata,\n            }\n</code></pre>"},{"location":"dynamiq/nodes/converters/csv/#dynamiq.nodes.converters.csv.CSVConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the CSV conversion process.</p> <p>Processes one or more CSV files according to the input configuration, converting each row into a document with specified content and metadata. If some files fail to process but at least one succeeds, logs errors and continues. If all files fail, raises the last encountered error.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>CSVConverterInputSchema</code> <p>Validated input parameters for the conversion.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional runtime configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to execution callbacks.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>dict[str, list[Any]]: Dictionary containing the list of processed documents under the 'documents' key.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there are errors reading or processing the CSV files.</p> Source code in <code>dynamiq/nodes/converters/csv.py</code> <pre><code>def execute(\n    self, input_data: CSVConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Executes the CSV conversion process.\n\n    Processes one or more CSV files according to the input configuration,\n    converting each row into a document with specified content and metadata.\n    If some files fail to process but at least one succeeds, logs errors and continues.\n    If all files fail, raises the last encountered error.\n\n    Args:\n        input_data (CSVConverterInputSchema): Validated input parameters for the conversion.\n        config (RunnableConfig | None): Optional runtime configuration.\n        **kwargs: Additional keyword arguments passed to execution callbacks.\n\n    Returns:\n        dict[str, list[Any]]: Dictionary containing the list of processed documents\n            under the 'documents' key.\n\n    Raises:\n        Exception: If there are errors reading or processing the CSV files.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    all_documents = []\n    last_error = None\n    success_count = 0\n    total_files = len(input_data.file_paths or []) + len(input_data.files or [])\n    delimiter = input_data.delimiter or self.delimiter\n    content_column = input_data.content_column or self.content_column\n    metadata_columns = input_data.metadata_columns or self.metadata_columns\n    external_metadata = input_data.metadata\n\n    if input_data.file_paths:\n        for path in input_data.file_paths:\n            try:\n                with open(path, encoding=\"utf-8\") as csv_file:\n                    reader = csv.DictReader(csv_file, delimiter=delimiter)\n                    for doc in self._process_rows_generator(\n                        reader,\n                        source=path,\n                        content_column=content_column,\n                        metadata_columns=metadata_columns,\n                        external_metadata=external_metadata,\n\n                    ):\n                        all_documents.append(doc)\n                    success_count += 1\n            except Exception as e:\n                logger.error(f\"Error processing file {path}: {str(e)}\")\n                last_error = e\n\n    if input_data.files:\n        for file_obj in input_data.files:\n            source_name = getattr(file_obj, \"name\", \"in-memory file\")\n            try:\n                if isinstance(file_obj, bytes):\n                    file_obj = BytesIO(file_obj)\n\n                file_text = TextIOWrapper(file_obj, encoding=\"utf-8\")\n                reader = csv.DictReader(file_text, delimiter=delimiter)\n\n                for doc in self._process_rows_generator(\n                    reader,\n                    source=source_name,\n                    content_column=content_column,\n                    metadata_columns=metadata_columns,\n                    external_metadata=external_metadata,\n\n                ):\n                    all_documents.append(doc)\n                success_count += 1\n            except Exception as e:\n                logger.error(f\"Error processing file {source_name}: {str(e)}\")\n                last_error = e\n\n    if success_count == 0 and last_error is not None:\n        raise last_error\n\n    if success_count &lt; total_files:\n        logger.warning(f\"Processed {success_count} out of {total_files} files successfully\")\n\n    return {\"documents\": all_documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/csv/#dynamiq.nodes.converters.csv.CSVConverterInputSchema","title":"<code>CSVConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema defining the input parameters for CSV file conversion.</p> <p>This model validates and structures the input configuration for converting CSV files into a standardized document format. It ensures that either file paths or file objects are provided, along with the necessary column specifications.</p> <p>Attributes:</p> Name Type Description <code>file_paths</code> <code>list[str] | None</code> <p>List of paths to CSV files on the filesystem.</p> <code>files</code> <code>list[BytesIO | bytes] | None</code> <p>List of file objects or bytes containing CSV data.</p> <code>delimiter</code> <code>str</code> <p>Character used to separate fields in the CSV files. Defaults to comma.</p> <code>content_column</code> <code>str</code> <p>Name of the column to use as the main document content.</p> <code>metadata_columns</code> <code>list[str] | None</code> <p>Column names to extract as metadata for each document.</p> Source code in <code>dynamiq/nodes/converters/csv.py</code> <pre><code>class CSVConverterInputSchema(BaseModel):\n    \"\"\"\n    Schema defining the input parameters for CSV file conversion.\n\n    This model validates and structures the input configuration for converting CSV files\n    into a standardized document format. It ensures that either file paths or file objects\n    are provided, along with the necessary column specifications.\n\n    Attributes:\n        file_paths (list[str] | None): List of paths to CSV files on the filesystem.\n        files (list[BytesIO | bytes] | None): List of file objects or bytes containing CSV data.\n        delimiter (str): Character used to separate fields in the CSV files. Defaults to comma.\n        content_column (str): Name of the column to use as the main document content.\n        metadata_columns (list[str] | None): Column names to extract as metadata for each document.\n    \"\"\"\n\n    file_paths: list[str] | None = Field(\n        default=None, description=\"List of CSV file paths. Either file_paths or files must be provided.\"\n    )\n    files: list[BytesIO | bytes] | None = Field(\n        default=None, description=\"List of file objects or bytes representing CSV files.\"\n    )\n    delimiter: str | None = Field(\n        default=None,\n        description=\"Delimiter used in the CSV files. If not provided, the Node's configured delimiter is used.\"\n    )\n    content_column: str | None = Field(\n        default=None, description=\"Name of the column that will be used as the document's main content.\"\n    )\n    metadata_columns: list[str] | None = Field(\n        default=None,\n        description=\"Optional list of column names to extract as metadata for each document. Can be None.\",\n    )\n    metadata: dict | list | None = Field(\n        default=None,\n        description=\"External metadata to be merged with metadata extracted from CSV rows.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_source(cls, values):\n        file_paths, files = values.file_paths, values.files\n        if not file_paths and not files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return values\n</code></pre>"},{"location":"dynamiq/nodes/converters/docx/","title":"Docx","text":""},{"location":"dynamiq/nodes/converters/docx/#dynamiq.nodes.converters.docx.DOCXFileConverter","title":"<code>DOCXFileConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A component for converting files to Documents using the docx converter.</p> Source code in <code>dynamiq/nodes/converters/docx.py</code> <pre><code>class DOCXFileConverter(Node):\n    \"\"\"\n    A component for converting files to Documents using the docx converter.\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"DOCX File Converter\"\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    file_converter: DOCXConverterComponent | None = None\n    input_schema: ClassVar[type[DOCXFileConverterInputSchema]] = DOCXFileConverterInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the DOCXConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = DOCXConverterComponent(\n                document_creation_mode=self.document_creation_mode,\n            )\n\n    def execute(\n        self, input_data: DOCXFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the DOCXConverter to convert files to Documents.\n\n        Args:\n            input_data (DOCXFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n            config (RunnableConfig): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.docx\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.file_paths\n        files = input_data.files\n        metadata = input_data.metadata\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/docx/#dynamiq.nodes.converters.docx.DOCXFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the DOCXConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DOCXFileConverterInputSchema</code> <p>An instance containing 'file_paths', 'files', and/or 'metadata'.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.docx\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/docx.py</code> <pre><code>def execute(\n    self, input_data: DOCXFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the DOCXConverter to convert files to Documents.\n\n    Args:\n        input_data (DOCXFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n        config (RunnableConfig): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.docx\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.file_paths\n    files = input_data.files\n    metadata = input_data.metadata\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/docx/#dynamiq.nodes.converters.docx.DOCXFileConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the DOCXConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/docx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the DOCXConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = DOCXConverterComponent(\n            document_creation_mode=self.document_creation_mode,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/docx/#dynamiq.nodes.converters.docx.DOCXFileConverterInputSchema","title":"<code>DOCXFileConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/docx.py</code> <pre><code>class DOCXFileConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/docx/#dynamiq.nodes.converters.docx.DOCXFileConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/docx.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/html/","title":"Html","text":""},{"location":"dynamiq/nodes/converters/html/#dynamiq.nodes.converters.html.HTMLConverter","title":"<code>HTMLConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A component for converting HTML files to Documents using the HTML converter.</p> Source code in <code>dynamiq/nodes/converters/html.py</code> <pre><code>class HTMLConverter(Node):\n    \"\"\"\n    A component for converting HTML files to Documents using the HTML converter.\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"HTML File Converter\"\n    file_converter: HTMLConverterComponent | None = None\n    input_schema: ClassVar[type[HTMLConverterInputSchema]] = HTMLConverterInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the HTMLConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = HTMLConverterComponent()\n\n    def execute(\n        self, input_data: HTMLConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the HTMLConverter to convert files to Documents.\n\n        Args:\n            input_data (HTMLConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n            config (RunnableConfig): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.html\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.file_paths\n        files = input_data.files\n        metadata = input_data.metadata\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/html/#dynamiq.nodes.converters.html.HTMLConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the HTMLConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>HTMLConverterInputSchema</code> <p>An instance containing 'file_paths', 'files', and/or 'metadata'.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.html\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/html.py</code> <pre><code>def execute(\n    self, input_data: HTMLConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the HTMLConverter to convert files to Documents.\n\n    Args:\n        input_data (HTMLConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n        config (RunnableConfig): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.html\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.file_paths\n    files = input_data.files\n    metadata = input_data.metadata\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/html/#dynamiq.nodes.converters.html.HTMLConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the HTMLConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/html.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the HTMLConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = HTMLConverterComponent()\n</code></pre>"},{"location":"dynamiq/nodes/converters/html/#dynamiq.nodes.converters.html.HTMLConverterInputSchema","title":"<code>HTMLConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/html.py</code> <pre><code>class HTMLConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/html/#dynamiq.nodes.converters.html.HTMLConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/html.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/","title":"Llm text extractor","text":""},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter","title":"<code>LLMImageConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node class for extracting text from images using a Large Language Model (LLM).</p> <p>This class extracts text from the images using an LLM and saves the text as documents with metadata.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[CONVERTERS]</code> <p>The group the node belongs to. Default is NodeGroup.CONVERTERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMImageConverter\".</p> <code>extraction_instruction</code> <code>str</code> <p>The instruction for text extraction. Default is DEFAULT_EXTRACTION_INSTRUCTION.</p> <code>document_creation_mode</code> <code>DocumentCreationMode</code> <p>The mode for document creation. Default is DocumentCreationMode.ONE_DOC_PER_FILE.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for text extraction. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.extractors import ImageLLMExtractor\nfrom io import BytesIO\n\n# Initialize the extractor\nextractor = ImageLLMExtractor(llm=my_llm_instance)\n\n# Example input data\ninput_data = {\n    \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n    \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\n# Execute the extractor\noutput = extractor.execute(input_data)\n\n# Output will be a dictionary with extracted documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMImageConverter(Node):\n    \"\"\"\n    A Node class for extracting text from images using a Large Language Model (LLM).\n\n    This class extracts text from the images using an LLM and saves the text as documents with metadata.\n\n    Attributes:\n        group (Literal[NodeGroup.CONVERTERS]): The group the node belongs to. Default is NodeGroup.CONVERTERS.\n        name (str): The name of the node. Default is \"LLMImageConverter\".\n        extraction_instruction (str): The instruction for text extraction.\n            Default is DEFAULT_EXTRACTION_INSTRUCTION.\n        document_creation_mode (DocumentCreationMode): The mode for document creation.\n            Default is DocumentCreationMode.ONE_DOC_PER_FILE.\n        llm (BaseLLM): The LLM instance used for text extraction. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.extractors import ImageLLMExtractor\n        from io import BytesIO\n\n        # Initialize the extractor\n        extractor = ImageLLMExtractor(llm=my_llm_instance)\n\n        # Example input data\n        input_data = {\n            \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n            \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        # Execute the extractor\n        output = extractor.execute(input_data)\n\n        # Output will be a dictionary with extracted documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"LLMImageConverter\"\n    extraction_instruction: str = DEFAULT_EXTRACTION_INSTRUCTION\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    llm: Node\n    vision_prompt: Prompt = Field(default_factory=create_vision_prompt_template)\n    input_schema: ClassVar[type[LLMImageConverterInputSchema]] = LLMImageConverterInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMImageConverter with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the document extractor component.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def execute(\n        self, input_data: LLMImageConverterInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the image text extraction process.\n\n        Args:\n            input_data (LLMImageConverterInputSchema): An instance containing the images to be processed.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the extracted documents.\n\n        Example:\n\n            input_data = {\n                \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n                \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n                \"metadata\": {\"source\": \"example source\"}\n            }\n\n            output = extractor.execute(input_data)\n\n            # output will be a dictionary with extracted documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = self.extract_text_from_images(\n            file_paths=input_data.file_paths,\n            files=input_data.files,\n            metadata=input_data.metadata,\n            config=config,\n            **kwargs,\n        )\n\n        return {\"documents\": documents}\n\n    def extract_text_from_images(\n        self,\n        file_paths: list[str] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from images using an LLM.\n\n        Args:\n            file_paths (list[str], optional): List of paths to image files. Default is None.\n            files (list[BytesIO], optional): List of image files as BytesIO objects. Default is None.\n            metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath\n                for path in paths_obj\n                if path.is_dir()\n                for filepath in path.glob(\"*.*\")\n                if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `file_paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = filepaths + filepaths_in_directories\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for file_path, meta in zip(all_filepaths, meta_list):\n                with open(file_path, \"rb\") as upload_file:\n                    file = BytesIO(upload_file.read())\n                    file.name = upload_file.name\n\n                image = self._load_image(file)\n                meta[\"filename\"] = str(file_path)\n                documents.extend(self._process_images([image], meta, config, **kwargs))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n\n            for file, meta in zip(files, meta_list):\n                if not isinstance(file, BytesIO):\n                    raise ValueError(\"All files must be of type BytesIO.\")\n                image = self._load_image(file)\n                meta[\"filename\"] = get_filename_for_bytesio(file)\n                documents.extend(self._process_images([image], meta, config, **kwargs))\n\n        return documents\n\n    def _load_image(self, file: BytesIO) -&gt; \"Image\":\n        \"\"\"\n        Loads an image from a BytesIO object.\n\n        Args:\n            file (BytesIO): The BytesIO object containing the image data.\n\n        Returns:\n            Image: The loaded image.\n        \"\"\"\n        from PIL import Image\n\n        return Image.open(file)\n\n    def _process_images(\n        self,\n        images: list[\"Image\"],\n        metadata: dict[str, Any],\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from images using a vision prompt.\n\n        Args:\n            images (list[Image]): List of images.\n            metadata (dict[str, Any]): Metadata for the documents.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n        urls = self._convert_images_to_urls(images)\n\n        outputs = self.perform_llm_extraction(urls, config, **kwargs)\n\n        if self.document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            document_content = \"\".join(output[\"content\"] for output in outputs)\n            return [Document(content=document_content, metadata=metadata)]\n        else:\n            documents = [\n                Document(content=output[\"content\"], metadata=metadata)\n                for output in outputs\n            ]\n            return documents\n\n    def perform_llm_extraction(\n        self, urls: list[str], config: RunnableConfig, **kwargs\n    ) -&gt; list[dict]:\n        \"\"\"\n        Performs the actual extraction of text from images using the LLM.\n\n        Args:\n            urls (list[str]): The list of image URLs to extract text from.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[dict]: A list of extracted text results from the LLM.\n\n        Example:\n\n            urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\n            extracted_texts = extractor.perform_llm_extraction(urls, config)\n\n            # extracted_texts will be a list of dictionaries with extracted text\n        \"\"\"\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        inputs = [\n            {\"extraction_instruction\": self.extraction_instruction, \"img_url\": url}\n            for url in urls\n        ]\n\n        prompt = self.vision_prompt\n\n        with ContextAwareThreadPoolExecutor() as executor:\n            llm_results = list(\n                executor.map(\n                    lambda input_data: self.call_llm(input_data, prompt, config, **run_kwargs),\n                    inputs,\n                )\n            )\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} images\"\n        )\n\n        return llm_results\n\n    def call_llm(self, input_data, prompt, config, **run_kwargs):\n        \"\"\"\n        Calls the LLM with the given input data and prompt.\n\n        Args:\n            input_data (dict): The input data for the LLM.\n            prompt (Prompt): The prompt to be used with the LLM.\n            config (RunnableConfig): Configuration for the execution.\n            **run_kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: The result from the LLM.\n        \"\"\"\n        llm_result = self.llm.run(\n            input_data=input_data,\n            prompt=prompt,\n            config=config,\n            **(run_kwargs | {\"run_depends\": self._run_depends}),\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n\n        if llm_result.status != RunnableStatus.SUCCESS:\n            logger.error(f\"Node {self.name} - {self.id}: LLM execution failed: {llm_result.error.to_dict()}\")\n            raise ValueError(\"ImageLLMExtractor LLM execution failed\")\n        return llm_result.output\n\n    @staticmethod\n    def _convert_image_to_url(image: \"Image\") -&gt; str:\n        \"\"\"\n        Converts a PIL Image to a base64-encoded URL.\n\n        Args:\n            image (Image): The image to convert.\n\n        Returns:\n            str: The base64-encoded URL of the image.\n        \"\"\"\n        # Ensure the image is in RGB mode (required for JPEG)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        buffered = BytesIO()\n        image.save(buffered, format=\"JPEG\")\n        buffered.seek(0)  # Ensure the buffer is at the beginning\n        decoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n        url = f\"data:image/jpeg;base64,{decoded_image}\"\n        return url\n\n    @staticmethod\n    def _convert_images_to_urls(images: list[\"Image\"]) -&gt; list[str]:\n        \"\"\"\n        Converts a list of PIL Images to a list of base64-encoded URLs.\n\n        Args:\n            images (List[Image]): The list of images to convert.\n\n        Returns:\n            List[str]: The list of base64-encoded URLs.\n        \"\"\"\n        return [LLMImageConverter._convert_image_to_url(image) for image in images]\n\n    @staticmethod\n    def _normalize_metadata(\n        metadata: dict[str, Any] | list[dict[str, Any]] | None, sources_count: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Normalizes metadata input for a converter.\n\n        Given all possible values of the metadata input for a converter (None, dictionary, or list of\n        dicts), ensures to return a list of dictionaries of the correct length for the converter to use.\n\n        Args:\n            metadata: The meta input of the converter, as-is. Can be None, a dictionary, or a list of\n                dictionaries.\n            sources_count: The number of sources the converter received.\n\n        Returns:\n            A list of dictionaries of the same length as the sources list.\n\n        Raises:\n            ValueError: If metadata is not None, a dictionary, or a list of dictionaries, or if the length\n                of the metadata list doesn't match the number of sources.\n        \"\"\"\n        if metadata is None:\n            return [{} for _ in range(sources_count)]\n        if isinstance(metadata, dict):\n            return [copy.deepcopy(metadata) for _ in range(sources_count)]\n        if isinstance(metadata, list):\n            if sources_count != len(metadata):\n                raise ValueError(\n                    \"The length of the metadata list must match the number of sources.\"\n                )\n            return metadata\n        raise ValueError(\n            \"metadata must be either None, a dictionary or a list of dictionaries.\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMImageConverter with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMImageConverter with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.call_llm","title":"<code>call_llm(input_data, prompt, config, **run_kwargs)</code>","text":"<p>Calls the LLM with the given input data and prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>The input data for the LLM.</p> required <code>prompt</code> <code>Prompt</code> <p>The prompt to be used with the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**run_kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The result from the LLM.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def call_llm(self, input_data, prompt, config, **run_kwargs):\n    \"\"\"\n    Calls the LLM with the given input data and prompt.\n\n    Args:\n        input_data (dict): The input data for the LLM.\n        prompt (Prompt): The prompt to be used with the LLM.\n        config (RunnableConfig): Configuration for the execution.\n        **run_kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The result from the LLM.\n    \"\"\"\n    llm_result = self.llm.run(\n        input_data=input_data,\n        prompt=prompt,\n        config=config,\n        **(run_kwargs | {\"run_depends\": self._run_depends}),\n    )\n    self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n\n    if llm_result.status != RunnableStatus.SUCCESS:\n        logger.error(f\"Node {self.name} - {self.id}: LLM execution failed: {llm_result.error.to_dict()}\")\n        raise ValueError(\"ImageLLMExtractor LLM execution failed\")\n    return llm_result.output\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the image text extraction process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>LLMImageConverterInputSchema</code> <p>An instance containing the images to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the extracted documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n    \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\noutput = extractor.execute(input_data)\n\n# output will be a dictionary with extracted documents\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def execute(\n    self, input_data: LLMImageConverterInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the image text extraction process.\n\n    Args:\n        input_data (LLMImageConverterInputSchema): An instance containing the images to be processed.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the extracted documents.\n\n    Example:\n\n        input_data = {\n            \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n            \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        output = extractor.execute(input_data)\n\n        # output will be a dictionary with extracted documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = self.extract_text_from_images(\n        file_paths=input_data.file_paths,\n        files=input_data.files,\n        metadata=input_data.metadata,\n        config=config,\n        **kwargs,\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.extract_text_from_images","title":"<code>extract_text_from_images(file_paths=None, files=None, metadata=None, config=None, **kwargs)</code>","text":"<p>Extracts text from images using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of paths to image files. Default is None.</p> <code>None</code> <code>files</code> <code>list[BytesIO]</code> <p>List of image files as BytesIO objects. Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]]</code> <p>Metadata for the documents. Default is None.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of extracted documents.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def extract_text_from_images(\n    self,\n    file_paths: list[str] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; list[Document]:\n    \"\"\"\n    Extracts text from images using an LLM.\n\n    Args:\n        file_paths (list[str], optional): List of paths to image files. Default is None.\n        files (list[BytesIO], optional): List of image files as BytesIO objects. Default is None.\n        metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of extracted documents.\n    \"\"\"\n\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath\n            for path in paths_obj\n            if path.is_dir()\n            for filepath in path.glob(\"*.*\")\n            if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `file_paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = filepaths + filepaths_in_directories\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for file_path, meta in zip(all_filepaths, meta_list):\n            with open(file_path, \"rb\") as upload_file:\n                file = BytesIO(upload_file.read())\n                file.name = upload_file.name\n\n            image = self._load_image(file)\n            meta[\"filename\"] = str(file_path)\n            documents.extend(self._process_images([image], meta, config, **kwargs))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n\n        for file, meta in zip(files, meta_list):\n            if not isinstance(file, BytesIO):\n                raise ValueError(\"All files must be of type BytesIO.\")\n            image = self._load_image(file)\n            meta[\"filename\"] = get_filename_for_bytesio(file)\n            documents.extend(self._process_images([image], meta, config, **kwargs))\n\n    return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the document extractor component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the document extractor component.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.perform_llm_extraction","title":"<code>perform_llm_extraction(urls, config, **kwargs)</code>","text":"<p>Performs the actual extraction of text from images using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>The list of image URLs to extract text from.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of extracted text results from the LLM.</p> <p>Example:</p> <pre><code>urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\nextracted_texts = extractor.perform_llm_extraction(urls, config)\n\n# extracted_texts will be a list of dictionaries with extracted text\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def perform_llm_extraction(\n    self, urls: list[str], config: RunnableConfig, **kwargs\n) -&gt; list[dict]:\n    \"\"\"\n    Performs the actual extraction of text from images using the LLM.\n\n    Args:\n        urls (list[str]): The list of image URLs to extract text from.\n        config (RunnableConfig): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[dict]: A list of extracted text results from the LLM.\n\n    Example:\n\n        urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\n        extracted_texts = extractor.perform_llm_extraction(urls, config)\n\n        # extracted_texts will be a list of dictionaries with extracted text\n    \"\"\"\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    inputs = [\n        {\"extraction_instruction\": self.extraction_instruction, \"img_url\": url}\n        for url in urls\n    ]\n\n    prompt = self.vision_prompt\n\n    with ContextAwareThreadPoolExecutor() as executor:\n        llm_results = list(\n            executor.map(\n                lambda input_data: self.call_llm(input_data, prompt, config, **run_kwargs),\n                inputs,\n            )\n        )\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} images\"\n    )\n\n    return llm_results\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverterInputSchema","title":"<code>LLMImageConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMImageConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide paths to files.\")\n    files: list[BytesIO | bytes] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter","title":"<code>LLMPDFConverter</code>","text":"<p>               Bases: <code>LLMImageConverter</code></p> <p>A Node class for extracting text from PDFs using a Large Language Model (LLM).</p> <p>This class converts PDFs to images, extracts text from the images using an LLM, and saves the text as documents with metadata.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[CONVERTERS]</code> <p>The group the node belongs to. Default is NodeGroup.CONVERTERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMPDFConverter\".</p> <code>extraction_instruction</code> <code>str</code> <p>The instruction for text extraction. Default is DEFAULT_EXTRACTION_INSTRUCTION.</p> <code>document_creation_mode</code> <code>DocumentCreationMode</code> <p>The mode for document creation. Default is DocumentCreationMode.ONE_DOC_PER_FILE.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for text extraction. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.converters import LLMPDFConverter\nfrom io import BytesIO\n\n# Initialize the extractor\nconverter = LLMPDFConverter(llm=my_llm_instance)\n\n# Example input data\ninput_data = {\n    \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n    \"files\": [BytesIO(b\"pdf1 content\"), BytesIO(b\"pdf2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\n# Execute the converter\noutput = converter.execute(input_data)\n\n# Output will be a dictionary with extracted documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMPDFConverter(LLMImageConverter):\n    \"\"\"\n    A Node class for extracting text from PDFs using a Large Language Model (LLM).\n\n    This class converts PDFs to images, extracts text from the images using an LLM,\n    and saves the text as documents with metadata.\n\n    Attributes:\n        group (Literal[NodeGroup.CONVERTERS]): The group the node belongs to. Default is NodeGroup.CONVERTERS.\n        name (str): The name of the node. Default is \"LLMPDFConverter\".\n        extraction_instruction (str): The instruction for text extraction.\n            Default is DEFAULT_EXTRACTION_INSTRUCTION.\n        document_creation_mode (DocumentCreationMode): The mode for document creation.\n            Default is DocumentCreationMode.ONE_DOC_PER_FILE.\n        llm (BaseLLM): The LLM instance used for text extraction. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.converters import LLMPDFConverter\n        from io import BytesIO\n\n        # Initialize the extractor\n        converter = LLMPDFConverter(llm=my_llm_instance)\n\n        # Example input data\n        input_data = {\n            \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n            \"files\": [BytesIO(b\"pdf1 content\"), BytesIO(b\"pdf2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        # Execute the converter\n        output = converter.execute(input_data)\n\n        # Output will be a dictionary with extracted documents\n        print(output)\n    \"\"\"\n\n    _convert_from_bytes: Any = PrivateAttr()\n    _convert_from_path: Any = PrivateAttr()\n    input_schema: ClassVar[type[LLMPDFConverterInputSchema]] = LLMPDFConverterInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        from pdf2image import convert_from_bytes, convert_from_path\n\n        super().__init__(**kwargs)\n        self._convert_from_bytes = convert_from_bytes\n        self._convert_from_path = convert_from_path\n\n    def execute(\n        self, input_data: LLMPDFConverterInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the PDF text extraction process.\n\n        Args:\n            input_data (LLMPDFConverterInputSchema): An instance containing the file paths or\n              files of PDFs to be processed.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the extracted documents.\n\n        Example:\n\n            input_data = {\n                \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n                \"metadata\": {\"source\": \"example source\"}\n            }\n\n            output = extractor.execute(input_data)\n\n            # output will be a dictionary with extracted documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = self.extract_text_from_pdfs(\n            file_paths=input_data.file_paths,\n            files=input_data.files,\n            metadata=input_data.metadata,\n            config=config,\n            **kwargs,\n        )\n\n        return {\"documents\": documents}\n\n    def extract_text_from_pdfs(\n        self,\n        file_paths: list[str] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from PDFs by converting them to images and using an LLM.\n\n        Args:\n            file_paths (list[str], optional): List of paths to PDF files. Default is None.\n            files (list[BytesIO], optional): List of PDF files as BytesIO objects. Default is None.\n            metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath\n                for path in paths_obj\n                if path.is_dir()\n                for filepath in path.glob(\"*.*\")\n                if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `file_paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = filepaths + filepaths_in_directories\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for file_path, meta in zip(all_filepaths, meta_list):\n                images = self._convert_from_path(file_path)\n                meta[\"filename\"] = str(file_path)\n                documents.extend(self._process_images(images, meta, config, **kwargs))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n\n            for file, meta in zip(files, meta_list):\n                if not isinstance(file, BytesIO):\n                    raise ValueError(\"All files must be of type BytesIO.\")\n                images = self._convert_from_bytes(file.read())\n                meta[\"filename\"] = get_filename_for_bytesio(file)\n                documents.extend(self._process_images(images, meta, config, **kwargs))\n\n        return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    from pdf2image import convert_from_bytes, convert_from_path\n\n    super().__init__(**kwargs)\n    self._convert_from_bytes = convert_from_bytes\n    self._convert_from_path = convert_from_path\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the PDF text extraction process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>LLMPDFConverterInputSchema</code> <p>An instance containing the file paths or files of PDFs to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the extracted documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\noutput = extractor.execute(input_data)\n\n# output will be a dictionary with extracted documents\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def execute(\n    self, input_data: LLMPDFConverterInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the PDF text extraction process.\n\n    Args:\n        input_data (LLMPDFConverterInputSchema): An instance containing the file paths or\n          files of PDFs to be processed.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the extracted documents.\n\n    Example:\n\n        input_data = {\n            \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        output = extractor.execute(input_data)\n\n        # output will be a dictionary with extracted documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = self.extract_text_from_pdfs(\n        file_paths=input_data.file_paths,\n        files=input_data.files,\n        metadata=input_data.metadata,\n        config=config,\n        **kwargs,\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.extract_text_from_pdfs","title":"<code>extract_text_from_pdfs(file_paths=None, files=None, metadata=None, config=None, **kwargs)</code>","text":"<p>Extracts text from PDFs by converting them to images and using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of paths to PDF files. Default is None.</p> <code>None</code> <code>files</code> <code>list[BytesIO]</code> <p>List of PDF files as BytesIO objects. Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]]</code> <p>Metadata for the documents. Default is None.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of extracted documents.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def extract_text_from_pdfs(\n    self,\n    file_paths: list[str] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; list[Document]:\n    \"\"\"\n    Extracts text from PDFs by converting them to images and using an LLM.\n\n    Args:\n        file_paths (list[str], optional): List of paths to PDF files. Default is None.\n        files (list[BytesIO], optional): List of PDF files as BytesIO objects. Default is None.\n        metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of extracted documents.\n    \"\"\"\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath\n            for path in paths_obj\n            if path.is_dir()\n            for filepath in path.glob(\"*.*\")\n            if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `file_paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = filepaths + filepaths_in_directories\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for file_path, meta in zip(all_filepaths, meta_list):\n            images = self._convert_from_path(file_path)\n            meta[\"filename\"] = str(file_path)\n            documents.extend(self._process_images(images, meta, config, **kwargs))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n\n        for file, meta in zip(files, meta_list):\n            if not isinstance(file, BytesIO):\n                raise ValueError(\"All files must be of type BytesIO.\")\n            images = self._convert_from_bytes(file.read())\n            meta[\"filename\"] = get_filename_for_bytesio(file)\n            documents.extend(self._process_images(images, meta, config, **kwargs))\n\n    return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverterInputSchema","title":"<code>LLMPDFConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMPDFConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO | bytes] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.create_vision_prompt_template","title":"<code>create_vision_prompt_template()</code>","text":"<p>Creates a vision prompt template.</p> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The vision prompt template.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def create_vision_prompt_template() -&gt; Prompt:\n    \"\"\"\n    Creates a vision prompt template.\n\n    Returns:\n        Prompt: The vision prompt template.\n    \"\"\"\n    text_message = VisionMessageTextContent(text=\"{{extraction_instruction}}\")\n    image_message = VisionMessageImageContent(image_url=VisionMessageImageURL(url=\"{{img_url}}\"))\n    vision_message = VisionMessage(content=[text_message, image_message], role=\"user\")\n    vision_prompt = Prompt(messages=[vision_message])\n    return vision_prompt\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/","title":"Pptx","text":""},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter","title":"<code>PPTXFileConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A component for converting files to Documents using the pptx converter.</p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>class PPTXFileConverter(Node):\n    \"\"\"\n    A component for converting files to Documents using the pptx converter.\n\n    Args:\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"],\n            optional): Determines how to create Documents from the elements returned by PdfReader.\n            Options are:\n            - \"one-doc-per-file\": Creates one Document per file.\n                All elements are concatenated into one text field.\n            - \"one-doc-per-page\": Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            Defaults to \"one-doc-per-file\".\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"PPTX File Converter\"\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    file_converter: PPTXConverterComponent | None = None\n    input_schema: ClassVar[type[PPTXFileConverterInputSchema]] = PPTXFileConverterInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the PPTXConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = PPTXConverterComponent(\n                document_creation_mode=self.document_creation_mode,\n            )\n\n    def execute(\n        self, input_data: PPTXFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the PPTXConverter to convert files to Documents.\n\n        Args:\n            input_data (PPTXFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n            config (RunnableConfig): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.pptx\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.file_paths\n        files = input_data.files\n        metadata = input_data.metadata\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the PPTXConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>PPTXFileConverterInputSchema</code> <p>An instance containing 'file_paths', 'files', and/or 'metadata'.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.pptx\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>def execute(\n    self, input_data: PPTXFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the PPTXConverter to convert files to Documents.\n\n    Args:\n        input_data (PPTXFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n        config (RunnableConfig): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.pptx\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.file_paths\n    files = input_data.files\n    metadata = input_data.metadata\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the PPTXConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the PPTXConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = PPTXConverterComponent(\n            document_creation_mode=self.document_creation_mode,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverterInputSchema","title":"<code>PPTXFileConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>class PPTXFileConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/pypdf/","title":"Pypdf","text":"<p>pdf</p>"},{"location":"dynamiq/nodes/converters/text/","title":"Text","text":""},{"location":"dynamiq/nodes/converters/text/#dynamiq.nodes.converters.text.TextFileConverter","title":"<code>TextFileConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A component for converting text files to Documents using the TextFileConverter.</p> Source code in <code>dynamiq/nodes/converters/text.py</code> <pre><code>class TextFileConverter(Node):\n    \"\"\"\n    A component for converting text files to Documents using the TextFileConverter.\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"Text File Converter\"\n    file_converter: TextFileConverterComponent | None = None\n    input_schema: ClassVar[type[TextFileConverterInputSchema]] = TextFileConverterInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the TextFileConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = TextFileConverterComponent()\n\n    def execute(\n        self, input_data: TextFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the TextFileConverter to convert files to Documents.\n\n        Args:\n            input_data (TextFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n            config (RunnableConfig): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.txt\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.file_paths\n        files = input_data.files\n        metadata = input_data.metadata\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/text/#dynamiq.nodes.converters.text.TextFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the TextFileConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TextFileConverterInputSchema</code> <p>An instance containing 'file_paths', 'files', and/or 'metadata'.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.txt\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/text.py</code> <pre><code>def execute(\n    self, input_data: TextFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the TextFileConverter to convert files to Documents.\n\n    Args:\n        input_data (TextFileConverterInputSchema): An instance containing 'file_paths', 'files', and/or 'metadata'.\n        config (RunnableConfig): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.txt\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.file_paths\n    files = input_data.files\n    metadata = input_data.metadata\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/text/#dynamiq.nodes.converters.text.TextFileConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the TextFileConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/text.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the TextFileConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = TextFileConverterComponent()\n</code></pre>"},{"location":"dynamiq/nodes/converters/text/#dynamiq.nodes.converters.text.TextFileConverterInputSchema","title":"<code>TextFileConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/text.py</code> <pre><code>class TextFileConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/text/#dynamiq.nodes.converters.text.TextFileConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/text.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/","title":"Unstructured","text":""},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter","title":"<code>UnstructuredFileConverter</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for converting files to Documents using the Unstructured API (hosted or running locally).</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>UnstructuredConnection</code> <p>The connection to use for the Unstructured API. Defaults to None, which will initialize a new UnstructuredConnection.</p> required <code>strategy</code> <code>Literal['auto', 'fast', 'hi_res', 'ocr_only']</code> <p>The strategy to use for document processing. Defaults to \"auto\".</p> required <code>unstructured_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional parameters to pass to the Unstructured API. See Unstructured API docs for available parameters. Defaults to None.</p> required <code>extract_image_block_types_enabled</code> <code>bool</code> <p>Whether to extract and embed images/tables in the result. When enabled, Base64-encoded images and tables will be decoded and included in the document content. Defaults to False.</p> required <code>extract_image_block_types</code> <code>Optional[list[UnstructuredElementTypes]]</code> <p>List of element types to extract when <code>extract_image_block_types_enabled</code> is True. If None and extract_image_block_types_enabled is True, defaults to [Image, Table] Defaults to None.</p> required Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>class UnstructuredFileConverter(ConnectionNode):\n    \"\"\"\n    A component for converting files to Documents using the Unstructured API (hosted or running locally).\n\n    Args:\n        connection (UnstructuredConnection, optional): The connection to use for the Unstructured API.\n            Defaults to None, which will initialize a new UnstructuredConnection.\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"],\n            optional): Determines how to create Documents from the elements returned by Unstructured.\n            Options are:\n            - \"one-doc-per-file\": Creates one Document per file.\n                All elements are concatenated into one text field.\n            - \"one-doc-per-page\": Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            - \"one-doc-per-element\": Creates one Document per element.\n                Each element is converted to a separate Document.\n            Defaults to \"one-doc-per-file\".\n        strategy (Literal[\"auto\", \"fast\", \"hi_res\", \"ocr_only\"], optional): The strategy to use for\n            document processing. Defaults to \"auto\".\n        unstructured_kwargs (Optional[dict[str, Any]], optional): Additional parameters to pass to the\n            Unstructured API. See Unstructured API docs for available parameters. Defaults to None.\n        extract_image_block_types_enabled (bool, optional): Whether to extract and embed images/tables in the result.\n            When enabled, Base64-encoded images and tables will be decoded and included in the document content.\n            Defaults to False.\n        extract_image_block_types (Optional[list[UnstructuredElementTypes]], optional): List of element types to extract\n            when `extract_image_block_types_enabled` is True.\n            If None and extract_image_block_types_enabled is True, defaults to [Image, Table]\n            Defaults to None.\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"Unstructured File Converter\"\n    connection: Unstructured = None\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    strategy: ConvertStrategy = ConvertStrategy.AUTO\n    unstructured_kwargs: dict[str, Any] | None = None\n    extract_image_block_types_enabled: bool = False\n    extract_image_block_types: list[UnstructuredElementTypes] | None = None\n    file_converter: UnstructuredFileConverterComponent | None = None\n    input_schema: ClassVar[type[UnstructuredFileConverterInputSchema]] = UnstructuredFileConverterInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the UnstructuredFileConverter.\n\n        If no connection is provided, a new Unstructured connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the UnstructuredFileConverter.\n        \"\"\"\n        if kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Unstructured()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the UnstructuredFileConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = UnstructuredFileConverterComponent(\n                connection=self.connection,\n                document_creation_mode=self.document_creation_mode,\n                strategy=self.strategy,\n                unstructured_kwargs=self.unstructured_kwargs,\n                extract_image_block_types_enabled=self.extract_image_block_types_enabled,\n                extract_image_block_types=self.extract_image_block_types,\n            )\n\n    def execute(\n        self, input_data: UnstructuredFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the UnstructuredFileConverter to convert files to Documents.\n\n        Args:\n            input_data (UnstructuredFileConverterInputSchema): An instance containing 'file_paths',\n              'files', and/or 'metadata'.\n            config (RunnableConfig): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, list[Any]]: Dictionary with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.file_paths\n        files = input_data.files\n        metadata = input_data.metadata\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \"\n            f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the UnstructuredFileConverter.</p> <p>If no connection is provided, a new Unstructured connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the UnstructuredFileConverter.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the UnstructuredFileConverter.\n\n    If no connection is provided, a new Unstructured connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the UnstructuredFileConverter.\n    \"\"\"\n    if kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Unstructured()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the UnstructuredFileConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>UnstructuredFileConverterInputSchema</code> <p>An instance containing 'file_paths', 'files', and/or 'metadata'.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>dict[str, list[Any]]: Dictionary with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def execute(\n    self, input_data: UnstructuredFileConverterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the UnstructuredFileConverter to convert files to Documents.\n\n    Args:\n        input_data (UnstructuredFileConverterInputSchema): An instance containing 'file_paths',\n          'files', and/or 'metadata'.\n        config (RunnableConfig): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, list[Any]]: Dictionary with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.file_paths\n    files = input_data.files\n    metadata = input_data.metadata\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \"\n        f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the UnstructuredFileConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the UnstructuredFileConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = UnstructuredFileConverterComponent(\n            connection=self.connection,\n            document_creation_mode=self.document_creation_mode,\n            strategy=self.strategy,\n            unstructured_kwargs=self.unstructured_kwargs,\n            extract_image_block_types_enabled=self.extract_image_block_types_enabled,\n            extract_image_block_types=self.extract_image_block_types,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverterInputSchema","title":"<code>UnstructuredFileConverterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>class UnstructuredFileConverterInputSchema(BaseModel):\n    file_paths: list[str] = Field(default=None, description=\"Parameter to provide path to files.\")\n    files: list[BytesIO | bytes] = Field(default=None, description=\"Parameter to provide files.\")\n    metadata: dict | list = Field(default=None, description=\"Parameter to provide metadata.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_file_source(self):\n        \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n        if not self.file_paths and not self.files:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverterInputSchema.validate_file_source","title":"<code>validate_file_source()</code>","text":"<p>Validate that either <code>file_paths</code> or <code>files</code> is specified</p> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_file_source(self):\n    \"\"\"Validate that either `file_paths` or `files` is specified\"\"\"\n    if not self.file_paths and not self.files:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/embedders/base/","title":"Base","text":""},{"location":"dynamiq/nodes/embedders/base/#dynamiq.nodes.embedders.base.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> Source code in <code>dynamiq/nodes/embedders/base.py</code> <pre><code>class DocumentEmbedder(ConnectionNode):\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    document_embedder: BaseEmbedder | None = None\n    input_schema: ClassVar[type[DocumentEmbedderInputSchema]] = DocumentEmbedderInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def execute(self, input_data: DocumentEmbedderInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings, and returns the result.\n\n        Args:\n            input_data (DocumentEmbedderInputSchema): An instance containing the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data.documents)\n        logger.debug(f\"{self.name} executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/base/#dynamiq.nodes.embedders.base.DocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DocumentEmbedderInputSchema</code> <p>An instance containing the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/base.py</code> <pre><code>def execute(self, input_data: DocumentEmbedderInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings, and returns the result.\n\n    Args:\n        input_data (DocumentEmbedderInputSchema): An instance containing the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data.documents)\n    logger.debug(f\"{self.name} executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/base/#dynamiq.nodes.embedders.base.TextEmbedder","title":"<code>TextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> Source code in <code>dynamiq/nodes/embedders/base.py</code> <pre><code>class TextEmbedder(ConnectionNode):\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    text_embedder: BaseEmbedder | None = None\n    input_schema: ClassVar[type[TextEmbedderInputSchema]] = TextEmbedderInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def execute(self, input_data: TextEmbedderInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes text input data, computes its embeddings, and returns the result.\n\n        Args:\n            input_data (TextEmbedderInputSchema): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        raw_output = self.text_embedder.embed_text(input_data.query)\n        logger.debug(f\"{self.name}: {raw_output['meta']}\")\n        result = TextEmbeddingOutput(\n            embedding=raw_output[\"embedding\"],\n            query=input_data.query,\n        )\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/embedders/base/#dynamiq.nodes.embedders.base.TextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes text input data, computes its embeddings, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TextEmbedderInputSchema</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/base.py</code> <pre><code>def execute(self, input_data: TextEmbedderInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes text input data, computes its embeddings, and returns the result.\n\n    Args:\n        input_data (TextEmbedderInputSchema): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    raw_output = self.text_embedder.embed_text(input_data.query)\n    logger.debug(f\"{self.name}: {raw_output['meta']}\")\n    result = TextEmbeddingOutput(\n        embedding=raw_output[\"embedding\"],\n        query=input_data.query,\n    )\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/embedders/base/#dynamiq.nodes.embedders.base.TextEmbeddingOutput","title":"<code>TextEmbeddingOutput</code>","text":"<p>               Bases: <code>dict</code></p> <p>Dict-like output for text embedders with tracing-aware to_dict().</p> Source code in <code>dynamiq/nodes/embedders/base.py</code> <pre><code>class TextEmbeddingOutput(dict):\n    \"\"\"Dict-like output for text embedders with tracing-aware to_dict().\"\"\"\n\n    query: str\n    embedding: list[float]\n\n    def to_dict(self, for_tracing: bool = False, truncate_limit: int = TRUNCATE_EMBEDDINGS_LIMIT, **kwargs) -&gt; dict:\n        data = dict(self)\n        if for_tracing and isinstance(data.get(\"embedding\"), list):\n            if len(data[\"embedding\"]) &gt; truncate_limit:\n                data[\"embedding\"] = data[\"embedding\"][:truncate_limit]\n        return data\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder","title":"<code>BedrockDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using Bedrock models.</p> <p>This class extends ConnectionNode to create embeddings for documents using Bedrock API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>AWS | None</code> <p>The connection to the Bedrock API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>BedrockDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[AWS]</code> <p>The connection to the Bedrock API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'amazon.titan-embed-text-v1'.</p> required Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>class BedrockDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Bedrock models.\n\n    This class extends ConnectionNode to create embeddings for documents using Bedrock API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (BedrockConnection | None): The connection to the Bedrock API.\n        model (str): The model name to use for embedding.\n        document_embedder (BedrockDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[BedrockConnection]): The connection to the Bedrock API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'amazon.titan-embed-text-v1'.\n    \"\"\"\n\n    name: str = \"AmazonBedrockDocumentEmbedder\"\n    connection: BedrockConnection | None = None\n    model: str = \"amazon.titan-embed-text-v1\"\n    document_embedder: BedrockEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the BedrockDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the BedrockDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = BedrockEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the BedrockDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new BedrockConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the BedrockDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = BedrockConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the BedrockDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the BedrockDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = BedrockEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder","title":"<code>BedrockTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified Cohere models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using Bedrock API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[AWS]</code> <p>An existing connection to Bedrock API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Bedrock model for text embeddings. Defaults to 'amazon.titan-embed-text-v1'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>AWS | None</code> <p>The connection to Bedrock API.</p> <code>model</code> <code>str</code> <p>The Bedrock model identifier for text embeddings.</p> <code>text_embedder</code> <code>BedrockTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>class BedrockTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified Cohere models.\n\n    This class extends ConnectionNode to provide text embedding functionality using Bedrock API.\n\n    Args:\n        connection (Optional[BedrockConnection]): An existing connection to Bedrock API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Bedrock model for text embeddings. Defaults to\n            'amazon.titan-embed-text-v1'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (BedrockConnection | None): The connection to Bedrock API.\n        model (str): The Bedrock model identifier for text embeddings.\n        text_embedder (BedrockTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    name: str = \"BedrockTextEmbedder\"\n    connection: BedrockConnection | None = None\n    model: str = \"amazon.titan-embed-text-v1\"\n    text_embedder: BedrockEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the BedrockTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the BedrockTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = BedrockEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BedrockTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new BedrockConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the BedrockTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = BedrockConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the BedrockTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the BedrockTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = BedrockEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/","title":"Cohere","text":""},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder","title":"<code>CohereDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using Cohere models.</p> <p>This class extends ConnectionNode to create embeddings for documents using Cohere API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Cohere | None</code> <p>The connection to the Cohere API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>CohereDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Cohere]</code> <p>The connection to the Cohere API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'cohere/embed-english-v2.0'.</p> required Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>class CohereDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Cohere models.\n\n    This class extends ConnectionNode to create embeddings for documents using Cohere API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (CohereConnection | None): The connection to the Cohere API.\n        model (str): The model name to use for embedding.\n        document_embedder (CohereDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[CohereConnection]): The connection to the Cohere API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'cohere/embed-english-v2.0'.\n    \"\"\"\n\n    name: str = \"CohereDocumentEmbedder\"\n    connection: CohereConnection | None = None\n    model: str = \"cohere/embed-english-v2.0\"\n    document_embedder: CohereEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the CohereDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the CohereDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = CohereEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the CohereDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new CohereConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the CohereDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the CohereDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the CohereDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = CohereEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder","title":"<code>CohereTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified Cohere models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using litellm embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Cohere]</code> <p>An existing connection to Cohere API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Cohere model for text embeddings. Defaults to 'cohere/embed-english-v2.0'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Cohere | None</code> <p>The connection to Cohere API.</p> <code>model</code> <code>str</code> <p>The Cohere model identifier for text embeddings.</p> <code>text_embedder</code> <code>CohereTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>class CohereTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified Cohere models.\n\n    This class extends ConnectionNode to provide text embedding functionality using litellm embedding.\n\n    Args:\n        connection (Optional[CohereConnection]): An existing connection to Cohere API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Cohere model for text embeddings. Defaults to\n            'cohere/embed-english-v2.0'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (CohereConnection | None): The connection to Cohere API.\n        model (str): The Cohere model identifier for text embeddings.\n        text_embedder (CohereTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    name: str = \"CohereTextEmbedder\"\n    connection: CohereConnection | None = None\n    model: str = \"cohere/embed-english-v2.0\"\n    text_embedder: CohereEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the CohereTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the CohereTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = CohereEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the CohereTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new CohereConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the CohereTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the CohereTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the CohereTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = CohereEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/","title":"Gemini","text":""},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiDocumentEmbedder","title":"<code>GeminiDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using Gemini models.</p> <p>This class extends DocumentEmbedder to create embeddings for documents using Gemini API.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Gemini | None</code> <p>The connection to the Gemini API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>GeminiEmbedder</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Gemini]</code> <p>The connection to the Gemini API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'gemini/gemini-embedding-exp-03-07'.</p> required <code>input_type</code> <code>str</code> <p>Specifies the type of embedding task. Defaults to \"search_document\".</p> required Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>class GeminiDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Gemini models.\n\n    This class extends DocumentEmbedder to create embeddings for documents using Gemini API.\n\n    Attributes:\n        name (str): The name of the node.\n        connection (GeminiConnection | None): The connection to the Gemini API.\n        model (str): The model name to use for embedding.\n        document_embedder (GeminiEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[GeminiConnection]): The connection to the Gemini API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'gemini/gemini-embedding-exp-03-07'.\n        input_type (str): Specifies the type of embedding task. Defaults to \"search_document\".\n    \"\"\"\n\n    name: str = \"GeminiDocumentEmbedder\"\n    connection: GeminiConnection | None = None\n    model: str = \"gemini/gemini-embedding-exp-03-07\"\n    input_type: str = \"search_document\"\n    document_embedder: GeminiEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the GeminiDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new GeminiConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GeminiConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the GeminiDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = GeminiEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client, input_type=self.input_type\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the GeminiDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new GeminiConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the GeminiDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new GeminiConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GeminiConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the GeminiDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the GeminiDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = GeminiEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client, input_type=self.input_type\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiTextEmbedder","title":"<code>GeminiTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified Gemini models.</p> <p>This class extends TextEmbedder to provide text embedding functionality using Gemini API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Gemini]</code> <p>An existing connection to Gemini API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Gemini model for text embeddings. Defaults to 'gemini/gemini-embedding-exp-03-07'.</p> required <code>input_type</code> <code>str</code> <p>Specifies the type of embedding task. Defaults to \"search_query\".</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Gemini | None</code> <p>The connection to Gemini API.</p> <code>model</code> <code>str</code> <p>The Gemini model identifier for text embeddings.</p> <code>text_embedder</code> <code>GeminiEmbedder</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>class GeminiTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified Gemini models.\n\n    This class extends TextEmbedder to provide text embedding functionality using Gemini API.\n\n    Args:\n        connection (Optional[GeminiConnection]): An existing connection to Gemini API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Gemini model for text embeddings. Defaults to\n            'gemini/gemini-embedding-exp-03-07'.\n        input_type (str): Specifies the type of embedding task. Defaults to \"search_query\".\n\n    Attributes:\n        name (str): The name of the node.\n        connection (GeminiConnection | None): The connection to Gemini API.\n        model (str): The Gemini model identifier for text embeddings.\n        text_embedder (GeminiEmbedderComponent): The component for text embedding.\n    \"\"\"\n\n    name: str = \"GeminiTextEmbedder\"\n    connection: GeminiConnection | None = None\n    model: str = \"gemini/gemini-embedding-exp-03-07\"\n    input_type: str = \"search_query\"\n    text_embedder: GeminiEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the GeminiTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new GeminiConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GeminiConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the GeminiTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = GeminiEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client, input_type=self.input_type\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the GeminiTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new GeminiConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the GeminiTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new GeminiConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GeminiConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/gemini/#dynamiq.nodes.embedders.gemini.GeminiTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the GeminiTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/gemini.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the GeminiTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = GeminiEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client, input_type=self.input_type\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder","title":"<code>HuggingFaceDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using HuggingFace models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>HuggingFace | None</code> <p>The connection to the HuggingFace API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>HuggingFaceDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[HuggingFace]</code> <p>The connection to the HuggingFace API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'huggingface/microsoft/codebert-base'.</p> required Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>class HuggingFaceDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using HuggingFace models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (HuggingFaceConnection | None): The connection to the HuggingFace API.\n        model (str): The model name to use for embedding.\n        document_embedder (HuggingFaceDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[HuggingFaceConnection]): The connection to the HuggingFace API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'huggingface/microsoft/codebert-base'.\n    \"\"\"\n    name: str = \"HuggingFaceDocumentEmbedder\"\n    connection: HuggingFaceConnection | None = None\n    model: str = \"huggingface/BAAI/bge-large-zh\"\n    document_embedder: HuggingFaceEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the HuggingFaceDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the HuggingFaceDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = HuggingFaceEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the HuggingFaceDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the HuggingFaceDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the HuggingFaceDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the HuggingFaceDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = HuggingFaceEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder","title":"<code>HuggingFaceTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified HuggingFace models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using litellm embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[HuggingFace]</code> <p>An existing connection to HuggingFace's API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the HuggingFace model for text embeddings. Defaults to 'huggingface/microsoft/codebert-base'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>HuggingFace | None</code> <p>The connection to HuggingFace API.</p> <code>model</code> <code>str</code> <p>The HuggingFace model identifier for text embeddings.</p> <code>text_embedder</code> <code>HuggingFaceTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>class HuggingFaceTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified HuggingFace models.\n\n    This class extends ConnectionNode to provide text embedding functionality using litellm embedding.\n\n    Args:\n        connection (Optional[HuggingFaceConnection]): An existing connection to HuggingFace's API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the HuggingFace model for text embeddings. Defaults to\n            'huggingface/microsoft/codebert-base'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (HuggingFaceConnection | None): The connection to HuggingFace API.\n        model (str): The HuggingFace model identifier for text embeddings.\n        text_embedder (HuggingFaceTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    name: str = \"HuggingFaceTextEmbedder\"\n    connection: HuggingFaceConnection | None = None\n    model: str = \"huggingface/microsoft/codebert-base\"\n    text_embedder: HuggingFaceEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the HuggingFaceTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the HuggingFaceTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = HuggingFaceEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the HuggingFaceTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the HuggingFaceTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the HuggingFaceTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the HuggingFaceTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = HuggingFaceEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/","title":"Mistral","text":""},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder","title":"<code>MistralDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using Mistral models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Mistral | None</code> <p>The connection to the Mistral API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>MistralDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Mistral]</code> <p>The connection to the Mistral API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'mistral/mistral-embed'. only by 'text-embedding-3' and later models. Defaults to None.</p> required Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>class MistralDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Mistral models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (MistralConnection | None): The connection to the Mistral API.\n        model (str): The model name to use for embedding.\n        document_embedder (MistralDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[MistralConnection]): The connection to the Mistral API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'mistral/mistral-embed'.\n            only by 'text-embedding-3' and later models. Defaults to None.\n    \"\"\"\n\n    name: str = \"MistralDocumentEmbedder\"\n    connection: MistralConnection | None = None\n    model: str = \"mistral/mistral-embed\"\n    document_embedder: MistralEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the MistralDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the MistralDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = MistralEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the MistralDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new MistralConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the MistralDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the MistralDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the MistralDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = MistralEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder","title":"<code>MistralTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified Mistral models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using Mistral API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Mistral]</code> <p>An existing connection to Mistral API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Mistral model for text embeddings. Defaults to 'mistral/mistral-embed'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Mistral | None</code> <p>The connection to Mistral's API.</p> <code>model</code> <code>str</code> <p>The Mistral model identifier for text embeddings.</p> <code>text_embedder</code> <code>MistralTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>class MistralTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified Mistral models.\n\n    This class extends ConnectionNode to provide text embedding functionality using Mistral API.\n\n    Args:\n        connection (Optional[MistralConnection]): An existing connection to Mistral API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Mistral model for text embeddings. Defaults to\n            'mistral/mistral-embed'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (MistralConnection | None): The connection to Mistral's API.\n        model (str): The Mistral model identifier for text embeddings.\n        text_embedder (MistralTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    name: str = \"MistralTextEmbedder\"\n    connection: MistralConnection | None = None\n    model: str = \"mistral/mistral-embed\"\n    text_embedder: MistralEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the MistralTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the MistralTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = MistralEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MistralTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new MistralConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the MistralTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the MistralTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the MistralTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = MistralEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/","title":"Openai","text":""},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder","title":"<code>OpenAIDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using OpenAI's models.</p> <p>This class extends ConnectionNode to create embeddings for documents using OpenAI's API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>OpenAI | None</code> <p>The connection to the OpenAI API.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>dimensions</code> <code>int | None</code> <p>The number of dimensions for the output embeddings.</p> <code>document_embedder</code> <code>OpenAIDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[OpenAI]</code> <p>The connection to the OpenAI API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'text-embedding-3-small'.</p> required <code>dimensions</code> <code>Optional[int]</code> <p>The number of dimensions for the output embeddings. Supported only by 'text-embedding-3' and later models. Defaults to None.</p> required Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>class OpenAIDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using OpenAI's models.\n\n    This class extends ConnectionNode to create embeddings for documents using OpenAI's API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (OpenAIConnection | None): The connection to the OpenAI API.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The model name to use for embedding.\n        dimensions (int | None): The number of dimensions for the output embeddings.\n        document_embedder (OpenAIDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[OpenAIConnection]): The connection to the OpenAI API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'text-embedding-3-small'.\n        dimensions (Optional[int]): The number of dimensions for the output embeddings. Supported\n            only by 'text-embedding-3' and later models. Defaults to None.\n    \"\"\"\n\n    name: str = \"OpenAIDocumentEmbedder\"\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n    dimensions: int | None = None\n    document_embedder: OpenAIEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the OpenAIDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the OpenAIDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = OpenAIEmbedderComponent(\n                connection=self.connection,\n                model=self.model,\n                dimensions=self.dimensions,\n                client=self.client,\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the OpenAIDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the OpenAIDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the OpenAIDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the OpenAIDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = OpenAIEmbedderComponent(\n            connection=self.connection,\n            model=self.model,\n            dimensions=self.dimensions,\n            client=self.client,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder","title":"<code>OpenAITextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified OpenAI models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[OpenAI]</code> <p>An existing connection to OpenAI's API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the OpenAI model for text embeddings. Defaults to 'text-embedding-3-small'.</p> required <code>dimensions</code> <code>Optional[int]</code> <p>Desired dimensionality of output embeddings. Defaults to None, using the model's default output dimensionality.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>OpenAI | None</code> <p>The connection to OpenAI's API.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The OpenAI model identifier for text embeddings.</p> <code>dimensions</code> <code>int | None</code> <p>The desired dimensionality of output embeddings.</p> <code>text_embedder</code> <code>OpenAITextEmbedderComponent</code> <p>The component for text embedding.</p> Notes <p>The <code>dimensions</code> parameter is model-dependent and may not be supported by all models.</p> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>class OpenAITextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified OpenAI models.\n\n    This class extends ConnectionNode to provide text embedding functionality using OpenAI's API.\n\n    Args:\n        connection (Optional[OpenAIConnection]): An existing connection to OpenAI's API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the OpenAI model for text embeddings. Defaults to\n            'text-embedding-3-small'.\n        dimensions (Optional[int]): Desired dimensionality of output embeddings. Defaults to None,\n            using the model's default output dimensionality.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (OpenAIConnection | None): The connection to OpenAI's API.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The OpenAI model identifier for text embeddings.\n        dimensions (int | None): The desired dimensionality of output embeddings.\n        text_embedder (OpenAITextEmbedderComponent): The component for text embedding.\n\n    Notes:\n        The `dimensions` parameter is model-dependent and may not be supported by all models.\n    \"\"\"\n\n    name: str = \"OpenAITextEmbedder\"\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n    dimensions: int | None = None\n    text_embedder: OpenAIEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the OpenAITextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the OpenAITextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = OpenAIEmbedderComponent(\n                connection=self.connection,\n                model=self.model,\n                dimensions=self.dimensions,\n                client=self.client,\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenAITextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the OpenAITextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the OpenAITextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the OpenAITextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = OpenAIEmbedderComponent(\n            connection=self.connection,\n            model=self.model,\n            dimensions=self.dimensions,\n            client=self.client,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/","title":"Vertexai","text":""},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAIDocumentEmbedder","title":"<code>VertexAIDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Computes embeddings for documents using Vertex AI.</p> <p>This class extends DocumentEmbedder to generate document embeddings via the Vertex AI embedding API.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of this node.</p> <code>connection</code> <code>VertexAI</code> <p>The Vertex AI connection instance.</p> <code>model</code> <code>str</code> <p>The Vertex AI embedding model name.</p> <code>input_type</code> <code>str</code> <p>The embedding task type (defaults to \"search_document\").</p> <code>document_embedder</code> <code>VertexAIEmbedder</code> <p>The embedder component.</p> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>class VertexAIDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Computes embeddings for documents using Vertex AI.\n\n    This class extends DocumentEmbedder to generate document embeddings\n    via the Vertex AI embedding API.\n\n    Attributes:\n        name (str): The name of this node.\n        connection (VertexAIConnection): The Vertex AI connection instance.\n        model (str): The Vertex AI embedding model name.\n        input_type (str): The embedding task type (defaults to \"search_document\").\n        document_embedder (VertexAIEmbedderComponent): The embedder component.\n    \"\"\"\n\n    name: str = \"VertexAIDocumentEmbedder\"\n    connection: VertexAIConnection\n    model: str = \"vertex_ai/text-embedding-005\"\n    input_type: str = \"search_document\"\n    document_embedder: VertexAIEmbedderComponent | None = None\n\n    def __init__(\n        self,\n        *,\n        connection: VertexAIConnection | None = None,\n        model: str | None = None,\n        input_type: str | None = None,\n        **kwargs: Any\n    ):\n        \"\"\"\n        Initialize the document embedder.\n\n        Args:\n            connection: Optional existing Vertex AI connection.\n            model: Optional override for the embedding model.\n            input_type: Optional override for the embedding task type.\n            **kwargs: Additional keyword args for the base node.\n        \"\"\"\n        if connection is None:\n            connection = VertexAIConnection()\n        super().__init__(connection=connection, **kwargs)\n\n        if model:\n            self.model = model\n        if input_type:\n            self.input_type = input_type\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize or reuse the Vertex AI embedder component.\n\n        Args:\n            connection_manager: Optional connection manager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n\n        if self.document_embedder is None:\n            self.document_embedder = VertexAIEmbedderComponent(\n                connection=self.connection, model=self.model, input_type=self.input_type\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAIDocumentEmbedder.__init__","title":"<code>__init__(*, connection=None, model=None, input_type=None, **kwargs)</code>","text":"<p>Initialize the document embedder.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>VertexAI | None</code> <p>Optional existing Vertex AI connection.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Optional override for the embedding model.</p> <code>None</code> <code>input_type</code> <code>str | None</code> <p>Optional override for the embedding task type.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword args for the base node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    connection: VertexAIConnection | None = None,\n    model: str | None = None,\n    input_type: str | None = None,\n    **kwargs: Any\n):\n    \"\"\"\n    Initialize the document embedder.\n\n    Args:\n        connection: Optional existing Vertex AI connection.\n        model: Optional override for the embedding model.\n        input_type: Optional override for the embedding task type.\n        **kwargs: Additional keyword args for the base node.\n    \"\"\"\n    if connection is None:\n        connection = VertexAIConnection()\n    super().__init__(connection=connection, **kwargs)\n\n    if model:\n        self.model = model\n    if input_type:\n        self.input_type = input_type\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAIDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize or reuse the Vertex AI embedder component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize or reuse the Vertex AI embedder component.\n\n    Args:\n        connection_manager: Optional connection manager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n\n    if self.document_embedder is None:\n        self.document_embedder = VertexAIEmbedderComponent(\n            connection=self.connection, model=self.model, input_type=self.input_type\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAITextEmbedder","title":"<code>VertexAITextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>Computes embeddings for text strings using Vertex AI.</p> <p>This class extends TextEmbedder to generate text embeddings via the Vertex AI embedding API.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of this node.</p> <code>connection</code> <code>VertexAI</code> <p>The Vertex AI connection instance.</p> <code>model</code> <code>str</code> <p>The Vertex AI embedding model name.</p> <code>input_type</code> <code>str</code> <p>The embedding task type (defaults to \"search_query\").</p> <code>text_embedder</code> <code>VertexAIEmbedder</code> <p>The embedder component.</p> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>class VertexAITextEmbedder(TextEmbedder):\n    \"\"\"\n    Computes embeddings for text strings using Vertex AI.\n\n    This class extends TextEmbedder to generate text embeddings\n    via the Vertex AI embedding API.\n\n    Attributes:\n        name (str): The name of this node.\n        connection (VertexAIConnection): The Vertex AI connection instance.\n        model (str): The Vertex AI embedding model name.\n        input_type (str): The embedding task type (defaults to \"search_query\").\n        text_embedder (VertexAIEmbedderComponent): The embedder component.\n    \"\"\"\n\n    name: str = \"VertexAITextEmbedder\"\n    connection: VertexAIConnection\n    model: str = \"vertex_ai/text-embedding-005\"\n    input_type: str = \"search_query\"\n    text_embedder: VertexAIEmbedderComponent | None = None\n\n    def __init__(\n        self,\n        *,\n        connection: VertexAIConnection | None = None,\n        model: str | None = None,\n        input_type: str | None = None,\n        **kwargs: Any\n    ):\n        \"\"\"\n        Initialize the text embedder.\n\n        Args:\n            connection: Optional existing Vertex AI connection.\n            model: Optional override for the embedding model.\n            input_type: Optional override for the embedding task type.\n            **kwargs: Additional keyword args for the base node.\n        \"\"\"\n        if connection is None:\n            connection = VertexAIConnection()\n        super().__init__(connection=connection, **kwargs)\n\n        if model:\n            self.model = model\n        if input_type:\n            self.input_type = input_type\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize or reuse the Vertex AI embedder component.\n\n        Args:\n            connection_manager: Optional connection manager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n\n        if self.text_embedder is None:\n            self.text_embedder = VertexAIEmbedderComponent(\n                connection=self.connection, model=self.model, input_type=self.input_type\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAITextEmbedder.__init__","title":"<code>__init__(*, connection=None, model=None, input_type=None, **kwargs)</code>","text":"<p>Initialize the text embedder.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>VertexAI | None</code> <p>Optional existing Vertex AI connection.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Optional override for the embedding model.</p> <code>None</code> <code>input_type</code> <code>str | None</code> <p>Optional override for the embedding task type.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword args for the base node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    connection: VertexAIConnection | None = None,\n    model: str | None = None,\n    input_type: str | None = None,\n    **kwargs: Any\n):\n    \"\"\"\n    Initialize the text embedder.\n\n    Args:\n        connection: Optional existing Vertex AI connection.\n        model: Optional override for the embedding model.\n        input_type: Optional override for the embedding task type.\n        **kwargs: Additional keyword args for the base node.\n    \"\"\"\n    if connection is None:\n        connection = VertexAIConnection()\n    super().__init__(connection=connection, **kwargs)\n\n    if model:\n        self.model = model\n    if input_type:\n        self.input_type = input_type\n</code></pre>"},{"location":"dynamiq/nodes/embedders/vertexai/#dynamiq.nodes.embedders.vertexai.VertexAITextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize or reuse the Vertex AI embedder component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/vertexai.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize or reuse the Vertex AI embedder component.\n\n    Args:\n        connection_manager: Optional connection manager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n\n    if self.text_embedder is None:\n        self.text_embedder = VertexAIEmbedderComponent(\n            connection=self.connection, model=self.model, input_type=self.input_type\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder","title":"<code>WatsonXDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using WatsonX models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>WatsonX | None</code> <p>The connection to the WatsonX API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>WatsonXDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[WatsonX]</code> <p>The connection to the WatsonX API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.</p> required Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>class WatsonXDocumentEmbedder(DocumentEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using WatsonX models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WatsonXConnection | None): The connection to the WatsonX API.\n        model (str): The model name to use for embedding.\n        document_embedder (WatsonXDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[WatsonXConnection]): The connection to the WatsonX API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.\n    \"\"\"\n\n    name: str = \"WatsonXDocumentEmbedder\"\n    connection: WatsonXConnection | None = None\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n    document_embedder: WatsonXEmbedderComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the WatsonXDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initializes the components of the WatsonXDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = WatsonXEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the WatsonXDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the WatsonXDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the WatsonXDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initializes the components of the WatsonXDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = WatsonXEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder","title":"<code>WatsonXTextEmbedder</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A component designed to embed strings using specified WatsonX models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using WatsonX API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[WatsonX]</code> <p>An existing connection to WatsonX API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the WatsonX model for text embeddings. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>WatsonX | None</code> <p>The connection to WatsonX's API.</p> <code>model</code> <code>str</code> <p>The WatsonX model identifier for text embeddings.</p> <code>text_embedder</code> <code>WatsonXTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>class WatsonXTextEmbedder(TextEmbedder):\n    \"\"\"\n    A component designed to embed strings using specified WatsonX models.\n\n    This class extends ConnectionNode to provide text embedding functionality using WatsonX API.\n\n    Args:\n        connection (Optional[WatsonXConnection]): An existing connection to WatsonX API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the WatsonX model for text embeddings. Defaults to\n            'watsonx/ibm/slate-30m-english-rtrvr'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WatsonXConnection | None): The connection to WatsonX's API.\n        model (str): The WatsonX model identifier for text embeddings.\n        text_embedder (WatsonXTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    name: str = \"WatsonXTextEmbedder\"\n    connection: WatsonXConnection | None = None\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n    text_embedder: WatsonXEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the WatsonXTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the WatsonXTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = WatsonXEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WatsonXTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the WatsonXTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the WatsonXTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the WatsonXTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = WatsonXEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/images/edit/","title":"Edit","text":""},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.ImageEdit","title":"<code>ImageEdit</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Node for editing images using AI models.</p> <p>Takes an existing image and a text prompt to generate edited versions. Optionally accepts a mask to specify which areas to modify.</p> <p>Attributes:</p> Name Type Description <code>FILE_PREFIX</code> <code>str</code> <p>Prefix for new file names. Default to \"edited\".</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>model</code> <code>str</code> <p>The model to use for image editing (e.g., 'dall-e-2', 'gpt-image-1').</p> <code>connection</code> <code>OpenAI</code> <p>The connection to the API.</p> <code>n</code> <code>int</code> <p>Number of edited images to generate.</p> <code>size</code> <code>ImageSize | str</code> <p>Size of the output images.</p> <code>response_format</code> <code>ImageResponseFormat | str | None</code> <p>Response format (e.g., 'url', 'b64_json'). Only supported</p> Source code in <code>dynamiq/nodes/images/edit.py</code> <pre><code>class ImageEdit(ConnectionNode):\n    \"\"\"\n    Node for editing images using AI models.\n\n    Takes an existing image and a text prompt to generate edited versions.\n    Optionally accepts a mask to specify which areas to modify.\n\n    Attributes:\n        FILE_PREFIX (str): Prefix for new file names. Default to \"edited\".\n        name (str): The name of the node.\n        model (str): The model to use for image editing (e.g., 'dall-e-2', 'gpt-image-1').\n        connection (OpenAIConnection): The connection to the API.\n        n (int): Number of edited images to generate.\n        size (ImageSize | str): Size of the output images.\n        response_format (ImageResponseFormat | str | None): Response format (e.g., 'url', 'b64_json'). Only supported\n        by some models.\n    \"\"\"\n\n    FILE_PREFIX: ClassVar[str] = \"edited\"\n\n    group: Literal[NodeGroup.IMAGES] = NodeGroup.IMAGES\n    name: str = \"Image Edit\"\n    description: str = \"\"\"Edit and modify existing images with text prompt and optional masking.\n\nKey Capabilities:\n- Image editing with natural language prompts\n- Selective area editing using optional mask images\n- Multiple variations generation (set n parameter)\n- Configurable output sizes (256x256 to 1792x1024)\n- URL or base64 JSON response formats\n\nUsage Strategy:\n- Provide clear, descriptive prompts for desired edits\n- Use masks to target specific areas for modification\n- Generate multiple variations to explore different results\n\nParameter Guide:\n- prompt: Text description of desired edits (required)\n- files: Image file/files to edit, auto-injected from agent's file store\n- mask: Optional mask image to specify areas to edit\n- n: Number of edited versions to generate\n- size: Output dimensions (e.g., '1024x1024', '1792x1024')\n- response_format: 'url' or 'b64_json' output format\n\nExamples:\n- {\"prompt\": \"Add a sunset background behind the subject\", \"files\": &lt;source_image&gt;}\n- {\"prompt\": \"Change the shirt color to blue\", \"files\": &lt;source_image&gt;, \"mask\": &lt;mask_image&gt;}\n- {\"prompt\": \"Make the image more vibrant and colorful\", \"n\": 3, \"files\": &lt;source_image&gt;}\"\"\"\n    model: str = \"gpt-image-1\"\n    connection: OpenAIConnection | None = None\n    n: int | None = None\n    size: ImageSize | str = ImageSize.SIZE_1024x1024\n    response_format: ImageResponseFormat | str | None = Field(\n        default=None,\n        description=\"Response format (e.g., 'url', 'b64_json'). Only supported by some models. Will be dropped \"\n        \"if not supported.\",\n    )\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    is_files_allowed: bool = True\n    input_schema: ClassVar[type[ImageEditInputSchema]] = ImageEditInputSchema\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    _image_edit: Callable = PrivateAttr()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ImageEdit node.\n\n        If neither client nor connection is provided, a new OpenAI connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n        from litellm import image_edit\n\n        self._image_edit = image_edit\n\n    @property\n    def edit_params(self) -&gt; dict:\n        \"\"\"Get parameters for the image edit API call.\"\"\"\n        params = self.connection.conn_params.copy() if self.connection else {}\n        if self.client:\n            params[\"client\"] = self.client\n        if self.response_format is not None:\n            response_format_value = (\n                self.response_format.value\n                if isinstance(self.response_format, ImageResponseFormat)\n                else self.response_format\n            )\n            params[\"response_format\"] = response_format_value\n\n        if model_extra := getattr(self, \"model_extra\", None):\n            extra = copy.deepcopy(model_extra)\n            params.update(extra)\n\n        return params\n\n    def _prepare_image(self, image: list[io.BytesIO | bytes] | io.BytesIO | bytes) -&gt; list[io.BytesIO] | io.BytesIO:\n        \"\"\"Prepare the image(s) for the API call.\n\n        Args:\n            image: Image as list of BytesIO/bytes (from FileStore), single BytesIO, or bytes.\n\n        Returns:\n            List of BytesIO file-like objects for API submission, or single BytesIO object.\n        \"\"\"\n        if image is None:\n            raise ValueError(\"No image provided. Please upload an image file.\")\n\n        if isinstance(image, list):\n            if not image:\n                raise ValueError(\"No image files found in storage.\")\n            return [prepare_single_image(img) for img in image]\n        else:\n            return prepare_single_image(image)\n\n    def execute(self, input_data: ImageEditInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Execute the image editing.\n\n        Args:\n            input_data (ImageEditInputSchema): Input containing the image and prompt.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: Based on response_format:\n                - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects\n                - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data\n                and BytesIO file objects\n                Also includes \"model\" and \"created\" fields.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        size = self.size.value if isinstance(self.size, ImageSize) else self.size\n\n        original_filenames = []\n        raw_image = input_data.files\n        images = raw_image if isinstance(raw_image, list) else [raw_image]\n        for img in images:\n            if img_name := getattr(img, \"name\", None):\n                original_filenames.append(img_name)\n\n        image = self._prepare_image(input_data.files)\n\n        edit_kwargs = {\n            \"model\": self.model,\n            \"image\": image,\n            \"prompt\": input_data.prompt,\n            \"size\": size,\n            \"drop_params\": True,\n            **self.edit_params,\n        }\n\n        n = input_data.n or self.n\n        if n:\n            edit_kwargs[\"n\"] = n\n\n        if input_data.mask:\n            mask = self._prepare_image(input_data.mask)\n            edit_kwargs[\"mask\"] = mask\n\n        try:\n            response = self._image_edit(**edit_kwargs)\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' encountered an unexpected error during image editing. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        content = []\n        files = []\n\n        try:\n            file_idx = 0\n            for idx, img_data in enumerate(response.data):\n                original_name = original_filenames[file_idx % len(original_filenames)] if original_filenames else None\n\n                if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                    content.append(img_url)\n                    image_bytes = download_image_from_url(img_url)\n                    file = create_image_file(\n                        image_bytes, file_idx, original_name=original_name, prefix=self.FILE_PREFIX\n                    )\n                    files.append(file)\n                    file_idx += 1\n\n                elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                    image_bytes = base64.b64decode(img_b64)\n                    file = create_image_file(\n                        image_bytes, file_idx, original_name=original_name, prefix=self.FILE_PREFIX\n                    )\n                    content.append(f\"{file.name} created\")\n                    files.append(file)\n                    file_idx += 1\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' failed to process edited image. \" f\"Error: {str(e)}. Please retry the request.\",\n                recoverable=True,\n            )\n\n        logger.debug(f\"{self.name} edited image, generated {len(content)} result(s)\")\n\n        if self.is_optimized_for_agents:\n            formatted_content = \"## Edited Images\\n\\n\"\n            formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n            formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n            has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n            if has_urls:\n                for idx, url in enumerate(content):\n                    formatted_content += f\"### Edited Image {idx + 1}\\n- URL: {url}\\n\\n\"\n            else:\n                for idx, file_name in enumerate(content):\n                    formatted_content += f\"### Edited Image {idx + 1}\\n- File: {file_name}\\n\\n\"\n            formatted_content += f\"## Files Generated\\n{len(files)} edited image file(s) available.\\n\"\n\n            result = {\"content\": formatted_content}\n        else:\n            result = {\n                \"content\": content,\n                \"created\": getattr(response, \"created\", None),\n            }\n\n        result[\"files\"] = files\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.ImageEdit.edit_params","title":"<code>edit_params: dict</code>  <code>property</code>","text":"<p>Get parameters for the image edit API call.</p>"},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.ImageEdit.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ImageEdit node.</p> <p>If neither client nor connection is provided, a new OpenAI connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/images/edit.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ImageEdit node.\n\n    If neither client nor connection is provided, a new OpenAI connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n\n    from litellm import image_edit\n\n    self._image_edit = image_edit\n</code></pre>"},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.ImageEdit.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the image editing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ImageEditInputSchema</code> <p>Input containing the image and prompt.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Based on response_format: - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data and BytesIO file objects Also includes \"model\" and \"created\" fields.</p> Source code in <code>dynamiq/nodes/images/edit.py</code> <pre><code>def execute(self, input_data: ImageEditInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Execute the image editing.\n\n    Args:\n        input_data (ImageEditInputSchema): Input containing the image and prompt.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: Based on response_format:\n            - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects\n            - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data\n            and BytesIO file objects\n            Also includes \"model\" and \"created\" fields.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    size = self.size.value if isinstance(self.size, ImageSize) else self.size\n\n    original_filenames = []\n    raw_image = input_data.files\n    images = raw_image if isinstance(raw_image, list) else [raw_image]\n    for img in images:\n        if img_name := getattr(img, \"name\", None):\n            original_filenames.append(img_name)\n\n    image = self._prepare_image(input_data.files)\n\n    edit_kwargs = {\n        \"model\": self.model,\n        \"image\": image,\n        \"prompt\": input_data.prompt,\n        \"size\": size,\n        \"drop_params\": True,\n        **self.edit_params,\n    }\n\n    n = input_data.n or self.n\n    if n:\n        edit_kwargs[\"n\"] = n\n\n    if input_data.mask:\n        mask = self._prepare_image(input_data.mask)\n        edit_kwargs[\"mask\"] = mask\n\n    try:\n        response = self._image_edit(**edit_kwargs)\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' encountered an unexpected error during image editing. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    content = []\n    files = []\n\n    try:\n        file_idx = 0\n        for idx, img_data in enumerate(response.data):\n            original_name = original_filenames[file_idx % len(original_filenames)] if original_filenames else None\n\n            if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                content.append(img_url)\n                image_bytes = download_image_from_url(img_url)\n                file = create_image_file(\n                    image_bytes, file_idx, original_name=original_name, prefix=self.FILE_PREFIX\n                )\n                files.append(file)\n                file_idx += 1\n\n            elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                image_bytes = base64.b64decode(img_b64)\n                file = create_image_file(\n                    image_bytes, file_idx, original_name=original_name, prefix=self.FILE_PREFIX\n                )\n                content.append(f\"{file.name} created\")\n                files.append(file)\n                file_idx += 1\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' failed to process edited image. \" f\"Error: {str(e)}. Please retry the request.\",\n            recoverable=True,\n        )\n\n    logger.debug(f\"{self.name} edited image, generated {len(content)} result(s)\")\n\n    if self.is_optimized_for_agents:\n        formatted_content = \"## Edited Images\\n\\n\"\n        formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n        formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n        has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n        if has_urls:\n            for idx, url in enumerate(content):\n                formatted_content += f\"### Edited Image {idx + 1}\\n- URL: {url}\\n\\n\"\n        else:\n            for idx, file_name in enumerate(content):\n                formatted_content += f\"### Edited Image {idx + 1}\\n- File: {file_name}\\n\\n\"\n        formatted_content += f\"## Files Generated\\n{len(files)} edited image file(s) available.\\n\"\n\n        result = {\"content\": formatted_content}\n    else:\n        result = {\n            \"content\": content,\n            \"created\": getattr(response, \"created\", None),\n        }\n\n    result[\"files\"] = files\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.ImageEditInputSchema","title":"<code>ImageEditInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for image editing.</p> Source code in <code>dynamiq/nodes/images/edit.py</code> <pre><code>class ImageEditInputSchema(BaseModel):\n    \"\"\"Input schema for image editing.\"\"\"\n\n    prompt: str = Field(..., description=\"Text prompt describing the desired edits.\")\n    files: list[io.BytesIO | bytes] | io.BytesIO | bytes = Field(\n        ...,\n        description=\"The image(s) to edit. Can be a single image or a list of images.\",\n        json_schema_extra={\"map_from_storage\": True, \"is_accessible_to_agent\": False},\n    )\n    mask: io.BytesIO | bytes | None = Field(\n        default=None,\n        description=\"Optional mask image indicating areas to edit.\",\n    )\n    n: int | None = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/nodes/images/edit/#dynamiq.nodes.images.edit.prepare_single_image","title":"<code>prepare_single_image(img)</code>","text":"<p>Prepare the image for the API call.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>bytes | BytesIO</code> <p>Image to prepare (bytes, BytesIO).</p> required <p>Returns:</p> Type Description <code>BytesIO</code> <p>BytesIO file-like object for API submission.</p> Source code in <code>dynamiq/nodes/images/edit.py</code> <pre><code>def prepare_single_image(img: bytes | io.BytesIO) -&gt; io.BytesIO:\n    \"\"\"Prepare the image for the API call.\n\n    Args:\n        img: Image to prepare (bytes, BytesIO).\n\n    Returns:\n        BytesIO file-like object for API submission.\n    \"\"\"\n    if isinstance(img, bytes):\n        image_bytes = img\n    elif isinstance(img, io.BytesIO):\n        img.seek(0)\n        image_bytes = img.read()\n    else:\n        raise ValueError(f\"Unsupported image type: {type(img)}\")\n\n    try:\n        img_obj = Image.open(io.BytesIO(image_bytes))\n        img_obj.load()\n    except Exception as e:\n        raise ValueError(\"Invalid image data\") from e\n\n    original_format = img_obj.format\n\n    if not original_format:\n        kind = filetype.guess(image_bytes)\n        if not kind:\n            raise ValueError(\"Unable to detect image format\")\n\n        original_format = kind.extension.upper()\n        original_format = \"JPEG\" if original_format == \"JPG\" else original_format\n\n    if original_format in (\"JPEG\", \"JPG\"):\n        if img_obj.mode not in (\"RGB\", \"L\"):\n            img_obj = img_obj.convert(\"RGB\")\n    else:\n        if img_obj.mode not in (\"RGBA\", \"LA\", \"L\"):\n            img_obj = img_obj.convert(\"RGBA\")\n\n    output_bytes = io.BytesIO()\n    img_obj.save(output_bytes, format=original_format)\n    output_bytes.seek(0)\n    return output_bytes\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/","title":"Generation","text":""},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageGeneration","title":"<code>ImageGeneration</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Node for generating images using various AI models.</p> <p>Supports multiple providers including OpenAI (DALL-E), Azure, Bedrock (Stability AI), Vertex AI, and more through a unified interface.</p> <p>Attributes:</p> Name Type Description <code>FILE_PREFIX</code> <code>str</code> <p>Prefix for new file names. Default to \"generated\".</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>model</code> <code>str</code> <p>The model to use for image generation (e.g., 'dall-e-3', 'gpt-image-1', 'bedrock/stability.stable-diffusion-xl-v0').</p> <code>connection</code> <code>OpenAI | Gemini | VertexAI | AWS | AzureAI</code> <code>n</code> <code>int</code> <p>Number of images to generate.</p> <code>size</code> <code>ImageSize | str</code> <p>Size of the generated images.</p> <code>quality</code> <code>str | None</code> <p>Quality of the generated images (e.g., 'standard', 'hd'). Only supported by some models.</p> <code>response_format</code> <code>ImageResponseFormat | str | None</code> <p>Response format (e.g., 'url', 'b64_json').</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>class ImageGeneration(ConnectionNode):\n    \"\"\"\n    Node for generating images using various AI models.\n\n    Supports multiple providers including OpenAI (DALL-E), Azure, Bedrock (Stability AI),\n    Vertex AI, and more through a unified interface.\n\n    Attributes:\n        FILE_PREFIX (str): Prefix for new file names. Default to \"generated\".\n        name (str): The name of the node.\n        model (str): The model to use for image generation (e.g., 'dall-e-3', 'gpt-image-1',\n            'bedrock/stability.stable-diffusion-xl-v0').\n        connection (OpenAIConnection | GeminiConnection | VertexAIConnection | AWSConnection | AzureAIConnection):\n        The connection to the API.\n        n (int): Number of images to generate.\n        size (ImageSize | str): Size of the generated images.\n        quality (str | None): Quality of the generated images (e.g., 'standard', 'hd'). Only supported by some models.\n        response_format (ImageResponseFormat | str | None): Response format (e.g., 'url', 'b64_json').\n        Only supported by some models.\n    \"\"\"\n\n    FILE_PREFIX: ClassVar[str] = \"generated\"\n\n    group: Literal[NodeGroup.IMAGES] = NodeGroup.IMAGES\n    name: str = \"Image Generation\"\n    description: str = \"\"\"Generate images from text prompt using image generation models.\n\nKey Capabilities:\n- Text-to-image generation with natural language prompts\n- Multi-provider support (OpenAI, Azure, AWS Bedrock, Vertex AI, Gemini)\n- Multiple image generation (set n parameter)\n- Configurable sizes (256x256 to 1792x1024)\n- Quality control (standard/hd for supported models)\n- URL or base64 JSON response formats\n\nUsage Strategy:\n- Write detailed, descriptive prompts for best results\n- Specify artistic style, composition, and key elements clearly\n- Generate multiple variations to explore creative options\n- Use quality='hd' for high-detail requirements\n\nParameter Guide:\n- prompt: Detailed text description of the image to generate (required)\n- n: Number of images to create (default: 1)\n- size: Output dimensions (e.g., '1024x1024', '1024x1792', '1792x1024')\n- quality: 'standard' or 'hd' (model-dependent)\n- response_format: 'url' or 'b64_json' output format\n\nExamples:\n- {\"prompt\": \"A serene mountain landscape at sunset, photorealistic\"}\n- {\"prompt\": \"Modern minimalist logo for tech startup, blue and white\", \"n\": 3}\n- {\"prompt\": \"Abstract art with vibrant colors\", \"size\": \"1792x1024\", \"quality\": \"hd\"}\"\"\"\n    model: str = \"gpt-image-1\"\n    connection: OpenAIConnection | GeminiConnection | VertexAIConnection | AWSConnection | AzureAIConnection | None = (\n        None\n    )\n    n: int | None = None\n    size: ImageSize | str = ImageSize.SIZE_1024x1024\n    quality: str | None = Field(\n        default=None,\n        description=\"Image quality (e.g., 'standard', 'hd'). Only supported by some models. Will be dropped \"\n        \"if not supported.\",\n    )\n    response_format: ImageResponseFormat | str | None = Field(\n        default=None,\n        description=\"Response format (e.g., 'url', 'b64_json'). Only supported by some models. Will be dropped \"\n        \"if not supported.\",\n    )\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    input_schema: ClassVar[type[ImageGenerationInputSchema]] = ImageGenerationInputSchema\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    _image_generation: Callable = PrivateAttr()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ImageGeneration node.\n\n        If neither client nor connection is provided, a new OpenAI connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n        from litellm import image_generation\n\n        self._image_generation = image_generation\n\n    @property\n    def generation_params(self) -&gt; dict:\n        \"\"\"Get parameters for the image generation API call.\"\"\"\n        params = self.connection.conn_params.copy() if self.connection else {}\n        if self.client:\n            params[\"client\"] = self.client\n        if self.quality is not None:\n            params[\"quality\"] = self.quality\n        if self.response_format is not None:\n            response_format_value = (\n                self.response_format.value\n                if isinstance(self.response_format, ImageResponseFormat)\n                else self.response_format\n            )\n            params[\"response_format\"] = response_format_value\n\n        if model_extra := getattr(self, \"model_extra\", None):\n            extra = copy.deepcopy(model_extra)\n            params.update(extra)\n\n        return params\n\n    def execute(\n        self, input_data: ImageGenerationInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the image generation.\n\n        Args:\n            input_data (ImageGenerationInputSchema): Input containing the prompt.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: Based on actual response fields:\n                - If response has \"url\": {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and\n                BytesIO file objects\n                - If response has \"b64_json\": {\"content\": list[str], \"files\": list[BytesIO]} - list of created files\n                data and BytesIO file objects\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        size = self.size.value if isinstance(self.size, ImageSize) else self.size\n\n        api_params = {\n            \"model\": self.model,\n            \"prompt\": input_data.prompt,\n            \"size\": size,\n            \"drop_params\": True,\n            **self.generation_params,\n        }\n\n        n = input_data.n or self.n\n        if n:\n            api_params[\"n\"] = n\n\n        try:\n            response = self._image_generation(**api_params)\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' encountered an unexpected error during image generation. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        content = []\n        files = []\n\n        try:\n            for idx, img_data in enumerate(response.data):\n                if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                    content.append(img_url)\n                    image_bytes = download_image_from_url(img_url)\n                    file = create_image_file(image_bytes, idx, prefix=self.FILE_PREFIX)\n                    files.append(file)\n\n                elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                    image_bytes = base64.b64decode(img_b64)\n                    file = create_image_file(image_bytes, idx, prefix=self.FILE_PREFIX)\n                    content.append(f\"{file.name} created\")\n                    files.append(file)\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' failed to process generated image. \" f\"Error: {str(e)}. Please retry the request.\",\n                recoverable=True,\n            )\n\n        logger.debug(f\"{self.name} generated {len(content)} image(s)\")\n\n        if self.is_optimized_for_agents:\n            formatted_content = \"## Generated Images\\n\\n\"\n            formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n            formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n            has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n            if has_urls:\n                for idx, url in enumerate(content):\n                    formatted_content += f\"### Image {idx + 1}\\n- URL: {url}\\n\\n\"\n            else:\n                for idx, file_name in enumerate(content):\n                    formatted_content += f\"### Image {idx + 1}\\n- File: {file_name}\\n\\n\"\n            formatted_content += f\"## Files Generated\\n{len(files)} image file(s) available.\\n\"\n\n            result = {\"content\": formatted_content}\n        else:\n            result = {\n                \"content\": content,\n                \"created\": getattr(response, \"created\", None),\n            }\n\n        result[\"files\"] = files\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageGeneration.generation_params","title":"<code>generation_params: dict</code>  <code>property</code>","text":"<p>Get parameters for the image generation API call.</p>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageGeneration.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ImageGeneration node.</p> <p>If neither client nor connection is provided, a new OpenAI connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ImageGeneration node.\n\n    If neither client nor connection is provided, a new OpenAI connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n\n    from litellm import image_generation\n\n    self._image_generation = image_generation\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageGeneration.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the image generation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ImageGenerationInputSchema</code> <p>Input containing the prompt.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Based on actual response fields: - If response has \"url\": {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects - If response has \"b64_json\": {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data and BytesIO file objects</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>def execute(\n    self, input_data: ImageGenerationInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the image generation.\n\n    Args:\n        input_data (ImageGenerationInputSchema): Input containing the prompt.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: Based on actual response fields:\n            - If response has \"url\": {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and\n            BytesIO file objects\n            - If response has \"b64_json\": {\"content\": list[str], \"files\": list[BytesIO]} - list of created files\n            data and BytesIO file objects\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    size = self.size.value if isinstance(self.size, ImageSize) else self.size\n\n    api_params = {\n        \"model\": self.model,\n        \"prompt\": input_data.prompt,\n        \"size\": size,\n        \"drop_params\": True,\n        **self.generation_params,\n    }\n\n    n = input_data.n or self.n\n    if n:\n        api_params[\"n\"] = n\n\n    try:\n        response = self._image_generation(**api_params)\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' encountered an unexpected error during image generation. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    content = []\n    files = []\n\n    try:\n        for idx, img_data in enumerate(response.data):\n            if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                content.append(img_url)\n                image_bytes = download_image_from_url(img_url)\n                file = create_image_file(image_bytes, idx, prefix=self.FILE_PREFIX)\n                files.append(file)\n\n            elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                image_bytes = base64.b64decode(img_b64)\n                file = create_image_file(image_bytes, idx, prefix=self.FILE_PREFIX)\n                content.append(f\"{file.name} created\")\n                files.append(file)\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' failed to process generated image. \" f\"Error: {str(e)}. Please retry the request.\",\n            recoverable=True,\n        )\n\n    logger.debug(f\"{self.name} generated {len(content)} image(s)\")\n\n    if self.is_optimized_for_agents:\n        formatted_content = \"## Generated Images\\n\\n\"\n        formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n        formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n        has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n        if has_urls:\n            for idx, url in enumerate(content):\n                formatted_content += f\"### Image {idx + 1}\\n- URL: {url}\\n\\n\"\n        else:\n            for idx, file_name in enumerate(content):\n                formatted_content += f\"### Image {idx + 1}\\n- File: {file_name}\\n\\n\"\n        formatted_content += f\"## Files Generated\\n{len(files)} image file(s) available.\\n\"\n\n        result = {\"content\": formatted_content}\n    else:\n        result = {\n            \"content\": content,\n            \"created\": getattr(response, \"created\", None),\n        }\n\n    result[\"files\"] = files\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageGenerationInputSchema","title":"<code>ImageGenerationInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for image generation.</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>class ImageGenerationInputSchema(BaseModel):\n    \"\"\"Input schema for image generation.\"\"\"\n\n    prompt: str = Field(..., description=\"Text prompt describing the image to generate.\")\n    n: int | None = None\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageResponseFormat","title":"<code>ImageResponseFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Response format for generated images.</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>class ImageResponseFormat(str, Enum):\n    \"\"\"Response format for generated images.\"\"\"\n\n    URL = \"url\"\n    B64_JSON = \"b64_json\"\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.ImageSize","title":"<code>ImageSize</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported image sizes for generation.</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>class ImageSize(str, Enum):\n    \"\"\"Supported image sizes for generation.\"\"\"\n\n    SIZE_256x256 = \"256x256\"\n    SIZE_512x512 = \"512x512\"\n    SIZE_1024x1024 = \"1024x1024\"\n    SIZE_1024x1792 = \"1024x1792\"\n    SIZE_1792x1024 = \"1792x1024\"\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.create_image_file","title":"<code>create_image_file(image_bytes, index=0, original_name=None, prefix=None)</code>","text":"<p>Create a properly configured BytesIO file from image bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image_bytes</code> <code>bytes</code> <p>Raw image bytes.</p> required <code>index</code> <code>int</code> <p>Index for naming multiple images.</p> <code>0</code> <code>original_name</code> <code>str | None</code> <p>Optional original filename to base the new name on.            If provided, will create names like \"original_0.png\", \"original_1.png\".            If None, will use generic names like \"generated_image_0.png\" or \"image_0.png\" if prefix is None.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>Optional prefix for the filename (e.g., \"generated\", \"edited\", \"variation\").</p> <code>None</code> <p>Returns:</p> Type Description <code>BytesIO</code> <p>BytesIO object with name and content_type attributes.</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>def create_image_file(\n    image_bytes: bytes, index: int = 0, original_name: str | None = None, prefix: str | None = None\n) -&gt; io.BytesIO:\n    \"\"\"Create a properly configured BytesIO file from image bytes.\n\n    Args:\n        image_bytes: Raw image bytes.\n        index: Index for naming multiple images.\n        original_name: Optional original filename to base the new name on.\n                       If provided, will create names like \"original_0.png\", \"original_1.png\".\n                       If None, will use generic names like \"generated_image_0.png\" or \"image_0.png\" if prefix is None.\n        prefix: Optional prefix for the filename (e.g., \"generated\", \"edited\", \"variation\").\n\n    Returns:\n        BytesIO object with name and content_type attributes.\n    \"\"\"\n    image_file = io.BytesIO(image_bytes)\n\n    kind = filetype.guess(image_bytes)\n    ext = f\".{kind.extension}\" if kind else \".png\"\n    prefix_str = f\"{prefix}_\" if prefix else \"\"\n    if original_name:\n        base_name = Path(original_name).stem\n        image_file.name = f\"{prefix_str}{base_name}_{index}{ext}\"\n    else:\n        image_file.name = f\"{prefix_str}image_{index}{ext}\"\n\n    image_file.content_type = kind.mime if kind else \"image/png\"\n    return image_file\n</code></pre>"},{"location":"dynamiq/nodes/images/generation/#dynamiq.nodes.images.generation.download_image_from_url","title":"<code>download_image_from_url(url)</code>","text":"<p>Download image from URL and return bytes.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the image to download.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Image bytes.</p> Source code in <code>dynamiq/nodes/images/generation.py</code> <pre><code>def download_image_from_url(url: str) -&gt; bytes:\n    \"\"\"Download image from URL and return bytes.\n\n    Args:\n        url: URL of the image to download.\n\n    Returns:\n        Image bytes.\n    \"\"\"\n    response = requests.get(url, timeout=60.0)\n    response.raise_for_status()\n    return response.content\n</code></pre>"},{"location":"dynamiq/nodes/images/variation/","title":"Variation","text":""},{"location":"dynamiq/nodes/images/variation/#dynamiq.nodes.images.variation.ImageVariation","title":"<code>ImageVariation</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Node for creating variations of images using AI models.</p> <p>Takes an existing image and generates variations of it.</p> <p>Attributes:</p> Name Type Description <code>FILE_PREFIX</code> <code>str</code> <p>Prefix for new file names. Default to \"variation\".</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>model</code> <code>str</code> <p>The model to use for image variation (e.g., 'dall-e-2').</p> <code>connection</code> <code>OpenAI</code> <p>The connection to the API.</p> <code>n</code> <code>int</code> <p>Number of variations to generate.</p> <code>size</code> <code>ImageSize | str</code> <p>Size of the output images.</p> <code>response_format</code> <code>ImageResponseFormat | str | None</code> <p>Response format (e.g., 'url', 'b64_json'). Only supported</p> Source code in <code>dynamiq/nodes/images/variation.py</code> <pre><code>class ImageVariation(ConnectionNode):\n    \"\"\"\n    Node for creating variations of images using AI models.\n\n    Takes an existing image and generates variations of it.\n\n    Attributes:\n        FILE_PREFIX (str): Prefix for new file names. Default to \"variation\".\n        name (str): The name of the node.\n        model (str): The model to use for image variation (e.g., 'dall-e-2').\n        connection (OpenAIConnection): The connection to the API.\n        n (int): Number of variations to generate.\n        size (ImageSize | str): Size of the output images.\n        response_format (ImageResponseFormat | str | None): Response format (e.g., 'url', 'b64_json'). Only supported\n        by some models.\n    \"\"\"\n\n    FILE_PREFIX: ClassVar[str] = \"variation\"\n\n    group: Literal[NodeGroup.IMAGES] = NodeGroup.IMAGES\n    name: str = \"Image Variation\"\n    description: str = \"\"\"Create AI-generated variations of existing images while preserving core composition.\n\nKey Capabilities:\n- Generate creative variations from source images\n- Maintain overall composition and subject matter\n- Multiple variation generation (set n parameter)\n- Configurable output sizes (256x256 to 1024x1024)\n- URL or base64 JSON response formats\n\nUsage Strategy:\n- Provide a clear, high-quality source image\n- Generate multiple variations to explore creative alternatives\n- Use for style exploration or creative ideation\n\nParameter Guide:\n- n: Number of variations to generate (default: 1)\n- size: Output dimensions (e.g., '1024x1024', '512x512')\n- response_format: 'url' or 'b64_json' output format\n\nExamples:\n- {\"n\": 5} - Generate 5 different variations\"\"\"\n    model: str = \"gpt-image-1\"\n    connection: OpenAIConnection | None = None\n    n: int | None = None\n    size: ImageSize | str = ImageSize.SIZE_1024x1024\n    response_format: ImageResponseFormat | str | None = Field(\n        default=None,\n        description=\"Response format (e.g., 'url', 'b64_json'). Only supported by some models. Will be dropped \"\n        \"if not supported.\",\n    )\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    is_files_allowed: bool = True\n    input_schema: ClassVar[type[ImageVariationInputSchema]] = ImageVariationInputSchema\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    _image_variation: Callable = PrivateAttr()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Image Variation node.\n\n        If neither client nor connection is provided, a new OpenAI connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n        from litellm import image_variation\n\n        self._image_variation = image_variation\n\n    @property\n    def variation_params(self) -&gt; dict:\n        \"\"\"Get parameters for the image variation API call.\"\"\"\n        params = self.connection.conn_params.copy() if self.connection else {}\n        if self.client:\n            params[\"client\"] = self.client\n        if self.response_format is not None:\n            response_format_value = (\n                self.response_format.value\n                if isinstance(self.response_format, ImageResponseFormat)\n                else self.response_format\n            )\n            params[\"response_format\"] = response_format_value\n\n        if model_extra := getattr(self, \"model_extra\", None):\n            extra = copy.deepcopy(model_extra)\n            params.update(extra)\n\n        return params\n\n    def execute(self, input_data: ImageVariationInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Execute the image variation generation.\n\n        Args:\n            input_data (ImageVariationInputSchema): Input containing the image.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: Based on response_format:\n                - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects\n                - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data\n                and BytesIO file objects\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        size = self.size.value if isinstance(self.size, ImageSize) else self.size\n        raw_image = input_data.files\n        if isinstance(raw_image, list):\n            if not raw_image:\n                raise ValueError(\"No image provided. List is empty.\")\n            if len(raw_image) &gt; 1:\n                logger.warning(\"Multiple images provided, using first image for variation.\")\n            raw_image = raw_image[0]\n        original_filename = None\n        if img_name := getattr(raw_image, \"name\", None):\n            original_filename = img_name\n\n        image = prepare_single_image(raw_image)\n\n        api_params = {\n            \"model\": self.model,\n            \"image\": image,\n            \"size\": size,\n            \"drop_params\": True,\n            **self.variation_params,\n        }\n\n        n = input_data.n or self.n\n        if n:\n            api_params[\"n\"] = n\n        try:\n            response = self._image_variation(**api_params)\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' encountered an unexpected error during image variation generation. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        content = []\n        files = []\n\n        try:\n            for idx, img_data in enumerate(response.data):\n                if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                    content.append(img_url)\n                    image_bytes = download_image_from_url(img_url)\n                    file = create_image_file(image_bytes, idx, original_name=original_filename, prefix=self.FILE_PREFIX)\n                    files.append(file)\n\n                elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                    image_bytes = base64.b64decode(img_b64)\n                    file = create_image_file(image_bytes, idx, original_name=original_filename, prefix=self.FILE_PREFIX)\n                    content.append(f\"{file.name} created\")\n                    files.append(file)\n        except Exception as e:\n            logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Node '{self.name}' failed to process image variation. \" f\"Error: {str(e)}. Please retry the request.\",\n                recoverable=True,\n            )\n\n        logger.debug(f\"{self.name} generated {len(content)} variation(s)\")\n\n        if self.is_optimized_for_agents:\n            formatted_content = \"## Image Variations\\n\\n\"\n            formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n            formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n            has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n            if has_urls:\n                for idx, url in enumerate(content):\n                    formatted_content += f\"### Variation {idx + 1}\\n- URL: {url}\\n\\n\"\n            else:\n                for idx, file_name in enumerate(content):\n                    formatted_content += f\"### Variation {idx + 1}\\n- File: {file_name}\\n\\n\"\n            formatted_content += f\"## Files Generated\\n{len(files)} variation file(s) available.\\n\"\n\n            result = {\"content\": formatted_content}\n        else:\n            result = {\n                \"content\": content,\n                \"created\": getattr(response, \"created\", None),\n            }\n\n        result[\"files\"] = files\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/images/variation/#dynamiq.nodes.images.variation.ImageVariation.variation_params","title":"<code>variation_params: dict</code>  <code>property</code>","text":"<p>Get parameters for the image variation API call.</p>"},{"location":"dynamiq/nodes/images/variation/#dynamiq.nodes.images.variation.ImageVariation.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Image Variation node.</p> <p>If neither client nor connection is provided, a new OpenAI connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/images/variation.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Image Variation node.\n\n    If neither client nor connection is provided, a new OpenAI connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n\n    from litellm import image_variation\n\n    self._image_variation = image_variation\n</code></pre>"},{"location":"dynamiq/nodes/images/variation/#dynamiq.nodes.images.variation.ImageVariation.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the image variation generation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ImageVariationInputSchema</code> <p>Input containing the image.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Based on response_format: - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data and BytesIO file objects</p> Source code in <code>dynamiq/nodes/images/variation.py</code> <pre><code>def execute(self, input_data: ImageVariationInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Execute the image variation generation.\n\n    Args:\n        input_data (ImageVariationInputSchema): Input containing the image.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: Based on response_format:\n            - URL: {\"content\": list[str], \"files\": list[BytesIO]} - list of image URLs and BytesIO file objects\n            - B64_JSON: {\"content\": list[str], \"files\": list[BytesIO]} - list of created files data\n            and BytesIO file objects\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    size = self.size.value if isinstance(self.size, ImageSize) else self.size\n    raw_image = input_data.files\n    if isinstance(raw_image, list):\n        if not raw_image:\n            raise ValueError(\"No image provided. List is empty.\")\n        if len(raw_image) &gt; 1:\n            logger.warning(\"Multiple images provided, using first image for variation.\")\n        raw_image = raw_image[0]\n    original_filename = None\n    if img_name := getattr(raw_image, \"name\", None):\n        original_filename = img_name\n\n    image = prepare_single_image(raw_image)\n\n    api_params = {\n        \"model\": self.model,\n        \"image\": image,\n        \"size\": size,\n        \"drop_params\": True,\n        **self.variation_params,\n    }\n\n    n = input_data.n or self.n\n    if n:\n        api_params[\"n\"] = n\n    try:\n        response = self._image_variation(**api_params)\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' encountered an unexpected error during image variation generation. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    content = []\n    files = []\n\n    try:\n        for idx, img_data in enumerate(response.data):\n            if img_url := getattr(img_data, ImageResponseFormat.URL.value, None):\n                content.append(img_url)\n                image_bytes = download_image_from_url(img_url)\n                file = create_image_file(image_bytes, idx, original_name=original_filename, prefix=self.FILE_PREFIX)\n                files.append(file)\n\n            elif img_b64 := getattr(img_data, ImageResponseFormat.B64_JSON.value, None):\n                image_bytes = base64.b64decode(img_b64)\n                file = create_image_file(image_bytes, idx, original_name=original_filename, prefix=self.FILE_PREFIX)\n                content.append(f\"{file.name} created\")\n                files.append(file)\n    except Exception as e:\n        logger.error(f\"Node {self.name} - {self.id}: failed to process response. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Node '{self.name}' failed to process image variation. \" f\"Error: {str(e)}. Please retry the request.\",\n            recoverable=True,\n        )\n\n    logger.debug(f\"{self.name} generated {len(content)} variation(s)\")\n\n    if self.is_optimized_for_agents:\n        formatted_content = \"## Image Variations\\n\\n\"\n        formatted_content += f\"Created: {getattr(response, 'created', 'N/A')}\\n\"\n        formatted_content += f\"Count: {len(content)}\\n\\n\"\n\n        has_urls = content and isinstance(content[0], str) and content[0].startswith(\"http\")\n        if has_urls:\n            for idx, url in enumerate(content):\n                formatted_content += f\"### Variation {idx + 1}\\n- URL: {url}\\n\\n\"\n        else:\n            for idx, file_name in enumerate(content):\n                formatted_content += f\"### Variation {idx + 1}\\n- File: {file_name}\\n\\n\"\n        formatted_content += f\"## Files Generated\\n{len(files)} variation file(s) available.\\n\"\n\n        result = {\"content\": formatted_content}\n    else:\n        result = {\n            \"content\": content,\n            \"created\": getattr(response, \"created\", None),\n        }\n\n    result[\"files\"] = files\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/images/variation/#dynamiq.nodes.images.variation.ImageVariationInputSchema","title":"<code>ImageVariationInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for image variation.</p> Source code in <code>dynamiq/nodes/images/variation.py</code> <pre><code>class ImageVariationInputSchema(BaseModel):\n    \"\"\"Input schema for image variation.\"\"\"\n\n    files: list[io.BytesIO | bytes] | io.BytesIO | bytes = Field(\n        ...,\n        description=\"The image to create variations of.\",\n        json_schema_extra={\"map_from_storage\": True, \"is_accessible_to_agent\": False},\n    )\n    n: int | None = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/nodes/llms/ai21/","title":"Ai21","text":""},{"location":"dynamiq/nodes/llms/ai21/#dynamiq.nodes.llms.ai21.AI21","title":"<code>AI21</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>AI21 LLM node.</p> <p>This class provides an implementation for the AI21 Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AI21</code> <p>The connection to use for the AI21 LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the AI21 model name.</p> Source code in <code>dynamiq/nodes/llms/ai21.py</code> <pre><code>class AI21(BaseLLM):\n    \"\"\"AI21 LLM node.\n\n    This class provides an implementation for the AI21 Language Model node.\n\n    Attributes:\n        connection (AI21Connection): The connection to use for the AI21 LLM.\n        MODEL_PREFIX (str): The prefix for the AI21 model name.\n    \"\"\"\n    connection: AI21Connection\n    MODEL_PREFIX = \"ai21/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AI21 LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AI21Connection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/ai21/#dynamiq.nodes.llms.ai21.AI21.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AI21 LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/ai21.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AI21 LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AI21Connection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anthropic/","title":"Anthropic","text":""},{"location":"dynamiq/nodes/llms/anthropic/#dynamiq.nodes.llms.anthropic.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Anthropic LLM node.</p> <p>This class provides an implementation for the Anthropic Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Anthropic | None</code> <p>The connection to use for the Anthropic LLM.</p> Source code in <code>dynamiq/nodes/llms/anthropic.py</code> <pre><code>class Anthropic(BaseLLM):\n    \"\"\"Anthropic LLM node.\n\n    This class provides an implementation for the Anthropic Language Model node.\n\n    Attributes:\n        connection (AnthropicConnection | None): The connection to use for the Anthropic LLM.\n    \"\"\"\n    connection: AnthropicConnection | None = None\n    MODEL_PREFIX = \"anthropic/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Anthropic LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AnthropicConnection()\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def _convert_non_image_to_file_content(messages: list[dict]) -&gt; list[dict]:\n        for message in messages:\n            content = message.get(\"content\")\n            if not isinstance(content, list):\n                continue\n\n            new_content = []\n            for item in content:\n                if isinstance(item, dict) and item.get(\"type\") == VisionMessageType.IMAGE_URL and \"image_url\" in item:\n                    url = item[\"image_url\"].get(\"url\", \"\")\n                    if url.startswith(\"data:\") and not url.startswith(\"data:image/\"):\n                        logger.debug(\"Anthropic: converting non-image image_url to file content format\")\n                        new_content.append(\n                            {\n                                \"type\": VisionMessageType.FILE,\n                                \"file\": {\"file_data\": url},\n                            }\n                        )\n                    else:\n                        new_content.append(item)\n                else:\n                    new_content.append(item)\n\n            message[\"content\"] = new_content\n\n        return messages\n\n    def get_messages(self, prompt, input_data) -&gt; list[dict]:\n        \"\"\"\n        Format messages and convert non-image files to Anthropic file content format.\n        \"\"\"\n        messages = super().get_messages(prompt, input_data)\n        return self._convert_non_image_to_file_content(messages)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anthropic/#dynamiq.nodes.llms.anthropic.Anthropic.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Anthropic LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/anthropic.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Anthropic LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AnthropicConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anthropic/#dynamiq.nodes.llms.anthropic.Anthropic.get_messages","title":"<code>get_messages(prompt, input_data)</code>","text":"<p>Format messages and convert non-image files to Anthropic file content format.</p> Source code in <code>dynamiq/nodes/llms/anthropic.py</code> <pre><code>def get_messages(self, prompt, input_data) -&gt; list[dict]:\n    \"\"\"\n    Format messages and convert non-image files to Anthropic file content format.\n    \"\"\"\n    messages = super().get_messages(prompt, input_data)\n    return self._convert_non_image_to_file_content(messages)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anyscale/","title":"Anyscale","text":""},{"location":"dynamiq/nodes/llms/anyscale/#dynamiq.nodes.llms.anyscale.Anyscale","title":"<code>Anyscale</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Anyscale LLM node.</p> <p>This class provides an implementation for the Anyscale Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Anyscale | None</code> <p>The connection to use for the Anyscale LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Anyscale model name.</p> Source code in <code>dynamiq/nodes/llms/anyscale.py</code> <pre><code>class Anyscale(BaseLLM):\n    \"\"\"Anyscale LLM node.\n\n    This class provides an implementation for the Anyscale Language Model node.\n\n    Attributes:\n        connection (AnyscaleConnection | None): The connection to use for the Anyscale LLM.\n        MODEL_PREFIX (str): The prefix for the Anyscale model name.\n    \"\"\"\n    connection: AnyscaleConnection | None = None\n    MODEL_PREFIX = \"anyscale/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Anyscale LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AnyscaleConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anyscale/#dynamiq.nodes.llms.anyscale.Anyscale.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Anyscale LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/anyscale.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Anyscale LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AnyscaleConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/azureai/","title":"Azureai","text":""},{"location":"dynamiq/nodes/llms/azureai/#dynamiq.nodes.llms.azureai.AzureAI","title":"<code>AzureAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>AzureAI LLM node.</p> <p>This class provides an implementation for the AzureAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AzureAI | None</code> <p>The connection to use for the AzureAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the AzureAI model name.</p> Source code in <code>dynamiq/nodes/llms/azureai.py</code> <pre><code>class AzureAI(BaseLLM):\n    \"\"\"AzureAI LLM node.\n\n    This class provides an implementation for the AzureAI Language Model node.\n\n    Attributes:\n        connection (AzureAIConnection | None): The connection to use for the AzureAI LLM.\n        MODEL_PREFIX (str): The prefix for the AzureAI model name.\n    \"\"\"\n    connection: AzureAIConnection | None = None\n    MODEL_PREFIX = \"azure/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AzureAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AzureAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/azureai/#dynamiq.nodes.llms.azureai.AzureAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AzureAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/azureai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AzureAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AzureAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/","title":"Base","text":""},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM","title":"<code>BaseLLM</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Base class for all LLM nodes.</p> <p>Attributes:</p> Name Type Description <code>MODEL_PREFIX</code> <code>ClassVar[str | None]</code> <p>Optional model prefix.</p> <code>name</code> <code>str | None</code> <p>Name of the LLM node. Defaults to \"LLM\".</p> <code>model</code> <code>str</code> <p>Model to use for the LLM.</p> <code>prompt</code> <code>Prompt | None</code> <p>Prompt to use for the LLM.</p> <code>connection</code> <code>BaseConnection</code> <p>Connection to use for the LLM.</p> <code>group</code> <code>Literal[LLMS]</code> <p>Group for the node. Defaults to NodeGroup.LLMS.</p> <code>temperature</code> <code>float | None</code> <p>Temperature for the LLM.</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the LLM.</p> <code>stop</code> <code>list[str]</code> <p>List of tokens to stop at for the LLM.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling config. Defaults to ErrorHandling(timeout_seconds=600).</p> <code>top_p</code> <code>float | None</code> <p>Value to consider tokens with top_p probability.</p> <code>seed</code> <code>int | None</code> <p>Seed for generating the same result for repeated requests.</p> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on their existence in the text.</p> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on their frequency in the text.</p> <code>tool_choice</code> <code>str | None</code> <p>Value to control which function is called by the model.</p> <code>thinking_enabled</code> <code>bool</code> <p>Enables advanced reasoning if set to True.</p> <code>budget_tokens</code> <code>int</code> <p>Maximum number of tokens allocated for thinking.</p> <code>response_format</code> <code>dict[str, Any]</code> <p>JSON schema that specifies the structure of the llm's output.</p> <code>tools</code> <code>list[Tool]</code> <p>List of tools that llm can call.</p> <code>fallback</code> <code>FallbackConfig</code> <p>Configuration for fallback behavior.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>class BaseLLM(ConnectionNode):\n    \"\"\"Base class for all LLM nodes.\n\n    Attributes:\n        MODEL_PREFIX (ClassVar[str | None]): Optional model prefix.\n        name (str | None): Name of the LLM node. Defaults to \"LLM\".\n        model (str): Model to use for the LLM.\n        prompt (Prompt | None): Prompt to use for the LLM.\n        connection (BaseConnection): Connection to use for the LLM.\n        group (Literal[NodeGroup.LLMS]): Group for the node. Defaults to NodeGroup.LLMS.\n        temperature (float | None): Temperature for the LLM.\n        max_tokens (int | None): Maximum number of tokens for the LLM.\n        stop (list[str]): List of tokens to stop at for the LLM.\n        error_handling (ErrorHandling): Error handling config. Defaults to ErrorHandling(timeout_seconds=600).\n        top_p (float | None): Value to consider tokens with top_p probability.\n        seed (int | None): Seed for generating the same result for repeated requests.\n        presence_penalty (float | None): Penalize new tokens based on their existence in the text.\n        frequency_penalty (float | None): Penalize new tokens based on their frequency in the text.\n        tool_choice (str | None): Value to control which function is called by the model.\n        thinking_enabled (bool): Enables advanced reasoning if set to True.\n        budget_tokens (int): Maximum number of tokens allocated for thinking.\n        response_format (dict[str, Any]): JSON schema that specifies the structure of the llm's output.\n        tools (list[Tool]): List of tools that llm can call.\n        fallback (FallbackConfig): Configuration for fallback behavior.\n    \"\"\"\n\n    MODEL_PREFIX: ClassVar[str | None] = None\n    name: str | None = \"LLM\"\n    model: str\n    prompt: Prompt | None = None\n    connection: BaseConnection\n    group: Literal[NodeGroup.LLMS] = NodeGroup.LLMS\n    temperature: float | None = None\n    max_tokens: int | None = None\n    stop: list[str] | None = None\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    top_p: float | None = None\n    seed: int | None = None\n    presence_penalty: float | None = None\n    frequency_penalty: float | None = None\n    tool_choice: str | None = None\n    thinking_enabled: bool | None = None\n    budget_tokens: int = 1024\n    response_format: dict[str, Any] | None = None\n    tools: list[Tool | dict] | None = None\n    input_schema: ClassVar[type[BaseLLMInputSchema]] = BaseLLMInputSchema\n    inference_mode: InferenceMode = Field(\n        default=InferenceMode.DEFAULT,\n        deprecated=\"Please use `tools` and `response_format` parameters \"\n        \"for selecting between function calling and structured output.\",\n    )\n    schema_: dict[str, Any] | type[BaseModel] | None = Field(\n        None,\n        description=\"Schema for structured output or function calling.\",\n        alias=\"schema\",\n        deprecated=\"Please use `tools` and `response_format` parameters \"\n        \"for function calling and structured output respectively.\",\n    )\n    fallback: FallbackConfig | None = Field(\n        default=None,\n        description=\"Configuration for fallback behavior including the fallback LLM.\",\n    )\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    _completion: Callable = PrivateAttr()\n    _stream_chunk_builder: Callable = PrivateAttr()\n    _is_fallback_run: bool = PrivateAttr(default=False)\n    _json_schema_fields: ClassVar[list[str]] = [\"model\", \"temperature\", \"max_tokens\", \"prompt\"]\n\n    @classmethod\n    def _generate_json_schema(cls, models: list[str], **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Generates full json schema of BaseLLM Node.\n\n        This schema is designed for compatibility with the WorkflowYamlParser,\n        containing enough partial information to instantiate an BaseLLM.\n        Parameters name to be included in the schema are either defined in the _json_schema_fields class variable or\n        passed via the fields parameter.\n\n        It generates a schema using provided models.\n\n        Args:\n            models (list[str]): List of available models.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Generated json schema.\n        \"\"\"\n        schema = super()._generate_json_schema(**kwargs)\n        schema[\"properties\"][\"model\"][\"enum\"] = models\n        return schema\n\n    @field_validator(\"model\")\n    @classmethod\n    def set_model(cls, value: str | None) -&gt; str:\n        \"\"\"Set the model with the appropriate prefix.\n\n        Args:\n            value (str | None): The model value.\n\n        Returns:\n            str: The model value with the prefix.\n        \"\"\"\n        if cls.MODEL_PREFIX is not None and not value.startswith(cls.MODEL_PREFIX):\n            value = f\"{cls.MODEL_PREFIX}{value}\"\n        return value\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the BaseLLM instance.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        # Save a bit of loading time as litellm is slow\n        from litellm import completion, stream_chunk_builder\n\n        # Avoid the same imports multiple times and for future usage in execute\n        self._completion = completion\n        self._stream_chunk_builder = stream_chunk_builder\n\n    def init_components(self, connection_manager=None):\n        \"\"\"Initialize components including fallback LLM if configured.\n\n        Args:\n            connection_manager: The connection manager for initializing connections.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.fallback and self.fallback.llm and self.fallback.llm.is_postponed_component_init:\n            self.fallback.llm.init_components(connection_manager)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        \"\"\"Exclude fallback configuration during serialization.\"\"\"\n        return super().to_dict_exclude_params | {\"fallback\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert to dictionary representation.\"\"\"\n        data = super().to_dict(**kwargs)\n        if self.fallback:\n            data[\"fallback\"] = self.fallback.model_dump(exclude={\"llm\": True})\n            data[\"fallback\"][\"llm\"] = self.fallback.llm.to_dict(**kwargs) if self.fallback.llm else None\n        if self._is_fallback_run:\n            data[\"is_fallback\"] = True\n        return data\n\n    def reset_run_state(self):\n        \"\"\"Reset the run state of the LLM.\"\"\"\n        self._is_fallback_run = False\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n        return {\"instance_prompt\": self.prompt}\n\n    def get_token_limit(self) -&gt; int:\n        \"\"\"Returns token limits of a llm.\n\n        Returns:\n            int: Number of tokens.\n        \"\"\"\n        return get_max_tokens(self.model)\n\n    @property\n    def is_vision_supported(self) -&gt; bool:\n        \"\"\"Check if the LLM supports vision/image processing.\"\"\"\n        return supports_vision(self.model)\n\n    @property\n    def is_pdf_input_supported(self) -&gt; bool:\n        \"\"\"Check if the LLM supports PDF input.\"\"\"\n        return supports_pdf_input(self.model)\n\n    def get_messages(\n        self,\n        prompt,\n        input_data,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Format and filter message parameters based on provider requirements.\n        Override this in provider-specific subclasses.\n        \"\"\"\n        messages = prompt.format_messages(**dict(input_data))\n        return messages\n\n    @classmethod\n    def get_usage_data(\n        cls,\n        model: str,\n        completion: \"ModelResponse\",\n    ) -&gt; BaseLLMUsageData:\n        \"\"\"Get usage data for the LLM.\n\n        This method generates usage data for the LLM based on the provided messages.\n\n        Args:\n            model (str): The model to use for generating the usage data.\n            completion (ModelResponse): The completion response from the LLM.\n\n        Returns:\n            BaseLLMUsageData: A model containing the usage data for the LLM.\n        \"\"\"\n        from litellm import cost_per_token\n\n        usage = completion.model_extra[\"usage\"]\n        prompt_tokens = usage.prompt_tokens\n        completion_tokens = usage.completion_tokens\n        total_tokens = usage.total_tokens\n\n        try:\n            prompt_tokens_cost_usd, completion_tokens_cost_usd = cost_per_token(\n                model=model, prompt_tokens=prompt_tokens, completion_tokens=completion_tokens\n            )\n            total_tokens_cost_usd = prompt_tokens_cost_usd + completion_tokens_cost_usd\n        except Exception:\n            prompt_tokens_cost_usd, completion_tokens_cost_usd, total_tokens_cost_usd = None, None, None\n\n        return BaseLLMUsageData(\n            prompt_tokens=prompt_tokens,\n            prompt_tokens_cost_usd=prompt_tokens_cost_usd,\n            completion_tokens=completion_tokens,\n            completion_tokens_cost_usd=completion_tokens_cost_usd,\n            total_tokens=total_tokens,\n            total_tokens_cost_usd=total_tokens_cost_usd,\n        )\n\n    def _handle_completion_response(\n        self,\n        response: Union[\"ModelResponse\", \"CustomStreamWrapper\"],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"Handle completion response.\n\n        Args:\n            response (ModelResponse | CustomStreamWrapper): The response from the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls if present.\n        \"\"\"\n        content = response.choices[0].message.content\n        result = {\"content\": content}\n        if tool_calls := response.choices[0].message.tool_calls:\n            tool_calls_parsed = {}\n            for tc in tool_calls:\n                call = tc.model_dump()\n                call[\"function\"][\"arguments\"] = json.loads(call[\"function\"][\"arguments\"])\n                tool_calls_parsed[call[\"function\"][\"name\"]] = call\n            result[\"tool_calls\"] = tool_calls_parsed\n\n        usage_data = self.get_usage_data(model=self.model, completion=response).model_dump()\n        self.run_on_node_execute_run(callbacks=config.callbacks, usage_data=usage_data, **kwargs)\n\n        return result\n\n    def _handle_streaming_completion_response(\n        self,\n        response: Union[\"ModelResponse\", \"CustomStreamWrapper\"],\n        messages: list[dict],\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"Handle streaming completion response.\n\n        Args:\n            response (ModelResponse | CustomStreamWrapper): The response from the LLM.\n            messages (list[dict]): The messages used for the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls.\n        \"\"\"\n        chunks = []\n        for chunk in response:\n            chunks.append(chunk)\n\n            self.run_on_node_execute_stream(\n                config.callbacks,\n                chunk.model_dump(),\n                **kwargs,\n            )\n\n        full_response = self._stream_chunk_builder(chunks=chunks, messages=messages)\n        return self._handle_completion_response(response=full_response, config=config, **kwargs)\n\n    def _get_response_format_and_tools(\n        self,\n        prompt: Prompt | None = None,\n        tools: list[Tool | dict] | None = None,\n        response_format: dict[str, Any] | None = None,\n    ) -&gt; tuple[dict[str, Any] | None, dict[str, Any] | None]:\n        \"\"\"Get response format and tools\n        Args:\n            input_data (BaseLLMInputSchema): The input data for the LLM.\n            prompt (Prompt | None): The prompt to use.\n            tools (list[Tool] | None): The tools to use.\n            response_format (dict[str, Any] | None): The response format to use.\n        Returns:\n            tuple[dict[str, Any] | None, dict[str, Any] | None]: Response format and tools.\n        Raises:\n            ValueError: If schema is None when using STRUCTURED_OUTPUT or FUNCTION_CALLING modes.\n        \"\"\"\n        response_format = response_format or self.response_format or prompt.response_format\n        tools = tools or self.tools or prompt.tools\n\n        # Suppress DeprecationWarning if deprecated parameters are not set\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n            use_inference_mode = (not response_format or not tools) and self.inference_mode != InferenceMode.DEFAULT\n\n        if use_inference_mode:\n            schema = self.schema_\n            match self.inference_mode:\n                case InferenceMode.STRUCTURED_OUTPUT:\n                    if schema is None:\n                        raise ValueError(\"Schema must be provided when using STRUCTURED_OUTPUT inference mode\")\n                    response_format = response_format or schema\n                case InferenceMode.FUNCTION_CALLING:\n                    if schema is None:\n                        raise ValueError(\"Schema must be provided when using FUNCTION_CALLING inference mode\")\n                    tools = tools or schema\n\n        if tools:\n            tools = [tool.model_dump() if isinstance(tool, Tool) else tool for tool in tools]\n\n        return response_format, tools\n\n    def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Updates or modifies the parameters for the completion method.\n\n        This method can be overridden by subclasses to customize the parameters\n        passed to the completion method. By default, it enables usage information\n        in streaming mode if streaming is enabled and include_usage is set.\n        Args:\n            params (dict[str, Any]): The parameters to be updated.\n\n        Returns:\n            dict[str, Any]: The updated parameters.\n        \"\"\"\n        if self.streaming and self.streaming.enabled and self.streaming.include_usage and params.get(\"stream\", False):\n            params.setdefault(\"stream_options\", {})\n            params[\"stream_options\"][\"include_usage\"] = True\n        return params\n\n    def execute(\n        self,\n        input_data: BaseLLMInputSchema,\n        config: RunnableConfig = None,\n        prompt: Prompt | None = None,\n        tools: list[Tool | dict] | None = None,\n        response_format: dict[str, Any] | None = None,\n        **kwargs,\n    ):\n        \"\"\"Execute the LLM node.\n\n        This method processes the input data, formats the prompt, and generates a response using\n        the configured LLM.\n\n        Args:\n            input_data (BaseLLMInputSchema): The input data for the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            prompt (Prompt, optional): The prompt to use for this execution. Defaults to None.\n            tools (list[Tool|dict]): List of tools that llm can call.\n            response_format (dict[str, Any]): JSON schema that specifies the structure of the llm's output\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls.\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        prompt = prompt or self.prompt or Prompt(messages=[], tools=None, response_format=None)\n        messages = self.get_messages(prompt, input_data)\n        self.run_on_node_execute_run(callbacks=config.callbacks, prompt_messages=messages, **kwargs)\n\n        extra = copy.deepcopy(self.__pydantic_extra__)\n        params = self.connection.conn_params.copy()\n        if self.client and not isinstance(self.connection, HttpApiKey):\n            params.update({\"client\": self.client})\n        if self.thinking_enabled:\n            params.update({\"thinking\": {\"type\": \"enabled\", \"budget_tokens\": self.budget_tokens}})\n        if extra:\n            params.update(extra)\n\n        response_format, tools = self._get_response_format_and_tools(\n            prompt=prompt,\n            tools=tools,\n            response_format=response_format,\n        )\n        # Check if a streaming callback is available in the config and enable streaming only if it is\n        # This is to avoid unnecessary streaming to reduce CPU usage\n        is_streaming_callback_available = any(\n            isinstance(callback, BaseStreamingCallbackHandler) for callback in config.callbacks\n        )\n        common_params: dict[str, Any] = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"stream\": self.streaming.enabled and is_streaming_callback_available,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"tools\": tools,\n            \"tool_choice\": self.tool_choice,\n            \"stop\": self.stop if self.stop else None,\n            \"top_p\": self.top_p,\n            \"seed\": self.seed,\n            \"presence_penalty\": self.presence_penalty,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"response_format\": response_format,\n            \"drop_params\": True,\n            **params,\n        }\n\n        common_params = self.update_completion_params(common_params)\n\n        response = self._completion(**common_params)\n        handle_completion = (\n            self._handle_streaming_completion_response\n            if self.streaming.enabled and is_streaming_callback_available\n            else self._handle_completion_response\n        )\n\n        return handle_completion(\n            response=response, messages=messages, config=config, input_data=dict(input_data), **kwargs\n        )\n\n    def _is_rate_limit_error(self, exception_type: type[Exception], error_str: str) -&gt; bool:\n        \"\"\"Check if the error is a rate limit error.\n\n        Args:\n            exception_type: The type of exception.\n            error_str: Lowercase error message string.\n\n        Returns:\n            bool: True if it's a rate limit error.\n        \"\"\"\n        if issubclass(exception_type, (RateLimitError, BudgetExceededError)):\n            return True\n        return any(indicator in error_str for indicator in LLM_RATE_LIMIT_ERROR_INDICATORS)\n\n    def _is_connection_error(self, exception_type: type[Exception], error_str: str) -&gt; bool:\n        \"\"\"Check if the error is a connection error.\n\n        Args:\n            exception_type: The type of exception.\n            error_str: Lowercase error message string.\n\n        Returns:\n            bool: True if it's a connection error.\n        \"\"\"\n        if issubclass(exception_type, (APIConnectionError, Timeout, ServiceUnavailableError, InternalServerError)):\n            return True\n        if issubclass(exception_type, (ConnectionError, TimeoutError, OSError)):\n            return True\n        return any(indicator in error_str for indicator in LLM_CONNECTION_ERROR_INDICATORS)\n\n    def _should_trigger_fallback(self, exception_type: type[Exception], exception_message: str | None = None) -&gt; bool:\n        \"\"\"Determine if exception should trigger fallback to secondary LLM.\n\n        Args:\n            exception_type: The type of exception that caused the primary LLM to fail.\n            exception_message: The exception message string for string-based detection.\n\n        Returns:\n            bool: True if fallback should be triggered, False otherwise.\n        \"\"\"\n        if not self.fallback or not self.fallback.enabled or not self.fallback.llm:\n            return False\n\n        triggers = set(self.fallback.triggers)\n        if FallbackTrigger.ANY in triggers:\n            return True\n\n        error_str = (exception_message or \"\").lower()\n\n        if FallbackTrigger.RATE_LIMIT in triggers and self._is_rate_limit_error(exception_type, error_str):\n            return True\n        if FallbackTrigger.CONNECTION in triggers and self._is_connection_error(exception_type, error_str):\n            return True\n\n        return False\n\n    def run_sync(\n        self,\n        input_data: dict,\n        config: RunnableConfig = None,\n        depends_result: dict = None,\n        **kwargs,\n    ) -&gt; RunnableResult:\n        \"\"\"Run the LLM with fallback support.\n\n        If the primary LLM fails and a fallback is configured, the primary failure\n        is traced first, then the fallback LLM is executed separately.\n\n        The fallback receives the same transformed input that the primary received,\n        and the primary's output_transformer is applied to the fallback's output.\n\n        Args:\n            input_data: Input data for the LLM.\n            config: Configuration for the run.\n            depends_result: Results of dependent nodes.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the LLM execution.\n        \"\"\"\n        result = super().run_sync(input_data=input_data, config=config, depends_result=depends_result, **kwargs)\n\n        if result.status != RunnableStatus.FAILURE:\n            return result\n\n        if not self.fallback or not self.fallback.llm:\n            return result\n\n        if not result.error:\n            return result\n\n        if not self._should_trigger_fallback(result.error.type, result.error.message):\n            return result\n\n        fallback_llm = self.fallback.llm\n        fallback_llm._is_fallback_run = True\n        logger.warning(\n            f\"LLM {self.name} - {self.id}: Primary LLM ({self.model}) failed. \"\n            f\"Error: {result.error.type.__name__}: {result.error.message}. \"\n            f\"Attempting fallback to {fallback_llm.name} - {fallback_llm.id}\"\n        )\n\n        # Use the primary's already transformed input for fallback\n        # This ensures fallback works with the same prepared input as primary\n        fallback_kwargs = {k: v for k, v in kwargs.items() if k != \"run_depends\"}\n        fallback_kwargs[\"parent_run_id\"] = kwargs.get(\"parent_run_id\")\n\n        fallback_input = result.input.model_dump() if hasattr(result.input, \"model_dump\") else result.input\n        fallback_result = fallback_llm.run_sync(\n            input_data=fallback_input,\n            config=config,\n            depends_result=None,  # Input is already transformed, no need to merge depends\n            **fallback_kwargs,\n        )\n\n        if fallback_result.status == RunnableStatus.SUCCESS:\n            logger.info(f\"LLM {self.name} - {self.id}: Fallback LLM ({fallback_llm.model}) succeeded\")\n            # Apply primary node's output_transformer to fallback result\n            transformed_output = self.transform_output(fallback_result.output, config=config, **kwargs)\n            return RunnableResult(\n                status=RunnableStatus.SUCCESS,\n                input=result.input,\n                output=transformed_output,\n            )\n\n        logger.error(f\"LLM {self.name} - {self.id}: Fallback LLM ({fallback_llm.model}) failed.\")\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.is_pdf_input_supported","title":"<code>is_pdf_input_supported: bool</code>  <code>property</code>","text":"<p>Check if the LLM supports PDF input.</p>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.is_vision_supported","title":"<code>is_vision_supported: bool</code>  <code>property</code>","text":"<p>Check if the LLM supports vision/image processing.</p>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict</code>  <code>property</code>","text":"<p>Exclude fallback configuration during serialization.</p>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BaseLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the BaseLLM instance.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # Save a bit of loading time as litellm is slow\n    from litellm import completion, stream_chunk_builder\n\n    # Avoid the same imports multiple times and for future usage in execute\n    self._completion = completion\n    self._stream_chunk_builder = stream_chunk_builder\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.execute","title":"<code>execute(input_data, config=None, prompt=None, tools=None, response_format=None, **kwargs)</code>","text":"<p>Execute the LLM node.</p> <p>This method processes the input data, formats the prompt, and generates a response using the configured LLM.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>BaseLLMInputSchema</code> <p>The input data for the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>prompt</code> <code>Prompt</code> <p>The prompt to use for this execution. Defaults to None.</p> <code>None</code> <code>tools</code> <code>list[Tool | dict]</code> <p>List of tools that llm can call.</p> <code>None</code> <code>response_format</code> <code>dict[str, Any]</code> <p>JSON schema that specifies the structure of the llm's output</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the generated content and tool calls.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def execute(\n    self,\n    input_data: BaseLLMInputSchema,\n    config: RunnableConfig = None,\n    prompt: Prompt | None = None,\n    tools: list[Tool | dict] | None = None,\n    response_format: dict[str, Any] | None = None,\n    **kwargs,\n):\n    \"\"\"Execute the LLM node.\n\n    This method processes the input data, formats the prompt, and generates a response using\n    the configured LLM.\n\n    Args:\n        input_data (BaseLLMInputSchema): The input data for the LLM.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        prompt (Prompt, optional): The prompt to use for this execution. Defaults to None.\n        tools (list[Tool|dict]): List of tools that llm can call.\n        response_format (dict[str, Any]): JSON schema that specifies the structure of the llm's output\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the generated content and tool calls.\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    prompt = prompt or self.prompt or Prompt(messages=[], tools=None, response_format=None)\n    messages = self.get_messages(prompt, input_data)\n    self.run_on_node_execute_run(callbacks=config.callbacks, prompt_messages=messages, **kwargs)\n\n    extra = copy.deepcopy(self.__pydantic_extra__)\n    params = self.connection.conn_params.copy()\n    if self.client and not isinstance(self.connection, HttpApiKey):\n        params.update({\"client\": self.client})\n    if self.thinking_enabled:\n        params.update({\"thinking\": {\"type\": \"enabled\", \"budget_tokens\": self.budget_tokens}})\n    if extra:\n        params.update(extra)\n\n    response_format, tools = self._get_response_format_and_tools(\n        prompt=prompt,\n        tools=tools,\n        response_format=response_format,\n    )\n    # Check if a streaming callback is available in the config and enable streaming only if it is\n    # This is to avoid unnecessary streaming to reduce CPU usage\n    is_streaming_callback_available = any(\n        isinstance(callback, BaseStreamingCallbackHandler) for callback in config.callbacks\n    )\n    common_params: dict[str, Any] = {\n        \"model\": self.model,\n        \"messages\": messages,\n        \"stream\": self.streaming.enabled and is_streaming_callback_available,\n        \"temperature\": self.temperature,\n        \"max_tokens\": self.max_tokens,\n        \"tools\": tools,\n        \"tool_choice\": self.tool_choice,\n        \"stop\": self.stop if self.stop else None,\n        \"top_p\": self.top_p,\n        \"seed\": self.seed,\n        \"presence_penalty\": self.presence_penalty,\n        \"frequency_penalty\": self.frequency_penalty,\n        \"response_format\": response_format,\n        \"drop_params\": True,\n        **params,\n    }\n\n    common_params = self.update_completion_params(common_params)\n\n    response = self._completion(**common_params)\n    handle_completion = (\n        self._handle_streaming_completion_response\n        if self.streaming.enabled and is_streaming_callback_available\n        else self._handle_completion_response\n    )\n\n    return handle_completion(\n        response=response, messages=messages, config=config, input_data=dict(input_data), **kwargs\n    )\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.get_context_for_input_schema","title":"<code>get_context_for_input_schema()</code>","text":"<p>Provides context for input schema that is required for proper validation.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def get_context_for_input_schema(self) -&gt; dict:\n    \"\"\"Provides context for input schema that is required for proper validation.\"\"\"\n    return {\"instance_prompt\": self.prompt}\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.get_messages","title":"<code>get_messages(prompt, input_data)</code>","text":"<p>Format and filter message parameters based on provider requirements. Override this in provider-specific subclasses.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def get_messages(\n    self,\n    prompt,\n    input_data,\n) -&gt; list[dict]:\n    \"\"\"\n    Format and filter message parameters based on provider requirements.\n    Override this in provider-specific subclasses.\n    \"\"\"\n    messages = prompt.format_messages(**dict(input_data))\n    return messages\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.get_token_limit","title":"<code>get_token_limit()</code>","text":"<p>Returns token limits of a llm.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of tokens.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def get_token_limit(self) -&gt; int:\n    \"\"\"Returns token limits of a llm.\n\n    Returns:\n        int: Number of tokens.\n    \"\"\"\n    return get_max_tokens(self.model)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.get_usage_data","title":"<code>get_usage_data(model, completion)</code>  <code>classmethod</code>","text":"<p>Get usage data for the LLM.</p> <p>This method generates usage data for the LLM based on the provided messages.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use for generating the usage data.</p> required <code>completion</code> <code>ModelResponse</code> <p>The completion response from the LLM.</p> required <p>Returns:</p> Name Type Description <code>BaseLLMUsageData</code> <code>BaseLLMUsageData</code> <p>A model containing the usage data for the LLM.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>@classmethod\ndef get_usage_data(\n    cls,\n    model: str,\n    completion: \"ModelResponse\",\n) -&gt; BaseLLMUsageData:\n    \"\"\"Get usage data for the LLM.\n\n    This method generates usage data for the LLM based on the provided messages.\n\n    Args:\n        model (str): The model to use for generating the usage data.\n        completion (ModelResponse): The completion response from the LLM.\n\n    Returns:\n        BaseLLMUsageData: A model containing the usage data for the LLM.\n    \"\"\"\n    from litellm import cost_per_token\n\n    usage = completion.model_extra[\"usage\"]\n    prompt_tokens = usage.prompt_tokens\n    completion_tokens = usage.completion_tokens\n    total_tokens = usage.total_tokens\n\n    try:\n        prompt_tokens_cost_usd, completion_tokens_cost_usd = cost_per_token(\n            model=model, prompt_tokens=prompt_tokens, completion_tokens=completion_tokens\n        )\n        total_tokens_cost_usd = prompt_tokens_cost_usd + completion_tokens_cost_usd\n    except Exception:\n        prompt_tokens_cost_usd, completion_tokens_cost_usd, total_tokens_cost_usd = None, None, None\n\n    return BaseLLMUsageData(\n        prompt_tokens=prompt_tokens,\n        prompt_tokens_cost_usd=prompt_tokens_cost_usd,\n        completion_tokens=completion_tokens,\n        completion_tokens_cost_usd=completion_tokens_cost_usd,\n        total_tokens=total_tokens,\n        total_tokens_cost_usd=total_tokens_cost_usd,\n    )\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components including fallback LLM if configured.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <p>The connection manager for initializing connections.</p> <code>None</code> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def init_components(self, connection_manager=None):\n    \"\"\"Initialize components including fallback LLM if configured.\n\n    Args:\n        connection_manager: The connection manager for initializing connections.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.fallback and self.fallback.llm and self.fallback.llm.is_postponed_component_init:\n        self.fallback.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the run state of the LLM.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Reset the run state of the LLM.\"\"\"\n    self._is_fallback_run = False\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.run_sync","title":"<code>run_sync(input_data, config=None, depends_result=None, **kwargs)</code>","text":"<p>Run the LLM with fallback support.</p> <p>If the primary LLM fails and a fallback is configured, the primary failure is traced first, then the fallback LLM is executed separately.</p> <p>The fallback receives the same transformed input that the primary received, and the primary's output_transformer is applied to the fallback's output.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input data for the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run.</p> <code>None</code> <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the LLM execution.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def run_sync(\n    self,\n    input_data: dict,\n    config: RunnableConfig = None,\n    depends_result: dict = None,\n    **kwargs,\n) -&gt; RunnableResult:\n    \"\"\"Run the LLM with fallback support.\n\n    If the primary LLM fails and a fallback is configured, the primary failure\n    is traced first, then the fallback LLM is executed separately.\n\n    The fallback receives the same transformed input that the primary received,\n    and the primary's output_transformer is applied to the fallback's output.\n\n    Args:\n        input_data: Input data for the LLM.\n        config: Configuration for the run.\n        depends_result: Results of dependent nodes.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the LLM execution.\n    \"\"\"\n    result = super().run_sync(input_data=input_data, config=config, depends_result=depends_result, **kwargs)\n\n    if result.status != RunnableStatus.FAILURE:\n        return result\n\n    if not self.fallback or not self.fallback.llm:\n        return result\n\n    if not result.error:\n        return result\n\n    if not self._should_trigger_fallback(result.error.type, result.error.message):\n        return result\n\n    fallback_llm = self.fallback.llm\n    fallback_llm._is_fallback_run = True\n    logger.warning(\n        f\"LLM {self.name} - {self.id}: Primary LLM ({self.model}) failed. \"\n        f\"Error: {result.error.type.__name__}: {result.error.message}. \"\n        f\"Attempting fallback to {fallback_llm.name} - {fallback_llm.id}\"\n    )\n\n    # Use the primary's already transformed input for fallback\n    # This ensures fallback works with the same prepared input as primary\n    fallback_kwargs = {k: v for k, v in kwargs.items() if k != \"run_depends\"}\n    fallback_kwargs[\"parent_run_id\"] = kwargs.get(\"parent_run_id\")\n\n    fallback_input = result.input.model_dump() if hasattr(result.input, \"model_dump\") else result.input\n    fallback_result = fallback_llm.run_sync(\n        input_data=fallback_input,\n        config=config,\n        depends_result=None,  # Input is already transformed, no need to merge depends\n        **fallback_kwargs,\n    )\n\n    if fallback_result.status == RunnableStatus.SUCCESS:\n        logger.info(f\"LLM {self.name} - {self.id}: Fallback LLM ({fallback_llm.model}) succeeded\")\n        # Apply primary node's output_transformer to fallback result\n        transformed_output = self.transform_output(fallback_result.output, config=config, **kwargs)\n        return RunnableResult(\n            status=RunnableStatus.SUCCESS,\n            input=result.input,\n            output=transformed_output,\n        )\n\n    logger.error(f\"LLM {self.name} - {self.id}: Fallback LLM ({fallback_llm.model}) failed.\")\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.set_model","title":"<code>set_model(value)</code>  <code>classmethod</code>","text":"<p>Set the model with the appropriate prefix.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>The model value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The model value with the prefix.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>@field_validator(\"model\")\n@classmethod\ndef set_model(cls, value: str | None) -&gt; str:\n    \"\"\"Set the model with the appropriate prefix.\n\n    Args:\n        value (str | None): The model value.\n\n    Returns:\n        str: The model value with the prefix.\n    \"\"\"\n    if cls.MODEL_PREFIX is not None and not value.startswith(cls.MODEL_PREFIX):\n        value = f\"{cls.MODEL_PREFIX}{value}\"\n    return value\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert to dictionary representation.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    data = super().to_dict(**kwargs)\n    if self.fallback:\n        data[\"fallback\"] = self.fallback.model_dump(exclude={\"llm\": True})\n        data[\"fallback\"][\"llm\"] = self.fallback.llm.to_dict(**kwargs) if self.fallback.llm else None\n    if self._is_fallback_run:\n        data[\"is_fallback\"] = True\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.update_completion_params","title":"<code>update_completion_params(params)</code>","text":"<p>Updates or modifies the parameters for the completion method.</p> <p>This method can be overridden by subclasses to customize the parameters passed to the completion method. By default, it enables usage information in streaming mode if streaming is enabled and include_usage is set. Args:     params (dict[str, Any]): The parameters to be updated.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The updated parameters.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Updates or modifies the parameters for the completion method.\n\n    This method can be overridden by subclasses to customize the parameters\n    passed to the completion method. By default, it enables usage information\n    in streaming mode if streaming is enabled and include_usage is set.\n    Args:\n        params (dict[str, Any]): The parameters to be updated.\n\n    Returns:\n        dict[str, Any]: The updated parameters.\n    \"\"\"\n    if self.streaming and self.streaming.enabled and self.streaming.include_usage and params.get(\"stream\", False):\n        params.setdefault(\"stream_options\", {})\n        params[\"stream_options\"][\"include_usage\"] = True\n    return params\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLMUsageData","title":"<code>BaseLLMUsageData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for LLM usage data.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of prompt tokens.</p> <code>prompt_tokens_cost_usd</code> <code>float | None</code> <p>Cost of prompt tokens in USD.</p> <code>completion_tokens</code> <code>int</code> <p>Number of completion tokens.</p> <code>completion_tokens_cost_usd</code> <code>float | None</code> <p>Cost of completion tokens in USD.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens.</p> <code>total_tokens_cost_usd</code> <code>float | None</code> <p>Total cost of tokens in USD.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>class BaseLLMUsageData(BaseModel):\n    \"\"\"Model for LLM usage data.\n\n    Attributes:\n        prompt_tokens (int): Number of prompt tokens.\n        prompt_tokens_cost_usd (float | None): Cost of prompt tokens in USD.\n        completion_tokens (int): Number of completion tokens.\n        completion_tokens_cost_usd (float | None): Cost of completion tokens in USD.\n        total_tokens (int): Total number of tokens.\n        total_tokens_cost_usd (float | None): Total cost of tokens in USD.\n    \"\"\"\n    prompt_tokens: int\n    prompt_tokens_cost_usd: float | None\n    completion_tokens: int\n    completion_tokens_cost_usd: float | None\n    total_tokens: int\n    total_tokens_cost_usd: float | None\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.FallbackConfig","title":"<code>FallbackConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for LLM fallback behavior.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLLM | None</code> <p>The fallback LLM to use when the primary LLM fails. Required when enabled=True.</p> <code>enabled</code> <code>bool</code> <p>Whether fallback is enabled. Defaults to False.</p> <code>triggers</code> <code>list[FallbackTrigger]</code> <p>List of trigger conditions that will activate the fallback. Use FallbackTrigger.ANY to trigger on any error.</p> <p>Examples:</p>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.FallbackConfig--single-trigger","title":"Single trigger","text":"<p>FallbackConfig(llm=my_llm, enabled=True, triggers=[FallbackTrigger.RATE_LIMIT])</p>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.FallbackConfig--multiple-triggers","title":"Multiple triggers","text":"<p>FallbackConfig(llm=my_llm, enabled=True, triggers=[FallbackTrigger.RATE_LIMIT, FallbackTrigger.CONNECTION])</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>class FallbackConfig(BaseModel):\n    \"\"\"Configuration for LLM fallback behavior.\n\n    Attributes:\n        llm: The fallback LLM to use when the primary LLM fails. Required when enabled=True.\n        enabled: Whether fallback is enabled. Defaults to False.\n        triggers: List of trigger conditions that will activate the fallback.\n            Use FallbackTrigger.ANY to trigger on any error.\n\n    Examples:\n        # Single trigger\n        FallbackConfig(llm=my_llm, enabled=True, triggers=[FallbackTrigger.RATE_LIMIT])\n\n        # Multiple triggers\n        FallbackConfig(llm=my_llm, enabled=True, triggers=[FallbackTrigger.RATE_LIMIT, FallbackTrigger.CONNECTION])\n    \"\"\"\n\n    llm: \"BaseLLM | None\" = None\n    enabled: bool = False\n    triggers: list[FallbackTrigger] = Field(default_factory=lambda: [FallbackTrigger.ANY])\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_llm_required_when_enabled(self) -&gt; \"FallbackConfig\":\n        \"\"\"Validate that llm is provided when fallback is enabled.\"\"\"\n        if self.enabled and self.llm is None:\n            raise ValueError(\"FallbackConfig requires 'llm' when 'enabled' is True\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.FallbackConfig.validate_llm_required_when_enabled","title":"<code>validate_llm_required_when_enabled()</code>","text":"<p>Validate that llm is provided when fallback is enabled.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_llm_required_when_enabled(self) -&gt; \"FallbackConfig\":\n    \"\"\"Validate that llm is provided when fallback is enabled.\"\"\"\n    if self.enabled and self.llm is None:\n        raise ValueError(\"FallbackConfig requires 'llm' when 'enabled' is True\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/llms/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/nodes/llms/bedrock/#dynamiq.nodes.llms.bedrock.Bedrock","title":"<code>Bedrock</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Bedrock LLM node.</p> <p>This class provides an implementation for the Bedrock Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AWS | None</code> <p>The connection to use for the Bedrock LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Bedrock model name.</p> Source code in <code>dynamiq/nodes/llms/bedrock.py</code> <pre><code>class Bedrock(BaseLLM):\n    \"\"\"Bedrock LLM node.\n\n    This class provides an implementation for the Bedrock Language Model node.\n\n    Attributes:\n        connection (AWSConnection | None): The connection to use for the Bedrock LLM.\n        MODEL_PREFIX (str): The prefix for the Bedrock model name.\n    \"\"\"\n    connection: AWSConnection | None = None\n    MODEL_PREFIX = \"bedrock/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Bedrock LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AWSConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/bedrock/#dynamiq.nodes.llms.bedrock.Bedrock.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Bedrock LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Bedrock LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AWSConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cerebras/","title":"Cerebras","text":""},{"location":"dynamiq/nodes/llms/cerebras/#dynamiq.nodes.llms.cerebras.Cerebras","title":"<code>Cerebras</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Cerebras LLM node.</p> <p>This class provides an implementation for the Cerebras Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cerebras</code> <p>The connection to use for the Cerebras LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Cerebras model name.</p> Source code in <code>dynamiq/nodes/llms/cerebras.py</code> <pre><code>class Cerebras(BaseLLM):\n    \"\"\"Cerebras LLM node.\n\n    This class provides an implementation for the Cerebras Language Model node.\n\n    Attributes:\n        connection (CerebrasConnection): The connection to use for the Cerebras LLM.\n        MODEL_PREFIX (str): The prefix for the Cerebras model name.\n    \"\"\"\n    connection: CerebrasConnection\n    MODEL_PREFIX = \"cerebras/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Cerebras LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CerebrasConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cerebras/#dynamiq.nodes.llms.cerebras.Cerebras.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Cerebras LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/cerebras.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Cerebras LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CerebrasConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cohere/","title":"Cohere","text":""},{"location":"dynamiq/nodes/llms/cohere/#dynamiq.nodes.llms.cohere.Cohere","title":"<code>Cohere</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Cohere LLM node.</p> <p>This class provides an implementation for the Cohere Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cohere</code> <p>The connection to use for the Cohere LLM.</p> Source code in <code>dynamiq/nodes/llms/cohere.py</code> <pre><code>class Cohere(BaseLLM):\n    \"\"\"Cohere LLM node.\n\n    This class provides an implementation for the Cohere Language Model node.\n\n    Attributes:\n        connection (CohereConnection): The connection to use for the Cohere LLM.\n    \"\"\"\n    connection: CohereConnection\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Cohere LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cohere/#dynamiq.nodes.llms.cohere.Cohere.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Cohere LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Cohere LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/custom_llm/","title":"Custom llm","text":""},{"location":"dynamiq/nodes/llms/custom_llm/#dynamiq.nodes.llms.custom_llm.CustomLLM","title":"<code>CustomLLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Custom LLM implementation for third-party providers requiring specific formatting.</p> <p>This class extends BaseLLM to support providers like OpenRouter, Anthropic, or custom endpoints that need special request formatting. It allows adding provider prefixes to model names.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str | None</code> <p>Name of the LLM node. Defaults to \"CustomLLM\".</p> <code>connection</code> <code>HttpApiKey</code> <p>Connection to use for the LLM API.</p> <code>provider_prefix</code> <code>str | None</code> <p>Provider prefix to add to model names (e.g., \"openrouter\").                          When specified, this will automatically prepend                          \"{provider_prefix}/\" to the model name when sending requests.</p> Source code in <code>dynamiq/nodes/llms/custom_llm.py</code> <pre><code>class CustomLLM(BaseLLM):\n    \"\"\"\n    Custom LLM implementation for third-party providers requiring specific formatting.\n\n    This class extends BaseLLM to support providers like OpenRouter, Anthropic, or custom\n    endpoints that need special request formatting. It allows adding provider prefixes to\n    model names.\n\n    Attributes:\n        name (str | None): Name of the LLM node. Defaults to \"CustomLLM\".\n        connection (HttpApiKey): Connection to use for the LLM API.\n        provider_prefix (str | None): Provider prefix to add to model names (e.g., \"openrouter\").\n                                     When specified, this will automatically prepend\n                                     \"{provider_prefix}/\" to the model name when sending requests.\n\n    \"\"\"\n\n    name: str | None = \"CustomLLM\"\n    connection: HttpApiKey\n    provider_prefix: str | None = None\n\n    def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Customizes LLM request parameters for third-party providers.\n\n        This method adds the provider prefix to model name if specified\n        (e.g., \"openrouter/anthropic/claude-2\").\n\n        Args:\n            params (dict[str, Any]): The original parameters to be sent to the LLM provider.\n                                    This includes model, messages, temperature, etc.\n\n        Returns:\n            dict[str, Any]: The modified parameters with proper model name formatting.\n        \"\"\"\n        params = super().update_completion_params(params)\n\n        if self.provider_prefix and not params[\"model\"].startswith(self.provider_prefix + \"/\"):\n            params[\"model\"] = f\"{self.provider_prefix}/{params['model']}\"\n\n        return params\n</code></pre>"},{"location":"dynamiq/nodes/llms/custom_llm/#dynamiq.nodes.llms.custom_llm.CustomLLM.update_completion_params","title":"<code>update_completion_params(params)</code>","text":"<p>Customizes LLM request parameters for third-party providers.</p> <p>This method adds the provider prefix to model name if specified (e.g., \"openrouter/anthropic/claude-2\").</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, Any]</code> <p>The original parameters to be sent to the LLM provider.                     This includes model, messages, temperature, etc.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The modified parameters with proper model name formatting.</p> Source code in <code>dynamiq/nodes/llms/custom_llm.py</code> <pre><code>def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Customizes LLM request parameters for third-party providers.\n\n    This method adds the provider prefix to model name if specified\n    (e.g., \"openrouter/anthropic/claude-2\").\n\n    Args:\n        params (dict[str, Any]): The original parameters to be sent to the LLM provider.\n                                This includes model, messages, temperature, etc.\n\n    Returns:\n        dict[str, Any]: The modified parameters with proper model name formatting.\n    \"\"\"\n    params = super().update_completion_params(params)\n\n    if self.provider_prefix and not params[\"model\"].startswith(self.provider_prefix + \"/\"):\n        params[\"model\"] = f\"{self.provider_prefix}/{params['model']}\"\n\n    return params\n</code></pre>"},{"location":"dynamiq/nodes/llms/databricks/","title":"Databricks","text":""},{"location":"dynamiq/nodes/llms/databricks/#dynamiq.nodes.llms.databricks.Databricks","title":"<code>Databricks</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Databricks LLM node.</p> <p>This class provides an implementation for the Databricks Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Databricks</code> <p>The connection to use for the Databricks LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Databricks model name.</p> <code>reasoning_effort</code> <code>ReasoningEffort | None</code> <p>Controls the depth and complexity of reasoning</p> Source code in <code>dynamiq/nodes/llms/databricks.py</code> <pre><code>class Databricks(BaseLLM):\n    \"\"\"Databricks LLM node.\n\n    This class provides an implementation for the Databricks Language Model node.\n\n    Attributes:\n        connection (DatabricksConnection): The connection to use for the Databricks LLM.\n        MODEL_PREFIX (str): The prefix for the Databricks model name.\n        reasoning_effort (ReasoningEffort | None): Controls the depth and complexity of reasoning\n        performed by the model.\n    \"\"\"\n\n    connection: DatabricksConnection\n    MODEL_PREFIX = \"databricks/\"\n    reasoning_effort: ReasoningEffort | None = ReasoningEffort.MEDIUM\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Databricks LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = DatabricksConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/databricks/#dynamiq.nodes.llms.databricks.Databricks.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Databricks LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/databricks.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Databricks LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = DatabricksConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepinfra/","title":"Deepinfra","text":""},{"location":"dynamiq/nodes/llms/deepinfra/#dynamiq.nodes.llms.deepinfra.DeepInfra","title":"<code>DeepInfra</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>DeepInfra LLM node.</p> <p>This class provides an implementation for the DeepInfra Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>DeepInfra</code> <p>The connection to use for the DeepInfra LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the DeepInfra model name.</p> Source code in <code>dynamiq/nodes/llms/deepinfra.py</code> <pre><code>class DeepInfra(BaseLLM):\n    \"\"\"DeepInfra LLM node.\n\n    This class provides an implementation for the DeepInfra Language Model node.\n\n    Attributes:\n        connection (DeepInfraConnection): The connection to use for the DeepInfra LLM.\n        MODEL_PREFIX (str): The prefix for the DeepInfra model name.\n    \"\"\"\n    connection: DeepInfraConnection\n    MODEL_PREFIX = \"deepinfra/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the DeepInfra LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = DeepInfraConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepinfra/#dynamiq.nodes.llms.deepinfra.DeepInfra.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the DeepInfra LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/deepinfra.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the DeepInfra LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = DeepInfraConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepseek/","title":"Deepseek","text":""},{"location":"dynamiq/nodes/llms/deepseek/#dynamiq.nodes.llms.deepseek.DeepSeek","title":"<code>DeepSeek</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>DeepSeek LLM node.</p> <p>This class provides an implementation for the DeepSeek Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>DeepSeek</code> <p>The connection to use for the DeepSeek LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the DeepSeek model name.</p> Source code in <code>dynamiq/nodes/llms/deepseek.py</code> <pre><code>class DeepSeek(BaseLLM):\n    \"\"\"DeepSeek LLM node.\n\n    This class provides an implementation for the DeepSeek Language Model node.\n\n    Attributes:\n        connection (DeepSeekConnection): The connection to use for the DeepSeek LLM.\n        MODEL_PREFIX (str): The prefix for the DeepSeek model name.\n    \"\"\"\n\n    connection: DeepSeekConnection\n    MODEL_PREFIX = \"deepseek/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Replicate LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = DeepSeekConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepseek/#dynamiq.nodes.llms.deepseek.DeepSeek.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Replicate LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/deepseek.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Replicate LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = DeepSeekConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/fireworksai/","title":"Fireworksai","text":""},{"location":"dynamiq/nodes/llms/fireworksai/#dynamiq.nodes.llms.fireworksai.FireworksAI","title":"<code>FireworksAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>FireworksAI LLM node.</p> <p>This class provides an implementation for the Fireworks AI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>FireworksAI | None</code> <p>The connection to use for the Fireworks AI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Fireworks AI model name.</p> Source code in <code>dynamiq/nodes/llms/fireworksai.py</code> <pre><code>class FireworksAI(BaseLLM):\n    \"\"\"FireworksAI LLM node.\n\n    This class provides an implementation for the Fireworks AI Language Model node.\n\n    Attributes:\n        connection (FireworksAIConnection | None): The connection to use for the Fireworks AI LLM.\n        MODEL_PREFIX (str): The prefix for the Fireworks AI model name.\n    \"\"\"\n\n    connection: FireworksAIConnection | None = None\n    MODEL_PREFIX = \"fireworks_ai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the FireworksAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = FireworksAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/fireworksai/#dynamiq.nodes.llms.fireworksai.FireworksAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the FireworksAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/fireworksai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the FireworksAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = FireworksAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/gemini/","title":"Gemini","text":""},{"location":"dynamiq/nodes/llms/gemini/#dynamiq.nodes.llms.gemini.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Gemini LLM node.</p> <p>This class provides an implementation for the Gemini Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Gemini</code> <p>The connection to use for the Gemini LLM.</p> Source code in <code>dynamiq/nodes/llms/gemini.py</code> <pre><code>class Gemini(BaseLLM):\n    \"\"\"Gemini LLM node.\n\n    This class provides an implementation for the Gemini Language Model node.\n\n    Attributes:\n        connection (GeminiConnection): The connection to use for the Gemini LLM.\n    \"\"\"\n\n    connection: GeminiConnection\n    MODEL_PREFIX = \"gemini/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Gemini LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GeminiConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/gemini/#dynamiq.nodes.llms.gemini.Gemini.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Gemini LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/gemini.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Gemini LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GeminiConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/groq/","title":"Groq","text":""},{"location":"dynamiq/nodes/llms/groq/#dynamiq.nodes.llms.groq.Groq","title":"<code>Groq</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Groq LLM node.</p> <p>This class provides an implementation for the Groq Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Groq | None</code> <p>The connection to use for the Groq LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Groq model name.</p> Source code in <code>dynamiq/nodes/llms/groq.py</code> <pre><code>class Groq(BaseLLM):\n    \"\"\"Groq LLM node.\n\n    This class provides an implementation for the Groq Language Model node.\n\n    Attributes:\n        connection (GroqConnection | None): The connection to use for the Groq LLM.\n        MODEL_PREFIX (str): The prefix for the Groq model name.\n    \"\"\"\n    connection: GroqConnection | None = None\n    MODEL_PREFIX = \"groq/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Groq LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GroqConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/groq/#dynamiq.nodes.llms.groq.Groq.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Groq LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/groq.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Groq LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GroqConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/nodes/llms/huggingface/#dynamiq.nodes.llms.huggingface.HuggingFace","title":"<code>HuggingFace</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>HuggingFace LLM node.</p> <p>This class provides an implementation for the HuggingFace Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>HuggingFace | None</code> <p>The connection to use for the HuggingFace LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the HuggingFace model name.</p> Source code in <code>dynamiq/nodes/llms/huggingface.py</code> <pre><code>class HuggingFace(BaseLLM):\n    \"\"\"HuggingFace LLM node.\n\n    This class provides an implementation for the HuggingFace Language Model node.\n\n    Attributes:\n        connection (HuggingFaceConnection | None): The connection to use for the HuggingFace LLM.\n        MODEL_PREFIX (str): The prefix for the HuggingFace model name.\n    \"\"\"\n    connection: HuggingFaceConnection | None = None\n    MODEL_PREFIX = \"huggingface/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the HuggingFace LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/huggingface/#dynamiq.nodes.llms.huggingface.HuggingFace.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the HuggingFace LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the HuggingFace LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/mistral/","title":"Mistral","text":""},{"location":"dynamiq/nodes/llms/mistral/#dynamiq.nodes.llms.mistral.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Mistral LLM node.</p> <p>This class provides an implementation for the Mistral Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Mistral | None</code> <p>The connection to use for the Mistral LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Mistral model name.</p> Source code in <code>dynamiq/nodes/llms/mistral.py</code> <pre><code>class Mistral(BaseLLM):\n    \"\"\"Mistral LLM node.\n\n    This class provides an implementation for the Mistral Language Model node.\n\n    Attributes:\n        connection (MistralConnection | None): The connection to use for the Mistral LLM.\n        MODEL_PREFIX (str): The prefix for the Mistral model name.\n    \"\"\"\n    connection: MistralConnection | None = None\n    MODEL_PREFIX = \"mistral/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Mistral LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n\n    def get_messages(\n        self,\n        prompt,\n        input_data,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Format and filter message parameters based on provider requirements.\n        Override this in provider-specific subclasses.\n        \"\"\"\n        messages = prompt.format_messages(**dict(input_data))\n        formatted_messages = []\n        for i, msg in enumerate(messages):\n            msg_copy = msg.copy()\n\n            is_last_message = i == len(messages) - 1\n            if is_last_message and msg_copy[\"role\"] == MessageRole.ASSISTANT.value:\n                msg_copy[\"prefix\"] = True\n\n            formatted_messages.append(msg_copy)\n\n        return formatted_messages\n</code></pre>"},{"location":"dynamiq/nodes/llms/mistral/#dynamiq.nodes.llms.mistral.Mistral.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Mistral LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Mistral LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/mistral/#dynamiq.nodes.llms.mistral.Mistral.get_messages","title":"<code>get_messages(prompt, input_data)</code>","text":"<p>Format and filter message parameters based on provider requirements. Override this in provider-specific subclasses.</p> Source code in <code>dynamiq/nodes/llms/mistral.py</code> <pre><code>def get_messages(\n    self,\n    prompt,\n    input_data,\n) -&gt; list[dict]:\n    \"\"\"\n    Format and filter message parameters based on provider requirements.\n    Override this in provider-specific subclasses.\n    \"\"\"\n    messages = prompt.format_messages(**dict(input_data))\n    formatted_messages = []\n    for i, msg in enumerate(messages):\n        msg_copy = msg.copy()\n\n        is_last_message = i == len(messages) - 1\n        if is_last_message and msg_copy[\"role\"] == MessageRole.ASSISTANT.value:\n            msg_copy[\"prefix\"] = True\n\n        formatted_messages.append(msg_copy)\n\n    return formatted_messages\n</code></pre>"},{"location":"dynamiq/nodes/llms/nvidia_nim/","title":"Nvidia nim","text":""},{"location":"dynamiq/nodes/llms/nvidia_nim/#dynamiq.nodes.llms.nvidia_nim.NvidiaNIM","title":"<code>NvidiaNIM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Nvidia NIM LLM node.</p> <p>This class provides an implementation for the Nvidia NIM Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Nvidia_NIM_Connection | None</code> <p>The connection to use for the Nvidia NIM LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Nvidia NIM model name.</p> Source code in <code>dynamiq/nodes/llms/nvidia_nim.py</code> <pre><code>class NvidiaNIM(BaseLLM):\n    \"\"\"Nvidia NIM LLM node.\n\n    This class provides an implementation for the Nvidia NIM Language Model node.\n\n    Attributes:\n        connection (Nvidia_NIM_Connection | None): The connection to use for the Nvidia NIM LLM.\n        MODEL_PREFIX (str): The prefix for the Nvidia NIM model name.\n    \"\"\"\n\n    connection: NvidiaNIMConnection | None = None\n    MODEL_PREFIX = \"nvidia_nim/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Nvidia NIM LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = NvidiaNIMConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/nvidia_nim/#dynamiq.nodes.llms.nvidia_nim.NvidiaNIM.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Nvidia NIM LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/nvidia_nim.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Nvidia NIM LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = NvidiaNIMConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/ollama/","title":"Ollama","text":""},{"location":"dynamiq/nodes/llms/ollama/#dynamiq.nodes.llms.ollama.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Ollama LLM node.</p> <p>This class provides an implementation for the Ollama Language Model node. It supports both chat and completion endpoints through model prefixes.</p> <p>Attributes:</p> Name Type Description <code>MODEL_PREFIX</code> <code>ClassVar[str | None]</code> <p>Optional model prefix, None to allow both ollama/ and ollama_chat/.</p> <code>connection</code> <code>Ollama | None</code> <p>The connection to use for the Ollama LLM.</p> Source code in <code>dynamiq/nodes/llms/ollama.py</code> <pre><code>class Ollama(BaseLLM):\n    \"\"\"Ollama LLM node.\n\n    This class provides an implementation for the Ollama Language Model node.\n    It supports both chat and completion endpoints through model prefixes.\n\n    Attributes:\n        MODEL_PREFIX (ClassVar[str | None]): Optional model prefix, None to allow both ollama/ and ollama_chat/.\n        connection (OllamaConnection | None): The connection to use for the Ollama LLM.\n    \"\"\"\n\n    MODEL_PREFIX: ClassVar[str | None] = \"ollama/\"\n    connection: OllamaConnection | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Ollama LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OllamaConnection()\n\n        super().__init__(**kwargs)\n\n    @classmethod\n    def get_usage_data(cls, model: str, completion: \"ModelResponse\") -&gt; \"BaseLLMUsageData\":\n        \"\"\"Get usage data for the Ollama LLM.\n\n        Args:\n            model (str): The model used for generation.\n            completion (ModelResponse): The completion response from the LLM.\n\n        Returns:\n            BaseLLMUsageData: A model containing the usage data for the LLM.\n        \"\"\"\n        usage = completion.model_extra.get(\"usage\", {})\n        prompt_tokens = usage.get(\"prompt_eval_count\", 0)\n        completion_tokens = usage.get(\"eval_count\", 0)\n        total_tokens = prompt_tokens + completion_tokens\n\n        return BaseLLMUsageData(\n            prompt_tokens=prompt_tokens,\n            prompt_tokens_cost_usd=None,\n            completion_tokens=completion_tokens,\n            completion_tokens_cost_usd=None,\n            total_tokens=total_tokens,\n            total_tokens_cost_usd=None,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/llms/ollama/#dynamiq.nodes.llms.ollama.Ollama.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Ollama LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/ollama.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Ollama LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OllamaConnection()\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/ollama/#dynamiq.nodes.llms.ollama.Ollama.get_usage_data","title":"<code>get_usage_data(model, completion)</code>  <code>classmethod</code>","text":"<p>Get usage data for the Ollama LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model used for generation.</p> required <code>completion</code> <code>ModelResponse</code> <p>The completion response from the LLM.</p> required <p>Returns:</p> Name Type Description <code>BaseLLMUsageData</code> <code>BaseLLMUsageData</code> <p>A model containing the usage data for the LLM.</p> Source code in <code>dynamiq/nodes/llms/ollama.py</code> <pre><code>@classmethod\ndef get_usage_data(cls, model: str, completion: \"ModelResponse\") -&gt; \"BaseLLMUsageData\":\n    \"\"\"Get usage data for the Ollama LLM.\n\n    Args:\n        model (str): The model used for generation.\n        completion (ModelResponse): The completion response from the LLM.\n\n    Returns:\n        BaseLLMUsageData: A model containing the usage data for the LLM.\n    \"\"\"\n    usage = completion.model_extra.get(\"usage\", {})\n    prompt_tokens = usage.get(\"prompt_eval_count\", 0)\n    completion_tokens = usage.get(\"eval_count\", 0)\n    total_tokens = prompt_tokens + completion_tokens\n\n    return BaseLLMUsageData(\n        prompt_tokens=prompt_tokens,\n        prompt_tokens_cost_usd=None,\n        completion_tokens=completion_tokens,\n        completion_tokens_cost_usd=None,\n        total_tokens=total_tokens,\n        total_tokens_cost_usd=None,\n    )\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/","title":"Openai","text":""},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>OpenAI LLM node.</p> <p>This class provides an implementation for the OpenAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>OpenAI | None</code> <p>The connection to use for the OpenAI LLM.</p> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>class OpenAI(BaseLLM):\n    \"\"\"OpenAI LLM node.\n\n    This class provides an implementation for the OpenAI Language Model node.\n\n    Attributes:\n        connection (OpenAIConnection | None): The connection to use for the OpenAI LLM.\n    \"\"\"\n    connection: OpenAIConnection | None = None\n    reasoning_effort: ReasoningEffort | None = ReasoningEffort.MEDIUM\n    verbosity: Verbosity | None = Verbosity.MEDIUM\n    O_SERIES_MODEL_PREFIXES: ClassVar[tuple[str, ...]] = (\"o1\", \"o3\", \"o4\")\n    MODEL_PREFIX = \"openai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the OpenAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    @cached_property\n    def is_o_series_model(self) -&gt; bool:\n        \"\"\"\n        Determine if the model belongs to the O-series (e.g. o1 or o3, o4)\n        by checking if the model starts with any of the O-series prefixes.\n        \"\"\"\n        model_lower = self.model.lower().removeprefix(self.MODEL_PREFIX)\n        return any(model_lower.startswith(prefix) for prefix in self.O_SERIES_MODEL_PREFIXES)\n\n    def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Override the base method to update the completion parameters for OpenAI.\n        For new series models, use \"max_completion_tokens\" instead of \"max_tokens\".\n        \"\"\"\n        params = super().update_completion_params(params)\n\n        new_params = params.copy()\n        model_lower = self.model.lower().removeprefix(self.MODEL_PREFIX)\n        if self.is_o_series_model:\n            new_params[\"max_completion_tokens\"] = self.max_tokens\n            if model_lower.startswith(\"o3\") or model_lower.startswith(\"o4\"):\n                new_params[\"reasoning_effort\"] = self.reasoning_effort\n            if model_lower not in [\"o3-mini\"]:\n                new_params.pop(\"stop\", None)\n            new_params.pop(\"max_tokens\", None)\n            new_params.pop(\"temperature\", None)\n        elif model_lower.startswith(\"gpt-5\"):\n            if \"chat\" not in model_lower:\n                new_params[\"verbosity\"] = self.verbosity\n                if \"pro\" in model_lower:\n                    new_params[\"reasoning_effort\"] = ReasoningEffort.HIGH\n                else:\n                    new_params[\"reasoning_effort\"] = self.reasoning_effort\n            new_params[\"max_completion_tokens\"] = self.max_tokens\n            new_params.pop(\"stop\", None)\n            new_params.pop(\"max_tokens\", None)\n\n        return new_params\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI.is_o_series_model","title":"<code>is_o_series_model: bool</code>  <code>cached</code> <code>property</code>","text":"<p>Determine if the model belongs to the O-series (e.g. o1 or o3, o4) by checking if the model starts with any of the O-series prefixes.</p>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the OpenAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI.update_completion_params","title":"<code>update_completion_params(params)</code>","text":"<p>Override the base method to update the completion parameters for OpenAI. For new series models, use \"max_completion_tokens\" instead of \"max_tokens\".</p> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>def update_completion_params(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Override the base method to update the completion parameters for OpenAI.\n    For new series models, use \"max_completion_tokens\" instead of \"max_tokens\".\n    \"\"\"\n    params = super().update_completion_params(params)\n\n    new_params = params.copy()\n    model_lower = self.model.lower().removeprefix(self.MODEL_PREFIX)\n    if self.is_o_series_model:\n        new_params[\"max_completion_tokens\"] = self.max_tokens\n        if model_lower.startswith(\"o3\") or model_lower.startswith(\"o4\"):\n            new_params[\"reasoning_effort\"] = self.reasoning_effort\n        if model_lower not in [\"o3-mini\"]:\n            new_params.pop(\"stop\", None)\n        new_params.pop(\"max_tokens\", None)\n        new_params.pop(\"temperature\", None)\n    elif model_lower.startswith(\"gpt-5\"):\n        if \"chat\" not in model_lower:\n            new_params[\"verbosity\"] = self.verbosity\n            if \"pro\" in model_lower:\n                new_params[\"reasoning_effort\"] = ReasoningEffort.HIGH\n            else:\n                new_params[\"reasoning_effort\"] = self.reasoning_effort\n        new_params[\"max_completion_tokens\"] = self.max_tokens\n        new_params.pop(\"stop\", None)\n        new_params.pop(\"max_tokens\", None)\n\n    return new_params\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.ReasoningEffort","title":"<code>ReasoningEffort</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The reasoning effort to use for the OpenAI LLM.</p> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>class ReasoningEffort(str, enum.Enum):\n    \"\"\"\n    The reasoning effort to use for the OpenAI LLM.\n    \"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    MINIMAL = \"minimal\"\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.Verbosity","title":"<code>Verbosity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The verbosity level for the OpenAI LLM.</p> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>class Verbosity(str, enum.Enum):\n    \"\"\"\n    The verbosity level for the OpenAI LLM.\n    \"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n</code></pre>"},{"location":"dynamiq/nodes/llms/perplexity/","title":"Perplexity","text":""},{"location":"dynamiq/nodes/llms/perplexity/#dynamiq.nodes.llms.perplexity.Perplexity","title":"<code>Perplexity</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Perplexity LLM node.</p> <p>This class provides an implementation for the Perplexity Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Perplexity</code> <p>The connection to use for the Perplexity LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Perplexity model name.</p> <code>return_citations</code> <code>bool</code> <p>Whether to return citations in the response.</p> Source code in <code>dynamiq/nodes/llms/perplexity.py</code> <pre><code>class Perplexity(BaseLLM):\n    \"\"\"Perplexity LLM node.\n\n    This class provides an implementation for the Perplexity Language Model node.\n\n    Attributes:\n        connection (PerplexityConnection): The connection to use for the Perplexity LLM.\n        MODEL_PREFIX (str): The prefix for the Perplexity model name.\n        return_citations (bool): Whether to return citations in the response.\n    \"\"\"\n\n    connection: PerplexityConnection\n    return_citations: bool = Field(default=False, description=\"Whether to return citations in the response\")\n\n    MODEL_PREFIX = \"perplexity/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Perplexity LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = PerplexityConnection()\n\n        if \"connection\" in kwargs:\n            kwargs[\"connection\"].conn_params[\"return_citations\"] = kwargs.get(\"return_citations\", False)\n\n        super().__init__(**kwargs)\n\n    def _handle_completion_response(\n        self,\n        response: Union[\"ModelResponse\", \"CustomStreamWrapper\"],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"Handle completion response with citations.\n\n        Args:\n            response (ModelResponse | CustomStreamWrapper): The response from the LLM.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content, tool calls, and citations.\n        \"\"\"\n        result = super()._handle_completion_response(response, config, **kwargs)\n\n        if hasattr(response, \"citations\"):\n            result[\"citations\"] = response.citations\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/llms/perplexity/#dynamiq.nodes.llms.perplexity.Perplexity.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Perplexity LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/perplexity.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Perplexity LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = PerplexityConnection()\n\n    if \"connection\" in kwargs:\n        kwargs[\"connection\"].conn_params[\"return_citations\"] = kwargs.get(\"return_citations\", False)\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/replicate/","title":"Replicate","text":""},{"location":"dynamiq/nodes/llms/replicate/#dynamiq.nodes.llms.replicate.Replicate","title":"<code>Replicate</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Replicate LLM node.</p> <p>This class provides an implementation for the Replicate Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Replicate</code> <p>The connection to use for the Replicate LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Replicate model name.</p> Source code in <code>dynamiq/nodes/llms/replicate.py</code> <pre><code>class Replicate(BaseLLM):\n    \"\"\"Replicate LLM node.\n\n    This class provides an implementation for the Replicate Language Model node.\n\n    Attributes:\n        connection (ReplicateConnection): The connection to use for the Replicate LLM.\n        MODEL_PREFIX (str): The prefix for the Replicate model name.\n    \"\"\"\n    connection: ReplicateConnection\n    MODEL_PREFIX = \"replicate/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Replicate LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ReplicateConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/replicate/#dynamiq.nodes.llms.replicate.Replicate.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Replicate LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/replicate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Replicate LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ReplicateConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/sambanova/","title":"Sambanova","text":""},{"location":"dynamiq/nodes/llms/sambanova/#dynamiq.nodes.llms.sambanova.SambaNova","title":"<code>SambaNova</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>SambaNova LLM node.</p> <p>This class provides an implementation for the SambaNova Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>SambaNova</code> <p>The connection to use for the SambaNova LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the SambaNova model name.</p> Source code in <code>dynamiq/nodes/llms/sambanova.py</code> <pre><code>class SambaNova(BaseLLM):\n    \"\"\"SambaNova LLM node.\n\n    This class provides an implementation for the SambaNova Language Model node.\n\n    Attributes:\n        connection (SambaNovaConnection): The connection to use for the SambaNova LLM.\n        MODEL_PREFIX (str): The prefix for the SambaNova model name.\n    \"\"\"\n    connection: SambaNovaConnection\n    MODEL_PREFIX = \"sambanova/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the SambaNova LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = SambaNovaConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/sambanova/#dynamiq.nodes.llms.sambanova.SambaNova.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the SambaNova LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/sambanova.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the SambaNova LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = SambaNovaConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/togetherai/","title":"Togetherai","text":""},{"location":"dynamiq/nodes/llms/togetherai/#dynamiq.nodes.llms.togetherai.TogetherAI","title":"<code>TogetherAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>TogetherAI LLM node.</p> <p>This class provides an implementation for the TogetherAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>TogetherAI | None</code> <p>The connection to use for the TogetherAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the TogetherAI model name.</p> Source code in <code>dynamiq/nodes/llms/togetherai.py</code> <pre><code>class TogetherAI(BaseLLM):\n    \"\"\"TogetherAI LLM node.\n\n    This class provides an implementation for the TogetherAI Language Model node.\n\n    Attributes:\n        connection (TogetherAIConnection | None): The connection to use for the TogetherAI LLM.\n        MODEL_PREFIX (str): The prefix for the TogetherAI model name.\n    \"\"\"\n    connection: TogetherAIConnection | None = None\n    MODEL_PREFIX = \"together_ai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the TogetherAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = TogetherAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/togetherai/#dynamiq.nodes.llms.togetherai.TogetherAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the TogetherAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/togetherai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the TogetherAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = TogetherAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/vertexai/","title":"Vertexai","text":""},{"location":"dynamiq/nodes/llms/vertexai/#dynamiq.nodes.llms.vertexai.VertexAI","title":"<code>VertexAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>VertexAI LLM node.</p> <p>This class provides an implementation for the VertexAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>VertexAI | None</code> <p>The connection to use for the VertexAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the VertexAI model name.</p> Source code in <code>dynamiq/nodes/llms/vertexai.py</code> <pre><code>class VertexAI(BaseLLM):\n    \"\"\"VertexAI LLM node.\n\n    This class provides an implementation for the VertexAI Language Model node.\n\n    Attributes:\n        connection (VertexAIConnection | None): The connection to use for the VertexAI LLM.\n        MODEL_PREFIX (str): The prefix for the VertexAI model name.\n    \"\"\"\n\n    connection: VertexAIConnection | None = None\n    MODEL_PREFIX = \"vertex_ai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the VertexAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = VertexAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/vertexai/#dynamiq.nodes.llms.vertexai.VertexAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the VertexAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/vertexai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the VertexAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = VertexAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/nodes/llms/watsonx/#dynamiq.nodes.llms.watsonx.WatsonX","title":"<code>WatsonX</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>WatsonX LLM node.</p> <p>This class provides an implementation for the WatsonX Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>WatsonX | None</code> <p>The connection to use for the WatsonX LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the WatsonX model name.</p> Source code in <code>dynamiq/nodes/llms/watsonx.py</code> <pre><code>class WatsonX(BaseLLM):\n    \"\"\"WatsonX LLM node.\n\n    This class provides an implementation for the WatsonX Language Model node.\n\n    Attributes:\n        connection (WatsonXConnection | None): The connection to use for the WatsonX LLM.\n        MODEL_PREFIX (str): The prefix for the WatsonX model name.\n    \"\"\"\n    connection: WatsonXConnection | None = None\n    MODEL_PREFIX = \"watsonx/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the WatsonX LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/watsonx/#dynamiq.nodes.llms.watsonx.WatsonX.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WatsonX LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the WatsonX LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/xai/","title":"Xai","text":""},{"location":"dynamiq/nodes/llms/xai/#dynamiq.nodes.llms.xai.xAI","title":"<code>xAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>xAI LLM node.</p> <p>This class provides an implementation for the xAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>xAI | None</code> <p>The connection to use for the xAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the xAI model name.</p> Source code in <code>dynamiq/nodes/llms/xai.py</code> <pre><code>class xAI(BaseLLM):\n    \"\"\"xAI LLM node.\n\n    This class provides an implementation for the xAI Language Model node.\n\n    Attributes:\n        connection (xAIConnection | None): The connection to use for the xAI LLM.\n        MODEL_PREFIX (str): The prefix for the xAI model name.\n    \"\"\"\n\n    connection: xAIConnection | None = None\n    MODEL_PREFIX = \"xai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the xAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = xAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/xai/#dynamiq.nodes.llms.xai.xAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the xAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/xai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the xAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = xAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/","title":"Operators","text":""},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice","title":"<code>Choice</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a choice node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Choice(Node):\n    \"\"\"Represents a choice node in a flow.\"\"\"\n\n    name: str | None = \"Choice\"\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    options: list[ChoiceOption] = []\n    input_schema: ClassVar[type[ChoiceInputSchema]] = ChoiceInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"options\": True}\n\n    def to_dict(self, include_secure_params: bool = True, for_tracing=False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n        data[\"options\"] = [option.model_dump(**kwargs) for option in self.options]\n        return data\n\n    def execute(\n        self, input_data: ChoiceInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Executes the choice node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A dictionary of RunnableResults for each option.\n        \"\"\"\n        results = {}\n        if self.options:\n            run_id = kwargs.get(\"run_id\", uuid4())\n            config = ensure_config(config)\n            merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n            self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n            is_success_evaluation = False\n            for option in self.options:\n                if is_success_evaluation:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SKIP, input=input_data.model_dump(), output=None\n                    )\n                elif option.condition and self.evaluate(option.condition, input_data.model_dump()):\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SUCCESS, input=input_data.model_dump(), output=True\n                    )\n                    is_success_evaluation = True\n                elif not option.condition:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SUCCESS, input=input_data.model_dump(), output=True\n                    )\n                    is_success_evaluation = True\n                else:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.FAILURE, input=input_data.model_dump(), output=False\n                    )\n\n        return results\n\n    @staticmethod\n    def evaluate(cond: ChoiceCondition, input_data: Any) -&gt; bool:\n        \"\"\"\n        Evaluates a choice condition.\n\n        Args:\n            cond: The condition to evaluate.\n            input_data: The input data to evaluate against.\n\n        Returns:\n            A boolean indicating whether the condition is met.\n\n        Raises:\n            ValueError: If the operator is not supported.\n        \"\"\"\n        value = jsonpath.filter(input_data, cond.variable)\n\n        if cond.operator == ConditionOperator.OR:\n            return (\n                any(Choice.evaluate(cond, value) for cond in cond.operands)\n                and not cond.is_not\n            )\n        elif cond.operator == ConditionOperator.AND:\n            return (\n                all(Choice.evaluate(cond, value) for cond in cond.operands)\n                and not cond.is_not\n            )\n        # boolean\n        elif cond.operator == ConditionOperator.BOOLEAN_EQUALS:\n            return (value == cond.value) == (not cond.is_not)\n        # numeric\n        if cond.operator == ConditionOperator.NUMERIC_EQUALS:\n            return (value == cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN:\n            return (value &gt; cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN_OR_EQUALS:\n            return (value &gt;= cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN:\n            return (value &lt; cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN_OR_EQUALS:\n            return (value &lt;= cond.value) == (not cond.is_not)\n        # string\n        elif cond.operator == ConditionOperator.STRING_EQUALS:\n            return (value == cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_GREATER_THAN:\n            return (value &gt; cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_GREATER_THAN_OR_EQUALS:\n            return (value &gt;= cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_LESS_THAN:\n            return (value &lt; cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_LESS_THAN_OR_EQUALS:\n            return (value &lt;= cond.value) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_STARTS_WITH:\n            return (str(value).startswith(str(cond.value))) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_CONTAINS:\n            return (str(cond.value) in str(value)) == (not cond.is_not)\n        elif cond.operator == ConditionOperator.STRING_REGEXP:\n            try:\n                return bool(re.search(str(cond.value), str(value))) == (not cond.is_not)\n            except re.error as e:\n                raise ValueError(f\"Invalid regular expression '{cond.value}': {e}\")\n        elif cond.operator == ConditionOperator.STRING_ENDS_WITH:\n            return (str(value).endswith(str(cond.value))) == (not cond.is_not)\n        else:\n            raise ValueError(f\"Operator {cond.operator} not supported.\")\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.evaluate","title":"<code>evaluate(cond, input_data)</code>  <code>staticmethod</code>","text":"<p>Evaluates a choice condition.</p> <p>Parameters:</p> Name Type Description Default <code>cond</code> <code>ChoiceCondition</code> <p>The condition to evaluate.</p> required <code>input_data</code> <code>Any</code> <p>The input data to evaluate against.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean indicating whether the condition is met.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the operator is not supported.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>@staticmethod\ndef evaluate(cond: ChoiceCondition, input_data: Any) -&gt; bool:\n    \"\"\"\n    Evaluates a choice condition.\n\n    Args:\n        cond: The condition to evaluate.\n        input_data: The input data to evaluate against.\n\n    Returns:\n        A boolean indicating whether the condition is met.\n\n    Raises:\n        ValueError: If the operator is not supported.\n    \"\"\"\n    value = jsonpath.filter(input_data, cond.variable)\n\n    if cond.operator == ConditionOperator.OR:\n        return (\n            any(Choice.evaluate(cond, value) for cond in cond.operands)\n            and not cond.is_not\n        )\n    elif cond.operator == ConditionOperator.AND:\n        return (\n            all(Choice.evaluate(cond, value) for cond in cond.operands)\n            and not cond.is_not\n        )\n    # boolean\n    elif cond.operator == ConditionOperator.BOOLEAN_EQUALS:\n        return (value == cond.value) == (not cond.is_not)\n    # numeric\n    if cond.operator == ConditionOperator.NUMERIC_EQUALS:\n        return (value == cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN:\n        return (value &gt; cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN_OR_EQUALS:\n        return (value &gt;= cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN:\n        return (value &lt; cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN_OR_EQUALS:\n        return (value &lt;= cond.value) == (not cond.is_not)\n    # string\n    elif cond.operator == ConditionOperator.STRING_EQUALS:\n        return (value == cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_GREATER_THAN:\n        return (value &gt; cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_GREATER_THAN_OR_EQUALS:\n        return (value &gt;= cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_LESS_THAN:\n        return (value &lt; cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_LESS_THAN_OR_EQUALS:\n        return (value &lt;= cond.value) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_STARTS_WITH:\n        return (str(value).startswith(str(cond.value))) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_CONTAINS:\n        return (str(cond.value) in str(value)) == (not cond.is_not)\n    elif cond.operator == ConditionOperator.STRING_REGEXP:\n        try:\n            return bool(re.search(str(cond.value), str(value))) == (not cond.is_not)\n        except re.error as e:\n            raise ValueError(f\"Invalid regular expression '{cond.value}': {e}\")\n    elif cond.operator == ConditionOperator.STRING_ENDS_WITH:\n        return (str(value).endswith(str(cond.value))) == (not cond.is_not)\n    else:\n        raise ValueError(f\"Operator {cond.operator} not supported.\")\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the choice node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ChoiceInputSchema</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>A dictionary of RunnableResults for each option.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(\n    self, input_data: ChoiceInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Executes the choice node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A dictionary of RunnableResults for each option.\n    \"\"\"\n    results = {}\n    if self.options:\n        run_id = kwargs.get(\"run_id\", uuid4())\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n        self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n        is_success_evaluation = False\n        for option in self.options:\n            if is_success_evaluation:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SKIP, input=input_data.model_dump(), output=None\n                )\n            elif option.condition and self.evaluate(option.condition, input_data.model_dump()):\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SUCCESS, input=input_data.model_dump(), output=True\n                )\n                is_success_evaluation = True\n            elif not option.condition:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SUCCESS, input=input_data.model_dump(), output=True\n                )\n                is_success_evaluation = True\n            else:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.FAILURE, input=input_data.model_dump(), output=False\n                )\n\n    return results\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.to_dict","title":"<code>to_dict(include_secure_params=True, for_tracing=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def to_dict(self, include_secure_params: bool = True, for_tracing=False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n    data[\"options\"] = [option.model_dump(**kwargs) for option in self.options]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.ChoiceOption","title":"<code>ChoiceOption</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an option for a choice node.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class ChoiceOption(BaseModel):\n    \"\"\"Represents an option for a choice node.\"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n    name: str | None = None\n    condition: ChoiceCondition | None = None\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map","title":"<code>Map</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a map node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Map(Node):\n    \"\"\"Represents a map node in a flow.\"\"\"\n\n    name: str | None = \"Map\"\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    node: Node\n    behavior: Behavior | None = Behavior.RETURN\n    input_schema: ClassVar[type[MapInputSchema]] = MapInputSchema\n    max_workers: int = 1\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"node\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"node\"] = self.node.to_dict(**kwargs)\n        return data\n\n    def regenerate_ids(self, obj):\n        if isinstance(obj, BaseModel):\n            if hasattr(obj, \"id\"):\n                setattr(obj, \"id\", str(uuid.uuid4()))\n\n            for field_name in obj.model_fields:\n                value = getattr(obj, field_name)\n                if isinstance(value, list):\n                    new_list = [self.regenerate_ids(item) for item in value]\n                    setattr(obj, field_name, new_list)\n                elif isinstance(value, dict):\n                    new_dict = {k: self.regenerate_ids(v) for k, v in value.items()}\n                    setattr(obj, field_name, new_dict)\n                else:\n                    setattr(obj, field_name, self.regenerate_ids(value))\n            return obj\n        elif isinstance(obj, list):\n            return [self.regenerate_ids(item) for item in obj]\n        elif isinstance(obj, dict):\n            return {k: self.regenerate_ids(v) for k, v in obj.items()}\n        else:\n            return obj\n\n    def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n        \"\"\"Clean up resources created during dry run.\"\"\"\n        self.node.dry_run_cleanup(dry_run_config)\n\n    def execute_workflow(self, index, data, config, merged_kwargs):\n        \"\"\"Execute a single workflow and handle errors.\"\"\"\n        node_copy = self.node.clone()\n        node_copy = self.regenerate_ids(node_copy)\n\n        # Create an isolated config per iteration with unique streaming override for the cloned node\n        local_config = config\n        try:\n            local_config = config.model_copy(deep=False) if config is not None else RunnableConfig()\n            if node_config := local_config.nodes_override.get(self.node.id):\n                local_config.nodes_override[node_copy.id] = node_config\n        except Exception as e:\n            logger.warning(f\"Map: failed to prepare isolated streaming config for iteration {index}: {e}\")\n\n        result = node_copy.run(data, local_config, **merged_kwargs)\n        if result.status != RunnableStatus.SUCCESS:\n            if self.behavior == Behavior.RAISE:\n                raise ValueError(f\"Node under iteration index {index + 1} has failed.\")\n        return result.output\n\n    def execute(self, input_data: MapInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Executes the map node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A list of outputs from executing the flow on each input item.\n\n        Raises:\n            Exception: If the input is not a list or if any flow execution fails.\n        \"\"\"\n        input_data = input_data.input\n\n        run_id = kwargs.get(\"run_id\", uuid4())\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        try:\n            with ContextAwareThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                results = executor.map(\n                    lambda args: self.execute_workflow(args[0], args[1], config, merged_kwargs),\n                    enumerate(input_data),\n                )\n        except Exception as e:\n            logger.error(str(e))\n            raise ValueError(f\"Map node failed to execute:{str(e)}\")\n\n        return {\"output\": list(results)}\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.dry_run_cleanup","title":"<code>dry_run_cleanup(dry_run_config=None)</code>","text":"<p>Clean up resources created during dry run.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n    \"\"\"Clean up resources created during dry run.\"\"\"\n    self.node.dry_run_cleanup(dry_run_config)\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the map node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>MapInputSchema</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A list of outputs from executing the flow on each input item.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the input is not a list or if any flow execution fails.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(self, input_data: MapInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Executes the map node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A list of outputs from executing the flow on each input item.\n\n    Raises:\n        Exception: If the input is not a list or if any flow execution fails.\n    \"\"\"\n    input_data = input_data.input\n\n    run_id = kwargs.get(\"run_id\", uuid4())\n    config = ensure_config(config)\n    merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    try:\n        with ContextAwareThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            results = executor.map(\n                lambda args: self.execute_workflow(args[0], args[1], config, merged_kwargs),\n                enumerate(input_data),\n            )\n    except Exception as e:\n        logger.error(str(e))\n        raise ValueError(f\"Map node failed to execute:{str(e)}\")\n\n    return {\"output\": list(results)}\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.execute_workflow","title":"<code>execute_workflow(index, data, config, merged_kwargs)</code>","text":"<p>Execute a single workflow and handle errors.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute_workflow(self, index, data, config, merged_kwargs):\n    \"\"\"Execute a single workflow and handle errors.\"\"\"\n    node_copy = self.node.clone()\n    node_copy = self.regenerate_ids(node_copy)\n\n    # Create an isolated config per iteration with unique streaming override for the cloned node\n    local_config = config\n    try:\n        local_config = config.model_copy(deep=False) if config is not None else RunnableConfig()\n        if node_config := local_config.nodes_override.get(self.node.id):\n            local_config.nodes_override[node_copy.id] = node_config\n    except Exception as e:\n        logger.warning(f\"Map: failed to prepare isolated streaming config for iteration {index}: {e}\")\n\n    result = node_copy.run(data, local_config, **merged_kwargs)\n    if result.status != RunnableStatus.SUCCESS:\n        if self.behavior == Behavior.RAISE:\n            raise ValueError(f\"Node under iteration index {index + 1} has failed.\")\n    return result.output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"node\"] = self.node.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Pass","title":"<code>Pass</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a pass node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Pass(Node):\n    \"\"\"Represents a pass node in a flow.\"\"\"\n\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    transformers: list[Transformer] = []\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Executes the pass node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The input data if no transformers are present, otherwise the transformed data.\n        \"\"\"\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": kwargs.get(\"run_id\", uuid4())}\n        self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n        output = input_data\n        for transformer in self.transformers:\n            output = self.transform(output, transformer)\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Pass.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the pass node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The input data if no transformers are present, otherwise the transformed data.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Executes the pass node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The input data if no transformers are present, otherwise the transformed data.\n    \"\"\"\n    config = ensure_config(config)\n    merged_kwargs = {**kwargs, \"parent_run_id\": kwargs.get(\"run_id\", uuid4())}\n    self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n    output = input_data\n    for transformer in self.transformers:\n        output = self.transform(output, transformer)\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/rankers/cohere/","title":"Cohere","text":""},{"location":"dynamiq/nodes/rankers/cohere/#dynamiq.nodes.rankers.cohere.CohereReranker","title":"<code>CohereReranker</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node class for reranking documents using Cohere's reranking model.</p> <p>This ranker uses Cohere's API to rerank documents based on their relevance to a query.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RANKERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>top_k</code> <code>int</code> <p>The number of top documents to return.</p> <code>model</code> <code>str</code> <p>The Cohere model to use for reranking.</p> <code>threshold</code> <code>float</code> <p>The threshold for relevance score. Default is 0.</p> <code>connection</code> <code>Cohere</code> <p>The Cohere connection instance.</p> Source code in <code>dynamiq/nodes/rankers/cohere.py</code> <pre><code>class CohereReranker(Node):\n    \"\"\"\n    A Node class for reranking documents using Cohere's reranking model.\n\n    This ranker uses Cohere's API to rerank documents based on their relevance to a query.\n\n    Attributes:\n        group (Literal[NodeGroup.RANKERS]): The group the node belongs to.\n        name (str): The name of the node.\n        top_k (int): The number of top documents to return.\n        model (str): The Cohere model to use for reranking.\n        threshold (float): The threshold for relevance score. Default is 0.\n        connection (Cohere): The Cohere connection instance.\n    \"\"\"\n\n    group: Literal[NodeGroup.RANKERS] = NodeGroup.RANKERS\n    name: str = \"CohereReranker\"\n    top_k: int = 5\n    model: str = \"cohere/rerank-v3.5\"\n    threshold: float = 0\n    connection: Cohere\n    input_schema: ClassVar[type[CohereRerankerInputSchema]] = CohereRerankerInputSchema\n    _rerank: Callable = PrivateAttr()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the CohereReranker instance.\"\"\"\n        super().__init__(**kwargs)\n\n        from litellm import rerank\n\n        self._rerank = rerank\n\n    def execute(self, input_data: CohereRerankerInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document reranking process.\n\n        Args:\n            input_data (CohereRerankerInputSchema): The input data containing documents and query.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the reranked documents.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query = input_data.query\n        documents = input_data.documents\n\n        if not documents:\n            logger.warning(f\"Node {self.name} - {self.id}: No documents provided for reranking\")\n            return {\"documents\": []}\n\n        document_texts = [doc.content for doc in documents]\n\n        logger.debug(f\"Node {self.name} - {self.id}: Reranking {len(documents)} documents\")\n\n        response = self._rerank(\n            model=self.model, query=query, documents=document_texts, top_n=self.top_k, **self.connection.conn_params\n        )\n\n        reranked_documents = []\n        for result in response.results:\n            doc = documents[result.get(\"index\")]\n            doc.score = result.get(\"relevance_score\")\n            if doc.score &gt; self.threshold:\n                reranked_documents.append(doc)\n\n        logger.debug(f\"Node {self.name} - {self.id}: Successfully reranked {len(reranked_documents)} documents\")\n\n        return {\"documents\": reranked_documents[: self.top_k]}\n</code></pre>"},{"location":"dynamiq/nodes/rankers/cohere/#dynamiq.nodes.rankers.cohere.CohereReranker.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the CohereReranker instance.</p> Source code in <code>dynamiq/nodes/rankers/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the CohereReranker instance.\"\"\"\n    super().__init__(**kwargs)\n\n    from litellm import rerank\n\n    self._rerank = rerank\n</code></pre>"},{"location":"dynamiq/nodes/rankers/cohere/#dynamiq.nodes.rankers.cohere.CohereReranker.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document reranking process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>CohereRerankerInputSchema</code> <p>The input data containing documents and query.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the reranked documents.</p> Source code in <code>dynamiq/nodes/rankers/cohere.py</code> <pre><code>def execute(self, input_data: CohereRerankerInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document reranking process.\n\n    Args:\n        input_data (CohereRerankerInputSchema): The input data containing documents and query.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the reranked documents.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query = input_data.query\n    documents = input_data.documents\n\n    if not documents:\n        logger.warning(f\"Node {self.name} - {self.id}: No documents provided for reranking\")\n        return {\"documents\": []}\n\n    document_texts = [doc.content for doc in documents]\n\n    logger.debug(f\"Node {self.name} - {self.id}: Reranking {len(documents)} documents\")\n\n    response = self._rerank(\n        model=self.model, query=query, documents=document_texts, top_n=self.top_k, **self.connection.conn_params\n    )\n\n    reranked_documents = []\n    for result in response.results:\n        doc = documents[result.get(\"index\")]\n        doc.score = result.get(\"relevance_score\")\n        if doc.score &gt; self.threshold:\n            reranked_documents.append(doc)\n\n    logger.debug(f\"Node {self.name} - {self.id}: Successfully reranked {len(reranked_documents)} documents\")\n\n    return {\"documents\": reranked_documents[: self.top_k]}\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/","title":"Llm","text":""},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker","title":"<code>LLMDocumentRanker</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node class for ranking documents using a Large Language Model (LLM).</p> <p>This class can use any LLM to rank and select relevant documents based on a query. By default, it utilizes an OpenAI language model.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RANKERS]</code> <p>The group the node belongs to. Default is NodeGroup.RANKERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMDocumentRanker\".</p> <code>prompt_template</code> <code>str</code> <p>The template for the prompt to be used with the LLM. Default is DEFAULT_PROMPT.</p> <code>top_k</code> <code>int</code> <p>The number of top documents to return. Default is 5.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for ranking. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.rankers import LLMDocumentRanker\nfrom dynamiq.types import Document\n\n# Initialize the ranker\nranker = LLMDocumentRanker()\n\n# Example input data\ninput_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\n# Execute the ranker\noutput = ranker.execute(input_data)\n\n# Output will be a dictionary with ranked documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>class LLMDocumentRanker(Node):\n    \"\"\"\n    A Node class for ranking documents using a Large Language Model (LLM).\n\n    This class can use any LLM to rank and select relevant documents based on a query. By default, it utilizes an OpenAI\n    language model.\n\n    Attributes:\n        group (Literal[NodeGroup.RANKERS]): The group the node belongs to. Default is NodeGroup.RANKERS.\n        name (str): The name of the node. Default is \"LLMDocumentRanker\".\n        prompt_template (str): The template for the prompt to be used with the LLM. Default is DEFAULT_PROMPT.\n        top_k (int): The number of top documents to return. Default is 5.\n        llm (BaseLLM): The LLM instance used for ranking. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.rankers import LLMDocumentRanker\n        from dynamiq.types import Document\n\n        # Initialize the ranker\n        ranker = LLMDocumentRanker()\n\n        # Example input data\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        # Execute the ranker\n        output = ranker.execute(input_data)\n\n        # Output will be a dictionary with ranked documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.RANKERS] = NodeGroup.RANKERS\n    name: str = \"LLMDocumentRanker\"\n    prompt_template: str = DEFAULT_PROMPT\n    top_k: int = 5\n    llm: Node\n    input_schema: ClassVar[type[LLMDocumentRankerInputSchema]] = LLMDocumentRankerInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the document ranker component.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def execute(\n        self, input_data: LLMDocumentRankerInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the document ranking process.\n\n        Args:\n            input_data (LLMDocumentRankerInputSchema): A dictionary containing the query and documents to be ranked.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n        Example:\n\n            input_data = {\n                \"query\": \"example query\",\n                \"documents\": [\n                    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n                ]\n            }\n\n            output = ranker.execute(input_data)\n\n            # output will be a dictionary with ranked documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        ranked_documents = self.perform_llm_ranking(\n            query=input_data.query,\n            documents=input_data.documents,\n            config=config,\n            **kwargs,\n        )\n\n        return {\n            \"documents\": ranked_documents,\n        }\n\n    def perform_llm_ranking(\n        self, query: str, documents: list[Document], config: RunnableConfig, **kwargs\n    ) -&gt; list[Document]:\n        \"\"\"\n        Performs the actual ranking of documents using the LLM.\n\n        Args:\n            query (str): The query to rank documents against.\n            documents (list[Document]): The list of documents to be ranked.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of selected documents deemed relevant by the LLM.\n\n        Example:\n\n            query = \"example query\"\n            documents = [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n\n            ranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n            # ranked_documents will be a list of documents deemed relevant by the LLM\n        \"\"\"\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        inputs = [\n            {\"query\": query, \"passage\": document.content} for document in documents\n        ]\n\n        prompt = prompts.Prompt(\n            messages=[prompts.Message(role=\"user\", content=self.prompt_template)]\n        )\n\n        with ContextAwareThreadPoolExecutor() as executor:\n            llm_results = list(\n                executor.map(\n                    lambda input_data: self.call_llm(input_data, prompt, config, **run_kwargs),\n                    inputs,\n                )\n            )\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} documents\"\n        )\n\n        selected_documents = []\n\n        for result, document in zip(llm_results, documents):\n            if result == \"Yes\":\n                selected_documents.append(document)\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM selected {len(selected_documents)} documents for context\"\n        )\n        return selected_documents\n\n    def call_llm(self, input_data, prompt, config, **run_kwargs):\n        \"\"\"\n        Calls the LLM with the given input data and prompt.\n\n        Args:\n            input_data (dict): The input data for the LLM.\n            prompt (prompts.Prompt): The prompt to be used with the LLM.\n            config (RunnableConfig): Configuration for the execution.\n            **run_kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The result from the LLM.\n\n        Example:\n\n            input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\n            prompt = prompts.Prompt(\n                messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n            )\n            config = RunnableConfig()\n\n            result = ranker.call_llm(input_data, prompt, config)\n\n            # result will be the LLM's response, either 'Yes' or 'No'\n        \"\"\"\n        llm_result = self.llm.run(\n            input_data=input_data,\n            prompt=prompt,\n            config=config,\n            run_depends=self._run_depends,\n            **run_kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n        if llm_result.status != RunnableStatus.SUCCESS:\n            logger.error(f\"Node {self.name} - {self.id}: LLM execution failed: {llm_result.error.to_dict()}\")\n            raise ValueError(\"LLMDocumentRanker LLM execution failed\")\n        return llm_result.output[\"content\"]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.call_llm","title":"<code>call_llm(input_data, prompt, config, **run_kwargs)</code>","text":"<p>Calls the LLM with the given input data and prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>The input data for the LLM.</p> required <code>prompt</code> <code>Prompt</code> <p>The prompt to be used with the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**run_kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The result from the LLM.</p> <p>Example:</p> <pre><code>input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\nprompt = prompts.Prompt(\n    messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n)\nconfig = RunnableConfig()\n\nresult = ranker.call_llm(input_data, prompt, config)\n\n# result will be the LLM's response, either 'Yes' or 'No'\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def call_llm(self, input_data, prompt, config, **run_kwargs):\n    \"\"\"\n    Calls the LLM with the given input data and prompt.\n\n    Args:\n        input_data (dict): The input data for the LLM.\n        prompt (prompts.Prompt): The prompt to be used with the LLM.\n        config (RunnableConfig): Configuration for the execution.\n        **run_kwargs: Additional keyword arguments.\n\n    Returns:\n        str: The result from the LLM.\n\n    Example:\n\n        input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\n        prompt = prompts.Prompt(\n            messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n        )\n        config = RunnableConfig()\n\n        result = ranker.call_llm(input_data, prompt, config)\n\n        # result will be the LLM's response, either 'Yes' or 'No'\n    \"\"\"\n    llm_result = self.llm.run(\n        input_data=input_data,\n        prompt=prompt,\n        config=config,\n        run_depends=self._run_depends,\n        **run_kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n    if llm_result.status != RunnableStatus.SUCCESS:\n        logger.error(f\"Node {self.name} - {self.id}: LLM execution failed: {llm_result.error.to_dict()}\")\n        raise ValueError(\"LLMDocumentRanker LLM execution failed\")\n    return llm_result.output[\"content\"]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document ranking process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>LLMDocumentRankerInputSchema</code> <p>A dictionary containing the query and documents to be ranked.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the original query and the ranked documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\noutput = ranker.execute(input_data)\n\n# output will be a dictionary with ranked documents\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def execute(\n    self, input_data: LLMDocumentRankerInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the document ranking process.\n\n    Args:\n        input_data (LLMDocumentRankerInputSchema): A dictionary containing the query and documents to be ranked.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n    Example:\n\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        output = ranker.execute(input_data)\n\n        # output will be a dictionary with ranked documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    ranked_documents = self.perform_llm_ranking(\n        query=input_data.query,\n        documents=input_data.documents,\n        config=config,\n        **kwargs,\n    )\n\n    return {\n        \"documents\": ranked_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the document ranker component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Default is a new instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the document ranker component.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.perform_llm_ranking","title":"<code>perform_llm_ranking(query, documents, config, **kwargs)</code>","text":"<p>Performs the actual ranking of documents using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to rank documents against.</p> required <code>documents</code> <code>list[Document]</code> <p>The list of documents to be ranked.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of selected documents deemed relevant by the LLM.</p> <p>Example:</p> <pre><code>query = \"example query\"\ndocuments = [\n    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n]\n\nranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n# ranked_documents will be a list of documents deemed relevant by the LLM\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def perform_llm_ranking(\n    self, query: str, documents: list[Document], config: RunnableConfig, **kwargs\n) -&gt; list[Document]:\n    \"\"\"\n    Performs the actual ranking of documents using the LLM.\n\n    Args:\n        query (str): The query to rank documents against.\n        documents (list[Document]): The list of documents to be ranked.\n        config (RunnableConfig): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of selected documents deemed relevant by the LLM.\n\n    Example:\n\n        query = \"example query\"\n        documents = [\n            Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n            Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n        ]\n\n        ranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n        # ranked_documents will be a list of documents deemed relevant by the LLM\n    \"\"\"\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    inputs = [\n        {\"query\": query, \"passage\": document.content} for document in documents\n    ]\n\n    prompt = prompts.Prompt(\n        messages=[prompts.Message(role=\"user\", content=self.prompt_template)]\n    )\n\n    with ContextAwareThreadPoolExecutor() as executor:\n        llm_results = list(\n            executor.map(\n                lambda input_data: self.call_llm(input_data, prompt, config, **run_kwargs),\n                inputs,\n            )\n        )\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} documents\"\n    )\n\n    selected_documents = []\n\n    for result, document in zip(llm_results, documents):\n        if result == \"Yes\":\n            selected_documents.append(document)\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM selected {len(selected_documents)} documents for context\"\n    )\n    return selected_documents\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/","title":"Recency","text":""},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker","title":"<code>TimeWeightedDocumentRanker</code>","text":"<p>               Bases: <code>Node</code></p> <p>A document ranker node boosting the recent content more.</p> <p>This ranker adjusts the initial scores of documents based on their recency. The recency coefficient depends on the number of days from today. The initial score is multiplied by the recency coefficient, and the documents are re-ranked based on the adjusted score.</p> The formula for the adjustment is <p>adjusted_score = score * recency_coefficient</p> The recency coefficient is calculated as follows <p>min_coefficient &lt;= coefficient &lt;= 1 (if the same date)</p> <p>The coefficient is determined based on the number of days since the content was created.</p> <p>An exponential decay formula is used to ensure that the coefficient decreases as the number of days increases, but never goes below the specified minimum coefficient.</p> The formula used is <p>coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)</p> This ensures that <ul> <li>If days &lt;= 0, the coefficient is 1.0 (no decay).</li> <li>If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).</li> <li>For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.</li> </ul> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RANKERS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>top_k</code> <code>int</code> <p>The number of top documents to return. Default is 5.</p> <code>max_days</code> <code>int</code> <p>The maximum number of days to consider for adjustment. Default is 3600.</p> <code>min_coefficient</code> <code>float</code> <p>The minimum coefficient for score adjustment. Default is 0.9.</p> <code>date_field</code> <code>str</code> <p>The field name in the metadata containing the date. Default is \"date\".</p> <code>date_format</code> <code>str</code> <p>The format of the date string. Default is \"%d %B, %Y\".</p> <p>Example:</p> <pre><code>from dynamiq.nodes.rankers import TimeWeightedDocumentRanker\nfrom dynamiq.types import Document\n\n# Initialize the ranker\nranker = TimeWeightedDocumentRanker()\n\n# Example input data\ninput_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\n# Execute the ranker\noutput = ranker.execute(input_data)\n\n# Output will be a dictionary with ranked documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>class TimeWeightedDocumentRanker(Node):\n    \"\"\"\n    A document ranker node boosting the recent content more.\n\n    This ranker adjusts the initial scores of documents based on their recency. The recency coefficient\n    depends on the number of days from today. The initial score is multiplied by the recency coefficient,\n    and the documents are re-ranked based on the adjusted score.\n\n    The formula for the adjustment is:\n        adjusted_score = score * recency_coefficient\n\n    The recency coefficient is calculated as follows:\n        min_coefficient &lt;= coefficient &lt;= 1 (if the same date)\n\n    The coefficient is determined based on the number of days since the content was created.\n\n    An exponential decay formula is used to ensure that the coefficient decreases as the number of days\n    increases, but never goes below the specified minimum coefficient.\n\n    The formula used is:\n        coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n    This ensures that:\n        - If days &lt;= 0, the coefficient is 1.0 (no decay).\n        - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n        - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n    Attributes:\n        group (Literal[NodeGroup.RANKERS]): The group this node belongs to.\n        name (str): The name of the node.\n        top_k (int): The number of top documents to return. Default is 5.\n        max_days (int): The maximum number of days to consider for adjustment. Default is 3600.\n        min_coefficient (float): The minimum coefficient for score adjustment. Default is 0.9.\n        date_field (str): The field name in the metadata containing the date. Default is \"date\".\n        date_format (str): The format of the date string. Default is \"%d %B, %Y\".\n\n    Example:\n\n        from dynamiq.nodes.rankers import TimeWeightedDocumentRanker\n        from dynamiq.types import Document\n\n        # Initialize the ranker\n        ranker = TimeWeightedDocumentRanker()\n\n        # Example input data\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        # Execute the ranker\n        output = ranker.execute(input_data)\n\n        # Output will be a dictionary with ranked documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.RANKERS] = NodeGroup.RANKERS\n    name: str = \"Time Weighted Document Ranker\"\n    top_k: int = 5\n    max_days: int = 3600\n    min_coefficient: float = 0.9\n    date_field: str = \"date\"\n    date_format: str = \"%d %B, %Y\"\n    input_schema: ClassVar[type[TimeWeightedDocumentRankerInputSchema]] = TimeWeightedDocumentRankerInputSchema\n\n    def execute(\n        self, input_data: TimeWeightedDocumentRankerInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document ranking process.\n\n        Args:\n            input_data (TimeWeightedDocumentRankerInputSchema): The input data containing documents and query.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n        Example:\n\n            input_data = {\n                \"documents\": [\n                    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n                ]\n            }\n\n            output = ranker.execute(input_data)\n\n            # output will be a dictionary with ranked documents\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n\n        ranked_documents = self.adjust_similarity_scores(\n            documents,\n            date_field=self.date_field,\n            max_days=self.max_days,\n            min_coefficient=self.min_coefficient,\n            date_format=self.date_format,\n        )\n\n        return {\n            \"documents\": ranked_documents,\n        }\n\n    @staticmethod\n    def date_to_days(date_string: str, date_format: str = \"%d %B, %Y\") -&gt; int:\n        \"\"\"\n        Convert a date string to the number of days since that date.\n\n        Args:\n            date_string (str): Date in the format \"dd Month, YYYY\"\n            date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n        Returns:\n            int: Number of days since the given date.\n\n        Example:\n\n            days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n            # days will be the number of days since 01 January, 2022\n        \"\"\"\n        date_object = datetime.strptime(date_string, date_format)\n        current_date = datetime.now()\n        return (current_date - date_object).days\n\n    @staticmethod\n    def days_to_coefficient(\n        days: int, max_days: int = 3600, min_coefficient: float = 0.1\n    ) -&gt; float:\n        \"\"\"\n        Transform number of days into a coefficient for score adjustment.\n\n        The coefficient is calculated based on the number of days since the content was created.\n\n        The function uses an exponential decay formula to ensure that the coefficient decreases\n        as the number of days increases, but never goes below the specified minimum coefficient.\n\n        The formula used is:\n            coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n        This ensures that:\n            - If days &lt;= 0, the coefficient is 1.0 (no decay).\n            - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n            - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n        Args:\n            days (int): Number of days since the content was created.\n            max_days (int): Maximum number of days to consider (default: 3600, about 12 years).\n            min_coefficient (float): Minimum coefficient value (default: 0.1).\n\n        Returns:\n            float: Coefficient between min_coefficient and 1.\n\n        Example:\n\n            coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n            # coefficient will be a value between 0.1 and 1 based on the number of days\n        \"\"\"\n        if days &lt;= 0:\n            return 1.0\n        elif days &gt;= max_days:\n            return min_coefficient\n        else:\n            return min_coefficient + (1 - min_coefficient) * math.exp(\n                -3 * days / max_days\n            )\n\n    @staticmethod\n    def adjust_similarity_scores(\n        candidates: list[Document],\n        date_field: str = \"date\",\n        max_days: int = 3600,\n        min_coefficient: float = 0.9,\n        date_format: str = \"%d %B, %Y\",\n    ) -&gt; list[Document]:\n        \"\"\"\n        Adjust cosine similarity scores based on content recency.\n\n        Args:\n            candidates (list[Document]): List of Document objects containing candidates with 'score' and date fields.\n            date_field (str): Name of the field containing the date string (default: 'date').\n            max_days (int): Maximum number of days to consider for adjustment.\n            min_coefficient (float): Minimum coefficient for score adjustment.\n            date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n        Returns:\n            list[Document]: List of candidates with adjusted scores, sorted by the new scores.\n\n        Example:\n\n            candidates = [\n                Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n            ]\n\n            adjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n            # adjusted_candidates will be sorted by adjusted scores\n        \"\"\"\n        for candidate in candidates:\n            if date := candidate.metadata.get(date_field):\n                days = TimeWeightedDocumentRanker.date_to_days(\n                    date,\n                    date_format=date_format,\n                )\n                coefficient = TimeWeightedDocumentRanker.days_to_coefficient(\n                    days, max_days=max_days, min_coefficient=min_coefficient\n                )\n                candidate.score = candidate.score * coefficient\n\n        documents = [\n            {\"score\": candidate.score, \"document\": candidate}\n            for candidate in candidates\n        ]\n\n        sorted_documents = sorted(documents, key=lambda x: x[\"score\"], reverse=True)\n\n        return [document[\"document\"] for document in sorted_documents]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.adjust_similarity_scores","title":"<code>adjust_similarity_scores(candidates, date_field='date', max_days=3600, min_coefficient=0.9, date_format='%d %B, %Y')</code>  <code>staticmethod</code>","text":"<p>Adjust cosine similarity scores based on content recency.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>list[Document]</code> <p>List of Document objects containing candidates with 'score' and date fields.</p> required <code>date_field</code> <code>str</code> <p>Name of the field containing the date string (default: 'date').</p> <code>'date'</code> <code>max_days</code> <code>int</code> <p>Maximum number of days to consider for adjustment.</p> <code>3600</code> <code>min_coefficient</code> <code>float</code> <p>Minimum coefficient for score adjustment.</p> <code>0.9</code> <code>date_format</code> <code>str</code> <p>The format of the date string (default: \"%d %B, %Y\").</p> <code>'%d %B, %Y'</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of candidates with adjusted scores, sorted by the new scores.</p> <p>Example:</p> <pre><code>candidates = [\n    Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n    Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n]\n\nadjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n# adjusted_candidates will be sorted by adjusted scores\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef adjust_similarity_scores(\n    candidates: list[Document],\n    date_field: str = \"date\",\n    max_days: int = 3600,\n    min_coefficient: float = 0.9,\n    date_format: str = \"%d %B, %Y\",\n) -&gt; list[Document]:\n    \"\"\"\n    Adjust cosine similarity scores based on content recency.\n\n    Args:\n        candidates (list[Document]): List of Document objects containing candidates with 'score' and date fields.\n        date_field (str): Name of the field containing the date string (default: 'date').\n        max_days (int): Maximum number of days to consider for adjustment.\n        min_coefficient (float): Minimum coefficient for score adjustment.\n        date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n    Returns:\n        list[Document]: List of candidates with adjusted scores, sorted by the new scores.\n\n    Example:\n\n        candidates = [\n            Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n            Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n        ]\n\n        adjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n        # adjusted_candidates will be sorted by adjusted scores\n    \"\"\"\n    for candidate in candidates:\n        if date := candidate.metadata.get(date_field):\n            days = TimeWeightedDocumentRanker.date_to_days(\n                date,\n                date_format=date_format,\n            )\n            coefficient = TimeWeightedDocumentRanker.days_to_coefficient(\n                days, max_days=max_days, min_coefficient=min_coefficient\n            )\n            candidate.score = candidate.score * coefficient\n\n    documents = [\n        {\"score\": candidate.score, \"document\": candidate}\n        for candidate in candidates\n    ]\n\n    sorted_documents = sorted(documents, key=lambda x: x[\"score\"], reverse=True)\n\n    return [document[\"document\"] for document in sorted_documents]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.date_to_days","title":"<code>date_to_days(date_string, date_format='%d %B, %Y')</code>  <code>staticmethod</code>","text":"<p>Convert a date string to the number of days since that date.</p> <p>Parameters:</p> Name Type Description Default <code>date_string</code> <code>str</code> <p>Date in the format \"dd Month, YYYY\"</p> required <code>date_format</code> <code>str</code> <p>The format of the date string (default: \"%d %B, %Y\").</p> <code>'%d %B, %Y'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of days since the given date.</p> <p>Example:</p> <pre><code>days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n# days will be the number of days since 01 January, 2022\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef date_to_days(date_string: str, date_format: str = \"%d %B, %Y\") -&gt; int:\n    \"\"\"\n    Convert a date string to the number of days since that date.\n\n    Args:\n        date_string (str): Date in the format \"dd Month, YYYY\"\n        date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n    Returns:\n        int: Number of days since the given date.\n\n    Example:\n\n        days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n        # days will be the number of days since 01 January, 2022\n    \"\"\"\n    date_object = datetime.strptime(date_string, date_format)\n    current_date = datetime.now()\n    return (current_date - date_object).days\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.days_to_coefficient","title":"<code>days_to_coefficient(days, max_days=3600, min_coefficient=0.1)</code>  <code>staticmethod</code>","text":"<p>Transform number of days into a coefficient for score adjustment.</p> <p>The coefficient is calculated based on the number of days since the content was created.</p> <p>The function uses an exponential decay formula to ensure that the coefficient decreases as the number of days increases, but never goes below the specified minimum coefficient.</p> The formula used is <p>coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)</p> This ensures that <ul> <li>If days &lt;= 0, the coefficient is 1.0 (no decay).</li> <li>If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).</li> <li>For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Number of days since the content was created.</p> required <code>max_days</code> <code>int</code> <p>Maximum number of days to consider (default: 3600, about 12 years).</p> <code>3600</code> <code>min_coefficient</code> <code>float</code> <p>Minimum coefficient value (default: 0.1).</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Coefficient between min_coefficient and 1.</p> <p>Example:</p> <pre><code>coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n# coefficient will be a value between 0.1 and 1 based on the number of days\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef days_to_coefficient(\n    days: int, max_days: int = 3600, min_coefficient: float = 0.1\n) -&gt; float:\n    \"\"\"\n    Transform number of days into a coefficient for score adjustment.\n\n    The coefficient is calculated based on the number of days since the content was created.\n\n    The function uses an exponential decay formula to ensure that the coefficient decreases\n    as the number of days increases, but never goes below the specified minimum coefficient.\n\n    The formula used is:\n        coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n    This ensures that:\n        - If days &lt;= 0, the coefficient is 1.0 (no decay).\n        - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n        - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n    Args:\n        days (int): Number of days since the content was created.\n        max_days (int): Maximum number of days to consider (default: 3600, about 12 years).\n        min_coefficient (float): Minimum coefficient value (default: 0.1).\n\n    Returns:\n        float: Coefficient between min_coefficient and 1.\n\n    Example:\n\n        coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n        # coefficient will be a value between 0.1 and 1 based on the number of days\n    \"\"\"\n    if days &lt;= 0:\n        return 1.0\n    elif days &gt;= max_days:\n        return min_coefficient\n    else:\n        return min_coefficient + (1 - min_coefficient) * math.exp(\n            -3 * days / max_days\n        )\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document ranking process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TimeWeightedDocumentRankerInputSchema</code> <p>The input data containing documents and query.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the original query and the ranked documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\noutput = ranker.execute(input_data)\n\n# output will be a dictionary with ranked documents\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>def execute(\n    self, input_data: TimeWeightedDocumentRankerInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document ranking process.\n\n    Args:\n        input_data (TimeWeightedDocumentRankerInputSchema): The input data containing documents and query.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n    Example:\n\n        input_data = {\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        output = ranker.execute(input_data)\n\n        # output will be a dictionary with ranked documents\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n\n    ranked_documents = self.adjust_similarity_scores(\n        documents,\n        date_field=self.date_field,\n        max_days=self.max_days,\n        min_coefficient=self.min_coefficient,\n        date_format=self.date_format,\n    )\n\n    return {\n        \"documents\": ranked_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/base/","title":"Base","text":""},{"location":"dynamiq/nodes/retrievers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever","title":"<code>ChromaDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code></p> <p>Document Retriever using Chroma.</p> <p>This class implements a document retriever that uses Chroma as the underlying vector store. It extends the VectorStoreNode class and provides functionality to retrieve documents based on vector similarity.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>ChromaVectorStore | None</code> <p>The ChromaVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>ChromaDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>class ChromaDocumentRetriever(Retriever):\n    \"\"\"\n    Document Retriever using Chroma.\n\n    This class implements a document retriever that uses Chroma as the underlying vector store.\n    It extends the VectorStoreNode class and provides functionality to retrieve documents\n    based on vector similarity.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (ChromaVectorStore | None): The ChromaVectorStore instance.\n        filters (dict[str, Any] | None): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (ChromaDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    name: str = \"ChromaDocumentRetriever\"\n    connection: Chroma | None = None\n    vector_store: ChromaVectorStore | None = None\n    document_retriever: ChromaDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ChromaDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Chroma()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ChromaVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include={\"index_name\"}) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the ChromaDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = ChromaDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method takes an input embedding, retrieves similar documents using the\n        document retriever component, and returns the retrieved documents.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        output = self.document_retriever.run(\n            query_embedding,\n            filters=filters,\n            top_k=top_k,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ChromaDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ChromaDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Chroma()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes an input embedding, retrieves similar documents using the document retriever component, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method takes an input embedding, retrieves similar documents using the\n    document retriever component, and returns the retrieved documents.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    output = self.document_retriever.run(\n        query_embedding,\n        filters=filters,\n        top_k=top_k,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the ChromaDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the ChromaDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = ChromaDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/elasticsearch/","title":"Elasticsearch","text":""},{"location":"dynamiq/nodes/retrievers/elasticsearch/#dynamiq.nodes.retrievers.elasticsearch.ElasticsearchDocumentRetriever","title":"<code>ElasticsearchDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>ElasticsearchVectorStoreParams</code></p> <p>Document Retriever using Elasticsearch for vector similarity search.</p> <p>This class implements a document retriever that uses Elasticsearch as the underlying store for vector similarity search with optional metadata filtering.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>Optional[ElasticsearchVectorStore]</code> <p>The ElasticsearchVectorStore instance.</p> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>ElasticsearchDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/elasticsearch.py</code> <pre><code>class ElasticsearchDocumentRetriever(Retriever, ElasticsearchVectorStoreParams):\n    \"\"\"\n    Document Retriever using Elasticsearch for vector similarity search.\n\n    This class implements a document retriever that uses Elasticsearch as the underlying store\n    for vector similarity search with optional metadata filtering.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (Optional[ElasticsearchVectorStore]): The ElasticsearchVectorStore instance.\n        filters (Optional[dict[str, Any]]): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (ElasticsearchDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    name: str = \"ElasticsearchDocumentRetriever\"\n    connection: Elasticsearch | None = None\n    vector_store: ElasticsearchVectorStore | None = None\n    document_retriever: ElasticsearchDocumentRetrieverComponent | None = None\n    input_schema = ElasticsearchRetrieverInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ElasticsearchDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs,\n        a default Elasticsearch connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Elasticsearch()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ElasticsearchVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(ElasticsearchVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"Initialize the components of the ElasticsearchDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = ElasticsearchDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(\n        self, input_data: ElasticsearchRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the document retrieval process.\n\n        This method takes input data containing the vector query and search parameters,\n        retrieves relevant documents using vector similarity search,\n        and returns the retrieved documents.\n\n        Args:\n            input_data (ElasticsearchRetrieverInputSchema): The input data containing:\n            config (Optional[RunnableConfig]): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_retriever.run(\n            query_embedding=input_data.query_embedding,\n            filters=input_data.filters or self.filters,\n            top_k=input_data.top_k or self.top_k,\n            exclude_document_embeddings=input_data.exclude_document_embeddings,\n            scale_scores=input_data.scale_scores,\n            content_key=input_data.content_key,\n            embedding_key=input_data.embedding_key,\n            similarity_threshold=(\n                input_data.similarity_threshold\n                if input_data.similarity_threshold is not None\n                else self.similarity_threshold\n            ),\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/elasticsearch/#dynamiq.nodes.retrievers.elasticsearch.ElasticsearchDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElasticsearchDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Elasticsearch connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/elasticsearch.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ElasticsearchDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs,\n    a default Elasticsearch connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Elasticsearch()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/elasticsearch/#dynamiq.nodes.retrievers.elasticsearch.ElasticsearchDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes input data containing the vector query and search parameters, retrieves relevant documents using vector similarity search, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ElasticsearchRetrieverInputSchema</code> <p>The input data containing:</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/elasticsearch.py</code> <pre><code>def execute(\n    self, input_data: ElasticsearchRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the document retrieval process.\n\n    This method takes input data containing the vector query and search parameters,\n    retrieves relevant documents using vector similarity search,\n    and returns the retrieved documents.\n\n    Args:\n        input_data (ElasticsearchRetrieverInputSchema): The input data containing:\n        config (Optional[RunnableConfig]): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_retriever.run(\n        query_embedding=input_data.query_embedding,\n        filters=input_data.filters or self.filters,\n        top_k=input_data.top_k or self.top_k,\n        exclude_document_embeddings=input_data.exclude_document_embeddings,\n        scale_scores=input_data.scale_scores,\n        content_key=input_data.content_key,\n        embedding_key=input_data.embedding_key,\n        similarity_threshold=(\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        ),\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/elasticsearch/#dynamiq.nodes.retrievers.elasticsearch.ElasticsearchDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the ElasticsearchDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/elasticsearch.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"Initialize the components of the ElasticsearchDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = ElasticsearchDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/elasticsearch/#dynamiq.nodes.retrievers.elasticsearch.ElasticsearchRetrieverInputSchema","title":"<code>ElasticsearchRetrieverInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for Elasticsearch retriever.</p> <p>Attributes:</p> Name Type Description <code>query_embedding</code> <code>list[float]</code> <p>Vector query for similarity search.</p> <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to an empty dictionary.</p> <code>top_k</code> <code>int</code> <p>Number of documents to retrieve. Defaults to 0.</p> <code>exclude_document_embeddings</code> <code>bool</code> <p>Whether to exclude embeddings in the response. Defaults to True.</p> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to the 0-1 range. Defaults to False.</p> <code>content_key</code> <code>str</code> <p>Key to use for content in the response. Defaults to \"content\".</p> <code>embedding_key</code> <code>str</code> <p>Key to use for embedding in the response. Defaults to \"embedding\".</p> Source code in <code>dynamiq/nodes/retrievers/elasticsearch.py</code> <pre><code>class ElasticsearchRetrieverInputSchema(BaseModel):\n    \"\"\"\n    Input schema for Elasticsearch retriever.\n\n    Attributes:\n        query_embedding (list[float]): Vector query for similarity search.\n        filters (dict[str, Any]): Filters to apply for retrieving specific documents. Defaults to an empty dictionary.\n        top_k (int): Number of documents to retrieve. Defaults to 0.\n        exclude_document_embeddings (bool): Whether to exclude embeddings in the response. Defaults to True.\n        scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n        content_key (str): Key to use for content in the response. Defaults to \"content\".\n        embedding_key (str): Key to use for embedding in the response. Defaults to \"embedding\".\n    \"\"\"\n\n    query_embedding: list[float] = Field(..., description=\"Vector query for similarity search\")\n    filters: dict[str, Any] = Field(default={}, description=\"Filters to apply for retrieving specific documents\")\n    top_k: int = Field(default=0, description=\"Number of documents to retrieve\")\n    exclude_document_embeddings: bool = Field(default=True, description=\"Whether to exclude embeddings in response\")\n    scale_scores: bool = Field(default=False, description=\"Whether to scale scores to 0-1 range\")\n    content_key: str = Field(default=\"content\", description=\"Key to use for content in response\")\n    embedding_key: str = Field(default=\"embedding\", description=\"Key to use for embedding in response\")\n    similarity_threshold: float | None = Field(\n        default=None,\n        description=\"Minimal similarity or maximal distance score accepted for retrieved documents.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever","title":"<code>MilvusDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>MilvusVectorStoreParams</code></p> <p>Document Retriever using Milvus.</p> <p>This class implements a document retriever that uses Milvus as the underlying vector store. It extends the VectorStoreNode class and provides functionality to retrieve documents based on vector similarity.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>MilvusVectorStore | None</code> <p>The MilvusVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>MilvusDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>class MilvusDocumentRetriever(Retriever, MilvusVectorStoreParams):\n    \"\"\"\n    Document Retriever using Milvus.\n\n    This class implements a document retriever that uses Milvus as the underlying vector store.\n    It extends the VectorStoreNode class and provides functionality to retrieve documents\n    based on vector similarity.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (MilvusVectorStore | None): The MilvusVectorStore instance.\n        filters (dict[str, Any] | None): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (MilvusDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    name: str = \"MilvusDocumentRetriever\"\n    connection: Milvus | None = None\n    vector_store: MilvusVectorStore | None = None\n    document_retriever: MilvusDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the MilvusDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Milvus()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return MilvusVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(MilvusVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the MilvusDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = MilvusDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method takes an input embedding, retrieves similar documents using the\n        document retriever component, and returns the retrieved documents.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        query = input_data.query\n        content_key = input_data.content_key\n        embedding_key = input_data.embedding_key\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        output = self.document_retriever.run(\n            query_embedding,\n            query=query,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            embedding_key=embedding_key,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MilvusDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the MilvusDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Milvus()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes an input embedding, retrieves similar documents using the document retriever component, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method takes an input embedding, retrieves similar documents using the\n    document retriever component, and returns the retrieved documents.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    query = input_data.query\n    content_key = input_data.content_key\n    embedding_key = input_data.embedding_key\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    output = self.document_retriever.run(\n        query_embedding,\n        query=query,\n        filters=filters,\n        top_k=top_k,\n        content_key=content_key,\n        embedding_key=embedding_key,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the MilvusDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the MilvusDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = MilvusDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/opensearch/","title":"Opensearch","text":""},{"location":"dynamiq/nodes/retrievers/opensearch/#dynamiq.nodes.retrievers.opensearch.OpenSearchDocumentRetriever","title":"<code>OpenSearchDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>OpenSearchVectorStoreParams</code></p> <p>Document Retriever using AWS OpenSearch for vector similarity search.</p> <p>This class implements a document retriever that uses AWS OpenSearch as the underlying store for vector similarity search with optional metadata filtering.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>Optional[OpenSearchVectorStore]</code> <p>The OpenSearchVectorStore instance.</p> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>OpenSearchDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/opensearch.py</code> <pre><code>class OpenSearchDocumentRetriever(Retriever, OpenSearchVectorStoreParams):\n    \"\"\"\n    Document Retriever using AWS OpenSearch for vector similarity search.\n\n    This class implements a document retriever that uses AWS OpenSearch as the underlying store\n    for vector similarity search with optional metadata filtering.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (Optional[OpenSearchVectorStore]): The OpenSearchVectorStore instance.\n        filters (Optional[dict[str, Any]]): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (OpenSearchDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    name: str = \"OpenSearchDocumentRetriever\"\n    connection: AWSOpenSearch | None = None\n    vector_store: OpenSearchVectorStore | None = None\n    document_retriever: OpenSearchDocumentRetrieverComponent | None = None\n    input_schema = OpenSearchRetrieverInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the OpenSearchDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs,\n        a default AWSOpenSearch connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AWSOpenSearch()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return OpenSearchVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(OpenSearchVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"Initialize the components of the OpenSearchDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = OpenSearchDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(\n        self, input_data: OpenSearchRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the document retrieval process.\n\n        This method takes input data containing the vector query and search parameters,\n        retrieves relevant documents using vector similarity search,\n        and returns the retrieved documents.\n\n        Args:\n            input_data (OpenSearchRetrieverInputSchema): The input data containing:\n            config (Optional[RunnableConfig]): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_retriever.run(\n            query_embedding=input_data.query_embedding,\n            filters=input_data.filters or self.filters,\n            top_k=input_data.top_k or self.top_k,\n            exclude_document_embeddings=input_data.exclude_document_embeddings,\n            scale_scores=input_data.scale_scores,\n            content_key=input_data.content_key,\n            embedding_key=input_data.embedding_key,\n            similarity_threshold=(\n                input_data.similarity_threshold\n                if input_data.similarity_threshold is not None\n                else self.similarity_threshold\n            ),\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/opensearch/#dynamiq.nodes.retrievers.opensearch.OpenSearchDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenSearchDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default AWSOpenSearch connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/opensearch.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the OpenSearchDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs,\n    a default AWSOpenSearch connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AWSOpenSearch()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/opensearch/#dynamiq.nodes.retrievers.opensearch.OpenSearchDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes input data containing the vector query and search parameters, retrieves relevant documents using vector similarity search, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>OpenSearchRetrieverInputSchema</code> <p>The input data containing:</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/opensearch.py</code> <pre><code>def execute(\n    self, input_data: OpenSearchRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the document retrieval process.\n\n    This method takes input data containing the vector query and search parameters,\n    retrieves relevant documents using vector similarity search,\n    and returns the retrieved documents.\n\n    Args:\n        input_data (OpenSearchRetrieverInputSchema): The input data containing:\n        config (Optional[RunnableConfig]): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_retriever.run(\n        query_embedding=input_data.query_embedding,\n        filters=input_data.filters or self.filters,\n        top_k=input_data.top_k or self.top_k,\n        exclude_document_embeddings=input_data.exclude_document_embeddings,\n        scale_scores=input_data.scale_scores,\n        content_key=input_data.content_key,\n        embedding_key=input_data.embedding_key,\n        similarity_threshold=(\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        ),\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/opensearch/#dynamiq.nodes.retrievers.opensearch.OpenSearchDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the OpenSearchDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/opensearch.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"Initialize the components of the OpenSearchDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = OpenSearchDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/opensearch/#dynamiq.nodes.retrievers.opensearch.OpenSearchRetrieverInputSchema","title":"<code>OpenSearchRetrieverInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for OpenSearch retriever.</p> <p>Attributes:</p> Name Type Description <code>query_embedding</code> <code>list[float]</code> <p>Vector query for similarity search.</p> <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to an empty dictionary.</p> <code>top_k</code> <code>int</code> <p>Number of documents to retrieve. Defaults to 0.</p> <code>exclude_document_embeddings</code> <code>bool</code> <p>Whether to exclude embeddings in the response. Defaults to True.</p> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to the 0-1 range. Defaults to False.</p> <code>content_key</code> <code>str</code> <p>Key to use for content in the response. Defaults to \"content\".</p> <code>embedding_key</code> <code>str</code> <p>Key to use for embedding in the response. Defaults to \"embedding\".</p> Source code in <code>dynamiq/nodes/retrievers/opensearch.py</code> <pre><code>class OpenSearchRetrieverInputSchema(BaseModel):\n    \"\"\"\n    Input schema for OpenSearch retriever.\n\n    Attributes:\n        query_embedding (list[float]): Vector query for similarity search.\n        filters (dict[str, Any]): Filters to apply for retrieving specific documents. Defaults to an empty dictionary.\n        top_k (int): Number of documents to retrieve. Defaults to 0.\n        exclude_document_embeddings (bool): Whether to exclude embeddings in the response. Defaults to True.\n        scale_scores (bool): Whether to scale scores to the 0-1 range. Defaults to False.\n        content_key (str): Key to use for content in the response. Defaults to \"content\".\n        embedding_key (str): Key to use for embedding in the response. Defaults to \"embedding\".\n    \"\"\"\n\n    query_embedding: list[float] = Field(..., description=\"Vector query for similarity search\")\n    filters: dict[str, Any] = Field(default={}, description=\"Filters to apply for retrieving specific documents\")\n    top_k: int = Field(default=0, description=\"Number of documents to retrieve\")\n    exclude_document_embeddings: bool = Field(default=True, description=\"Whether to exclude embeddings in response\")\n    scale_scores: bool = Field(default=False, description=\"Whether to scale scores to 0-1 range\")\n    content_key: str = Field(default=\"content\", description=\"Key to use for content in response\")\n    embedding_key: str = Field(default=\"embedding\", description=\"Key to use for embedding in response\")\n    similarity_threshold: float | None = Field(\n        default=None,\n        description=\"Minimal similarity or maximal distance score accepted for retrieved documents.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pgvector/","title":"Pgvector","text":""},{"location":"dynamiq/nodes/retrievers/pgvector/#dynamiq.nodes.retrievers.pgvector.PGVectorDocumentRetriever","title":"<code>PGVectorDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>PGVectorStoreRetrieverParams</code></p> <p>Document Retriever using PGVector.</p> <p>This class implements a document retriever that uses PGVector as the underlying vector store. It extends the VectorStoreNode class and provides functionality to retrieve documents based on vector similarity.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>PGVectorStore | None</code> <p>The PGVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>PGVectorDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pgvector.py</code> <pre><code>class PGVectorDocumentRetriever(Retriever, PGVectorStoreRetrieverParams):\n    \"\"\"\n    Document Retriever using PGVector.\n\n    This class implements a document retriever that uses PGVector as the underlying vector store.\n    It extends the VectorStoreNode class and provides functionality to retrieve documents\n    based on vector similarity.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (PGVectorStore | None): The PGVectorStore instance.\n        filters (dict[str, Any] | None): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (PGVectorDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    name: str = \"PGVectorDocumentRetriever\"\n    connection: PostgreSQL | None = None\n    vector_store: PGVectorStore | None = None\n    document_retriever: PGVectorDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PGVectorDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = PostgreSQL()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return PGVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PGVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initialize the components of the PGVectorDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = PGVectorDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method takes an input embedding, retrieves similar documents using the\n        document retriever component, and returns the retrieved documents.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        content_key = input_data.content_key\n        embedding_key = input_data.embedding_key\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        alpha = input_data.alpha or self.alpha\n        query = input_data.query\n\n        output = self.document_retriever.run(\n            query_embedding,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            embedding_key=embedding_key,\n            query=query,\n            alpha=alpha,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pgvector/#dynamiq.nodes.retrievers.pgvector.PGVectorDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PGVectorDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pgvector.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PGVectorDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = PostgreSQL()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pgvector/#dynamiq.nodes.retrievers.pgvector.PGVectorDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes an input embedding, retrieves similar documents using the document retriever component, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/pgvector.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method takes an input embedding, retrieves similar documents using the\n    document retriever component, and returns the retrieved documents.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    content_key = input_data.content_key\n    embedding_key = input_data.embedding_key\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    alpha = input_data.alpha or self.alpha\n    query = input_data.query\n\n    output = self.document_retriever.run(\n        query_embedding,\n        filters=filters,\n        top_k=top_k,\n        content_key=content_key,\n        embedding_key=embedding_key,\n        query=query,\n        alpha=alpha,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pgvector/#dynamiq.nodes.retrievers.pgvector.PGVectorDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the PGVectorDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/pgvector.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initialize the components of the PGVectorDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = PGVectorDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever","title":"<code>PineconeDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>PineconeVectorStoreParams</code></p> <p>Document Retriever using Pinecone.</p> <p>This class implements a document retriever that uses Pinecone as the vector store backend.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>PineconeVectorStore | None</code> <p>The Pinecone vector store.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply for retrieving specific documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>PineconeDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>class PineconeDocumentRetriever(Retriever, PineconeVectorStoreParams):\n    \"\"\"Document Retriever using Pinecone.\n\n    This class implements a document retriever that uses Pinecone as the vector store backend.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (PineconeVectorStore | None): The Pinecone vector store.\n        filters (dict[str, Any] | None): Filters to apply for retrieving specific documents.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (PineconeDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    name: str = \"PineconeDocumentRetriever\"\n    connection: Pinecone | None = None\n    vector_store: PineconeVectorStore | None = None\n    document_retriever: PineconeDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PineconeDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Pinecone()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return PineconeVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PineconeVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the PineconeDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = PineconeDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        content_key = input_data.content_key\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        output = self.document_retriever.run(\n            query_embedding,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PineconeDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PineconeDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Pinecone()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    content_key = input_data.content_key\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    output = self.document_retriever.run(\n        query_embedding,\n        filters=filters,\n        top_k=top_k,\n        content_key=content_key,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the PineconeDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the PineconeDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = PineconeDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever","title":"<code>QdrantDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code></p> <p>Document Retriever using Qdrant.</p> <p>This class implements a document retriever that uses Qdrant as the vector store backend.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>QdrantVectorStore</code> <p>An instance of QdrantVectorStore to interface with Qdrant vectors.</p> required <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>QdrantVectorStore | None</code> <p>The QdrantVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters for document retrieval.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>QdrantDocumentRetriever</code> <p>The document retriever component.</p> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>class QdrantDocumentRetriever(Retriever):\n    \"\"\"Document Retriever using Qdrant.\n\n    This class implements a document retriever that uses Qdrant as the vector store backend.\n\n    Args:\n        vector_store (QdrantVectorStore, optional): An instance of QdrantVectorStore to interface\n            with Qdrant vectors.\n        filters (dict[str, Any], optional): Filters to apply for retrieving specific documents.\n            Defaults to None.\n        top_k (int, optional): The maximum number of documents to return. Defaults to 10.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (QdrantVectorStore | None): The QdrantVectorStore instance.\n        filters (dict[str, Any] | None): Filters for document retrieval.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (QdrantDocumentRetrieverComponent): The document retriever component.\n    \"\"\"\n\n    name: str = \"QdrantDocumentRetriever\"\n    connection: Qdrant | None = None\n    vector_store: QdrantVectorStore | None = None\n    document_retriever: QdrantDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the QdrantDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the retriever.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Qdrant()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return QdrantVectorStore\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the retriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = QdrantDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        content_key = input_data.content_key\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        output = self.document_retriever.run(\n            query_embedding,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the QdrantDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the retriever.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the QdrantDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the retriever.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Qdrant()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    content_key = input_data.content_key\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    output = self.document_retriever.run(\n        query_embedding,\n        filters=filters,\n        top_k=top_k,\n        content_key=content_key,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the retriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the retriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = QdrantDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/","title":"Retriever","text":""},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever","title":"<code>VectorStoreRetriever</code>","text":"<p>               Bases: <code>Node</code></p> <p>Node for retrieving relevant documents based on a query.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>Group for the node. Defaults to NodeGroup.TOOLS.</p> <code>name</code> <code>str</code> <p>Name of the tool. Defaults to \"Retrieval Tool\".</p> <code>description</code> <code>str</code> <p>Description of the tool.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>text_embedder</code> <code>TextEmbedder</code> <p>Text embedder node.</p> <code>document_retriever</code> <code>Retriever</code> <p>Document retriever node.</p> <code>document_reranker</code> <code>Node | None</code> <p>Optional document_reranker node for reranking retrieved documents.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters for document retrieval.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>alpha</code> <code>float</code> <p>The alpha parameter for hybrid retrieval.</p> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>class VectorStoreRetriever(Node):\n    \"\"\"Node for retrieving relevant documents based on a query.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): Group for the node. Defaults to NodeGroup.TOOLS.\n        name (str): Name of the tool. Defaults to \"Retrieval Tool\".\n        description (str): Description of the tool.\n        error_handling (ErrorHandling): Error handling configuration.\n        text_embedder (TextEmbedder): Text embedder node.\n        document_retriever (Retriever): Document retriever node.\n        document_reranker (Node | None): Optional document_reranker node for reranking retrieved documents.\n        filters (dict[str, Any] | None): Filters for document retrieval.\n        top_k (int): The maximum number of documents to return.\n        alpha (float): The alpha parameter for hybrid retrieval.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.SEMANTIC_SEARCH\n    name: str = \"VectorStore Retriever\"\n    description: str = \"A node for retrieving relevant documents based on a query.\"\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    text_embedder: TextEmbedder\n    document_retriever: Retriever\n    document_reranker: Node | None = None\n    filters: dict[str, Any] = {}\n    top_k: int | None = None\n    alpha: float = 0.0\n    similarity_threshold: float | None = None\n\n    input_schema: ClassVar[type[VectorStoreRetrieverInputSchema]] = VectorStoreRetrieverInputSchema\n    _EXCLUDED_METADATA_FIELDS: ClassVar[tuple[str, ...]] = (\n        \"embedding\",\n        \"embeddings\",\n        \"vector\",\n        \"vectors\",\n    )\n    _EXCLUDED_METADATA_TOKENS: ClassVar[tuple[str, ...]] = (\"id\", \"hash\")\n    _EXPECTED_METADATA_KEYWORDS: ClassVar[tuple[str, ...]] = (\"url\", \"link\", \"source\", \"file\", \"title\")\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the VectorStoreRetriever with the given parameters.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize the components of the tool.\n\n        Args:\n            connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.text_embedder.is_postponed_component_init:\n            self.text_embedder.init_components(connection_manager)\n        if self.document_retriever.is_postponed_component_init:\n            self.document_retriever.init_components(connection_manager)\n        if self.document_reranker and self.document_reranker.is_postponed_component_init:\n            self.document_reranker.init_components(connection_manager)\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\n            \"text_embedder\": True,\n            \"document_retriever\": True,\n            \"document_reranker\": True,\n        }\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"text_embedder\"] = self.text_embedder.to_dict(**kwargs)\n        data[\"document_retriever\"] = self.document_retriever.to_dict(**kwargs)\n        if self.document_reranker:\n            data[\"document_reranker\"] = self.document_reranker.to_dict(**kwargs)\n        return data\n\n    def format_content(self, documents: list[Document], metadata_fields: list[str] | None = None) -&gt; str:\n        \"\"\"Format the retrieved documents' metadata and content.\n\n        Args:\n            documents (list[Document]): List of retrieved documents.\n            metadata_fields (list[str]): Metadata fields to include. If None, uses all metadata except embeddings.\n\n        Returns:\n            str: Formatted content of the documents.\n        \"\"\"\n        formatted_docs: list[str] = []\n\n        normalized_metadata_fields: list[str] | None = None\n        include_score = False\n        if metadata_fields is not None:\n            seen_fields: set[str] = set()\n            cleaned_fields: list[str] = []\n            for field in metadata_fields:\n                stripped = field.strip() if field else \"\"\n                if not stripped:\n                    continue\n                lowered = stripped.lower()\n                if lowered in seen_fields:\n                    continue\n                if lowered == \"score\":\n                    include_score = True\n                    seen_fields.add(lowered)\n                    continue\n                seen_fields.add(lowered)\n                cleaned_fields.append(stripped)\n\n            if cleaned_fields:\n                normalized_metadata_fields = cleaned_fields\n            elif include_score:\n                normalized_metadata_fields = []\n\n        for index, doc in enumerate(documents):\n            metadata = doc.metadata or {}\n            metadata_lines: list[str] = []\n\n            if normalized_metadata_fields is not None:\n                if include_score and doc.score is not None:\n                    metadata_lines.append(self._format_metadata_line(\"Score\", doc.score))\n            else:\n                if doc.score is not None:\n                    metadata_lines.append(self._format_metadata_line(\"Score\", doc.score))\n\n            metadata_lines.extend(self._generate_metadata_lines(metadata, normalized_metadata_fields))\n\n            metadata_block = \"\\n\\n\".join(metadata_lines) if metadata_lines else \"No metadata available.\"\n            content_block = doc.content or \"\"\n\n            formatted_doc = (\n                f\"\\n\\n== Source {index + 1} ==\\n\\n\"\n                f\"\\n\\n== Metadata ==\\n{metadata_block}\\n\\n\"\n                f\"\\n\\n== Content (Source {index + 1}) ==\\n\\n{content_block}\"\n            ).rstrip()\n            formatted_docs.append(formatted_doc)\n\n        return \"\\n\\n\".join(formatted_docs)\n\n    @staticmethod\n    def _prettify_field_name(field_name: str) -&gt; str:\n        if not field_name:\n            return field_name\n\n        cleaned = field_name.replace(\"_\", \" \").strip()\n        lowered = cleaned.lower()\n        if lowered.startswith(\"dynamiq\"):\n            cleaned = cleaned[len(\"dynamiq\") :].lstrip(\" -_/\")\n\n        prettified = cleaned.strip().title()\n        return prettified or field_name\n\n    @classmethod\n    def _format_metadata_line(cls, field: str, value: Any) -&gt; str:\n        formatted_value = cls._stringify_metadata_value(value)\n        return f\"{field}: {formatted_value}\"\n\n    @classmethod\n    def _stringify_metadata_value(cls, value: Any) -&gt; str:\n        if isinstance(value, (dict, list)):\n            try:\n                serialized = json.dumps(value, indent=2, sort_keys=True)\n                return cls._postprocess_metadata_string(serialized)\n            except (TypeError, ValueError):\n                return cls._postprocess_metadata_string(str(value))\n        return cls._postprocess_metadata_string(str(value))\n\n    @staticmethod\n    def _postprocess_metadata_string(value: str) -&gt; str:\n        if not value:\n            return value\n\n        if value.lower().startswith(\"dynamiq\"):\n            trimmed = value[7:]\n            return trimmed.lstrip(\"/\\\\ \")\n        return value\n\n    def _resolve_metadata_path(\n        self,\n        metadata: dict[str, Any],\n        field: str,\n    ) -&gt; tuple[Any | None, list[str]]:\n        if not metadata:\n            return None, []\n\n        parts = field.split(\".\")\n        current: Any = metadata\n        actual_path: list[str] = []\n\n        for part in parts:\n            if not isinstance(current, dict):\n                return None, []\n\n            matching_key = next((key for key in current.keys() if key.lower() == part.lower()), None)\n            if matching_key is None:\n                return None, []\n\n            actual_path.append(matching_key)\n            current = current[matching_key]\n\n        return current, actual_path\n\n    def _iter_metadata_entries(\n        self,\n        metadata: dict[str, Any],\n        requested_fields: list[str] | None,\n    ) -&gt; Iterator[tuple[list[str], list[str], Any]]:\n        if not metadata:\n            return\n\n        if requested_fields is None:\n            for key, value in metadata.items():\n                if self._should_exclude_metadata_key(key):\n                    continue\n                yield from self._flatten_metadata(value, [self._prettify_field_name(key)], [key])\n            return\n\n        for field in requested_fields:\n            value, path = self._resolve_metadata_path(metadata, field)\n            if not path:\n                continue\n\n            if any(self._should_exclude_metadata_key(part) for part in path):\n                continue\n\n            label_parts = [self._prettify_field_name(part) for part in path]\n            yield from self._flatten_metadata(value, label_parts, path)\n\n    def _flatten_metadata(\n        self,\n        value: Any,\n        label_parts: list[str],\n        raw_parts: list[str],\n    ) -&gt; Iterator[tuple[list[str], list[str], Any]]:\n        if isinstance(value, dict):\n            for key, nested_value in value.items():\n                if self._should_exclude_metadata_key(key):\n                    continue\n                yield from self._flatten_metadata(\n                    nested_value,\n                    label_parts + [self._prettify_field_name(key)],\n                    raw_parts + [key],\n                )\n            return\n\n        yield (label_parts or [\"Metadata\"]), raw_parts, value\n\n    def _generate_metadata_lines(\n        self,\n        metadata: dict[str, Any],\n        requested_fields: list[str] | None,\n    ) -&gt; list[str]:\n        if not metadata:\n            return []\n\n        base_entries = list(self._iter_metadata_entries(metadata, requested_fields))\n\n        expected_source_entries = (\n            base_entries if requested_fields is None else list(self._iter_metadata_entries(metadata, None))\n        )\n\n        expected_entries: list[tuple[list[str], list[str], Any]] = []\n        expected_paths: set[tuple[str, ...]] = set()\n        for display_parts, raw_parts, value in expected_source_entries:\n            if not self._contains_expected_keyword(raw_parts):\n                continue\n            path_key = self._normalize_raw_path(raw_parts)\n            if path_key in expected_paths:\n                continue\n            expected_entries.append((display_parts, raw_parts, value))\n            expected_paths.add(path_key)\n\n        seen_paths: set[tuple[str, ...]] = set()\n        lines_by_path: dict[tuple[str, ...], str] = {}\n        general_lines: list[str] = []\n\n        for display_parts, raw_parts, value in base_entries:\n            path_key = self._normalize_raw_path(raw_parts)\n            if path_key in seen_paths:\n                continue\n            seen_paths.add(path_key)\n            line = self._format_metadata_line(\" - \".join(display_parts), value)\n            lines_by_path[path_key] = line\n            if path_key not in expected_paths:\n                general_lines.append(line)\n\n        expected_lines: list[str] = []\n        for display_parts, raw_parts, value in expected_entries:\n            path_key = self._normalize_raw_path(raw_parts)\n            if path_key not in seen_paths:\n                line = self._format_metadata_line(\" - \".join(display_parts), value)\n                lines_by_path[path_key] = line\n                seen_paths.add(path_key)\n            expected_lines.append(lines_by_path[path_key])\n\n        return general_lines + expected_lines\n\n    @staticmethod\n    def _normalize_raw_path(raw_parts: list[str]) -&gt; tuple[str, ...]:\n        return tuple(part.lower() for part in raw_parts)\n\n    @classmethod\n    def _should_exclude_metadata_key(cls, key: str) -&gt; bool:\n        if not key:\n            return False\n\n        lowered_key = key.lower()\n        if lowered_key in cls._EXCLUDED_METADATA_FIELDS:\n            return True\n\n        tokens = cls._tokenize_metadata_key(key)\n        return any(token in cls._EXCLUDED_METADATA_TOKENS for token in tokens)\n\n    @classmethod\n    def _contains_expected_keyword(cls, raw_parts: list[str]) -&gt; bool:\n        for part in raw_parts:\n            tokens = cls._tokenize_metadata_key(part)\n            if any(token in cls._EXPECTED_METADATA_KEYWORDS for token in tokens):\n                return True\n        return False\n\n    @staticmethod\n    def _tokenize_metadata_key(key: str) -&gt; list[str]:\n        if not key:\n            return []\n\n        normalized = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", key)\n        normalized = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", normalized)\n        return [token for token in normalized.lower().split(\"_\") if token]\n\n    def execute(\n        self, input_data: VectorStoreRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the retrieval tool.\n\n        Args:\n            input_data (dict[str, Any]): Input data for the tool.\n            config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Result of the retrieval.\n        \"\"\"\n\n        logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        alpha = input_data.alpha or self.alpha\n        query = input_data.query\n        try:\n            kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n            kwargs.pop(\"run_depends\", None)\n            text_embedder_output = self.text_embedder.run(\n                input_data={\"query\": query}, run_depends=self._run_depends, config=config, **kwargs\n            )\n            self._run_depends = [NodeDependency(node=self.text_embedder).to_dict(for_tracing=True)]\n            embedding = text_embedder_output.output.get(\"embedding\")\n\n            document_retriever_output = self.document_retriever.run(\n                input_data={\n                    \"embedding\": embedding,\n                    **({\"top_k\": top_k} if top_k else {}),\n                    \"filters\": filters,\n                    \"alpha\": alpha,\n                    **({\"query\": query} if alpha else {}),\n                    **({\"similarity_threshold\": similarity_threshold} if similarity_threshold is not None else {}),\n                },\n                run_depends=self._run_depends,\n                config=config,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.document_retriever).to_dict(for_tracing=True)]\n            retrieved_documents = document_retriever_output.output.get(\"documents\", [])\n            logger.info(f\"Tool {self.name} - {self.id}: retrieved {len(retrieved_documents)} documents\")\n\n            if self.document_reranker and retrieved_documents:\n                docs_before_rerank = len(retrieved_documents)\n                logger.info(\n                    f\"Tool {self.name} - {self.id}: Applying document_reranker '{self.document_reranker.name}' \"\n                    f\"to {docs_before_rerank} documents\"\n                )\n                document_reranker_result = self.document_reranker.run(\n                    input_data={\"query\": query, \"documents\": retrieved_documents},\n                    run_depends=self._run_depends,\n                    config=config,\n                    **kwargs,\n                )\n                self._run_depends = [NodeDependency(node=self.document_reranker).to_dict(for_tracing=True)]\n                retrieved_documents = document_reranker_result.output.get(\"documents\", [])\n                logger.info(\n                    f\"Tool {self.name} - {self.id}: Document_reranker finished. \"\n                    f\"Documents: {docs_before_rerank} -&gt; {len(retrieved_documents)}\"\n                )\n\n            result = self.format_content(retrieved_documents)\n            logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n            return {\"content\": result, \"documents\": retrieved_documents}\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to retrieve data using the specified action. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the VectorStoreRetriever with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the VectorStoreRetriever with the given parameters.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the retrieval tool.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the tool.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable, including callbacks.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Result of the retrieval.</p> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def execute(\n    self, input_data: VectorStoreRetrieverInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the retrieval tool.\n\n    Args:\n        input_data (dict[str, Any]): Input data for the tool.\n        config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Result of the retrieval.\n    \"\"\"\n\n    logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    alpha = input_data.alpha or self.alpha\n    query = input_data.query\n    try:\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n        text_embedder_output = self.text_embedder.run(\n            input_data={\"query\": query}, run_depends=self._run_depends, config=config, **kwargs\n        )\n        self._run_depends = [NodeDependency(node=self.text_embedder).to_dict(for_tracing=True)]\n        embedding = text_embedder_output.output.get(\"embedding\")\n\n        document_retriever_output = self.document_retriever.run(\n            input_data={\n                \"embedding\": embedding,\n                **({\"top_k\": top_k} if top_k else {}),\n                \"filters\": filters,\n                \"alpha\": alpha,\n                **({\"query\": query} if alpha else {}),\n                **({\"similarity_threshold\": similarity_threshold} if similarity_threshold is not None else {}),\n            },\n            run_depends=self._run_depends,\n            config=config,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.document_retriever).to_dict(for_tracing=True)]\n        retrieved_documents = document_retriever_output.output.get(\"documents\", [])\n        logger.info(f\"Tool {self.name} - {self.id}: retrieved {len(retrieved_documents)} documents\")\n\n        if self.document_reranker and retrieved_documents:\n            docs_before_rerank = len(retrieved_documents)\n            logger.info(\n                f\"Tool {self.name} - {self.id}: Applying document_reranker '{self.document_reranker.name}' \"\n                f\"to {docs_before_rerank} documents\"\n            )\n            document_reranker_result = self.document_reranker.run(\n                input_data={\"query\": query, \"documents\": retrieved_documents},\n                run_depends=self._run_depends,\n                config=config,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.document_reranker).to_dict(for_tracing=True)]\n            retrieved_documents = document_reranker_result.output.get(\"documents\", [])\n            logger.info(\n                f\"Tool {self.name} - {self.id}: Document_reranker finished. \"\n                f\"Documents: {docs_before_rerank} -&gt; {len(retrieved_documents)}\"\n            )\n\n        result = self.format_content(retrieved_documents)\n        logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n        return {\"content\": result, \"documents\": retrieved_documents}\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to retrieve data using the specified action. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.format_content","title":"<code>format_content(documents, metadata_fields=None)</code>","text":"<p>Format the retrieved documents' metadata and content.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of retrieved documents.</p> required <code>metadata_fields</code> <code>list[str]</code> <p>Metadata fields to include. If None, uses all metadata except embeddings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted content of the documents.</p> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def format_content(self, documents: list[Document], metadata_fields: list[str] | None = None) -&gt; str:\n    \"\"\"Format the retrieved documents' metadata and content.\n\n    Args:\n        documents (list[Document]): List of retrieved documents.\n        metadata_fields (list[str]): Metadata fields to include. If None, uses all metadata except embeddings.\n\n    Returns:\n        str: Formatted content of the documents.\n    \"\"\"\n    formatted_docs: list[str] = []\n\n    normalized_metadata_fields: list[str] | None = None\n    include_score = False\n    if metadata_fields is not None:\n        seen_fields: set[str] = set()\n        cleaned_fields: list[str] = []\n        for field in metadata_fields:\n            stripped = field.strip() if field else \"\"\n            if not stripped:\n                continue\n            lowered = stripped.lower()\n            if lowered in seen_fields:\n                continue\n            if lowered == \"score\":\n                include_score = True\n                seen_fields.add(lowered)\n                continue\n            seen_fields.add(lowered)\n            cleaned_fields.append(stripped)\n\n        if cleaned_fields:\n            normalized_metadata_fields = cleaned_fields\n        elif include_score:\n            normalized_metadata_fields = []\n\n    for index, doc in enumerate(documents):\n        metadata = doc.metadata or {}\n        metadata_lines: list[str] = []\n\n        if normalized_metadata_fields is not None:\n            if include_score and doc.score is not None:\n                metadata_lines.append(self._format_metadata_line(\"Score\", doc.score))\n        else:\n            if doc.score is not None:\n                metadata_lines.append(self._format_metadata_line(\"Score\", doc.score))\n\n        metadata_lines.extend(self._generate_metadata_lines(metadata, normalized_metadata_fields))\n\n        metadata_block = \"\\n\\n\".join(metadata_lines) if metadata_lines else \"No metadata available.\"\n        content_block = doc.content or \"\"\n\n        formatted_doc = (\n            f\"\\n\\n== Source {index + 1} ==\\n\\n\"\n            f\"\\n\\n== Metadata ==\\n{metadata_block}\\n\\n\"\n            f\"\\n\\n== Content (Source {index + 1}) ==\\n\\n{content_block}\"\n        ).rstrip()\n        formatted_docs.append(formatted_doc)\n\n    return \"\\n\\n\".join(formatted_docs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize the components of the tool.\n\n    Args:\n        connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.text_embedder.is_postponed_component_init:\n        self.text_embedder.init_components(connection_manager)\n    if self.document_retriever.is_postponed_component_init:\n        self.document_retriever.init_components(connection_manager)\n    if self.document_reranker and self.document_reranker.is_postponed_component_init:\n        self.document_reranker.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/retriever/#dynamiq.nodes.retrievers.retriever.VectorStoreRetriever.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/retrievers/retriever.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"text_embedder\"] = self.text_embedder.to_dict(**kwargs)\n    data[\"document_retriever\"] = self.document_retriever.to_dict(**kwargs)\n    if self.document_reranker:\n        data[\"document_reranker\"] = self.document_reranker.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever","title":"<code>WeaviateDocumentRetriever</code>","text":"<p>               Bases: <code>Retriever</code>, <code>WeaviateRetrieverVectorStoreParams</code></p> <p>Document Retriever using Weaviate.</p> <p>This class implements a document retriever that uses Weaviate as the vector store backend.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>WeaviateVectorStore</code> <p>An instance of WeaviateVectorStore to interface with Weaviate vectors.</p> required <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>WeaviateVectorStore | None</code> <p>The WeaviateVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters for document retrieval.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>WeaviateDocumentRetriever</code> <p>The document retriever component.</p> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>class WeaviateDocumentRetriever(Retriever, WeaviateRetrieverVectorStoreParams):\n    \"\"\"Document Retriever using Weaviate.\n\n    This class implements a document retriever that uses Weaviate as the vector store backend.\n\n    Args:\n        vector_store (WeaviateVectorStore, optional): An instance of WeaviateVectorStore to interface\n            with Weaviate vectors.\n        filters (dict[str, Any], optional): Filters to apply for retrieving specific documents.\n            Defaults to None.\n        top_k (int, optional): The maximum number of documents to return. Defaults to 10.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (WeaviateVectorStore | None): The WeaviateVectorStore instance.\n        filters (dict[str, Any] | None): Filters for document retrieval.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (WeaviateDocumentRetrieverComponent): The document retriever component.\n    \"\"\"\n\n    name: str = \"WeaviateDocumentRetriever\"\n    connection: Weaviate | None = None\n    vector_store: WeaviateVectorStore | None = None\n    document_retriever: WeaviateDocumentRetrieverComponent | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the WeaviateDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the retriever.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Weaviate()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return WeaviateVectorStore\n\n    @property\n    def vector_store_params(self):\n        params = self.model_dump(include=set(WeaviateRetrieverVectorStoreParams.model_fields))\n        params.update(\n            {\n                \"connection\": self.connection,\n                \"client\": self.client,\n            }\n        )\n        return params\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the retriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = WeaviateDocumentRetrieverComponent(\n                vector_store=self.vector_store,\n                filters=self.filters,\n                top_k=self.top_k,\n                similarity_threshold=self.similarity_threshold,\n            )\n\n    def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (RetrieverInputSchema): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data.embedding\n        content_key = input_data.content_key\n        filters = input_data.filters or self.filters\n        top_k = input_data.top_k or self.top_k\n        similarity_threshold = (\n            input_data.similarity_threshold\n            if input_data.similarity_threshold is not None\n            else self.similarity_threshold\n        )\n\n        alpha = input_data.alpha or self.alpha\n        query = input_data.query\n\n        output = self.document_retriever.run(\n            query_embedding,\n            filters=filters,\n            top_k=top_k,\n            content_key=content_key,\n            query=query,\n            alpha=alpha,\n            similarity_threshold=similarity_threshold,\n        )\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WeaviateDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the retriever.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the WeaviateDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the retriever.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Weaviate()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>RetrieverInputSchema</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def execute(self, input_data: RetrieverInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (RetrieverInputSchema): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data.embedding\n    content_key = input_data.content_key\n    filters = input_data.filters or self.filters\n    top_k = input_data.top_k or self.top_k\n    similarity_threshold = (\n        input_data.similarity_threshold\n        if input_data.similarity_threshold is not None\n        else self.similarity_threshold\n    )\n\n    alpha = input_data.alpha or self.alpha\n    query = input_data.query\n\n    output = self.document_retriever.run(\n        query_embedding,\n        filters=filters,\n        top_k=top_k,\n        content_key=content_key,\n        query=query,\n        alpha=alpha,\n        similarity_threshold=similarity_threshold,\n    )\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the retriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the retriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = WeaviateDocumentRetrieverComponent(\n            vector_store=self.vector_store,\n            filters=self.filters,\n            top_k=self.top_k,\n            similarity_threshold=self.similarity_threshold,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/","title":"Document","text":""},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter","title":"<code>DocumentSplitter</code>","text":"<p>               Bases: <code>Node</code></p> <p>Splits a list of text documents into a list of text documents with shorter texts.</p> <p>Splitting documents with long texts is a common preprocessing step during indexing. This allows Embedders to create significant semantic representations and avoids exceeding the maximum context length of language models.</p> <p>Parameters:</p> Name Type Description Default <code>split_by</code> <code>Literal['word', 'sentence', 'page', 'passage']</code> <p>Determines the unit by which the document should be split. Defaults to \"word\".</p> required <code>split_length</code> <code>int</code> <p>Maximum number of units (as defined by <code>split_by</code>) to include in each split. Defaults to 200.</p> required <code>split_overlap</code> <code>int</code> <p>Number of units that should overlap between consecutive splits. Defaults to 0.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[SPLITTERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>split_by</code> <code>DocumentSplitBy</code> <p>The unit by which the document should be split.</p> <code>split_length</code> <code>int</code> <p>The maximum number of units to include in each split.</p> <code>split_overlap</code> <code>int</code> <p>The number of units that should overlap between consecutive splits.</p> <code>document_splitter</code> <code>DocumentSplitter</code> <p>The component used for document splitting.</p> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>class DocumentSplitter(Node):\n    \"\"\"Splits a list of text documents into a list of text documents with shorter texts.\n\n    Splitting documents with long texts is a common preprocessing step during indexing.\n    This allows Embedders to create significant semantic representations\n    and avoids exceeding the maximum context length of language models.\n\n    Args:\n        split_by (Literal[\"word\", \"sentence\", \"page\", \"passage\"], optional): Determines the unit by\n            which the document should be split. Defaults to \"word\".\n        split_length (int, optional): Maximum number of units (as defined by `split_by`) to include\n            in each split. Defaults to 200.\n        split_overlap (int, optional): Number of units that should overlap between consecutive\n            splits. Defaults to 0.\n\n    Attributes:\n        group (Literal[NodeGroup.SPLITTERS]): The group of the node.\n        name (str): The name of the node.\n        split_by (DocumentSplitBy): The unit by which the document should be split.\n        split_length (int): The maximum number of units to include in each split.\n        split_overlap (int): The number of units that should overlap between consecutive splits.\n        document_splitter (DocumentSplitterComponent): The component used for document splitting.\n    \"\"\"\n\n    group: Literal[NodeGroup.SPLITTERS] = NodeGroup.SPLITTERS\n    name: str = \"DocumentSplitter\"\n    split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE\n    split_length: int = 10\n    split_overlap: int = 0\n    document_splitter: DocumentSplitterComponent = None\n    input_schema: ClassVar[type[DocumentSplitterInputSchema]] = DocumentSplitterInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_splitter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"Initializes the components of the DocumentSplitter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to ConnectionManager().\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_splitter is None:\n            self.document_splitter = DocumentSplitterComponent(\n                split_by=self.split_by,\n                split_length=self.split_length,\n                split_overlap=self.split_overlap,\n            )\n\n    def execute(\n        self, input_data: DocumentSplitterInputSchema, config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Executes the document splitting process.\n\n        Args:\n            input_data (DocumentSplitterInputSchema): The input data containing the documents to split.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        logger.debug(f\"Splitting {len(documents)} documents\")\n        output = self.document_splitter.run(documents=documents)\n\n        split_documents = output[\"documents\"]\n        logger.debug(\n            f\"Split {len(documents)} documents into {len(split_documents)} parts\"\n        )\n\n        return {\n            \"documents\": split_documents,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document splitting process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DocumentSplitterInputSchema</code> <p>The input data containing the documents to split.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the split documents under the key \"documents\".</p> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>def execute(\n    self, input_data: DocumentSplitterInputSchema, config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Executes the document splitting process.\n\n    Args:\n        input_data (DocumentSplitterInputSchema): The input data containing the documents to split.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    logger.debug(f\"Splitting {len(documents)} documents\")\n    output = self.document_splitter.run(documents=documents)\n\n    split_documents = output[\"documents\"]\n    logger.debug(\n        f\"Split {len(documents)} documents into {len(split_documents)} parts\"\n    )\n\n    return {\n        \"documents\": split_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initializes the components of the DocumentSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to ConnectionManager().</p> <code>None</code> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"Initializes the components of the DocumentSplitter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to ConnectionManager().\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_splitter is None:\n        self.document_splitter = DocumentSplitterComponent(\n            split_by=self.split_by,\n            split_length=self.split_length,\n            split_overlap=self.split_overlap,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/","title":"Context manager","text":""},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerInputSchema","title":"<code>ContextManagerInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for ContextManagerTool.</p> <ul> <li>notes: Verbatim content that must be preserved as-is and prepended to the summary.</li> <li>messages: List of messages to summarize.</li> </ul> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>class ContextManagerInputSchema(BaseModel):\n    \"\"\"Input for ContextManagerTool.\n\n    - notes: Verbatim content that must be preserved as-is and prepended to the summary.\n    - messages: List of messages to summarize.\n    \"\"\"\n\n    notes: str | None = Field(\n        default=None,\n        description=(\n            \"Optional notes to preserve verbatim (e.g., IDs, filenames, critical details). \"\n            \"This will be prepended to the automatic summary.\"\n        ),\n    )\n\n    messages: list[Message | VisionMessage] = Field(\n        default=[],\n        description=\"List of messages to summarize (conversation history).\",\n        json_schema_extra={\"is_accessible_to_agent\": False},\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool","title":"<code>ContextManagerTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool that generates a conversation summary.</p> <p>When called by the agent, this tool: 1. Generates a summary of the conversation using its own LLM 2. Returns the summary as tool result 3. Agent then decides how to apply the summary.</p> <p>The tool doesn't modify the agent's state - it just generates and returns the summary.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>Tool description with usage warning.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Configuration for error handling.</p> <code>llm</code> <code>Node</code> <p>LLM instance for generating summaries.</p> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>class ContextManagerTool(Node):\n    \"\"\"\n    A tool that generates a conversation summary.\n\n    When called by the agent, this tool:\n    1. Generates a summary of the conversation using its own LLM\n    2. Returns the summary as tool result\n    3. Agent then decides how to apply the summary.\n\n    The tool doesn't modify the agent's state - it just generates and returns the summary.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group this node belongs to.\n        name (str): The name of the tool.\n        description (str): Tool description with usage warning.\n        error_handling (ErrorHandling): Configuration for error handling.\n        llm (Node): LLM instance for generating summaries.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Context Manager Tool\"\n    description: str = (\n        \"Generates a conversation summary to help manage context.\\n\\n\"\n        \"WARNING: This tool will trigger context compression. Before calling it,\\n\"\n        \"save any necessary information because previous messages will be removed.\\n\"\n    )\n\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=60))\n    token_budget_ratio: float = Field(default=0.75, gt=0, lt=1)\n    llm: Node = Field(..., description=\"LLM instance for generating summaries\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[ContextManagerInputSchema]] = ContextManagerInputSchema\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"Initialize components for the tool.\"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        # Initialize the LLM if it is a postponed component\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def reset_run_state(self):\n        \"\"\"Reset the intermediate steps (run_depends) of the node.\"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def _get_token_budget(self) -&gt; int:\n        \"\"\"Return the max input tokens available for conversation messages per chunk.\n\n        Subtracts estimated prompt overhead (summarization/merge instructions)\n        from the ratio-based budget so that chunk + prompt always fits within\n        the LLM context window.\n        \"\"\"\n        raw_budget = int(self.llm.get_token_limit() * self.token_budget_ratio)\n        prompt_overhead = self._count_message_tokens(\n            [\n                Message(content=HISTORY_SUMMARIZATION_PROMPT_REPLACE, role=MessageRole.USER),\n            ]\n        )\n        return max(raw_budget - prompt_overhead, 1)\n\n    def _count_message_tokens(self, messages: list[Message | VisionMessage]) -&gt; int:\n        \"\"\"Count tokens for a list of messages using the summarization LLM's tokenizer.\"\"\"\n        return token_counter(\n            model=self.llm.model,\n            messages=[m.model_dump(exclude={\"metadata\"}) for m in messages],\n        )\n\n    _TRUNCATION_MARKER = \"\\n\\n[... truncated due to context limit ...]\"\n\n    def _truncate_message(self, msg: Message | VisionMessage, max_tokens: int) -&gt; Message | VisionMessage:\n        \"\"\"Truncate a single message so it fits within *max_tokens*.\n\n        Uses a ratio-based character estimate with token-count verification.\n        Falls back to halving the cut point up to 3 times if still over budget.\n        VisionMessages are returned as-is (image content can't be meaningfully truncated).\n        \"\"\"\n        if isinstance(msg, VisionMessage):\n            return msg\n\n        msg_tokens = self._count_message_tokens([msg])\n        if msg_tokens &lt;= max_tokens:\n            return msg\n\n        content = msg.content\n        ratio = max_tokens / msg_tokens\n        cut_point = max(1, int(len(content) * ratio * 0.9))\n\n        for _ in range(3):\n            candidate = msg.model_copy(update={\"content\": content[:cut_point] + self._TRUNCATION_MARKER})\n            if self._count_message_tokens([candidate]) &lt;= max_tokens:\n                break\n            cut_point = cut_point // 2\n\n        logger.warning(\n            f\"Context Manager Tool: Truncated oversized message \"\n            f\"({msg_tokens} tokens &gt; {max_tokens} token limit). Kept {cut_point}/{len(content)} chars.\"\n        )\n        return candidate\n\n    def _split_messages_into_chunks(\n        self,\n        messages: list[Message | VisionMessage],\n        budget: int,\n    ) -&gt; list[list[Message | VisionMessage]]:\n        \"\"\"Split messages into chunks that each fit within *budget* tokens.\n\n        Messages are kept in order. A single message that exceeds the budget\n        is truncated to fit.\n        \"\"\"\n        chunks: list[list[Message | VisionMessage]] = []\n        current_chunk: list[Message | VisionMessage] = []\n        current_tokens = 0\n\n        for msg in messages:\n            msg_tokens = self._count_message_tokens([msg])\n\n            if msg_tokens &gt; budget:\n                if current_chunk:\n                    chunks.append(current_chunk)\n                    current_chunk = []\n                    current_tokens = 0\n                truncated = self._truncate_message(msg, budget)\n                chunks.append([truncated])\n                continue\n\n            if current_chunk and current_tokens + msg_tokens &gt; budget:\n                chunks.append(current_chunk)\n                current_chunk = [msg]\n                current_tokens = msg_tokens\n            else:\n                current_chunk.append(msg)\n                current_tokens += msg_tokens\n\n        if current_chunk:\n            chunks.append(current_chunk)\n\n        return chunks\n\n    def _call_llm_for_summary(\n        self,\n        messages: list[Message | VisionMessage],\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Send *messages* to the LLM and return the generated text.\"\"\"\n        llm_result = self.llm.run(\n            input_data={},\n            prompt=Prompt(messages=messages),\n            config=config,\n            **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\"), \"run_depends\": []}),\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n\n        if llm_result.status != RunnableStatus.SUCCESS:\n            error_msg = llm_result.error.message if llm_result.error else \"Unknown error\"\n            raise ValueError(f\"Context Manager Tool: LLM failed to generate summary: {error_msg}\")\n\n        summary = llm_result.output.get(\"content\", \"\")\n        if not summary:\n            raise ValueError(\"Context Manager Tool: LLM returned empty summary.\")\n        return summary\n\n    def _summarize_replace_history(\n        self,\n        messages: list[Message | VisionMessage],\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a complete summary of the conversation (replace mode).\n\n        When the conversation history exceeds the LLM's context window, the\n        messages are split into chunks that fit, each chunk is summarised\n        independently, and the per-chunk summaries are merged into one final\n        summary.  If the merged text is still too large, a second merge pass\n        is applied.\n\n        Args:\n            messages: List of messages to summarize.\n            config: Configuration for the run.\n            **kwargs: Additional parameters.\n\n        Returns:\n            str: The generated summary.\n        \"\"\"\n        budget = self._get_token_budget()\n        message_tokens = self._count_message_tokens(messages)\n\n        if message_tokens &lt;= budget:\n            logger.info(\"Context Manager Tool: History fits in context, single-pass summary.\")\n            summary_messages = messages + [\n                Message(content=HISTORY_SUMMARIZATION_PROMPT_REPLACE, role=MessageRole.USER, static=True),\n            ]\n            summary = self._call_llm_for_summary(summary_messages, config, **kwargs)\n            logger.info(f\"Context Manager Tool: Summary generated. Length: {len(summary)}\")\n            return summary\n\n        chunks = self._split_messages_into_chunks(messages, budget)\n        logger.info(\n            f\"Context Manager Tool: History ({message_tokens} tokens) exceeds budget \"\n            f\"({budget} tokens). Splitting into {len(chunks)} chunks.\"\n        )\n\n        chunk_summaries: list[str] = []\n        for idx, chunk in enumerate(chunks):\n            logger.info(f\"Context Manager Tool: Summarizing chunk {idx + 1}/{len(chunks)}.\")\n            chunk_messages = chunk + [\n                Message(content=HISTORY_SUMMARIZATION_PROMPT_REPLACE, role=MessageRole.USER, static=True),\n            ]\n            chunk_summaries.append(self._call_llm_for_summary(chunk_messages, config, **kwargs))\n\n        combined = \"\\n\\n---\\n\\n\".join(chunk_summaries)\n        combined_messages = [\n            Message(content=combined, role=MessageRole.USER, static=True),\n            Message(content=MERGE_SUMMARIES_PROMPT, role=MessageRole.USER, static=True),\n        ]\n\n        combined_tokens = self._count_message_tokens(combined_messages)\n        if combined_tokens &gt; budget:\n            logger.info(\n                \"Context Manager Tool: Merged summaries still exceed budget \"\n                f\"({combined_tokens} &gt; {budget}). Running additional merge pass.\"\n            )\n            merge_chunks = self._split_messages_into_chunks(\n                [Message(content=s, role=MessageRole.USER, static=True) for s in chunk_summaries],\n                budget,\n            )\n            re_summaries: list[str] = []\n            for merge_chunk in merge_chunks:\n                merge_chunk.append(Message(content=MERGE_SUMMARIES_PROMPT, role=MessageRole.USER, static=True))\n                re_summaries.append(self._call_llm_for_summary(merge_chunk, config, **kwargs))\n            combined = \"\\n\\n\".join(re_summaries)\n            combined_messages = [\n                Message(content=combined, role=MessageRole.USER, static=True),\n                Message(content=MERGE_SUMMARIES_PROMPT, role=MessageRole.USER, static=True),\n            ]\n\n        summary = self._call_llm_for_summary(combined_messages, config, **kwargs)\n        logger.info(f\"Context Manager Tool: Chunked summary generated. Length: {len(summary)}\")\n        return summary\n\n    def execute(\n        self, input_data: ContextManagerInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Generate conversation summary from provided messages.\n\n        Returns:\n            dict[str, Any]:\n                - content: Short acknowledgment for the agent observation.\n                - summary: The full generated summary used by ``_compact_history``.\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        if not self.llm:\n            raise ValueError(\"Context Manager Tool: No LLM configured.\")\n\n        if not input_data.messages:\n            raise ValueError(\"Context Manager Tool: No messages provided to summarize.\")\n\n        logger.info(f\"Context Manager Tool: Generating summary for {len(input_data.messages)} messages.\")\n\n        summary_result = self._summarize_replace_history(\n            input_data.messages,\n            config,\n            **kwargs,\n        )\n\n        if input_data.notes:\n            summary_result = f\"Notes: {input_data.notes}\\n\\n{summary_result}\"\n\n        return {\n            \"content\": f\"Conversation history was summarized successfully ({len(input_data.messages)} \"\n            \"messages compressed).\",\n            \"summary\": summary_result,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Generate conversation summary from provided messages.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: - content: Short acknowledgment for the agent observation. - summary: The full generated summary used by <code>_compact_history</code>.</p> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>def execute(\n    self, input_data: ContextManagerInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Generate conversation summary from provided messages.\n\n    Returns:\n        dict[str, Any]:\n            - content: Short acknowledgment for the agent observation.\n            - summary: The full generated summary used by ``_compact_history``.\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    if not self.llm:\n        raise ValueError(\"Context Manager Tool: No LLM configured.\")\n\n    if not input_data.messages:\n        raise ValueError(\"Context Manager Tool: No messages provided to summarize.\")\n\n    logger.info(f\"Context Manager Tool: Generating summary for {len(input_data.messages)} messages.\")\n\n    summary_result = self._summarize_replace_history(\n        input_data.messages,\n        config,\n        **kwargs,\n    )\n\n    if input_data.notes:\n        summary_result = f\"Notes: {input_data.notes}\\n\\n{summary_result}\"\n\n    return {\n        \"content\": f\"Conversation history was summarized successfully ({len(input_data.messages)} \"\n        \"messages compressed).\",\n        \"summary\": summary_result,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize components for the tool.</p> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"Initialize components for the tool.\"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    # Initialize the LLM if it is a postponed component\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Reset the intermediate steps (run_depends) of the node.\"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/tools/context_manager/#dynamiq.nodes.tools.context_manager.ContextManagerTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/tools/context_manager.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/cypher_executor/","title":"Cypher executor","text":""},{"location":"dynamiq/nodes/tools/cypher_executor/#dynamiq.nodes.tools.cypher_executor.CypherExecutor","title":"<code>CypherExecutor</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Tool for executing Cypher queries against Neo4j, Apache AGE, or Neptune.</p> Source code in <code>dynamiq/nodes/tools/cypher_executor.py</code> <pre><code>class CypherExecutor(ConnectionNode):\n    \"\"\"Tool for executing Cypher queries against Neo4j, Apache AGE, or Neptune.\"\"\"\n\n    input_schema: ClassVar[type[CypherInputSchema]] = CypherInputSchema\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.DATABASE_QUERY\n    name: str = \"Cypher Executor\"\n    description: str = BASE_CYPHER_DESCRIPTION\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    connection: Neo4j | ApacheAGE | AWSNeptune\n    graph_name: str | None = None\n    create_graph_if_not_exists: bool = False\n    _graph_store: BaseGraphStore | None = PrivateAttr(default=None)\n    _backend_name: BackendName | None = PrivateAttr(default=None)\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"Initialize graph store and backend metadata.\n\n        Args:\n            connection_manager: Optional connection manager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if isinstance(self.connection, ApacheAGE):\n            self._backend_name = BackendName.AGE\n            self._graph_store = ApacheAgeGraphStore(\n                connection=self.connection,\n                client=self.client,\n                graph_name=self.graph_name,\n                create_graph_if_not_exists=self.create_graph_if_not_exists,\n            )\n        elif isinstance(self.connection, AWSNeptune):\n            self._backend_name = BackendName.NEPTUNE\n            self._graph_store = NeptuneGraphStore(\n                connection=self.connection,\n                client=self.client,\n                endpoint=self.connection.endpoint,\n                verify_ssl=self.connection.verify_ssl,\n                timeout=self.connection.timeout,\n            )\n        else:\n            self._backend_name = BackendName.NEO4J\n            self._graph_store = Neo4jGraphStore(connection=self.connection, client=self.client)\n        self.description = self._build_description()\n\n    def ensure_client(self) -&gt; None:\n        previous_client = self.client\n        super().ensure_client()\n        if self.client is previous_client:\n            return\n        if not self._graph_store:\n            return\n        graph_client = getattr(self._graph_store, \"client\", None)\n        if graph_client is not self.client:\n            self._graph_store.update_client(self.client)\n\n    def _build_description(self) -&gt; str:\n        if self._backend_name == BackendName.AGE:\n            return BASE_CYPHER_DESCRIPTION + AGE_BACKEND_NOTES\n        if self._backend_name == BackendName.NEPTUNE:\n            return BASE_CYPHER_DESCRIPTION + NEPTUNE_BACKEND_NOTES\n        if self._backend_name == BackendName.NEO4J:\n            return BASE_CYPHER_DESCRIPTION + NEO4J_BACKEND_NOTES\n        return BASE_CYPHER_DESCRIPTION\n\n    def execute(self, input_data: CypherInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Run Cypher queries or introspect schema via the configured backend.\n\n        Args:\n            input_data: Validated Cypher input payload.\n            config: Optional runnable configuration.\n            **kwargs: Extra execution context forwarded to callbacks.\n\n        Returns:\n            Dictionary payload containing records or graph output, plus metadata.\n\n        Raises:\n            ToolExecutionException: If execution fails or the graph store is not initialized.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        if not self._graph_store:\n            raise ToolExecutionException(\"Graph store is not initialized.\", recoverable=True)\n\n        database = input_data.database\n        routing = input_data.routing\n        result_payload: dict[str, Any] = {}\n\n        try:\n            if input_data.mode == \"introspect\":\n                result_payload = self._graph_store.introspect_schema(\n                    include_properties=input_data.property_metadata_enabled,\n                    database=database,\n                )\n                result_payload[\"mode\"] = input_data.mode\n                result_payload[\"content\"] = self._build_schema_content(result_payload)\n                logger.info(\n                    f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\"\n                )\n                return result_payload\n\n            if isinstance(input_data.query, list):\n                results = self._execute_batch(\n                    queries=input_data.query,\n                    parameters=input_data.parameters,\n                    database=database,\n                    routing=routing,\n                    graph_return_enabled=input_data.graph_return_enabled,\n                    writes_allowed=input_data.writes_allowed,\n                )\n                result_payload = {\n                    \"mode\": input_data.mode,\n                    \"queries\": [self._clean_query(query) for query in input_data.query],\n                    \"results\": results,\n                }\n                result_payload[\"content\"] = self._build_batch_content(results, input_data.graph_return_enabled)\n                logger.info(\n                    f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\"\n                )\n                return result_payload\n\n            result_payload = self._execute_single(\n                query=input_data.query or \"\",\n                parameters=input_data.parameters,\n                database=database,\n                routing=routing,\n                graph_return_enabled=input_data.graph_return_enabled,\n                writes_allowed=input_data.writes_allowed,\n            )\n            result_payload[\"mode\"] = input_data.mode\n            logger.info(f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\")\n            return result_payload\n        except Exception as exc:  # noqa: BLE001\n            logger.error(f\"Tool {self.name} - {self.id}: failed to execute Cypher. Error: {exc}\")\n            raise ToolExecutionException(str(exc), recoverable=True) from exc\n\n    def _execute_batch(\n        self,\n        *,\n        queries: list[str],\n        parameters: dict[str, Any] | list[dict[str, Any]],\n        database: str | None,\n        routing: str | None,\n        graph_return_enabled: bool,\n        writes_allowed: bool,\n    ) -&gt; list[dict[str, Any]]:\n        if isinstance(parameters, list):\n            params_list = parameters\n        else:\n            params_list = [parameters for _ in queries]\n        results: list[dict[str, Any]] = []\n        for query, query_params in zip(queries, params_list, strict=True):\n            results.append(\n                self._execute_single(\n                    query=query,\n                    parameters=query_params,\n                    database=database,\n                    routing=routing,\n                    graph_return_enabled=graph_return_enabled,\n                    writes_allowed=writes_allowed,\n                )\n            )\n        return results\n\n    def _execute_single(\n        self,\n        *,\n        query: str,\n        parameters: dict[str, Any],\n        database: str | None,\n        routing: str | None,\n        graph_return_enabled: bool,\n        writes_allowed: bool,\n    ) -&gt; dict[str, Any]:\n        transformer = None\n        cleaned_query = self._clean_query(query or \"\")\n        self._validate_query(cleaned_query, writes_allowed=writes_allowed)\n\n        if graph_return_enabled:\n            if not self._graph_store.supports_graph_result():\n                raise ToolExecutionException(\n                    \"graph_return_enabled is only supported for Neo4j backends.\",\n                    recoverable=True,\n                )\n\n            def _graph_transformer(result: Any) -&gt; Any:\n                graph_attr = getattr(result, \"graph\", None)\n                return graph_attr() if callable(graph_attr) else graph_attr\n\n            transformer = _graph_transformer\n\n        records, summary, keys = self._graph_store.run_cypher(\n            query=cleaned_query,\n            parameters=parameters,\n            database=database,\n            routing=routing,\n            result_transformer=transformer,\n        )\n\n        result_payload: dict[str, Any] = {}\n        if graph_return_enabled:\n            result_payload[\"graph\"] = self._serialize_graph(records)\n            result_payload[\"keys\"] = []\n        else:\n            result_payload[\"records\"] = self._graph_store.format_records(records)\n            result_payload[\"keys\"] = keys or []\n\n        result_payload[\"summary\"] = self._build_summary(summary, cleaned_query)\n        result_payload[\"query\"] = cleaned_query\n        result_payload[\"parameters_used\"] = parameters\n        result_payload[\"content\"] = self._build_content(result_payload, graph_return_enabled)\n        return result_payload\n\n    @staticmethod\n    def _build_content(payload: dict[str, Any], is_graph: bool) -&gt; str:\n        summary = payload.get(\"summary\", {})\n        counters = summary.get(\"counters\")\n        counters_text = str(counters) if counters is not None else \"None\"\n        query_text = payload.get(\"query\", \"\")\n        params = payload.get(\"parameters_used\", {})\n        if is_graph:\n            graph = payload.get(\"graph\", {})\n            node_count = len(graph.get(\"nodes\", []))\n            rel_count = len(graph.get(\"relationships\", []))\n            return (\n                f\"Query: {query_text}. Params: {params}. \"\n                f\"Executed graph query. Nodes: {node_count}, Relationships: {rel_count}. \"\n                f\"Counters: {counters_text}.\"\n            )\n        records = payload.get(\"records\", [])\n        preview = records[:3] if records else []\n        return (\n            f\"Query: {query_text}. Params: {params}. \"\n            f\"Returned {len(records)} records. Preview: {preview}. \"\n            f\"Counters: {counters_text}.\"\n        )\n\n    @staticmethod\n    def _build_schema_content(payload: dict[str, Any]) -&gt; str:\n        def _first_label(value: Any) -&gt; str:\n            if isinstance(value, list):\n                return value[0] if value else \"?\"\n            return value or \"?\"\n\n        labels = payload.get(\"labels\") or []\n        rels = payload.get(\"relationship_types\") or []\n        node_props = payload.get(\"node_properties\") or []\n        rel_props = payload.get(\"relationship_properties\") or []\n        node_samples = []\n        for entry in node_props[:5]:\n            if \"nodeLabels\" in entry:\n                node_samples.append(\n                    f\"{_first_label(entry.get('nodeLabels'))}.{entry.get('propertyName')}:{entry.get('propertyTypes')}\"\n                )\n            else:\n                props = entry.get(\"properties\") or []\n                if props:\n                    node_samples.append(f\"{entry.get('labels')}.{props[0].get('property')}:{props[0].get('type')}\")\n        rel_samples = []\n        for entry in rel_props[:5]:\n            if \"relType\" in entry:\n                rel_samples.append(\n                    f\"{_first_label(entry.get('relType'))}.{entry.get('propertyName')}:{entry.get('propertyTypes')}\"\n                )\n            else:\n                props = entry.get(\"properties\") or []\n                if props:\n                    rel_samples.append(f\"{entry.get('type')}.{props[0].get('property')}:{props[0].get('type')}\")\n        return (\n            f\"Labels: {labels}. \"\n            f\"Relationship types: {rels}. \"\n            f\"Node properties entries: {len(node_props)} (samples: {node_samples}). \"\n            f\"Relationship properties entries: {len(rel_props)} (samples: {rel_samples}).\"\n        )\n\n    @staticmethod\n    def _build_batch_content(results: list[dict[str, Any]], is_graph: bool) -&gt; str:\n        if not results:\n            return \"No queries executed.\"\n        snippets = []\n        for index, payload in enumerate(results, start=1):\n            summary = payload.get(\"summary\", {})\n            counters = summary.get(\"counters\")\n            counters_text = str(counters) if counters is not None else \"None\"\n            query_text = payload.get(\"query\", \"\")\n            params = payload.get(\"parameters_used\", {})\n            if is_graph:\n                graph = payload.get(\"graph\", {})\n                node_count = len(graph.get(\"nodes\", []))\n                rel_count = len(graph.get(\"relationships\", []))\n                snippets.append(\n                    f\"[{index}] Query: {query_text}. Params: {params}. \"\n                    f\"Nodes: {node_count}, Relationships: {rel_count}. Counters: {counters_text}.\"\n                )\n            else:\n                records = payload.get(\"records\", [])\n                preview = records[:3] if records else []\n                snippets.append(\n                    f\"[{index}] Query: {query_text}. Params: {params}. \"\n                    f\"Returned {len(records)} records. Preview: {preview}. Counters: {counters_text}.\"\n                )\n        return \" \".join(snippets)\n\n    @classmethod\n    def _build_summary(cls, summary: Any, fallback_query: str) -&gt; dict[str, Any]:\n        payload = {\"query\": fallback_query, \"counters\": {}, \"result_available_after\": None}\n        if summary is None:\n            return payload\n\n        def _extract_query(source: Any) -&gt; str:\n            if isinstance(source, dict):\n                value = source.get(\"query\", payload[\"query\"])\n            else:\n                value = getattr(source, \"query\", payload[\"query\"])\n            return value.text if hasattr(value, \"text\") else value\n\n        if isinstance(summary, dict):\n            payload[\"query\"] = _extract_query(summary)\n            payload[\"counters\"] = summary.get(\"counters\", payload[\"counters\"])\n            payload[\"result_available_after\"] = summary.get(\n                \"result_available_after\",\n                payload[\"result_available_after\"],\n            )\n            return payload\n\n        payload[\"query\"] = _extract_query(summary)\n        counters = getattr(summary, \"counters\", None)\n        payload[\"counters\"] = cls._serialize_counters(counters) if counters is not None else {}\n        payload[\"result_available_after\"] = getattr(summary, \"result_available_after\", None)\n        return payload\n\n    @staticmethod\n    def _clean_query(query: str) -&gt; str:\n        cleaned = (query or \"\").strip()\n        if cleaned.startswith(\"```\"):\n            cleaned = cleaned.strip(\"`\").strip()\n            if cleaned.lower().startswith(\"cypher\"):\n                cleaned = cleaned[len(\"cypher\") :].strip()\n        return cleaned\n\n    @classmethod\n    def _validate_query(cls, query: str, *, writes_allowed: bool) -&gt; None:\n        if not query:\n            raise ToolExecutionException(\"Cypher query cannot be empty.\", recoverable=True)\n        if not writes_allowed and cls._contains_write(query):\n            raise ToolExecutionException(\n                \"Cypher contains write operations \" \"but writes_allowed is false.\", recoverable=True\n            )\n        if writes_allowed and cls._contains_write(query) and cls._contains_cartesian_match(query):\n            raise ToolExecutionException(\n                \"Cypher contains comma-separated MATCH/MERGE patterns that may create cartesian products. \"\n                \"Use chained MATCH with WITH, or a single MATCH with relationship patterns.\",\n                recoverable=True,\n            )\n\n    @staticmethod\n    def _contains_write(cypher: str) -&gt; bool:\n        pattern = re.compile(r\"\\b(CREATE|MERGE|DELETE|DETACH|SET|DROP|REMOVE)\\b\", re.IGNORECASE)\n        return bool(pattern.search(cypher or \"\"))\n\n    @staticmethod\n    def _contains_cartesian_match(cypher: str) -&gt; bool:\n        if not cypher:\n            return False\n        pattern = re.compile(r\"\\b(MATCH|MERGE)\\b[\\s\\S]*?,\\s*\\(\", re.IGNORECASE)\n        return bool(pattern.search(cypher))\n\n    @staticmethod\n    def _serialize_graph(graph: Any | None) -&gt; dict[str, Any]:\n        \"\"\"Convert Neo4j Graph result into JSON-serializable structures.\"\"\"\n        if graph is None:\n            return {\"nodes\": [], \"relationships\": []}\n\n        def _node_to_dict(node: Any) -&gt; dict[str, Any]:\n            return {\n                \"id\": getattr(node, \"id\", None),\n                \"element_id\": getattr(node, \"element_id\", None),\n                \"labels\": list(getattr(node, \"labels\", [])),\n                \"properties\": dict(node),\n            }\n\n        def _relationship_to_dict(rel: Any) -&gt; dict[str, Any]:\n            start_node = getattr(rel, \"start_node\", None)\n            end_node = getattr(rel, \"end_node\", None)\n            start_node_id = (\n                getattr(start_node, \"id\", None) if start_node is not None else getattr(rel, \"start_node_id\", None)\n            )\n            end_node_id = getattr(end_node, \"id\", None) if end_node is not None else getattr(rel, \"end_node_id\", None)\n            start_node_element_id = (\n                getattr(start_node, \"element_id\", None)\n                if start_node is not None\n                else getattr(rel, \"start_node_element_id\", None)\n            )\n            end_node_element_id = (\n                getattr(end_node, \"element_id\", None)\n                if end_node is not None\n                else getattr(rel, \"end_node_element_id\", None)\n            )\n            return {\n                \"id\": getattr(rel, \"id\", None),\n                \"element_id\": getattr(rel, \"element_id\", None),\n                \"type\": getattr(rel, \"type\", None),\n                \"start_node_id\": start_node_id,\n                \"end_node_id\": end_node_id,\n                \"start_node_element_id\": start_node_element_id,\n                \"end_node_element_id\": end_node_element_id,\n                \"properties\": dict(rel),\n            }\n\n        nodes = [_node_to_dict(node) for node in getattr(graph, \"nodes\", [])]\n        relationships = [_relationship_to_dict(rel) for rel in getattr(graph, \"relationships\", [])]\n\n        return {\"nodes\": nodes, \"relationships\": relationships}\n\n    @staticmethod\n    def _serialize_counters(counters: Any | None) -&gt; dict[str, Any]:\n        \"\"\"Convert Neo4j SummaryCounters to a JSON-serializable dict.\"\"\"\n        if counters is None:\n            return {}\n\n        counter_fields = [\n            \"nodes_created\",\n            \"nodes_deleted\",\n            \"relationships_created\",\n            \"relationships_deleted\",\n            \"properties_set\",\n            \"labels_added\",\n            \"labels_removed\",\n            \"indexes_added\",\n            \"indexes_removed\",\n            \"constraints_added\",\n            \"constraints_removed\",\n            \"system_updates\",\n        ]\n\n        counters_dict = {field: getattr(counters, field, 0) for field in counter_fields}\n        if hasattr(counters, \"contains_updates\"):\n            value = counters.contains_updates\n            counters_dict[\"contains_updates\"] = value() if callable(value) else value\n        if hasattr(counters, \"contains_system_updates\"):\n            value = counters.contains_system_updates\n            counters_dict[\"contains_system_updates\"] = value() if callable(value) else value\n\n        return counters_dict\n</code></pre>"},{"location":"dynamiq/nodes/tools/cypher_executor/#dynamiq.nodes.tools.cypher_executor.CypherExecutor.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Run Cypher queries or introspect schema via the configured backend.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>CypherInputSchema</code> <p>Validated Cypher input payload.</p> required <code>config</code> <code>RunnableConfig</code> <p>Optional runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Extra execution context forwarded to callbacks.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary payload containing records or graph output, plus metadata.</p> <p>Raises:</p> Type Description <code>ToolExecutionException</code> <p>If execution fails or the graph store is not initialized.</p> Source code in <code>dynamiq/nodes/tools/cypher_executor.py</code> <pre><code>def execute(self, input_data: CypherInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Run Cypher queries or introspect schema via the configured backend.\n\n    Args:\n        input_data: Validated Cypher input payload.\n        config: Optional runnable configuration.\n        **kwargs: Extra execution context forwarded to callbacks.\n\n    Returns:\n        Dictionary payload containing records or graph output, plus metadata.\n\n    Raises:\n        ToolExecutionException: If execution fails or the graph store is not initialized.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    if not self._graph_store:\n        raise ToolExecutionException(\"Graph store is not initialized.\", recoverable=True)\n\n    database = input_data.database\n    routing = input_data.routing\n    result_payload: dict[str, Any] = {}\n\n    try:\n        if input_data.mode == \"introspect\":\n            result_payload = self._graph_store.introspect_schema(\n                include_properties=input_data.property_metadata_enabled,\n                database=database,\n            )\n            result_payload[\"mode\"] = input_data.mode\n            result_payload[\"content\"] = self._build_schema_content(result_payload)\n            logger.info(\n                f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\"\n            )\n            return result_payload\n\n        if isinstance(input_data.query, list):\n            results = self._execute_batch(\n                queries=input_data.query,\n                parameters=input_data.parameters,\n                database=database,\n                routing=routing,\n                graph_return_enabled=input_data.graph_return_enabled,\n                writes_allowed=input_data.writes_allowed,\n            )\n            result_payload = {\n                \"mode\": input_data.mode,\n                \"queries\": [self._clean_query(query) for query in input_data.query],\n                \"results\": results,\n            }\n            result_payload[\"content\"] = self._build_batch_content(results, input_data.graph_return_enabled)\n            logger.info(\n                f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\"\n            )\n            return result_payload\n\n        result_payload = self._execute_single(\n            query=input_data.query or \"\",\n            parameters=input_data.parameters,\n            database=database,\n            routing=routing,\n            graph_return_enabled=input_data.graph_return_enabled,\n            writes_allowed=input_data.writes_allowed,\n        )\n        result_payload[\"mode\"] = input_data.mode\n        logger.info(f\"Tool {self.name} - {self.id}: finished successfully. Content: {result_payload['content']}\")\n        return result_payload\n    except Exception as exc:  # noqa: BLE001\n        logger.error(f\"Tool {self.name} - {self.id}: failed to execute Cypher. Error: {exc}\")\n        raise ToolExecutionException(str(exc), recoverable=True) from exc\n</code></pre>"},{"location":"dynamiq/nodes/tools/cypher_executor/#dynamiq.nodes.tools.cypher_executor.CypherExecutor.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize graph store and backend metadata.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/tools/cypher_executor.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"Initialize graph store and backend metadata.\n\n    Args:\n        connection_manager: Optional connection manager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if isinstance(self.connection, ApacheAGE):\n        self._backend_name = BackendName.AGE\n        self._graph_store = ApacheAgeGraphStore(\n            connection=self.connection,\n            client=self.client,\n            graph_name=self.graph_name,\n            create_graph_if_not_exists=self.create_graph_if_not_exists,\n        )\n    elif isinstance(self.connection, AWSNeptune):\n        self._backend_name = BackendName.NEPTUNE\n        self._graph_store = NeptuneGraphStore(\n            connection=self.connection,\n            client=self.client,\n            endpoint=self.connection.endpoint,\n            verify_ssl=self.connection.verify_ssl,\n            timeout=self.connection.timeout,\n        )\n    else:\n        self._backend_name = BackendName.NEO4J\n        self._graph_store = Neo4jGraphStore(connection=self.connection, client=self.client)\n    self.description = self._build_description()\n</code></pre>"},{"location":"dynamiq/nodes/tools/cypher_executor/#dynamiq.nodes.tools.cypher_executor.CypherInputSchema","title":"<code>CypherInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Cypher tool inputs.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <p>Execution mode.</p> required <code>query</code> <p>Cypher query or list of queries in execute mode.</p> required <code>parameters</code> <p>Parameters for Cypher execution.</p> required <code>database</code> <p>Optional database name override.</p> required <code>routing</code> <p>Routing preference for clustered deployments.</p> required <code>graph_return_enabled</code> <p>Whether to return graph results instead of rows.</p> required <code>property_metadata_enabled</code> <p>Whether to include node and relationship property metadata.</p> required <code>writes_allowed</code> <p>Whether to allow write queries.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing or incompatible with the selected mode.</p> Source code in <code>dynamiq/nodes/tools/cypher_executor.py</code> <pre><code>class CypherInputSchema(BaseModel):\n    \"\"\"Schema for Cypher tool inputs.\n\n    Args:\n        mode: Execution mode.\n        query: Cypher query or list of queries in execute mode.\n        parameters: Parameters for Cypher execution.\n        database: Optional database name override.\n        routing: Routing preference for clustered deployments.\n        graph_return_enabled: Whether to return graph results instead of rows.\n        property_metadata_enabled: Whether to include node and relationship property metadata.\n        writes_allowed: Whether to allow write queries.\n\n    Raises:\n        ValueError: If required fields are missing or incompatible with the selected mode.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\", populate_by_name=True)\n\n    mode: Literal[\"execute\", \"introspect\"] = Field(default=\"execute\", description=\"Execution mode.\")\n    query: str | list[str] | None = Field(\n        default=None, description=\"Cypher query or list of queries (execute mode only).\"\n    )\n    parameters: dict[str, Any] | list[dict[str, Any]] = Field(\n        default_factory=dict,\n        description=\"Parameters for the Cypher query (use $param syntax in query).\",\n        json_schema_extra={\"type\": \"object\", \"properties\": {}, \"additionalProperties\": True},\n    )\n    database: str | None = Field(default=None, description=\"Optional database name override.\")\n    routing: str | None = Field(default=None, description=\"Routing preference ('r' for read, 'w' for write).\")\n    graph_return_enabled: bool = Field(\n        default=False,\n        description=\"If true, returns the Neo4j graph result (nodes/relationships) instead of rows.\",\n        validation_alias=\"return_graph\",\n    )\n    property_metadata_enabled: bool = Field(\n        default=True,\n        description=\"If true, include node and relationship property metadata (introspect mode).\",\n        validation_alias=\"include_properties\",\n    )\n    writes_allowed: bool = Field(\n        default=True,\n        description=\"If false, reject write queries by regex guardrails.\",\n        validation_alias=\"allow_writes\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_mode_inputs(self: \"CypherInputSchema\") -&gt; \"CypherInputSchema\":\n        if self.mode == \"execute\":\n            if isinstance(self.query, list):\n                if not self.query or any(not str(item).strip() for item in self.query):\n                    raise ValueError(\"query is required in execute mode.\")\n            elif not (self.query or \"\").strip():\n                raise ValueError(\"query is required in execute mode.\")\n        else:\n            if self.graph_return_enabled:\n                raise ValueError(\"graph_return_enabled is only supported in execute mode.\")\n            if self.query is not None:\n                raise ValueError(\"query is not supported in introspect mode.\")\n            if self.parameters:\n                raise ValueError(\"parameters are not supported in introspect mode.\")\n        if isinstance(self.query, list):\n            if isinstance(self.parameters, list):\n                if len(self.query) != len(self.parameters):\n                    raise ValueError(\"parameters list must match query list length.\")\n        elif isinstance(self.parameters, list):\n            raise ValueError(\"parameters list is only supported when query is a list.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/","title":"E2b sandbox","text":""},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterInputSchema","title":"<code>E2BInterpreterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for E2B interpreter tool.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>class E2BInterpreterInputSchema(BaseModel):\n    \"\"\"Input schema for E2B interpreter tool.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    packages: str = Field(default=\"\", description=\"Comma-separated pip packages to install.\")\n    shell_command: str = Field(default=\"\", description=\"Shell command to execute.\")\n    python: str = Field(default=\"\", description=\"Python code to execute.\")\n    download_files: list[str] = Field(default_factory=list, description=\"Exact file paths to fetch as base64.\")\n    files: list[io.BytesIO] | None = Field(\n        default=None,\n        description=\"Files to upload to the sandbox.\",\n        json_schema_extra={\"is_accessible_to_agent\": False, \"map_from_storage\": True},\n    )\n    params: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Arbitrary variables to inject as Python globals before executing 'python'.\",\n        json_schema_extra={\"is_accessible_to_agent\": False},\n    )\n    env: dict[str, str] = Field(\n        default_factory=dict,\n        description=\"Environment variables for shell commands.\",\n        json_schema_extra={\"is_accessible_to_agent\": False},\n    )\n    cwd: str = Field(default=\"/home/user/output\", description=\"Working directory for shell commands.\")\n    timeout: int | None = Field(default=None, description=\"Override sandbox timeout for this execution (seconds)\")\n\n    @model_validator(mode=\"after\")\n    def validate_execution_commands(self):\n        \"\"\"Validate that either shell command, python code, or download files is specified.\"\"\"\n        if not self.shell_command and not self.python and not self.download_files:\n            raise ValueError(\"shell_command, python code, or download_files has to be specified.\")\n        return self\n\n    @field_validator(\"files\", mode=\"before\")\n    @classmethod\n    def files_validator(cls, files: list[bytes | io.BytesIO | FileInfo], **kwargs):\n        \"\"\"Validate and process files.\"\"\"\n        if files in (None, [], ()):\n            return None\n\n        return handle_file_upload(files)\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterInputSchema.files_validator","title":"<code>files_validator(files, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate and process files.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>@field_validator(\"files\", mode=\"before\")\n@classmethod\ndef files_validator(cls, files: list[bytes | io.BytesIO | FileInfo], **kwargs):\n    \"\"\"Validate and process files.\"\"\"\n    if files in (None, [], ()):\n        return None\n\n    return handle_file_upload(files)\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterInputSchema.validate_execution_commands","title":"<code>validate_execution_commands()</code>","text":"<p>Validate that either shell command, python code, or download files is specified.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_execution_commands(self):\n    \"\"\"Validate that either shell command, python code, or download files is specified.\"\"\"\n    if not self.shell_command and not self.python and not self.download_files:\n        raise ValueError(\"shell_command, python code, or download_files has to be specified.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool","title":"<code>E2BInterpreterTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for executing code and managing files in an E2B sandbox environment.</p> <p>This tool provides a secure execution environment for running Python code, shell commands, and managing file operations.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The node group identifier.</p> <code>name</code> <code>str</code> <p>The unique name of the tool.</p> <code>description</code> <code>str</code> <p>Detailed usage instructions and capabilities.</p> <code>connection</code> <code>E2B</code> <p>Configuration for E2B connection.</p> <code>installed_packages</code> <code>list</code> <p>Pre-installed packages in the sandbox.</p> <code>files</code> <code>list[BytesIO] | None</code> <p>Files to be uploaded.</p> <code>persistent_sandbox</code> <code>bool</code> <p>Whether to maintain sandbox between executions.</p> <code>is_files_allowed</code> <code>bool</code> <p>Whether file uploads are permitted.</p> <code>_sandbox</code> <code>Sandbox | None</code> <p>Internal sandbox instance for persistent mode.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>class E2BInterpreterTool(ConnectionNode):\n    \"\"\"\n    A tool for executing code and managing files in an E2B sandbox environment.\n\n    This tool provides a secure execution environment for running Python code,\n    shell commands, and managing file operations.\n\n    Attributes:\n        group: The node group identifier.\n        name: The unique name of the tool.\n        description: Detailed usage instructions and capabilities.\n        connection: Configuration for E2B connection.\n        installed_packages: Pre-installed packages in the sandbox.\n        files: Files to be uploaded.\n        persistent_sandbox: Whether to maintain sandbox between executions.\n        is_files_allowed: Whether file uploads are permitted.\n        _sandbox: Internal sandbox instance for persistent mode.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.CODE_EXECUTION\n    name: str = \"E2b Code Interpreter Tool\"\n    description: str = DESCRIPTION_E2B\n    connection: E2BConnection\n    installed_packages: list = []\n    files: list[io.BytesIO] | None = None\n    persistent_sandbox: bool = True\n    timeout: int = Field(default=600, description=\"Sandbox timeout in seconds (default: 600 seconds)\")\n    is_files_allowed: bool = True\n    creation_error_handling: SandboxCreationErrorHandling = Field(default_factory=SandboxCreationErrorHandling)\n\n    _sandbox: Sandbox | None = None\n\n    input_schema: ClassVar[type[E2BInterpreterInputSchema]] = E2BInterpreterInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the E2B interpreter tool.\"\"\"\n        super().__init__(**kwargs)\n        if self.persistent_sandbox and self.connection.api_key:\n            self._initialize_persistent_sandbox()\n        else:\n            logger.debug(f\"Tool {self.name} - {self.id}: Will initialize sandbox on each execute\")\n\n    @property\n    def to_dict_exclude_params(self) -&gt; set:\n        \"\"\"\n        Get parameters to exclude from dictionary representation.\n\n        Returns:\n            set: Set of parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"files\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert instance to dictionary format.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        if self.files:\n            data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n        return data\n\n    def _initialize_persistent_sandbox(self):\n        \"\"\"Initialize the persistent sandbox, install packages, and upload initial files.\"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: \" f\"Initializing Persistent Sandbox with timeout {self.timeout}s\")\n        self._sandbox = self._create_sandbox_with_retry()\n        self._install_default_packages(self._sandbox)\n        if self.files:\n            self._upload_files(files=self.files, sandbox=self._sandbox)\n\n    def _install_default_packages(self, sandbox: Sandbox) -&gt; None:\n        \"\"\"Install the default packages in the specified sandbox.\"\"\"\n        if self.installed_packages:\n            for package in self.installed_packages:\n                self._install_packages(sandbox, package)\n\n    def _install_packages(self, sandbox: Sandbox, packages: str) -&gt; None:\n        \"\"\"\n        Install the specified packages in the given sandbox.\n\n        Args:\n            sandbox: The sandbox instance to install packages in.\n            packages: Comma-separated string of package names.\n\n        Raises:\n            ToolExecutionException: If package installation fails.\n        \"\"\"\n        if packages:\n            logger.debug(f\"Tool {self.name} - {self.id}: Installing packages: {packages}\")\n            try:\n                process = sandbox.commands.run(f\"pip install -qq {' '.join(packages.split(','))}\")\n            except Exception as e:\n                raise ToolExecutionException(f\"Error during package installation: {e}\", recoverable=True)\n\n            if process.exit_code != 0:\n                raise ToolExecutionException(f\"Error during package installation: {process.stderr}\", recoverable=True)\n\n    def _upload_files(self, files: list[io.BytesIO], sandbox: Sandbox) -&gt; str:\n        \"\"\"\n        Upload multiple files to the sandbox and return details for each file.\n\n        Args:\n            files: List of file data objects to upload.\n            sandbox: The sandbox instance to upload files to.\n\n        Returns:\n            str: Details of uploaded files.\n        \"\"\"\n        upload_details = []\n        for file in files:\n            uploaded_path = self._upload_file(file, sandbox)\n            file_name = getattr(file, \"name\", \"unknown_file\")\n            upload_details.append(\n                {\n                    \"original_name\": file_name,\n                    \"description\": getattr(file, \"description\", \"\"),\n                    \"uploaded_path\": uploaded_path,\n                }\n            )\n        self._update_description_with_files(upload_details)\n        return \"\\n\".join([f\"{file['original_name']} -&gt; {file['uploaded_path']}\" for file in upload_details])\n\n    def _upload_file(self, file: io.BytesIO, sandbox: Sandbox | None = None) -&gt; str:\n        \"\"\"\n        Upload a single file to the specified sandbox and return the uploaded path.\n\n        Args:\n            file: The file data to upload.\n            sandbox: The sandbox instance to upload to.\n\n        Returns:\n            str: The path where the file was uploaded.\n\n        Raises:\n            ValueError: If sandbox instance is not provided.\n        \"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for file upload.\")\n\n        file_name = getattr(file, \"name\", \"unknown_file\")\n\n        if \"/\" in file_name:\n            dir_path = \"/\".join(file_name.split(\"/\")[:-1])\n            sandbox.commands.run(f\"mkdir -p /home/user/input/{shlex.quote(dir_path)}\")\n\n        # Reset file position to beginning\n        file.seek(0)\n\n        # Upload to /home/user/input directory\n        target_path = (\n            f\"/home/user/input/{file_name}\" if not file_name.startswith(\"/\") else f\"/home/user/input{file_name}\"\n        )\n        uploaded_info = sandbox.files.write(target_path, file)\n        logger.debug(f\"Tool {self.name} - {self.id}: Uploaded file info: {uploaded_info}\")\n\n        return uploaded_info.path\n\n    def _update_description_with_files(self, upload_details: list[dict]) -&gt; None:\n        \"\"\"\n        Update the tool description with detailed information about the uploaded files.\n\n        Args:\n            upload_details: List of dictionaries containing file upload details.\n        \"\"\"\n        if upload_details:\n            self.description = self.description.strip().replace(\"&lt;/tool_description&gt;\", \"\")\n            self.description += \"\\n\\n**Uploaded Files Details:**\"\n            for file_info in upload_details:\n                self.description += (\n                    f\"\\n- **Original File Name**: {file_info['original_name']}\\n\"\n                    f\"  **Description**: {file_info['description']}\\n\"\n                    f\"  **Uploaded Path**: {file_info['uploaded_path']}\\n\"\n                )\n            self.description += \"\\n&lt;/tool_description&gt;\"\n\n    def _execute_python_code(self, code: str, sandbox: Sandbox | None = None, params: dict = None) -&gt; str:\n        \"\"\"\n        Execute Python code in the specified sandbox with persistent session state.\n\n        Args:\n            code: The Python code to execute.\n            sandbox: The sandbox instance to execute code in.\n            params: Variables to inject into the execution environment.\n\n        Returns:\n            str: The output from code execution.\n\n        Raises:\n            ValueError: If sandbox instance is not provided.\n            ToolExecutionException: If code execution fails.\n        \"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for code execution.\")\n\n        if params:\n            vars_code = \"\\n# Tool params variables injected by framework\\n\"\n            for key, value in params.items():\n                if isinstance(value, str):\n                    vars_code += f\"{key} = {repr(value)}\\n\"\n                elif isinstance(value, (int, float, bool)) or value is None:\n                    vars_code += f\"{key} = {value}\\n\"\n                elif isinstance(value, (list, dict)):\n                    vars_code += f\"{key} = {repr(value)}\\n\"\n                else:\n                    vars_code += f\"{key} = {repr(str(value))}\\n\"\n\n            code = vars_code + \"\\n\" + code\n\n        try:\n            logger.info(f\"Executing Python code: {code}\")\n            execution = sandbox.run_code(code)\n            output_parts = []\n\n            if execution.text:\n                output_parts.append(execution.text)\n\n            if execution.error:\n                if \"NameError\" in str(execution.error) and self.persistent_sandbox:\n                    logger.debug(\n                        f\"Tool {self.name}: Recoverable NameError in persistent session: \" f\"{execution.error}\"\n                    )\n                raise ToolExecutionException(f\"Error during Python code execution: {execution.error}\", recoverable=True)\n\n            if hasattr(execution, \"logs\") and execution.logs:\n                if hasattr(execution.logs, \"stdout\") and execution.logs.stdout:\n                    for log in execution.logs.stdout:\n                        output_parts.append(log)\n                if hasattr(execution.logs, \"stderr\") and execution.logs.stderr:\n                    for log in execution.logs.stderr:\n                        output_parts.append(f\"[stderr] {log}\")\n\n            return \"\\n\".join(output_parts) if output_parts else \"\"\n\n        except Exception as e:\n            raise ToolExecutionException(f\"Error during Python code execution: {e}\", recoverable=True)\n\n    def _execute_shell_command(\n        self, command: str, sandbox: Sandbox | None = None, env: dict | None = None, cwd: str | None = None\n    ) -&gt; str:\n        \"\"\"\n        Execute a shell command in the specified sandbox.\n\n        Args:\n            command: The shell command to execute.\n            sandbox: The sandbox instance to execute command in.\n            env: Environment variables for the command.\n            cwd: Working directory for the command.\n\n        Returns:\n            str: The output from command execution.\n\n        Raises:\n            ValueError: If sandbox instance is not provided.\n            ToolExecutionException: If command execution fails.\n        \"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for command execution.\")\n\n        try:\n            process = sandbox.commands.run(command, background=True, envs=env or {}, cwd=cwd or \"/home/user\")\n        except Exception as e:\n            raise ToolExecutionException(f\"Error during shell command execution: {e}\", recoverable=True)\n\n        output = process.wait()\n        if output.exit_code != 0:\n            raise ToolExecutionException(f\"Error during shell command execution: {output.stderr}\", recoverable=True)\n        return output.stdout\n\n    def _download_files(self, file_paths: list[str], sandbox: Sandbox | None = None) -&gt; dict[str, str]:\n        \"\"\"\n        Download files from sandbox and return them with proper MIME types and data URIs.\n\n        Args:\n            file_paths: List of file paths to download.\n            sandbox: The sandbox instance to download from.\n\n        Returns:\n            dict[str, str]: Dictionary mapping file paths to base64 or data URI content.\n\n        Raises:\n            ValueError: If sandbox instance is not provided.\n        \"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for file download.\")\n\n        downloaded_files = {}\n        for file_path in file_paths:\n            try:\n\n                file_bytes = sandbox.files.read(file_path, \"bytes\")\n\n                base64_content = base64.b64encode(file_bytes).decode(\"utf-8\")\n\n                downloaded_files[file_path] = base64_content\n\n            except Exception as e:\n                logger.warning(f\"Tool {self.name} - {self.id}: Failed to download {file_path}: {e}\")\n                downloaded_files[file_path] = f\"Error: {str(e)}\"\n\n        return downloaded_files\n\n    def _is_simple_structure(self, obj: Any, max_depth: int = 3) -&gt; bool:\n        \"\"\"\n        Check if object contains only simple, serializable types.\n\n        Args:\n            obj: The object to check.\n            max_depth: Maximum depth to check for nested structures.\n\n        Returns:\n            bool: True if object contains only simple types.\n        \"\"\"\n        if max_depth &lt;= 0:\n            return False\n        if isinstance(obj, (str, int, float, bool, type(None))):\n            return True\n        elif isinstance(obj, list):\n            return all(self._is_simple_structure(item, max_depth - 1) for item in obj[:10])  # Limit list size\n        elif isinstance(obj, dict):\n            return all(\n                isinstance(k, str) and self._is_simple_structure(v, max_depth - 1)\n                for k, v in list(obj.items())[:10]  # Limit dict size\n            )\n        else:\n            return False\n\n    def _collect_output_files(self, sandbox: Sandbox, base_dir: str = \"\") -&gt; dict[str, str]:\n        \"\"\"\n        Collect common output files from /home/user/output directory.\n\n        Args:\n            sandbox: The sandbox instance to collect files from.\n            base_dir: Base directory to search for files.\n\n        Returns:\n            dict[str, str]: Dictionary mapping file paths to base64 or data URI content.\n        \"\"\"\n        try:\n            collected_files = {}\n\n            search_dirs = [\"/home/user/output\"]\n\n            for search_dir in search_dirs:\n                check_cmd = f\"test -d {shlex.quote(search_dir)} &amp;&amp; echo exists\"\n                check_res = sandbox.commands.run(check_cmd)\n                if hasattr(check_res, \"wait\"):\n                    check_out = check_res.wait()\n                else:\n                    check_out = check_res\n\n                if check_out.exit_code != 0 or \"exists\" not in check_out.stdout:\n                    continue\n\n                max_depth = \"3\"  # Allow deeper search in /home/user/output directory\n                cmd = (\n                    f\"cd {shlex.quote(search_dir)} &amp;&amp; find . -maxdepth {max_depth} \"\n                    f\"-type f -printf '%P\\\\n' 2&gt;/dev/null | head -20\"\n                )\n                res = sandbox.commands.run(cmd)\n\n                if hasattr(res, \"wait\"):\n                    out = res.wait()\n                else:\n                    out = res\n\n                if out.exit_code != 0 or not out.stdout.strip():\n                    continue\n\n                file_paths = [f for f in out.stdout.splitlines() if f.strip()]\n                if file_paths:\n                    abs_paths = [str(PurePosixPath(search_dir) / p) for p in file_paths]\n                    files = self._download_files(abs_paths, sandbox=sandbox)\n                    collected_files.update(files)\n\n            return collected_files\n\n        except Exception as e:\n            logger.warning(f\"Failed to collect output files: {e}\")\n            return {}\n\n    def execute(\n        self, input_data: E2BInterpreterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the requested action based on the input data.\n\n        Args:\n            input_data: The input schema containing execution parameters.\n            config: Optional runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Dictionary containing execution results.\n\n        Raises:\n            ToolExecutionException: If execution fails or invalid input provided.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n\" f\"{str(input_data.model_dump())[:300]}\")\n        config = ensure_config(config)\n\n        if self.persistent_sandbox and self._sandbox:\n            sandbox = self._sandbox\n        else:\n            sandbox = self._create_sandbox_with_retry()\n            self._install_default_packages(sandbox)\n            if self.files:\n                self._upload_files(files=self.files, sandbox=sandbox)\n\n        tool_data = {\n            \"tool_session_id\": sandbox.sandbox_id,\n            \"tool_session_host\": sandbox.get_host(port=sandbox.connection_config.envd_port),\n        }\n        self.run_on_node_execute_run(\n            config.callbacks,\n            tool_data=tool_data,\n            **kwargs,\n        )\n\n        if sandbox and self.is_files_allowed:\n            try:\n                sandbox.commands.run(\"mkdir -p /home/user/input /home/user/output\")\n                logger.debug(\"Created /home/user/input and /home/user/output directories\")\n            except Exception as e:\n                logger.warning(f\"Failed to create directories: {e}\")\n\n        if input_data.timeout and sandbox:\n            try:\n                sandbox.set_timeout(input_data.timeout)\n                logger.debug(f\"Set per-call timeout to {input_data.timeout}s\")\n            except Exception as e:\n                logger.warning(f\"Failed to set per-call timeout: {e}\")\n\n        try:\n            content = {}\n\n            if files := input_data.files:\n                content[\"files_uploaded\"] = self._upload_files(files=files, sandbox=sandbox)\n\n            if packages := input_data.packages:\n                self._install_packages(sandbox=sandbox, packages=packages)\n                content[\"packages_installation\"] = f\"Installed packages: {input_data.packages}\"\n\n            if shell_command := input_data.shell_command:\n                content[\"shell_command_execution\"] = self._execute_shell_command(\n                    shell_command, sandbox=sandbox, env=input_data.env, cwd=input_data.cwd\n                )\n\n            if python := input_data.python:\n                content[\"code_execution\"] = self._execute_python_code(python, sandbox=sandbox, params=input_data.params)\n\n            if download_files := input_data.download_files:\n                downloaded_files = self._download_files(download_files, sandbox=sandbox)\n                content.setdefault(\"files\", {}).update(downloaded_files)\n\n            if shell_command or python:\n                collected_files = self._collect_output_files(sandbox)\n                if collected_files:\n                    content.setdefault(\"files\", {}).update(collected_files)\n\n            if not (packages or files or shell_command or python or download_files):\n                raise ToolExecutionException(\n                    \"Error: Invalid input data. Please provide packages, files, shell_command, \"\n                    \"python code, or download_files.\",\n                    recoverable=True,\n                )\n\n            if python and not content.get(\"code_execution\") and not content.get(\"files\"):\n                raise ToolExecutionException(\n                    \"Error: No output from Python execution. \"\n                    \"Please use 'print()' to display the result of your Python code.\",\n                    recoverable=True,\n                )\n\n        finally:\n            if not self.persistent_sandbox:\n                logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n                sandbox.kill()\n\n        if self.is_optimized_for_agents:\n            result_text = \"\"\n\n            if code_execution := content.get(\"code_execution\"):\n                result_text += \"## Output\\n\\n\" + code_execution + \"\\n\\n\"\n\n            if shell_command_execution := content.get(\"shell_command_execution\"):\n                result_text += \"## Shell Output\\n\\n\" + shell_command_execution + \"\\n\\n\"\n\n            all_files = content.get(\"files\", {})\n\n            uploaded_files = set()\n            if files_uploaded := content.get(\"files_uploaded\"):\n                for line in files_uploaded.split(\"\\n\"):\n                    if \" -&gt; \" in line:\n                        uploaded_path = line.split(\" -&gt; \")[1].strip()\n                        uploaded_files.add(uploaded_path)\n\n            new_files = {k: v for k, v in all_files.items() if k not in uploaded_files}\n\n            # Convert files to BytesIO objects for proper storage handling\n            files_bytesio = []\n            if new_files:\n                result_text += \"## Generated Files (ready for download)\\n\\n\"\n                for file_path, file_content in new_files.items():\n                    if file_content.startswith(\"Error:\"):\n                        result_text += f\"- {file_path}: {file_content}\\n\"\n                    else:\n                        try:\n                            # Decode content to bytes\n                            if file_content.startswith(\"data:\"):\n                                # Handle data URI format\n                                mime_part = file_content.split(\";\")[0].replace(\"data:\", \"\")\n                                base64_part = file_content.split(\",\", 1)[1]\n                                content_bytes = base64.b64decode(base64_part)\n                                content_type = mime_part\n                            else:\n                                # Handle plain base64 content\n                                content_bytes = base64.b64decode(file_content)\n                                content_type = detect_mime_type(content_bytes, file_path)\n\n                            file_name = file_path.split(\"/\")[-1]\n                            file_size = len(content_bytes)\n                            result_text += f\"- **{file_name}** ({file_size:,} bytes, {content_type})\\n\"\n\n                            # Create BytesIO object with metadata\n                            file_bytesio = io.BytesIO(content_bytes)\n                            file_bytesio.name = file_name\n                            file_bytesio.description = f\"Generated file from E2B sandbox: {file_path}\"\n                            file_bytesio.content_type = content_type\n\n                            # Ensure the BytesIO object is positioned at the beginning for reading\n                            file_bytesio.seek(0)\n\n                            files_bytesio.append(file_bytesio)\n\n                        except (base64.binascii.Error, ValueError, Exception) as e:\n                            error_msg = f\"Failed to decode file {file_path}: {str(e)}\"\n                            result_text += f\"- {file_path}: {error_msg}\\n\"\n                            logger.warning(f\"Tool {self.name} - {self.id}: {error_msg}\")\n\n                result_text += \"\\n\"\n\n            if packages_installation := content.get(\"packages_installation\"):\n                packages = packages_installation.replace(\"Installed packages: \", \"\")\n                if packages:\n                    result_text += f\"*Packages installed: {packages}*\\n\\n\"\n\n            if files_uploaded := content.get(\"files_uploaded\"):\n                files_list = []\n                for line in files_uploaded.split(\"\\n\"):\n                    if \" -&gt; \" in line:\n                        file_name = line.split(\" -&gt; \")[0].strip()\n                        files_list.append(file_name)\n                if files_list:\n                    result_text += f\"*Files uploaded: {', '.join(files_list)}*\\n\"\n                    result_text += \"Note: Uploaded files are available under /home/user/input. \"\n            logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n\" f\"{str(result_text)[:200]}...\")\n\n            return {\"content\": result_text, \"files\": files_bytesio}\n\n        return {\"content\": content}\n\n    def _create_sandbox_with_retry(self) -&gt; Sandbox:\n        \"\"\"Create E2B Sandbox with tenacity retry on 429 responses.\n\n        Uses exponential backoff strategy for rate limit errors with configuration\n        from the node's error_handling settings.\n        \"\"\"\n\n        @retry(\n            retry=retry_if_exception_type(E2BRateLimitException),\n            stop=stop_after_attempt(self.creation_error_handling.max_retries),\n            wait=wait_exponential_jitter(\n                initial=self.creation_error_handling.initial_wait_seconds,\n                max=self.creation_error_handling.max_wait_seconds,\n                exp_base=self.creation_error_handling.exponential_base,\n                jitter=self.creation_error_handling.jitter,\n            ),\n            reraise=True,\n        )\n        def create_sandbox() -&gt; Sandbox:\n            try:\n                sandbox = Sandbox.create(\n                    api_key=self.connection.api_key,\n                    timeout=self.timeout,\n                    domain=self.connection.domain,\n                )\n                logger.debug(f\"Tool {self.name} - {self.id}: Successfully created sandbox\")\n                return sandbox\n            except E2BRateLimitException:\n                logger.warning(\n                    f\"Tool {self.name} - {self.id}: Sandbox creation rate-limited. \"\n                    f\"Retrying with exponential backoff.\"\n                )\n                raise\n            except Exception:\n                raise\n\n        return create_sandbox()\n\n    def set_timeout(self, timeout: int) -&gt; None:\n        \"\"\"\n        Update the timeout for the sandbox during runtime.\n\n        Args:\n            timeout: New timeout value in seconds.\n        \"\"\"\n        self.timeout = timeout\n        if self._sandbox and self.persistent_sandbox:\n            try:\n                self._sandbox.set_timeout(timeout)\n                logger.debug(f\"Tool {self.name} - {self.id}: Updated sandbox timeout to {timeout}s\")\n            except Exception as e:\n                logger.warning(f\"Tool {self.name} - {self.id}: Failed to update sandbox timeout: {e}\")\n\n    def close(self) -&gt; None:\n        \"\"\"Close the persistent sandbox if it exists.\"\"\"\n        if self._sandbox and self.persistent_sandbox:\n            logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n            self._sandbox.kill()\n            self._sandbox = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: set</code>  <code>property</code>","text":"<p>Get parameters to exclude from dictionary representation.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>Set of parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the E2B interpreter tool.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the E2B interpreter tool.\"\"\"\n    super().__init__(**kwargs)\n    if self.persistent_sandbox and self.connection.api_key:\n        self._initialize_persistent_sandbox()\n    else:\n        logger.debug(f\"Tool {self.name} - {self.id}: Will initialize sandbox on each execute\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.close","title":"<code>close()</code>","text":"<p>Close the persistent sandbox if it exists.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the persistent sandbox if it exists.\"\"\"\n    if self._sandbox and self.persistent_sandbox:\n        logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n        self._sandbox.kill()\n        self._sandbox = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the requested action based on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>E2BInterpreterInputSchema</code> <p>The input schema containing execution parameters.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing execution results.</p> <p>Raises:</p> Type Description <code>ToolExecutionException</code> <p>If execution fails or invalid input provided.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def execute(\n    self, input_data: E2BInterpreterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the requested action based on the input data.\n\n    Args:\n        input_data: The input schema containing execution parameters.\n        config: Optional runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Dictionary containing execution results.\n\n    Raises:\n        ToolExecutionException: If execution fails or invalid input provided.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n\" f\"{str(input_data.model_dump())[:300]}\")\n    config = ensure_config(config)\n\n    if self.persistent_sandbox and self._sandbox:\n        sandbox = self._sandbox\n    else:\n        sandbox = self._create_sandbox_with_retry()\n        self._install_default_packages(sandbox)\n        if self.files:\n            self._upload_files(files=self.files, sandbox=sandbox)\n\n    tool_data = {\n        \"tool_session_id\": sandbox.sandbox_id,\n        \"tool_session_host\": sandbox.get_host(port=sandbox.connection_config.envd_port),\n    }\n    self.run_on_node_execute_run(\n        config.callbacks,\n        tool_data=tool_data,\n        **kwargs,\n    )\n\n    if sandbox and self.is_files_allowed:\n        try:\n            sandbox.commands.run(\"mkdir -p /home/user/input /home/user/output\")\n            logger.debug(\"Created /home/user/input and /home/user/output directories\")\n        except Exception as e:\n            logger.warning(f\"Failed to create directories: {e}\")\n\n    if input_data.timeout and sandbox:\n        try:\n            sandbox.set_timeout(input_data.timeout)\n            logger.debug(f\"Set per-call timeout to {input_data.timeout}s\")\n        except Exception as e:\n            logger.warning(f\"Failed to set per-call timeout: {e}\")\n\n    try:\n        content = {}\n\n        if files := input_data.files:\n            content[\"files_uploaded\"] = self._upload_files(files=files, sandbox=sandbox)\n\n        if packages := input_data.packages:\n            self._install_packages(sandbox=sandbox, packages=packages)\n            content[\"packages_installation\"] = f\"Installed packages: {input_data.packages}\"\n\n        if shell_command := input_data.shell_command:\n            content[\"shell_command_execution\"] = self._execute_shell_command(\n                shell_command, sandbox=sandbox, env=input_data.env, cwd=input_data.cwd\n            )\n\n        if python := input_data.python:\n            content[\"code_execution\"] = self._execute_python_code(python, sandbox=sandbox, params=input_data.params)\n\n        if download_files := input_data.download_files:\n            downloaded_files = self._download_files(download_files, sandbox=sandbox)\n            content.setdefault(\"files\", {}).update(downloaded_files)\n\n        if shell_command or python:\n            collected_files = self._collect_output_files(sandbox)\n            if collected_files:\n                content.setdefault(\"files\", {}).update(collected_files)\n\n        if not (packages or files or shell_command or python or download_files):\n            raise ToolExecutionException(\n                \"Error: Invalid input data. Please provide packages, files, shell_command, \"\n                \"python code, or download_files.\",\n                recoverable=True,\n            )\n\n        if python and not content.get(\"code_execution\") and not content.get(\"files\"):\n            raise ToolExecutionException(\n                \"Error: No output from Python execution. \"\n                \"Please use 'print()' to display the result of your Python code.\",\n                recoverable=True,\n            )\n\n    finally:\n        if not self.persistent_sandbox:\n            logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n            sandbox.kill()\n\n    if self.is_optimized_for_agents:\n        result_text = \"\"\n\n        if code_execution := content.get(\"code_execution\"):\n            result_text += \"## Output\\n\\n\" + code_execution + \"\\n\\n\"\n\n        if shell_command_execution := content.get(\"shell_command_execution\"):\n            result_text += \"## Shell Output\\n\\n\" + shell_command_execution + \"\\n\\n\"\n\n        all_files = content.get(\"files\", {})\n\n        uploaded_files = set()\n        if files_uploaded := content.get(\"files_uploaded\"):\n            for line in files_uploaded.split(\"\\n\"):\n                if \" -&gt; \" in line:\n                    uploaded_path = line.split(\" -&gt; \")[1].strip()\n                    uploaded_files.add(uploaded_path)\n\n        new_files = {k: v for k, v in all_files.items() if k not in uploaded_files}\n\n        # Convert files to BytesIO objects for proper storage handling\n        files_bytesio = []\n        if new_files:\n            result_text += \"## Generated Files (ready for download)\\n\\n\"\n            for file_path, file_content in new_files.items():\n                if file_content.startswith(\"Error:\"):\n                    result_text += f\"- {file_path}: {file_content}\\n\"\n                else:\n                    try:\n                        # Decode content to bytes\n                        if file_content.startswith(\"data:\"):\n                            # Handle data URI format\n                            mime_part = file_content.split(\";\")[0].replace(\"data:\", \"\")\n                            base64_part = file_content.split(\",\", 1)[1]\n                            content_bytes = base64.b64decode(base64_part)\n                            content_type = mime_part\n                        else:\n                            # Handle plain base64 content\n                            content_bytes = base64.b64decode(file_content)\n                            content_type = detect_mime_type(content_bytes, file_path)\n\n                        file_name = file_path.split(\"/\")[-1]\n                        file_size = len(content_bytes)\n                        result_text += f\"- **{file_name}** ({file_size:,} bytes, {content_type})\\n\"\n\n                        # Create BytesIO object with metadata\n                        file_bytesio = io.BytesIO(content_bytes)\n                        file_bytesio.name = file_name\n                        file_bytesio.description = f\"Generated file from E2B sandbox: {file_path}\"\n                        file_bytesio.content_type = content_type\n\n                        # Ensure the BytesIO object is positioned at the beginning for reading\n                        file_bytesio.seek(0)\n\n                        files_bytesio.append(file_bytesio)\n\n                    except (base64.binascii.Error, ValueError, Exception) as e:\n                        error_msg = f\"Failed to decode file {file_path}: {str(e)}\"\n                        result_text += f\"- {file_path}: {error_msg}\\n\"\n                        logger.warning(f\"Tool {self.name} - {self.id}: {error_msg}\")\n\n            result_text += \"\\n\"\n\n        if packages_installation := content.get(\"packages_installation\"):\n            packages = packages_installation.replace(\"Installed packages: \", \"\")\n            if packages:\n                result_text += f\"*Packages installed: {packages}*\\n\\n\"\n\n        if files_uploaded := content.get(\"files_uploaded\"):\n            files_list = []\n            for line in files_uploaded.split(\"\\n\"):\n                if \" -&gt; \" in line:\n                    file_name = line.split(\" -&gt; \")[0].strip()\n                    files_list.append(file_name)\n            if files_list:\n                result_text += f\"*Files uploaded: {', '.join(files_list)}*\\n\"\n                result_text += \"Note: Uploaded files are available under /home/user/input. \"\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n\" f\"{str(result_text)[:200]}...\")\n\n        return {\"content\": result_text, \"files\": files_bytesio}\n\n    return {\"content\": content}\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.set_timeout","title":"<code>set_timeout(timeout)</code>","text":"<p>Update the timeout for the sandbox during runtime.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>New timeout value in seconds.</p> required Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def set_timeout(self, timeout: int) -&gt; None:\n    \"\"\"\n    Update the timeout for the sandbox during runtime.\n\n    Args:\n        timeout: New timeout value in seconds.\n    \"\"\"\n    self.timeout = timeout\n    if self._sandbox and self.persistent_sandbox:\n        try:\n            self._sandbox.set_timeout(timeout)\n            logger.debug(f\"Tool {self.name} - {self.id}: Updated sandbox timeout to {timeout}s\")\n        except Exception as e:\n            logger.warning(f\"Tool {self.name} - {self.id}: Failed to update sandbox timeout: {e}\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert instance to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert instance to dictionary format.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    if self.files:\n        data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.detect_mime_type","title":"<code>detect_mime_type(file_content, file_path)</code>","text":"<p>Detect MIME type using magic numbers and file extension.</p> <p>Parameters:</p> Name Type Description Default <code>file_content</code> <code>bytes</code> <p>The raw file content as bytes</p> required <code>file_path</code> <code>str</code> <p>The file path to extract extension from</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detected MIME type</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def detect_mime_type(file_content: bytes, file_path: str) -&gt; str:\n    \"\"\"\n    Detect MIME type using magic numbers and file extension.\n\n    Args:\n        file_content: The raw file content as bytes\n        file_path: The file path to extract extension from\n\n    Returns:\n        str: The detected MIME type\n    \"\"\"\n    magic_signatures = {\n        # Images\n        b\"\\x89PNG\\r\\n\\x1a\\n\": \"image/png\",\n        b\"\\xff\\xd8\\xff\": \"image/jpeg\",\n        b\"GIF87a\": \"image/gif\",\n        b\"GIF89a\": \"image/gif\",\n        b\"RIFF\": \"image/webp\",\n        b\"BM\": \"image/bmp\",\n        b\"\\x00\\x00\\x01\\x00\": \"image/x-icon\",\n        # Documents\n        b\"%PDF\": \"application/pdf\",\n        b\"PK\\x03\\x04\": \"application/zip\",\n        b\"\\xd0\\xcf\\x11\\xe0\\xa1\\xb1\\x1a\\xe1\": \"application/vnd.ms-office\",\n        # Text/Data\n        b\"{\\n\": \"application/json\",\n        b'{\"': \"application/json\",\n        b\"[\\n\": \"application/json\",\n        b\"[{\": \"application/json\",\n    }\n\n    for signature, mime_type in magic_signatures.items():\n        if file_content.startswith(signature):\n            if signature == b\"RIFF\" and len(file_content) &gt; 12:\n                if file_content[8:12] == b\"WEBP\":\n                    return \"image/webp\"\n                else:\n                    continue\n            return mime_type\n\n    extension = file_path.lower().split(\".\")[-1] if \".\" in file_path else \"\"\n\n    extension_map = {\n        \"png\": \"image/png\",\n        \"jpg\": \"image/jpeg\",\n        \"jpeg\": \"image/jpeg\",\n        \"gif\": \"image/gif\",\n        \"webp\": \"image/webp\",\n        \"bmp\": \"image/bmp\",\n        \"ico\": \"image/x-icon\",\n        \"svg\": \"image/svg+xml\",\n        \"pdf\": \"application/pdf\",\n        \"xlsx\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n        \"xls\": \"application/vnd.ms-excel\",\n        \"docx\": \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n        \"doc\": \"application/msword\",\n        \"pptx\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n        \"ppt\": \"application/vnd.ms-powerpoint\",\n        \"txt\": \"text/plain\",\n        \"csv\": \"text/csv\",\n        \"json\": \"application/json\",\n        \"xml\": \"application/xml\",\n        \"html\": \"text/html\",\n        \"htm\": \"text/html\",\n        \"css\": \"text/css\",\n        \"js\": \"application/javascript\",\n        \"md\": \"text/markdown\",\n        \"zip\": \"application/zip\",\n        \"tar\": \"application/x-tar\",\n        \"gz\": \"application/gzip\",\n        \"rar\": \"application/vnd.rar\",\n    }\n\n    return extension_map.get(extension, \"application/octet-stream\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.generate_fallback_filename","title":"<code>generate_fallback_filename(file)</code>","text":"<p>Generate a unique fallback filename for uploaded files.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes | BytesIO</code> <p>File content as bytes or BytesIO object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique filename based on the object's id.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def generate_fallback_filename(file: bytes | io.BytesIO) -&gt; str:\n    \"\"\"\n    Generate a unique fallback filename for uploaded files.\n\n    Args:\n        file: File content as bytes or BytesIO object.\n\n    Returns:\n        str: A unique filename based on the object's id.\n    \"\"\"\n    return f\"uploaded_file_{id(file)}.bin\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.generate_file_description","title":"<code>generate_file_description(file, length=20)</code>","text":"<p>Generate a description for a file based on its content.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes | BytesIO</code> <p>File content as bytes or BytesIO object.</p> required <code>length</code> <code>int</code> <p>Maximum number of bytes to include in the description.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A description of the file's content or existing description.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def generate_file_description(file: bytes | io.BytesIO, length: int = 20) -&gt; str:\n    \"\"\"\n    Generate a description for a file based on its content.\n\n    Args:\n        file: File content as bytes or BytesIO object.\n        length: Maximum number of bytes to include in the description.\n\n    Returns:\n        str: A description of the file's content or existing description.\n    \"\"\"\n    if description := getattr(file, \"description\", None):\n        return description\n\n    file_content = file.getbuffer()[:length] if isinstance(file, io.BytesIO) else file[:length]\n    return f\"File starting with: {file_content.hex()}\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.handle_file_upload","title":"<code>handle_file_upload(files)</code>","text":"<p>Handles file uploading and converts all inputs to BytesIO objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[bytes | BytesIO | FileInfo]</code> <p>List of file objects to upload.</p> required <p>Returns:</p> Type Description <code>list[BytesIO]</code> <p>list[io.BytesIO]: List of BytesIO objects with file data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid file data type is provided.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def handle_file_upload(files: list[bytes | io.BytesIO | FileInfo]) -&gt; list[io.BytesIO]:\n    \"\"\"\n    Handles file uploading and converts all inputs to BytesIO objects.\n\n    Args:\n        files: List of file objects to upload.\n\n    Returns:\n        list[io.BytesIO]: List of BytesIO objects with file data.\n\n    Raises:\n        ValueError: If invalid file data type is provided.\n    \"\"\"\n    files_data = []\n    for file in files:\n        if isinstance(file, io.BytesIO):\n            files_data.append(file)\n        elif isinstance(file, bytes):\n            file_name = getattr(file, \"name\", generate_fallback_filename(file))\n            bytes_io = io.BytesIO(file)\n            bytes_io.name = file_name\n            files_data.append(bytes_io)\n        elif isinstance(file, FileInfo):\n            bytes_io = io.BytesIO(file.content)\n            bytes_io.name = file.name\n            files_data.append(bytes_io)\n        else:\n            raise ValueError(f\"Error: Invalid file data type: {type(file)}. \" f\"Expected bytes or BytesIO or FileInfo.\")\n\n    return files_data\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.should_use_data_uri","title":"<code>should_use_data_uri(mime_type)</code>","text":"<p>Determine if a file should be returned as a data URI.</p> <p>Parameters:</p> Name Type Description Default <code>mime_type</code> <code>str</code> <p>The MIME type of the file</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if should use data URI format</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def should_use_data_uri(mime_type: str) -&gt; bool:\n    \"\"\"\n    Determine if a file should be returned as a data URI.\n\n    Args:\n        mime_type: The MIME type of the file\n\n    Returns:\n        bool: True if should use data URI format\n    \"\"\"\n    # Use data URIs for images and other web-renderable content\n    data_uri_types = [\n        \"image/\",\n        \"text/html\",\n        \"text/css\",\n        \"application/javascript\",\n        \"image/svg+xml\",\n    ]\n\n    return any(mime_type.startswith(prefix) for prefix in data_uri_types)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/","title":"Exa search","text":""},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContentsExtrasOptions","title":"<code>ContentsExtrasOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Extra metadata extraction configuration.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContentsExtrasOptions(BaseModel):\n    \"\"\"Extra metadata extraction configuration.\"\"\"\n\n    links: int | None = Field(\n        default=None,\n        ge=0,\n        description=\"Number of outbound links to return from each result (0 disables).\",\n    )\n    image_links: int | None = Field(\n        default=None,\n        alias=\"imageLinks\",\n        ge=0,\n        description=\"Number of image URLs to extract per result (0 disables).\",\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContentsHighlightsOptions","title":"<code>ContentsHighlightsOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for highlights snippets.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContentsHighlightsOptions(BaseModel):\n    \"\"\"Configuration for highlights snippets.\"\"\"\n\n    num_sentences: int | None = Field(\n        default=None,\n        alias=\"numSentences\",\n        ge=1,\n        description=\"Sentences per highlight snippet (minimum 1).\",\n    )\n    highlights_per_url: int | None = Field(\n        default=None,\n        alias=\"highlightsPerUrl\",\n        ge=1,\n        description=\"How many highlight snippets to emit for each result (min 1).\",\n    )\n    query: str | None = Field(\n        default=None,\n        description=\"Optional override query that guides which passages are highlighted.\",\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContentsRequest","title":"<code>ContentsRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema mirroring Exa's ContentsRequest payload.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContentsRequest(BaseModel):\n    \"\"\"Schema mirroring Exa's ContentsRequest payload.\"\"\"\n\n    text: bool | ContentsTextOptions | None = Field(\n        default=None,\n        description=(\n            \"Toggle or configure raw page text extraction. Use True for defaults, or provide ContentsTextOptions to \"\n            \"limit characters / include HTML tags.\"\n        ),\n    )\n    highlights: ContentsHighlightsOptions | None = Field(\n        default=None,\n        description=(\n            \"Snippet extraction tuned by numSentences/highlightsPerUrl and optional query to steer what's highlighted.\"\n        ),\n    )\n    summary: ContentsSummaryOptions | None = Field(\n        default=None,\n        description=\"Generate an LLM summary, optionally guided by a query and/or JSON schema for structured output.\",\n    )\n    livecrawl: Literal[\"never\", \"fallback\", \"always\", \"preferred\"] | None = Field(\n        default=None,\n        description=(\n            \"Livecrawl strategy: 'never' (disable), 'fallback' (crawl when cache missing), 'always', or 'preferred' \"\n            \"(try livecrawl but fall back to cache). Defaults align with Exa search type.\"\n        ),\n    )\n    livecrawl_timeout: int | None = Field(\n        default=None,\n        alias=\"livecrawlTimeout\",\n        description=\"Timeout in ms for livecrawl fetches (Exa default 10000).\",\n    )\n    subpages: int | None = Field(\n        default=None,\n        description=\"How many subpages to crawl per result (default 0; higher costs more).\",\n    )\n    subpage_target: str | list[str] | None = Field(\n        default=None,\n        alias=\"subpageTarget\",\n        description=\"Keyword(s) that help Exa find relevant subpages (string or list of strings).\",\n    )\n    extras: ContentsExtrasOptions | None = Field(\n        default=None,\n        description=\"Return extra metadata such as additional links or image URLs via ContentsExtrasOptions.\",\n    )\n    context: bool | ContextOptions | None = Field(\n        default=None,\n        description=(\n            \"Return a combined context string. True uses defaults; provide ContextOptions to cap characters to match \"\n            \"LLM context windows.\"\n        ),\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContentsSummaryOptions","title":"<code>ContentsSummaryOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for content summaries.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContentsSummaryOptions(BaseModel):\n    \"\"\"Configuration for content summaries.\"\"\"\n\n    query: str | None = Field(\n        default=None,\n        description=\"Prompt/question for the summary (e.g., 'Key developments', 'Company overview').\",\n    )\n    summary_schema: dict[str, Any] | None = Field(\n        default=None,\n        alias=\"schema\",\n        description=(\n            \"JSON Schema describing the structured summary output. Follow JSON Schema draft-07 syntax when requesting \"\n            \"structured data.\"\n        ),\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContentsTextOptions","title":"<code>ContentsTextOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Advanced controls for Exa text extraction.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContentsTextOptions(BaseModel):\n    \"\"\"Advanced controls for Exa text extraction.\"\"\"\n\n    max_characters: int | None = Field(\n        default=None,\n        alias=\"maxCharacters\",\n        description=(\n            \"Maximum characters of page text to return. Helps manage response size/cost; Exa recommends 10k+ chars \"\n            \"when building RAG context.\"\n        ),\n    )\n    include_html_tags: bool | None = Field(\n        default=None,\n        alias=\"includeHtmlTags\",\n        description=\"Include HTML tags to preserve structure (useful for tables, headings, bold markers).\",\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ContextOptions","title":"<code>ContextOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Controls Exa context string construction.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ContextOptions(BaseModel):\n    \"\"\"Controls Exa context string construction.\"\"\"\n\n    max_characters: int | None = Field(\n        default=None,\n        alias=\"maxCharacters\",\n        description=(\n            \"Total character budget for the concatenated context string. Characters are split across results \"\n            \"(roughly evenly). Exa suggests &gt;=10000 characters for best RAG quality.\"\n        ),\n    )\n\n    model_config = ConfigDict(populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ExaInputSchema","title":"<code>ExaInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Exa search input parameters.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ExaInputSchema(BaseModel):\n    \"\"\"Schema for Exa search input parameters.\"\"\"\n\n    query: str = Field(description=\"Natural-language search query.\")\n    include_full_content: bool | None = Field(\n        default=None,\n        description=(\n            \"Shortcut flag: True requests default text/highlight/summary payloads for each result \"\n            \"(equivalent to ContentsRequest with simple booleans).\"\n        ),\n    )\n    use_autoprompt: bool | None = Field(\n        default=None,\n        description=\"If true, query will be converted to a Exa query.\"\n        \"Enabled by default for auto search, optional for neural search, and not available for keyword search.\",\n        json_schema_extra={\"is_accessible_to_agent\": False},\n    )\n    query_type: QueryType | None = Field(\n        default=None,\n        description=\"Type of query to be used. Options are 'keyword', 'neural', or 'auto'.\"\n        \"Neural uses an embeddings-based model, keyword is google-like SERP. \"\n        \"Default is auto, which automatically decides between keyword and neural.\",\n    )\n    category: CategoryType | None = Field(\n        default=None,\n        description=\"A data category to focus on.\"\n        \"Options are company, research paper, news, pdf,\"\n        \" github, tweet, personal site, linkedin profile, financial report.\",\n    )\n    limit: int | None = Field(\n        default=None,\n        ge=1,\n        le=100,\n        description=(\"Number of search results to return (keyword max 10, neural max 100 per Exa's API).\"),\n    )\n    include_domains: list[str] | None = Field(\n        default=None,\n        description=\"Whitelist of domains (e.g. ['arxiv.org', 'nature.com']). Results restricted to these domains.\",\n    )\n    exclude_domains: list[str] | None = Field(\n        default=None,\n        description=\"Blacklist of domains to omit from search results.\",\n    )\n    include_text: list[str] | None = Field(\n        default=None,\n        description=\"String(s) that must appear in the page text (currently supports one phrase up to 5 words).\",\n    )\n    exclude_text: list[str] | None = Field(\n        default=None,\n        description=\"String(s) that must *not* appear in the first ~1000 words of the page text.\",\n    )\n    start_crawl_date: str | None = Field(\n        default=None,\n        description=(\"Only include links crawled after this ISO 8601 date. Expected format 2023-01-01T00:00:00.000Z.\"),\n    )\n    end_crawl_date: str | None = Field(\n        default=None,\n        description=(\"Only include links crawled before this ISO 8601 date. Expected format 2023-12-31T00:00:00.000Z.\"),\n    )\n    start_published_date: str | None = Field(\n        default=None,\n        description=\"Only include links with a published date after this ISO 8601 date.\",\n    )\n    end_published_date: str | None = Field(\n        default=None,\n        description=\"Only include links with a published date before this ISO 8601 date.\",\n    )\n    context: bool | ContextOptions | None = Field(\n        default=None,\n        description=(\n            \"Return all page contents concatenated into a single context string. True uses defaults; provide \"\n            \"ContextOptions to set a maxCharacters budget (Exa recommends &gt;=10000).\"\n        ),\n    )\n    moderation: bool | None = Field(\n        default=None,\n        description=\"Enable Exa's content moderation filter for unsafe content.\",\n    )\n    contents: ContentsRequest | None = Field(\n        default=None,\n        description=(\n            \"Full customization of Exa's contents payload (text/highlights/summary/livecrawl/subpages/extras/context). \"\n            \"Use this when include_full_content is insufficient.\"\n        ),\n    )\n    brief: str = Field(\n        default=\"Searching the web for information.\",\n        description=\"Very brief description of the action being performed. Example: 'Search for AI research papers'.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ExaTool","title":"<code>ExaTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for performing web searches using the Exa AI API.</p> <p>This tool accepts various search parameters and returns relevant search results with options for filtering by date, domain, and content.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>action_type</code> <code>ActionType</code> <p>The type of action this tool performs.</p> <code>connection</code> <code>Exa</code> <p>The connection instance for the Exa API.</p> <code>include_full_content</code> <code>bool</code> <p>If true, retrieve full content, highlights, and summaries.</p> <code>use_autoprompt</code> <code>bool</code> <p>If true, query will be converted to a Exa query.</p> <code>query_type</code> <code>QueryType</code> <p>Type of query to be used.</p> <code>category</code> <code>CategoryType</code> <p>A data category to focus on.</p> <code>limit</code> <code>int</code> <p>Number of search results to return.</p> <code>include_domains</code> <code>list[str]</code> <p>List of domains to include.</p> <code>exclude_domains</code> <code>list[str]</code> <p>List of domains to exclude.</p> <code>include_text</code> <code>list[str]</code> <p>Strings that must be present.</p> <code>exclude_text</code> <code>list[str]</code> <p>Strings that must not be present.</p> <code>start_crawl_date</code> <code>str</code> <p>Include links crawled after this date.</p> <code>end_crawl_date</code> <code>str</code> <p>Include links crawled before this date.</p> <code>start_published_date</code> <code>str</code> <p>Include links published after this date.</p> <code>end_published_date</code> <code>str</code> <p>Include links published before this date.</p> <code>context</code> <code>bool | dict</code> <p>Return combined context content for results.</p> <code>moderation</code> <code>bool</code> <p>Enable Exa moderation filter.</p> <code>contents</code> <code>dict</code> <p>Advanced contents configuration overriding include_full_content defaults.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>class ExaTool(ConnectionNode):\n    \"\"\"\n    A tool for performing web searches using the Exa AI API.\n\n    This tool accepts various search parameters and returns relevant search results\n    with options for filtering by date, domain, and content.\n\n    Attributes:\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        action_type (ActionType): The type of action this tool performs.\n        connection (Exa): The connection instance for the Exa API.\n        include_full_content (bool): If true, retrieve full content, highlights, and summaries.\n        use_autoprompt (bool): If true, query will be converted to a Exa query.\n        query_type (QueryType): Type of query to be used.\n        category (CategoryType): A data category to focus on.\n        limit (int): Number of search results to return.\n        include_domains (list[str], optional): List of domains to include.\n        exclude_domains (list[str], optional): List of domains to exclude.\n        include_text (list[str], optional): Strings that must be present.\n        exclude_text (list[str], optional): Strings that must not be present.\n        start_crawl_date (str, optional): Include links crawled after this date.\n        end_crawl_date (str, optional): Include links crawled before this date.\n        start_published_date (str, optional): Include links published after this date.\n        end_published_date (str, optional): Include links published before this date.\n        context (bool | dict, optional): Return combined context content for results.\n        moderation (bool, optional): Enable Exa moderation filter.\n        contents (dict, optional): Advanced contents configuration overriding include_full_content defaults.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Exa Search Tool\"\n    description: str = DESCRIPTION_EXA\n    action_type: ActionType = ActionType.WEB_SEARCH\n    is_parallel_execution_allowed: bool = True\n    connection: Exa\n\n    include_full_content: bool = Field(\n        default=False, description=\"If true, retrieve full content, highlights, and summaries for search results.\"\n    )\n    use_autoprompt: bool = Field(default=False, description=\"If true, query will be converted to a Exa query.\")\n    query_type: QueryType = Field(default=QueryType.auto, description=\"Type of query to be used.\")\n    category: CategoryType | None = Field(default=None, description=\"A data category to focus on.\")\n    limit: int = Field(default=10, ge=1, le=100, description=\"Number of search results to return.\")\n    include_domains: list[str] | None = Field(default=None, description=\"List of domains to include in the search.\")\n    exclude_domains: list[str] | None = Field(default=None, description=\"List of domains to exclude from the search.\")\n    include_text: list[str] | None = Field(default=None, description=\"Strings that must be present in webpage text.\")\n    exclude_text: list[str] | None = Field(\n        default=None, description=\"Strings that must not be present in webpage text.\"\n    )\n    start_crawl_date: str | None = Field(\n        default=None, description=\"Only include links crawled after this ISO 8601 date.\"\n    )\n    end_crawl_date: str | None = Field(\n        default=None, description=\"Only include links crawled before this ISO 8601 date.\"\n    )\n    start_published_date: str | None = Field(\n        default=None, description=\"Only include links published after this ISO 8601 date.\"\n    )\n    end_published_date: str | None = Field(\n        default=None, description=\"Only include links published before this ISO 8601 date.\"\n    )\n    context: bool | ContextOptions | None = Field(\n        default=None,\n        description=\"Return a combined context blob (True for defaults, ContextOptions to cap characters).\",\n    )\n    moderation: bool | None = Field(default=None, description=\"Enable Exa's content moderation filter.\")\n    contents: ContentsRequest | None = Field(\n        default=None, description=\"Advanced contents configuration mirroring Exa's ContentsRequest schema.\"\n    )\n\n    MAX_SNIPPET_CHARS: ClassVar[int] = 800\n    MAX_CONTEXT_CHARS: ClassVar[int] = 4000\n    MAX_HIGHLIGHTS: ClassVar[int] = 5\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[ExaInputSchema]] = ExaInputSchema\n\n    @staticmethod\n    def to_camel_case(snake_str: str) -&gt; str:\n        \"\"\"Convert snake_case to camelCase.\"\"\"\n        components = snake_str.split(\"_\")\n        return components[0] + \"\".join(x.title() for x in components[1:])\n\n    @staticmethod\n    def _truncate(text: str, limit: int) -&gt; str:\n        \"\"\"Trim text to the specified length while keeping whole sentences when possible.\"\"\"\n\n        text = (text or \"\").strip()\n        if not text or len(text) &lt;= limit:\n            return text\n\n        truncated = text[:limit].rsplit(\" \", 1)[0]\n        return truncated.rstrip(\"\\n\") + \"...\"\n\n    def _format_search_results(self, results: list[dict[str, Any]]) -&gt; str:\n        \"\"\"\n        Formats the search results into a human-readable string.\n\n        Args:\n            results (list[dict[str, Any]]): The raw search results.\n\n        Returns:\n            str: A formatted string containing the search results.\n        \"\"\"\n        if not results:\n            return \"No results returned by Exa.\"\n\n        formatted_results = []\n        for index, result in enumerate(results, start=1):\n            title = result.get(\"title\") or DEFAULT_RESULT_TITLE\n            url = result.get(\"url\")\n            published = result.get(\"publishedDate\") or DEFAULT_RESULT_PUBLISHED_DATE\n            author = result.get(\"author\") or DEFAULT_RESULT_AUTHOR\n            score = result.get(\"score\")\n\n            formatted_results.append(f\"### Result {index}: {title}\")\n            if url:\n                formatted_results.append(f\"- URL: [{url}]({url})\")\n            else:\n                formatted_results.append(\"- URL: Not available\")\n\n            formatted_results.extend(\n                [\n                    f\"- Published: {published}\",\n                    f\"- Author: {author}\",\n                    f\"- Score: {score if score is not None else 'N/A'}\",\n                ]\n            )\n\n            highlights = (result.get(\"highlights\") or [])[: self.MAX_HIGHLIGHTS]\n            if highlights:\n                formatted_results.append(\"- Highlights:\")\n                formatted_results.extend([f\"  * {highlight.strip()}\" for highlight in highlights])\n\n            summary = (result.get(\"summary\") or \"\").strip()\n            if summary:\n                formatted_results.append(f\"- Summary: {summary}\")\n\n            text = (result.get(\"text\") or \"\").strip()\n            if text:\n                snippet = self._truncate(text, self.MAX_SNIPPET_CHARS)\n                if snippet:\n                    formatted_results.append(f\"- Snippet: {snippet}\")\n\n            formatted_results.append(\"\")\n\n        return \"\\n\".join(formatted_results).strip()\n\n    def _format_sources(self, results: list[dict[str, Any]]) -&gt; list[str]:\n        \"\"\"Create markdown-friendly source list.\"\"\"\n\n        sources = []\n        for result in results:\n            title = result.get(\"title\") or DEFAULT_RESULT_TITLE\n            url = result.get(\"url\")\n            if url:\n                sources.append(f\"- [{title}]({url})\")\n            else:\n                sources.append(f\"- {title}\")\n        return sources\n\n    def _format_context_section(self, context_blob: str | None) -&gt; str | None:\n        \"\"\"Summarize the context blob returned by Exa without overwhelming the agent.\"\"\"\n\n        if not context_blob:\n            return None\n\n        return self._truncate(context_blob, self.MAX_CONTEXT_CHARS)\n\n    @staticmethod\n    def _serialize_contents(contents: ContentsRequest) -&gt; dict[str, Any]:\n        \"\"\"Serialize a ContentsRequest into the API payload shape.\"\"\"\n\n        return contents.model_dump(by_alias=True, exclude_none=True)\n\n    def execute(self, input_data: ExaInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the search using the Exa API and returns the formatted results.\n\n        Input parameters override node parameters when provided.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        payload = {\n            \"query\": input_data.query,\n            \"useAutoprompt\": (\n                input_data.use_autoprompt if input_data.use_autoprompt is not None else self.use_autoprompt\n            ),\n            \"type\": input_data.query_type if input_data.query_type is not None else self.query_type,\n            \"numResults\": input_data.limit if input_data.limit is not None else self.limit,\n            \"includeDomains\": (\n                input_data.include_domains if input_data.include_domains is not None else self.include_domains\n            ),\n            \"excludeDomains\": (\n                input_data.exclude_domains if input_data.exclude_domains is not None else self.exclude_domains\n            ),\n            \"includeText\": input_data.include_text if input_data.include_text is not None else self.include_text,\n            \"excludeText\": input_data.exclude_text if input_data.exclude_text is not None else self.exclude_text,\n            \"category\": input_data.category if input_data.category is not None else self.category,\n            \"startCrawlDate\": (\n                input_data.start_crawl_date if input_data.start_crawl_date is not None else self.start_crawl_date\n            ),\n            \"endCrawlDate\": (\n                input_data.end_crawl_date if input_data.end_crawl_date is not None else self.end_crawl_date\n            ),\n            \"startPublishedDate\": (\n                input_data.start_published_date\n                if input_data.start_published_date is not None\n                else self.start_published_date\n            ),\n            \"endPublishedDate\": (\n                input_data.end_published_date if input_data.end_published_date is not None else self.end_published_date\n            ),\n            \"context\": input_data.context if input_data.context is not None else self.context,\n            \"moderation\": input_data.moderation if input_data.moderation is not None else self.moderation,\n        }\n\n        if isinstance(payload[\"type\"], QueryType):\n            payload[\"type\"] = payload[\"type\"].value\n\n        context_value = payload.get(\"context\")\n        if isinstance(context_value, ContextOptions):\n            payload[\"context\"] = context_value.model_dump(by_alias=True, exclude_none=True)\n\n        payload = {k: v for k, v in payload.items() if v is not None}\n\n        include_full_content = (\n            input_data.include_full_content\n            if input_data.include_full_content is not None\n            else self.include_full_content\n        )\n        contents_configuration = input_data.contents if input_data.contents is not None else self.contents\n\n        if contents_configuration is not None:\n            payload[\"contents\"] = self._serialize_contents(contents_configuration)\n        elif include_full_content:\n            default_contents = ContentsRequest(\n                text=ContentsTextOptions(max_characters=1000, include_html_tags=False),\n                highlights=ContentsHighlightsOptions(\n                    num_sentences=3,\n                    highlights_per_url=2,\n                    query=payload[\"query\"],\n                ),\n                summary=ContentsSummaryOptions(query=f\"Summarize the main points about {payload['query']}\"),\n                context=True,\n            )\n            payload[\"contents\"] = self._serialize_contents(default_contents)\n\n        connection_url = urljoin(self.connection.url, \"search\")\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json=payload,\n                headers=self.connection.headers,\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to retrieve search results. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        results = search_result.get(\"results\", [])\n        formatted_results = self._format_search_results(results)\n\n        sources_with_url = self._format_sources(results)\n        formatted_context = self._format_context_section(search_result.get(\"context\"))\n\n        urls = [r.get(\"url\") for r in results]\n        if self.is_optimized_for_agents:\n            result_parts = [\"## Sources\", \"\\n\".join(sources_with_url)]\n            result_parts.extend([\"## Search Results\", formatted_results])\n            if formatted_context:\n                result_parts.extend([\"## Context\", formatted_context])\n            result = \"\\n\\n\".join(result_parts)\n\n            output = {\"content\": result, \"urls\": urls}\n        else:\n            result = {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"urls\": urls,\n                \"context\": formatted_context,\n                \"raw_response\": search_result,\n            }\n\n            output = {\"content\": result}\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ExaTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the search using the Exa API and returns the formatted results.</p> <p>Input parameters override node parameters when provided.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>def execute(self, input_data: ExaInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the search using the Exa API and returns the formatted results.\n\n    Input parameters override node parameters when provided.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    payload = {\n        \"query\": input_data.query,\n        \"useAutoprompt\": (\n            input_data.use_autoprompt if input_data.use_autoprompt is not None else self.use_autoprompt\n        ),\n        \"type\": input_data.query_type if input_data.query_type is not None else self.query_type,\n        \"numResults\": input_data.limit if input_data.limit is not None else self.limit,\n        \"includeDomains\": (\n            input_data.include_domains if input_data.include_domains is not None else self.include_domains\n        ),\n        \"excludeDomains\": (\n            input_data.exclude_domains if input_data.exclude_domains is not None else self.exclude_domains\n        ),\n        \"includeText\": input_data.include_text if input_data.include_text is not None else self.include_text,\n        \"excludeText\": input_data.exclude_text if input_data.exclude_text is not None else self.exclude_text,\n        \"category\": input_data.category if input_data.category is not None else self.category,\n        \"startCrawlDate\": (\n            input_data.start_crawl_date if input_data.start_crawl_date is not None else self.start_crawl_date\n        ),\n        \"endCrawlDate\": (\n            input_data.end_crawl_date if input_data.end_crawl_date is not None else self.end_crawl_date\n        ),\n        \"startPublishedDate\": (\n            input_data.start_published_date\n            if input_data.start_published_date is not None\n            else self.start_published_date\n        ),\n        \"endPublishedDate\": (\n            input_data.end_published_date if input_data.end_published_date is not None else self.end_published_date\n        ),\n        \"context\": input_data.context if input_data.context is not None else self.context,\n        \"moderation\": input_data.moderation if input_data.moderation is not None else self.moderation,\n    }\n\n    if isinstance(payload[\"type\"], QueryType):\n        payload[\"type\"] = payload[\"type\"].value\n\n    context_value = payload.get(\"context\")\n    if isinstance(context_value, ContextOptions):\n        payload[\"context\"] = context_value.model_dump(by_alias=True, exclude_none=True)\n\n    payload = {k: v for k, v in payload.items() if v is not None}\n\n    include_full_content = (\n        input_data.include_full_content\n        if input_data.include_full_content is not None\n        else self.include_full_content\n    )\n    contents_configuration = input_data.contents if input_data.contents is not None else self.contents\n\n    if contents_configuration is not None:\n        payload[\"contents\"] = self._serialize_contents(contents_configuration)\n    elif include_full_content:\n        default_contents = ContentsRequest(\n            text=ContentsTextOptions(max_characters=1000, include_html_tags=False),\n            highlights=ContentsHighlightsOptions(\n                num_sentences=3,\n                highlights_per_url=2,\n                query=payload[\"query\"],\n            ),\n            summary=ContentsSummaryOptions(query=f\"Summarize the main points about {payload['query']}\"),\n            context=True,\n        )\n        payload[\"contents\"] = self._serialize_contents(default_contents)\n\n    connection_url = urljoin(self.connection.url, \"search\")\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json=payload,\n            headers=self.connection.headers,\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to retrieve search results. Error: {str(e)}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    results = search_result.get(\"results\", [])\n    formatted_results = self._format_search_results(results)\n\n    sources_with_url = self._format_sources(results)\n    formatted_context = self._format_context_section(search_result.get(\"context\"))\n\n    urls = [r.get(\"url\") for r in results]\n    if self.is_optimized_for_agents:\n        result_parts = [\"## Sources\", \"\\n\".join(sources_with_url)]\n        result_parts.extend([\"## Search Results\", formatted_results])\n        if formatted_context:\n            result_parts.extend([\"## Context\", formatted_context])\n        result = \"\\n\\n\".join(result_parts)\n\n        output = {\"content\": result, \"urls\": urls}\n    else:\n        result = {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"urls\": urls,\n            \"context\": formatted_context,\n            \"raw_response\": search_result,\n        }\n\n        output = {\"content\": result}\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/tools/exa_search/#dynamiq.nodes.tools.exa_search.ExaTool.to_camel_case","title":"<code>to_camel_case(snake_str)</code>  <code>staticmethod</code>","text":"<p>Convert snake_case to camelCase.</p> Source code in <code>dynamiq/nodes/tools/exa_search.py</code> <pre><code>@staticmethod\ndef to_camel_case(snake_str: str) -&gt; str:\n    \"\"\"Convert snake_case to camelCase.\"\"\"\n    components = snake_str.split(\"_\")\n    return components[0] + \"\".join(x.title() for x in components[1:])\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/","title":"File tools","text":""},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.EditOperation","title":"<code>EditOperation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single find-and-replace operation.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class EditOperation(BaseModel):\n    \"\"\"A single find-and-replace operation.\"\"\"\n\n    find: str = Field(..., min_length=1, description=\"Exact string to locate in the file (literal match, no regex).\")\n    replace: str = Field(..., description=\"Replacement string.\")\n    replace_all: bool = Field(\n        default=False,\n        description=\"If true, replace all occurrences. Otherwise only the first.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileListInputSchema","title":"<code>FileListInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for file list input parameters.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileListInputSchema(BaseModel):\n    \"\"\"Schema for file list input parameters.\"\"\"\n\n    file_path: str = Field(\n        default=\"\", description=\"Path of the file to list. Default is the root path. Keep empty to list all files.\"\n    )\n    recursive: bool = Field(default=True, description=\"Whether to list files recursively. Default is True.\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileListTool","title":"<code>FileListTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for listing files in storage.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileListTool(Node):\n    \"\"\"\n    A tool for listing files in storage.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.FILE_OPERATION\n    name: str = \"FileListTool\"\n    description: str = \"\"\"Lists files in storage based on the provided file path.\"\"\"\n\n    file_store: FileStore = Field(..., description=\"File storage to list from.\")\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[FileListInputSchema]] = FileListInputSchema\n\n    def execute(\n        self,\n        input_data: FileListInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        try:\n            files_list = self.file_store.list_files(directory=input_data.file_path, recursive=input_data.recursive)\n            files_string = \"Files currently available in the filesystem storage:\\n\"\n            for file in files_list:\n                files_string += f\"File: {file.name} | Path: {file.path} | Size: {file.size} bytes\\n\"\n\n            logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{files_string}\")\n            return {\"content\": files_string}\n\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to list files. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to list files. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadInputSchema","title":"<code>FileReadInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for file read input parameters.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileReadInputSchema(BaseModel):\n    \"\"\"Schema for file read input parameters.\"\"\"\n\n    file_path: str = Field(default=\"\", description=\"Path of the file to read\")\n    instructions: str | None = Field(\n        default=None,\n        description=\"Instructions for the file read. If not provided, the file will be read in its entirety.\",\n    )\n    mode: Literal[\"auto\", \"full\", \"chunked\", \"summary\"] = Field(\n        default=\"auto\",\n        description=(\n            \"Controls how the file content is returned. \"\n            \"'auto' uses default heuristics, 'full' forces entire content, \"\n            \"'chunked' always returns segmented chunks, and 'summary' returns a short preview.\"\n        ),\n    )\n    chunk_size_override: int | None = Field(\n        default=None,\n        description=\"Optional chunk size override (\"\n        \"in bytes/chars depending on content type) when returning chunked output.\",\n    )\n    max_preview_bytes: int | None = Field(\n        default=None,\n        description=\"Optional maximum number of bytes/chars to include when returning summary previews.\",\n    )\n    document_mode: Literal[\"file\", \"page\"] = Field(\n        default=\"file\",\n        description=\"For PDF-like documents, 'page' keeps content separated per page (with metadata).\",\n    )\n    brief: str = Field(\n        default=\"Reading a file\",\n        description=\"Very brief description of the action being performed. \"\n        \"Example: 'Read the file report.txt', 'Read the PDF report.pdf.\",\n    )\n\n    @field_validator(\"file_path\")\n    @classmethod\n    def validate_path(cls, v: str, info: ValidationInfo) -&gt; str:\n        \"\"\"Validate file_path to prevent path traversal attacks.\"\"\"\n        allow_absolute = bool((info.context or {}).get(\"absolute_file_paths_allowed\"))\n        return validate_file_path(v, allow_absolute=allow_absolute)\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadInputSchema.validate_path","title":"<code>validate_path(v, info)</code>  <code>classmethod</code>","text":"<p>Validate file_path to prevent path traversal attacks.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>@field_validator(\"file_path\")\n@classmethod\ndef validate_path(cls, v: str, info: ValidationInfo) -&gt; str:\n    \"\"\"Validate file_path to prevent path traversal attacks.\"\"\"\n    allow_absolute = bool((info.context or {}).get(\"absolute_file_paths_allowed\"))\n    return validate_file_path(v, allow_absolute=allow_absolute)\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadTool","title":"<code>FileReadTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for reading files from storage with intelligent file processing.</p> <p>This tool can be passed to Agents to read files from the configured storage backend. It automatically detects file types and processes them using appropriate converters to extract text content. For large files, it automatically returns chunked content showing first, middle, and last parts. For images and PDFs with instructions, uses LLM processing if the model supports vision/PDF input.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>file_store</code> <code>FileStore</code> <p>File storage to read from.</p> <code>llm</code> <code>BaseLLM</code> <p>LLM that will be used to process files.</p> <code>max_size</code> <code>int</code> <p>Maximum size in bytes before chunking (default: 10000).</p> <code>chunk_size</code> <code>int</code> <p>Size of each chunk in bytes (default: 1000).</p> <code>converter_mapping</code> <code>dict[FileType, Node]</code> <p>Mapping of file types to converters.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileReadTool(Node):\n    \"\"\"\n    A tool for reading files from storage with intelligent file processing.\n\n    This tool can be passed to Agents to read files from the configured storage backend.\n    It automatically detects file types and processes them using appropriate converters to extract text content.\n    For large files, it automatically returns chunked content showing first, middle, and last parts.\n    For images and PDFs with instructions, uses LLM processing if the model supports vision/PDF input.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        file_store (FileStore): File storage to read from.\n        llm (BaseLLM): LLM that will be used to process files.\n        max_size (int): Maximum size in bytes before chunking (default: 10000).\n        chunk_size (int): Size of each chunk in bytes (default: 1000).\n        converter_mapping (dict[FileType, Node]): Mapping of file types to converters.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.FILE_OPERATION\n    name: str = \"FileReadTool\"\n    is_parallel_execution_allowed: bool = True\n    description: str = \"\"\"\n        Reads files from storage based on the provided file path with intelligent file processing.\n        Automatically detects file types (PDF, DOCX, PPTX, HTML, TXT, IMAGE, etc.) and extracts text content.\n        For large files (configurable threshold), returns first, middle, and last chunks as bytes with separators.\n        For images and PDFs with instructions, uses LLM processing if the model supports vision/PDF input.\n\n        Usage Examples:\n            - Read text file: {\"file_path\": \"config.txt\"}\n            - Read PDF: {\"file_path\": \"report.pdf\"}\n            - Per-page PDF read for downstream searches: {\"file_path\": \"report.pdf\", \"document_mode\": \"page\"}\n            - Read DOCX: {\"file_path\": \"document.docx\"}\n            - Read image: {\"file_path\": \"image.png\"} (extracts text using LLM)\n            - Read large file: {\"file_path\": \"large_data.json\"}\n            - Force summary preview: {\"file_path\": \"report.pdf\", \"mode\": \"summary\", \"max_preview_bytes\": 800}\n            - Always chunk: {\"file_path\": \"server.log\", \"mode\": \"chunked\", \"chunk_size_override\": 4000}\n            - Read image with instructions: {\"file_path\": \"image.png\", \"instructions\": \"Describe the image in detail\"}\n\n        Parameters:\n            - file_path: Path of the file to read\n            - instructions: Optional instructions for LLM processing of images.\n            - mode: \"auto\" (default), \"full\", \"chunked\", or \"summary\"\n            - chunk_size_override: Optional override for chunk sizes in bytes/chars\n            - max_preview_bytes: Optional cap for summary previews\n            - document_mode: \"file\" (default) or \"page\" for per-page PDF extraction\n\n        Notes:\n            - Whenever text is extracted from non-text sources (PDF, PPTX, spreadsheets, etc.), it is cached as\n              \"&lt;original_path&gt;.extracted.txt\" inside the same file store so FileSearchTool can reuse it without\n              re-running converters.\n    \"\"\"\n    llm: BaseLLM = Field(..., description=\"LLM used for image-aware file processing.\")\n    file_store: FileStore | Sandbox = Field(..., description=\"File storage to read from.\")\n    max_size: int = Field(default=10000, description=\"Maximum size in bytes before chunking (default: 10000)\")\n    chunk_size: int = Field(default=1000, description=\"Size of each chunk in bytes (default: 1000)\")\n    converter_mapping: dict[FileType, Node] | None = None\n    spreadsheet_preview_rows: int = Field(\n        default=5, description=\"Maximum number of rows to show per sheet when previewing spreadsheets.\"\n    )\n    spreadsheet_preview_max_chars: int = Field(\n        default=8000, description=\"Maximum characters to emit per sheet preview to avoid massive outputs.\"\n    )\n    absolute_file_paths_allowed: bool = Field(default=False, description=\"Whether to allow absolute paths.\")\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[FileReadInputSchema]] = FileReadInputSchema\n    _connection_manager: ConnectionManager | None = PrivateAttr(default=None)\n    _page_converter_cache: dict[FileType, Node] = PrivateAttr(default_factory=dict)\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        return {\"absolute_file_paths_allowed\": self.absolute_file_paths_allowed}\n\n    def init_components(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"\n        Initialize the components of the FileReadTool.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        self._connection_manager = connection_manager\n        super().init_components(connection_manager)\n\n        self._setup_converters(connection_manager)\n\n    def _setup_converters(self, connection_manager: ConnectionManager | None = None):\n        \"\"\"Setup internal converter components.\"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n\n        if not self.converter_mapping:\n            self.converter_mapping = {}\n            for file_type, converter_class in DEFAULT_FILE_TYPE_TO_CONVERTER_CLASS_MAP.items():\n                if file_type == FileType.IMAGE and converter_class == LLMImageConverter:\n                    self.converter_mapping[file_type] = converter_class(llm=self.llm)\n                else:\n                    self.converter_mapping[file_type] = converter_class()\n\n        initialized_converters = set()\n        for converter in self.converter_mapping.values():\n            if id(converter) not in initialized_converters:\n                if converter.is_postponed_component_init:\n                    converter.init_components(connection_manager)\n                initialized_converters.add(id(converter))\n                logger.info(f\"Initialized converter: {converter.name}\")\n\n    def _get_converter_for_type(self, file_type: FileType, document_mode: str) -&gt; Node | None:\n        \"\"\"Fetch appropriate converter, supporting per-page PDF extraction.\"\"\"\n        if document_mode == \"page\" and file_type == FileType.PDF:\n            cached = self._page_converter_cache.get(file_type)\n            if not cached:\n                page_converter = PyPDFConverter(document_creation_mode=PyPDFDocumentCreationMode.ONE_DOC_PER_PAGE)\n                page_converter.init_components(self._connection_manager or ConnectionManager())\n                self._page_converter_cache[file_type] = page_converter\n                cached = page_converter\n            return cached\n\n        if not self.converter_mapping:\n            return None\n        return self.converter_mapping.get(file_type)\n\n    def _detect_file_type(self, file: BytesIO, filename: str, config: RunnableConfig, **kwargs) -&gt; FileType | None:\n        \"\"\"\n        Detect the file type using custom detection function.\n\n        Args:\n            file: The file to analyze\n            filename: The filename for file type detection\n            config: Runtime configuration\n            **kwargs: Additional arguments\n\n        Returns:\n            FileType: The detected file type, or None if detection fails\n        \"\"\"\n        return detect_file_type(file, filename)\n\n    def _process_file_with_converter(\n        self,\n        file: BytesIO,\n        filename: str,\n        detected_type: FileType,\n        config: RunnableConfig,\n        instructions: str | None = None,\n        document_mode: str = \"file\",\n        **kwargs,\n    ) -&gt; tuple[str | None, list[dict[str, Any]] | None]:\n        \"\"\"\n        Process a file using the appropriate converter to extract text content.\n\n        Args:\n            file: The file to process\n            filename: The filename\n            detected_type: The detected file type\n            config: Runtime configuration\n            instructions: Custom instructions for image processing\n            **kwargs: Additional arguments\n\n        Returns:\n            tuple[str | None, list[dict]]: Extracted text content and optional structured page data.\n        \"\"\"\n\n        try:\n            if detected_type == FileType.SPREADSHEET:\n                spreadsheet_preview = self._render_spreadsheet_preview(file, filename)\n                if spreadsheet_preview:\n                    return spreadsheet_preview, None\n\n            if detected_type in self.converter_mapping:\n\n                converter = self._get_converter_for_type(detected_type, document_mode)\n                if not converter:\n                    logger.warning(f\"No converter available for file type: {detected_type}\")\n                    return None, None\n\n                if detected_type == FileType.IMAGE and instructions:\n                    converter = LLMImageConverter(llm=self.llm, extraction_instruction=instructions)\n                    converter_name = f\"{converter.name} (with custom instructions)\"\n                else:\n                    converter_name = converter.name\n\n                file.seek(0)\n                if not hasattr(file, \"name\"):\n                    file.name = filename\n\n                converter_input = {\"files\": [file]}\n                result = converter.run(\n                    input_data=converter_input,\n                    config=config,\n                    **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\"), \"run_depends\": []}),\n                )\n\n                if result.status == RunnableStatus.SUCCESS:\n                    documents = result.output.get(\"documents\", [])\n                    if documents:\n                        logger.info(f\"Successfully extracted text using {converter_name}\")\n                        if document_mode == \"page\":\n                            page_entries = []\n                            segments = []\n                            for idx, doc in enumerate(documents, start=1):\n                                page_num = doc.metadata.get(\"page_number\") if doc.metadata else None\n                                page_num = page_num or idx\n                                content = doc.content\n                                page_entries.append(\n                                    {\n                                        \"page\": page_num,\n                                        \"content\": content,\n                                        \"metadata\": doc.metadata or {},\n                                    }\n                                )\n                                segments.append(f\"=== PAGE {page_num} ===\\n{content}\")\n                            return \"\\n\\n\".join(segments), page_entries\n\n                        text_content = \"\\n\\n\".join([doc.content for doc in documents if hasattr(doc, \"content\")])\n                        return text_content, None\n                    else:\n                        logger.warning(f\"No documents extracted by {converter_name}\")\n                else:\n                    logger.warning(f\"Converter {converter_name} failed: {result.error}\")\n\n            else:\n                logger.warning(f\"No converter available for file type: {detected_type}\")\n\n        except Exception as e:\n            logger.warning(f\"File processing failed with converter: {str(e)}\")\n        return None, None\n\n    def _render_spreadsheet_preview(self, file: BytesIO, filename: str) -&gt; str | None:\n        \"\"\"Return a lightweight textual preview for spreadsheets using pandas head() per sheet.\"\"\"\n        try:\n            import pandas as pd\n        except Exception as exc:  # noqa: BLE001 optional dependency\n            logger.debug(\"pandas unavailable for spreadsheet preview: %s\", exc)\n            return None\n\n        try:\n            file.seek(0)\n            buffer = BytesIO(file.read())\n        except Exception as exc:\n            logger.warning(\"Failed to buffer spreadsheet %s for preview: %s\", filename, exc)\n            return None\n\n        try:\n            buffer.seek(0)\n            with pd.ExcelFile(buffer) as excel_file:\n\n                sheet_dimensions = self._sheet_dimensions_from_workbook(excel_file)\n\n                limit = self.spreadsheet_preview_rows\n                preview_segments: list[str] = [\n                    f\"Spreadsheet preview for '{filename}' \"\n                    f\"({len(excel_file.sheet_names)} sheet{'s' if len(excel_file.sheet_names) != 1 else ''}).\"\n                ]\n\n                pd_options = pd.option_context(\"display.width\", 120, \"display.max_colwidth\", 200)\n                with pd_options:\n                    for sheet_name in excel_file.sheet_names:\n                        try:\n                            preview_frame = excel_file.parse(sheet_name=sheet_name, nrows=limit)\n                        except Exception as exc:\n                            logger.warning(\"Failed to read preview rows for sheet %s: %s\", sheet_name, exc)\n                            continue\n\n                        total_rows, total_columns = sheet_dimensions.get(\n                            sheet_name,\n                            (len(preview_frame.index), len(preview_frame.columns)),\n                        )\n\n                        preview_segments.append(\n                            f\"=== Sheet '{sheet_name or '(Unnamed Sheet)'}' \"\n                            f\"\u2014 Rows: {total_rows:,}, Columns: {total_columns:,} \"\n                            f\"(showing up to {limit} row(s)) ===\"\n                        )\n\n                        if preview_frame.empty:\n                            preview_segments.append(\"[Sheet is empty]\")\n                            continue\n\n                        markdown = preview_frame.to_markdown(index=False)\n                        truncated = False\n                        max_chars = self.spreadsheet_preview_max_chars\n                        if max_chars and len(markdown) &gt; max_chars:\n                            markdown = f\"{markdown[: max_chars - 3]}...\"\n                            truncated = True\n\n                        preview_segments.append(markdown)\n                        if truncated:\n                            preview_segments.append(f\"[Preview truncated to {max_chars} characters]\")\n\n                        if total_rows &gt; limit:\n                            preview_segments.append(f\"... showing only the first {limit} row(s).\")\n\n                return \"\\n\\n\".join(preview_segments) if len(preview_segments) &gt; 1 else None\n\n        except Exception as exc:\n            logger.warning(\"Failed to open spreadsheet %s for preview: %s\", filename, exc)\n            return None\n\n    @staticmethod\n    def _sheet_dimensions_from_workbook(excel_file: Any) -&gt; dict[str, tuple[int, int]]:\n        workbook = getattr(excel_file, \"book\", None)\n        if workbook is None:\n            return {}\n\n        dimensions: dict[str, tuple[int, int]] = {}\n        for sheet_name in getattr(workbook, \"sheetnames\", []):\n            worksheet = workbook[sheet_name]\n\n            max_row = getattr(worksheet, \"max_row\", 0) or 0\n            max_column = getattr(worksheet, \"max_column\", 0) or 0\n\n            try:\n                header_row = next(worksheet.iter_rows(values_only=True, max_row=1), None)\n            except Exception:\n                header_row = None\n            has_header = bool(header_row and any(value is not None for value in header_row))\n\n            total_rows = max(max_row - (1 if has_header else 0), 0)\n            dimensions[sheet_name] = (total_rows, max_column)\n\n        return dimensions\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\n            \"llm\": True,\n            \"file_store\": True,\n            \"converter_mapping\": True,\n        }\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        data[\"file_store\"] = self.file_store.to_dict(**kwargs)\n        if self.converter_mapping:\n            data[\"converter_mapping\"] = {\n                file_type.value: converter.to_dict(**kwargs) for file_type, converter in self.converter_mapping.items()\n            }\n        return data\n\n    def _build_file_info(self, file_path: str, content: bytes) -&gt; FileInfo:\n        \"\"\"Build a FileInfo instance from a read file path and its raw content.\"\"\"\n        filename = os.path.basename(file_path)\n        return FileInfo(\n            name=filename,\n            path=file_path,\n            size=len(content),\n            content_type=mimetypes.guess_type(filename)[0] or \"application/octet-stream\",\n            content=content,\n        )\n\n    def execute(\n        self,\n        input_data: FileReadInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the file read operation and returns the file content.\n        For large files, returns first, middle, and last chunks instead of full content.\n        Automatically detects file type and extracts text content when possible.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        mode = input_data.mode or \"auto\"\n        chunk_size = input_data.chunk_size_override or self.chunk_size\n        chunk_size = max(chunk_size, 1)\n        preview_limit = input_data.max_preview_bytes or chunk_size\n        preview_limit = max(preview_limit, 1)\n        allow_cache = input_data.instructions is None and input_data.document_mode == \"file\"\n\n        try:\n            if not self.file_store.exists(input_data.file_path):\n                raise ToolExecutionException(\n                    f\"File '{input_data.file_path}' not found\",\n                    recoverable=True,\n                )\n\n            content = self.file_store.retrieve(input_data.file_path)\n            content_size = len(content)\n            file_info = self._build_file_info(input_data.file_path, content)\n\n            cached_text, cached_path = (None, None)\n            if allow_cache and not isinstance(self.file_store, Sandbox):\n                cached_text, cached_path = self._load_cached_text(input_data.file_path)\n\n            if cached_text:\n                self._log_text_preview(cached_text, \"cached extracted text\")\n                processed = self._render_text_content(\n                    text_content=cached_text,\n                    mode=mode,\n                    chunk_size=chunk_size,\n                    preview_limit=preview_limit,\n                    file_path=input_data.file_path,\n                )\n                processed = self._append_cache_hint(processed, cached_path, hint_enabled=False)\n                return {\n                    \"content\": processed,\n                    \"file_info\": file_info.model_dump(mode=\"json\"),\n                    \"cached_text_path\": cached_path,\n                }\n\n            try:\n                file_io = BytesIO(content)\n                filename = os.path.basename(input_data.file_path)\n\n                detected_type = self._detect_file_type(file_io, filename, config, **kwargs)\n\n                if detected_type:\n                    text_content, page_entries = self._process_file_with_converter(\n                        file_io,\n                        filename,\n                        detected_type,\n                        config,\n                        input_data.instructions,\n                        input_data.document_mode,\n                        **kwargs,\n                    )\n\n                    if text_content:\n                        logger.info(\n                            f\"Tool {self.name} - {self.id}: successfully processed file and extracted text content\"\n                        )\n                        self._log_text_preview(text_content, \"extracted text\")\n\n                        cached_path = None\n                        hint_enabled = False\n                        if allow_cache and not isinstance(self.file_store, Sandbox):\n                            cached_path = self._persist_extracted_text(input_data.file_path, text_content)\n                            hint_enabled = detected_type not in {FileType.TEXT, FileType.MARKDOWN}\n\n                        processed = self._render_text_content(\n                            text_content=text_content,\n                            mode=mode,\n                            chunk_size=chunk_size,\n                            preview_limit=preview_limit,\n                            file_path=input_data.file_path,\n                        )\n                        processed = self._append_cache_hint(processed, cached_path, hint_enabled)\n                        result_payload = {\"content\": processed, \"file_info\": file_info.model_dump(mode=\"json\")}\n                        if page_entries:\n                            result_payload[\"pages\"] = page_entries\n                        if cached_path:\n                            result_payload[\"cached_text_path\"] = cached_path\n                        return result_payload\n                    else:\n                        logger.warning(\n                            f\"Tool {self.name} - {self.id}: no text content extracted from file,\"\n                            \"falling back to raw content\"\n                        )\n                else:\n                    logger.warning(\n                        f\"Tool {self.name} - {self.id}: could not detect file type, falling back to raw content\"\n                    )\n\n            except Exception as e:\n                logger.warning(\n                    f\"Tool {self.name} - {self.id}: file processing failed: {str(e)}, falling back to raw content\"\n                )\n\n            rendered_content = self._render_binary_content(\n                content=content,\n                content_size=content_size,\n                mode=mode,\n                chunk_size=chunk_size,\n                preview_limit=preview_limit,\n                file_path=input_data.file_path,\n            )\n\n            return {\"content\": rendered_content, \"file_info\": file_info.model_dump(mode=\"json\")}\n\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to read file. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to read file. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n    def _create_chunked_content(self, content: bytes, chunk_size: int, file_path: str) -&gt; bytes:\n        \"\"\"\n        Create chunked content showing first, middle, and last parts of a large file.\n\n        Args:\n            content: The file content as bytes\n            chunk_size: Size of each chunk in bytes\n            file_path: Path of the file being read\n\n        Returns:\n            Concatenated bytes containing first, middle, and last chunks\n        \"\"\"\n        total_size = len(content)\n\n        first_chunk = content[:chunk_size]\n\n        middle_start = total_size // 2 - chunk_size // 2\n        middle_chunk = content[middle_start : middle_start + chunk_size]\n\n        last_chunk = content[-chunk_size:] if total_size &gt; chunk_size else content\n\n        separator = f\"\\n\\n--- CHUNKED FILE: {file_path} ({total_size:,} bytes total) ---\\n\".encode()\n        first_sep = f\"\\n--- FIRST {len(first_chunk):,} BYTES ---\\n\".encode()\n        middle_sep = f\"\\n--- MIDDLE {len(middle_chunk):,} BYTES (from position {middle_start:,}) ---\\n\".encode()\n        last_sep = f\"\\n--- LAST {len(last_chunk):,} BYTES ---\\n\".encode()\n\n        chunked_bytes = (\n            separator\n            + first_sep\n            + first_chunk\n            + middle_sep\n            + middle_chunk\n            + last_sep\n            + last_chunk\n            + b\"\\n\\n--- END OF CHUNKED FILE ---\\n\"\n        )\n\n        return chunked_bytes\n\n    def _create_chunked_text_content(self, content: str, chunk_size: int, file_path: str) -&gt; str:\n        \"\"\"\n        Create chunked text content showing first, middle, and last parts of a large text.\n\n        Args:\n            content: The text content as string\n            chunk_size: Size of each chunk in characters\n            file_path: Path of the file being read\n\n        Returns:\n            str: Concatenated string containing first, middle, and last chunks\n        \"\"\"\n        total_size = len(content)\n\n        first_chunk = content[:chunk_size]\n\n        middle_start = total_size // 2 - chunk_size // 2\n        middle_chunk = content[middle_start : middle_start + chunk_size]\n\n        last_chunk = content[-chunk_size:] if total_size &gt; chunk_size else content\n\n        separator = f\"\\n\\n--- CHUNKED TEXT FILE: {file_path} ({total_size:,} characters total) ---\\n\"\n        first_sep = f\"\\n--- FIRST {len(first_chunk):,} CHARACTERS ---\\n\"\n        middle_sep = f\"\\n--- MIDDLE {len(middle_chunk):,} CHARACTERS (from position {middle_start:,}) ---\\n\"\n        last_sep = f\"\\n--- LAST {len(last_chunk):,} CHARACTERS ---\\n\"\n\n        chunked_text = (\n            separator\n            + first_sep\n            + first_chunk\n            + middle_sep\n            + middle_chunk\n            + last_sep\n            + last_chunk\n            + \"\\n\\n--- END OF CHUNKED TEXT FILE ---\\n\"\n        )\n\n        return chunked_text\n\n    def _render_text_content(\n        self, text_content: str, mode: str, chunk_size: int, preview_limit: int, file_path: str\n    ) -&gt; str:\n        \"\"\"Render text output according to the requested mode.\"\"\"\n        match mode:\n            case \"full\":\n                return text_content\n            case \"chunked\":\n                return self._create_chunked_text_content(text_content, chunk_size, file_path)\n            case \"summary\":\n                return self._create_summary_text_content(text_content, preview_limit, file_path)\n            case _:\n                if len(text_content) &gt; self.max_size:\n                    return self._create_chunked_text_content(text_content, chunk_size, file_path)\n                return text_content\n\n    def _render_binary_content(\n        self, content: bytes, content_size: int, mode: str, chunk_size: int, preview_limit: int, file_path: str\n    ) -&gt; bytes | str:\n        \"\"\"Render binary output according to the requested mode.\"\"\"\n        match mode:\n            case \"full\":\n                return content\n            case \"chunked\":\n                return self._create_chunked_content(content, chunk_size, file_path)\n            case \"summary\":\n                return self._create_summary_bytes_content(content, preview_limit, file_path)\n            case _:\n                if content_size &lt;= self.max_size:\n                    return content\n                return self._create_chunked_content(content, chunk_size, file_path)\n\n    def _create_summary_text_content(self, content: str, max_chars: int, file_path: str) -&gt; str:\n        \"\"\"Return a short preview string for text documents.\"\"\"\n        preview = content[:max_chars]\n        suffix = \"...\" if len(content) &gt; len(preview) else \"\"\n        return (\n            f\"Preview of {file_path} (showing {len(preview):,} of {len(content):,} characters):\\n\" f\"{preview}{suffix}\"\n        )\n\n    def _create_summary_bytes_content(self, content: bytes, max_bytes: int, file_path: str) -&gt; str:\n        \"\"\"Return a short preview string for binary files.\"\"\"\n        preview = content[:max_bytes]\n        try:\n            preview_text = preview.decode(\"utf-8\")\n            descriptor = \"text\"\n        except UnicodeDecodeError:\n            preview_text = preview.hex()\n            descriptor = \"hex\"\n        suffix = \"...\" if len(content) &gt; len(preview) else \"\"\n        return (\n            f\"Preview of {file_path} ({descriptor}, showing {len(preview):,} of {len(content):,} bytes):\\n\"\n            f\"{preview_text}{suffix}\"\n        )\n\n    def _persist_extracted_text(self, original_path: str, text_content: str) -&gt; str | None:\n        \"\"\"Persist extracted text so future reads/searches can reuse it.\"\"\"\n        if not self.file_store:\n            return None\n\n        cache_path = self._derived_cache_path(original_path)\n        try:\n            self.file_store.store(\n                cache_path,\n                text_content.encode(\"utf-8\"),\n                content_type=\"text/plain\",\n                metadata={\n                    \"source\": \"file_read_tool\",\n                    \"original_path\": original_path,\n                },\n                overwrite=True,\n            )\n            logger.info(f\"Tool {self.name} - {self.id}: cached extracted text at {cache_path}\")\n            return cache_path\n        except Exception as exc:\n            logger.warning(f\"Tool {self.name} - {self.id}: failed to cache extracted text for {original_path}: {exc}\")\n            return None\n\n    def _load_cached_text(self, original_path: str) -&gt; tuple[str | None, str | None]:\n        \"\"\"Load cached extracted text if it exists.\"\"\"\n        if not self.file_store:\n            return None, None\n\n        cache_path = self._derived_cache_path(original_path)\n        try:\n            if not self.file_store.exists(cache_path):\n                return None, None\n            cached_bytes = self.file_store.retrieve(cache_path)\n            return cached_bytes.decode(\"utf-8\"), cache_path\n        except Exception as exc:\n            logger.warning(f\"Tool {self.name} - {self.id}: failed to load cached text for {original_path}: {exc}\")\n            return None, None\n\n    @staticmethod\n    def _derived_cache_path(original_path: str) -&gt; str:\n        return f\"{original_path}{EXTRACTED_TEXT_SUFFIX}\"\n\n    @staticmethod\n    def _append_cache_hint(content: str, cache_path: str | None, hint_enabled: bool = True) -&gt; str:\n        if cache_path and hint_enabled:\n            hint = (\n                f\"\\n\\n[Extracted text cached at '{cache_path}'. \"\n                \"Use FileSearchTool to search this processed content without re-reading the original file.]\"\n            )\n            return f\"{content}{hint}\"\n        return content\n\n    def _log_text_preview(self, text: str, context: str, limit: int = 200) -&gt; None:\n        \"\"\"Emit a short preview of extracted text so logs show what was parsed.\"\"\"\n        if not text:\n            return\n        preview = text.strip().replace(\"\\n\", \" \")\n        preview = preview[:limit]\n        suffix = \"...\" if len(text.strip()) &gt; limit else \"\"\n        logger.info(\n            f\"Tool {self.name} - {self.id}: {context} preview ({min(len(preview), limit)} chars) =&gt; {preview}{suffix}\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the file read operation and returns the file content. For large files, returns first, middle, and last chunks instead of full content. Automatically detects file type and extracts text content when possible.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def execute(\n    self,\n    input_data: FileReadInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the file read operation and returns the file content.\n    For large files, returns first, middle, and last chunks instead of full content.\n    Automatically detects file type and extracts text content when possible.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    mode = input_data.mode or \"auto\"\n    chunk_size = input_data.chunk_size_override or self.chunk_size\n    chunk_size = max(chunk_size, 1)\n    preview_limit = input_data.max_preview_bytes or chunk_size\n    preview_limit = max(preview_limit, 1)\n    allow_cache = input_data.instructions is None and input_data.document_mode == \"file\"\n\n    try:\n        if not self.file_store.exists(input_data.file_path):\n            raise ToolExecutionException(\n                f\"File '{input_data.file_path}' not found\",\n                recoverable=True,\n            )\n\n        content = self.file_store.retrieve(input_data.file_path)\n        content_size = len(content)\n        file_info = self._build_file_info(input_data.file_path, content)\n\n        cached_text, cached_path = (None, None)\n        if allow_cache and not isinstance(self.file_store, Sandbox):\n            cached_text, cached_path = self._load_cached_text(input_data.file_path)\n\n        if cached_text:\n            self._log_text_preview(cached_text, \"cached extracted text\")\n            processed = self._render_text_content(\n                text_content=cached_text,\n                mode=mode,\n                chunk_size=chunk_size,\n                preview_limit=preview_limit,\n                file_path=input_data.file_path,\n            )\n            processed = self._append_cache_hint(processed, cached_path, hint_enabled=False)\n            return {\n                \"content\": processed,\n                \"file_info\": file_info.model_dump(mode=\"json\"),\n                \"cached_text_path\": cached_path,\n            }\n\n        try:\n            file_io = BytesIO(content)\n            filename = os.path.basename(input_data.file_path)\n\n            detected_type = self._detect_file_type(file_io, filename, config, **kwargs)\n\n            if detected_type:\n                text_content, page_entries = self._process_file_with_converter(\n                    file_io,\n                    filename,\n                    detected_type,\n                    config,\n                    input_data.instructions,\n                    input_data.document_mode,\n                    **kwargs,\n                )\n\n                if text_content:\n                    logger.info(\n                        f\"Tool {self.name} - {self.id}: successfully processed file and extracted text content\"\n                    )\n                    self._log_text_preview(text_content, \"extracted text\")\n\n                    cached_path = None\n                    hint_enabled = False\n                    if allow_cache and not isinstance(self.file_store, Sandbox):\n                        cached_path = self._persist_extracted_text(input_data.file_path, text_content)\n                        hint_enabled = detected_type not in {FileType.TEXT, FileType.MARKDOWN}\n\n                    processed = self._render_text_content(\n                        text_content=text_content,\n                        mode=mode,\n                        chunk_size=chunk_size,\n                        preview_limit=preview_limit,\n                        file_path=input_data.file_path,\n                    )\n                    processed = self._append_cache_hint(processed, cached_path, hint_enabled)\n                    result_payload = {\"content\": processed, \"file_info\": file_info.model_dump(mode=\"json\")}\n                    if page_entries:\n                        result_payload[\"pages\"] = page_entries\n                    if cached_path:\n                        result_payload[\"cached_text_path\"] = cached_path\n                    return result_payload\n                else:\n                    logger.warning(\n                        f\"Tool {self.name} - {self.id}: no text content extracted from file,\"\n                        \"falling back to raw content\"\n                    )\n            else:\n                logger.warning(\n                    f\"Tool {self.name} - {self.id}: could not detect file type, falling back to raw content\"\n                )\n\n        except Exception as e:\n            logger.warning(\n                f\"Tool {self.name} - {self.id}: file processing failed: {str(e)}, falling back to raw content\"\n            )\n\n        rendered_content = self._render_binary_content(\n            content=content,\n            content_size=content_size,\n            mode=mode,\n            chunk_size=chunk_size,\n            preview_limit=preview_limit,\n            file_path=input_data.file_path,\n        )\n\n        return {\"content\": rendered_content, \"file_info\": file_info.model_dump(mode=\"json\")}\n\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to read file. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to read file. Error: {str(e)}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadTool.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the FileReadTool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>None</code> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None):\n    \"\"\"\n    Initialize the components of the FileReadTool.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    self._connection_manager = connection_manager\n    super().init_components(connection_manager)\n\n    self._setup_converters(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileReadTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    data[\"file_store\"] = self.file_store.to_dict(**kwargs)\n    if self.converter_mapping:\n        data[\"converter_mapping\"] = {\n            file_type.value: converter.to_dict(**kwargs) for file_type, converter in self.converter_mapping.items()\n        }\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileSearchInputSchema","title":"<code>FileSearchInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for file search operations.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileSearchInputSchema(BaseModel):\n    \"\"\"Schema for file search operations.\"\"\"\n\n    query: str = Field(..., description=\"Substring or regex to search for.\")\n    file_path: str | list[str] = Field(\n        default=\"\",\n        description=\"Single path, list of paths, or empty to scan all available files.\",\n    )\n    recursive: bool = Field(default=True, description=\"Whether to recurse when scanning directories.\")\n    mode: Literal[\"substring\", \"regex\"] = Field(\n        default=\"substring\", description=\"Search mode: plain substring or regular expression.\"\n    )\n    case_sensitive: bool = Field(default=False, description=\"Whether the search should be case sensitive.\")\n    max_matches_per_file: int = Field(default=5, description=\"Maximum matches returned per file.\")\n    max_files: int = Field(default=20, description=\"Maximum number of files to scan when file_path is empty.\")\n    context_chars: int = Field(default=120, description=\"Number of characters of context to include around matches.\")\n    max_file_bytes: int = Field(\n        default=1_000_000, description=\"Maximum number of bytes to load from each file while searching.\"\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileSearchTool","title":"<code>FileSearchTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for searching across stored files.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileSearchTool(Node):\n    \"\"\"\n    A tool for searching across stored files.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.FILE_OPERATION\n    name: str = \"FileSearchTool\"\n    description: str = \"\"\"\n        Searches stored files for substrings or regular expressions and returns contextual matches.\n        Usage Examples:\n            - {\"query\": \"TODO\"} (searches all files, case-insensitive substring)\n            - {\"query\": \"class Agent\", \"file_path\": \"dynamiq/nodes/agents/base.py\"}\n            - {\"query\": \"error.+timeout\", \"mode\": \"regex\", \"case_sensitive\": true}\n            - {\"query\": \"select\", \"context_chars\": 300, \"max_matches_per_file\": 10}\n        Notes:\n            - When the FileReadTool has already extracted text (e.g., from PDF/PPTX/XLSX/CSV), this tool automatically\n              searches the cached \"&lt;original&gt;.extracted.txt\" instead of re-reading the binary source.\n            - Start with concrete phrases (e.g., \"Global Drug Facility\", \"KPI tree\") and widen or switch to regex\n              only if needed; large, unfocused queries slow the agent down.\n            - When you need page-level attribution, read PDFs with {\"document_mode\": \"page\"} so matches reference\n              the same per-page extracts.\n            - `context_chars` controls how much surrounding text is returned; increase it instead of re-running the\n              same search repeatedly.\n    \"\"\"\n\n    file_store: FileStore = Field(..., description=\"File storage to search within.\")\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[FileSearchInputSchema]] = FileSearchInputSchema\n\n    def execute(\n        self,\n        input_data: FileSearchInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        try:\n            files_to_scan = self._resolve_files(input_data)\n            matches = []\n            total_scanned = 0\n\n            if input_data.mode == \"regex\":\n                flags = 0 if input_data.case_sensitive else re.IGNORECASE\n                try:\n                    pattern = re.compile(input_data.query, flags)\n                except re.error as e:\n                    raise ToolExecutionException(\n                        f\"Invalid regular expression '{input_data.query}': {e}\", recoverable=True\n                    )\n            else:\n                pattern = None\n                query = input_data.query if input_data.case_sensitive else input_data.query.lower()\n\n            for file_path in files_to_scan:\n                total_scanned += 1\n                file_matches = self._search_file(\n                    file_path=file_path,\n                    query=query if pattern is None else None,\n                    pattern=pattern,\n                    case_sensitive=input_data.case_sensitive,\n                    max_matches=input_data.max_matches_per_file,\n                    context_chars=input_data.context_chars,\n                    max_bytes=input_data.max_file_bytes,\n                )\n                if file_matches:\n                    matches.extend(file_matches)\n\n            result = {\n                \"content\": {\n                    \"matches\": matches,\n                    \"files_scanned\": total_scanned,\n                    \"total_matches\": len(matches),\n                }\n            }\n            logger.info(\n                f\"Tool {self.name} - {self.id}: finished searching {total_scanned} files, \"\n                f\"found {len(matches)} matches.\"\n            )\n            return result\n\n        except ToolExecutionException:\n            raise\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to search files. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed during search. Error: {str(e)}. \"\n                f\"Please adjust your query or file selection and try again.\",\n                recoverable=True,\n            )\n\n    def _resolve_files(self, input_data: FileSearchInputSchema) -&gt; list[str]:\n        \"\"\"Determine which files should be searched.\"\"\"\n        if isinstance(input_data.file_path, list) and input_data.file_path:\n            return input_data.file_path[: input_data.max_files]\n        if isinstance(input_data.file_path, str) and input_data.file_path:\n            return [input_data.file_path]\n\n        files = self.file_store.list_files(recursive=input_data.recursive)\n        return [file.path for file in files[: input_data.max_files]]\n\n    def _search_file(\n        self,\n        file_path: str,\n        query: str | None,\n        pattern: re.Pattern | None,\n        case_sensitive: bool,\n        max_matches: int,\n        context_chars: int,\n        max_bytes: int,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Run the search inside a single file.\"\"\"\n        text, source_path = self._load_search_text(file_path, max_bytes)\n        if not text:\n            return []\n\n        matches: list[dict[str, Any]] = []\n\n        if pattern:\n            for match in pattern.finditer(text):\n                matches.append(\n                    self._build_match_entry(\n                        file_path=file_path,\n                        source_path=source_path,\n                        text=text,\n                        start=match.start(),\n                        end=match.end(),\n                        context_chars=context_chars,\n                    )\n                )\n                if len(matches) &gt;= max_matches:\n                    break\n        else:\n            haystack = text if case_sensitive else text.lower()\n            start_idx = 0\n            query_len = len(query)\n            while len(matches) &lt; max_matches:\n                idx = haystack.find(query, start_idx)\n                if idx == -1:\n                    break\n                matches.append(\n                    self._build_match_entry(\n                        file_path=file_path,\n                        source_path=source_path,\n                        text=text,\n                        start=idx,\n                        end=idx + query_len,\n                        context_chars=context_chars,\n                    )\n                )\n                start_idx = idx + query_len\n\n        return matches\n\n    def _load_search_text(self, file_path: str, max_bytes: int) -&gt; tuple[str | None, str | None]:\n        \"\"\"Load search text preferring cached extracted content when available.\"\"\"\n        if not self.file_store:\n            return None, None\n\n        candidates = [f\"{file_path}{EXTRACTED_TEXT_SUFFIX}\", file_path]\n        for candidate in candidates:\n            try:\n                if not self.file_store.exists(candidate):\n                    continue\n                raw = self.file_store.retrieve(candidate)[:max_bytes]\n                if not raw:\n                    continue\n                text = raw.decode(\"utf-8\", errors=\"ignore\")\n                if text:\n                    if candidate.endswith(EXTRACTED_TEXT_SUFFIX):\n                        logger.info(\n                            f\"Tool {self.name} - {self.id}: using cached extracted text for search: {candidate}\"\n                        )\n                    return text, candidate\n            except Exception as exc:\n                logger.warning(f\"Tool {self.name} - {self.id}: failed to read {candidate} for search: {exc}\")\n        return None, None\n\n    @staticmethod\n    def _build_match_entry(\n        file_path: str, source_path: str | None, text: str, start: int, end: int, context_chars: int\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build a structured match entry with context and line number.\"\"\"\n        before = max(0, start - context_chars)\n        after = min(len(text), end + context_chars)\n        context = text[before:after]\n        line = text.count(\"\\n\", 0, start) + 1\n        return {\n            \"file\": file_path,\n            \"source_path\": source_path or file_path,\n            \"line\": line,\n            \"match\": text[start:end],\n            \"context\": context.strip(),\n        }\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileWriteAction","title":"<code>FileWriteAction</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Determines the operation mode for FileWriteTool.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileWriteAction(str, enum.Enum):\n    \"\"\"Determines the operation mode for FileWriteTool.\"\"\"\n\n    WRITE = \"write\"\n    EDIT = \"edit\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileWriteInputSchema","title":"<code>FileWriteInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for file write input parameters.</p> <ul> <li>write (default): provide <code>content</code> to create, overwrite, or append.</li> <li>edit: provide <code>edits</code> (find/replace list) for atomic in-place edits.</li> </ul> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileWriteInputSchema(BaseModel):\n    \"\"\"Schema for file write input parameters.\n\n    * **write** (default): provide ``content`` to create, overwrite, or append.\n    * **edit**: provide ``edits`` (find/replace list) for atomic in-place edits.\n    \"\"\"\n\n    action: FileWriteAction = Field(\n        default=FileWriteAction.WRITE,\n        description=\"Operation mode: 'write' to create/overwrite/append a file, \"\n        \"'edit' to perform atomic find-and-replace on an existing file.\",\n    )\n    file_path: str = Field(..., description=\"Path where the file should be written or edited\")\n    content: Any = Field(\n        default=None,\n        description=\"File content (string, bytes, or structured data for JSON). \" \"Required for 'write' action.\",\n    )\n    edits: list[EditOperation] | None = Field(\n        default=None,\n        description=\"Ordered list of find/replace operations for 'edit' action. \"\n        \"Each entry has 'find', 'replace', and optional 'replace_all' (default false). \"\n        \"Edits are applied sequentially with literal matching. \"\n        \"Atomic: if any find string is not found, the operation fails with no changes.\",\n    )\n    content_type: str | None = Field(default=None, description=\"MIME type (auto-detected if not provided)\")\n    metadata: dict[str, Any] | None = Field(default=None, description=\"Additional metadata for the file\")\n    overwrite: bool = Field(\n        default=True,\n        description=\"Whether to overwrite the file if it already exists. Defaults to True.\",\n    )\n    content_format: Literal[\"auto\", \"text\", \"json\", \"binary\"] = Field(\n        default=\"auto\",\n        description=(\n            \"Hints how to encode the provided content. 'auto' infers from the value, \"\n            \"'text' treats the payload as UTF-8 text, 'json' serializes with json.dumps, \"\n            \"and 'binary' writes raw bytes.\"\n        ),\n    )\n    brief: str = Field(\n        default=\"Writing a file\",\n        description=\"Very brief description of the action being performed. \"\n        \"Example: 'Create a new file called report.txt', 'Update the data in report.txt.\",\n    )\n    encoding: str = Field(default=\"utf-8\", description=\"Encoding to use when writing textual content.\")\n    append: bool = Field(\n        default=False,\n        description=\"If True, append to the existing file instead of replacing it. \"\n        \"Falls back to overwrite when file is missing.\",\n    )\n\n    @field_validator(\"file_path\")\n    @classmethod\n    def validate_path(cls, v: str, info: ValidationInfo) -&gt; str:\n        \"\"\"Validate file_path to prevent path traversal attacks.\"\"\"\n        allow_absolute = bool((info.context or {}).get(\"absolute_file_paths_allowed\"))\n        return validate_file_path(v, allow_absolute=allow_absolute)\n\n    @model_validator(mode=\"after\")\n    def validate_action_fields(self) -&gt; \"FileWriteInputSchema\":\n        if self.action == FileWriteAction.WRITE and self.content is None:\n            raise ValueError(\"'content' is required when action is 'write'\")\n        if self.action == FileWriteAction.EDIT and not self.edits:\n            raise ValueError(\"'edits' is required when action is 'edit'\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileWriteInputSchema.validate_path","title":"<code>validate_path(v, info)</code>  <code>classmethod</code>","text":"<p>Validate file_path to prevent path traversal attacks.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>@field_validator(\"file_path\")\n@classmethod\ndef validate_path(cls, v: str, info: ValidationInfo) -&gt; str:\n    \"\"\"Validate file_path to prevent path traversal attacks.\"\"\"\n    allow_absolute = bool((info.context or {}).get(\"absolute_file_paths_allowed\"))\n    return validate_file_path(v, allow_absolute=allow_absolute)\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileWriteTool","title":"<code>FileWriteTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for writing and editing files in storage.</p> <p>Supports two modes:</p> <ul> <li>Write mode: provide <code>content</code> to create, overwrite, or append to a file.</li> <li>Edit mode: provide <code>edits</code> (a list of find/replace operations) to perform   atomic in-place edits.  Edits are applied sequentially with literal string   matching.  If any <code>find</code> string is missing, the entire operation is aborted   and no changes are written.</li> </ul> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>file_store</code> <code>FileStore</code> <p>File storage to write to.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>class FileWriteTool(Node):\n    \"\"\"\n    A tool for writing and editing files in storage.\n\n    Supports two modes:\n\n    * **Write mode**: provide ``content`` to create, overwrite, or append to a file.\n    * **Edit mode**: provide ``edits`` (a list of find/replace operations) to perform\n      atomic in-place edits.  Edits are applied sequentially with literal string\n      matching.  If any ``find`` string is missing, the entire operation is aborted\n      and no changes are written.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        file_store (FileStore): File storage to write to.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.FILE_OPERATION\n    name: str = \"FileWriteTool\"\n    description: str = (\n        \"Writes or edits files in storage.\\n\\n\"\n        \"Actions:\\n\"\n        \"- write: Create or overwrite a file. Requires 'content'. Set 'append: true' to append.\\n\"\n        \"- edit: Atomic find-and-replace on an existing file. Requires 'edits' list with 'find'/'replace' pairs. \"\n        \"Edits are applied sequentially; a prior replacement may remove a later find string.\\n\"\n        \"Example:\\n\"\n        \"- Write text: {action: 'write', 'file_path': 'readme.txt', 'content': 'Hello World', \"\n        \"'brief': 'Create a new file called readme.txt'}\\n\"\n        \"- Edit file: {action: 'edit', 'file_path': 'app.py', 'edits': [{'find': 'old_func()', \"\n        \"'replace': 'new_func()'}], 'brief': 'Rename function old_func to new_func'}\\n\"\n    )\n\n    file_store: FileStore | Sandbox = Field(..., description=\"File storage to write to.\")\n    absolute_file_paths_allowed: bool = Field(default=False, description=\"Whether to allow absolute paths.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[FileWriteInputSchema]] = FileWriteInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_store\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        data = super().to_dict(**kwargs)\n        data[\"file_store\"] = self.file_store.to_dict(**kwargs)\n        return data\n\n    def get_context_for_input_schema(self) -&gt; dict:\n        return {\"absolute_file_paths_allowed\": self.absolute_file_paths_allowed}\n\n    def execute(\n        self,\n        input_data: FileWriteInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute a file write or edit operation.\n\n        Dispatches to ``_execute_write`` or ``_execute_edit`` based on\n        ``input_data.action``.  Any non-tool exception is wrapped in a\n        recoverable ``ToolExecutionException``.\n\n        Args:\n            input_data: Validated input containing action, file path, and\n                either content (write) or edits (edit).\n            config: Optional runnable configuration with callbacks.\n            **kwargs: Additional keyword arguments forwarded to callbacks.\n\n        Returns:\n            Dict with ``content`` (result message) and ``file_info``.\n\n        Raises:\n            ToolExecutionException: On file I/O errors or failed edit pre-checks.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        if input_data.file_path.startswith(f\"{RESERVED_AGENT_PATH_PREFIX}/\"):\n            raise ToolExecutionException(\n                f\"Path '{input_data.file_path}' is reserved for internal agent use. \"\n                f\"Use the dedicated tool to manage files under '{RESERVED_AGENT_PATH_PREFIX}/'.\",\n                recoverable=True,\n            )\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        try:\n            if input_data.action == FileWriteAction.EDIT:\n                return self._execute_edit(input_data)\n            return self._execute_write(input_data)\n        except Exception as e:\n            if isinstance(e, ToolExecutionException):\n                raise\n            action = input_data.action.value\n            logger.error(f\"Tool {self.name} - {self.id}: failed to {action} file. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to {action} file. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n    def _execute_write(self, input_data: FileWriteInputSchema) -&gt; dict[str, Any]:\n        \"\"\"Create, overwrite, or append to a file.\n\n        Content is serialised via ``_prepare_content_payload`` (handles str, bytes,\n        JSON-serialisable objects, and format overrides).  When ``append=True`` the\n        existing file bytes are prepended to the new payload before storing.\n\n        Returns:\n            Dict with ``content`` (success message) and ``file_info``.\n        \"\"\"\n        payload, inferred_type = self._prepare_content_payload(input_data)\n        content_type = input_data.content_type or inferred_type\n\n        overwrite_flag = input_data.overwrite or input_data.append\n        if input_data.append and self.file_store.exists(input_data.file_path):\n            existing = self.file_store.retrieve(input_data.file_path)\n            payload = existing + payload\n\n        file_info = self.file_store.store(\n            input_data.file_path,\n            payload,\n            content_type=content_type,\n            metadata=input_data.metadata,\n            overwrite=overwrite_flag,\n        )\n\n        message = f\"File '{input_data.file_path}' written successfully\"\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(file_info)[:200]}...\")\n\n        return {\n            \"content\": message,\n            \"file_info\": file_info.model_dump(mode=\"json\"),\n        }\n\n    def _execute_edit(self, input_data: FileWriteInputSchema) -&gt; dict[str, Any]:\n        \"\"\"Perform atomic sequential find-and-replace edits on an existing file.\n\n        Semantics:\n        1. **Pre-check**: all ``find`` strings are verified against the *original*\n           file content.  If any are missing the operation is aborted with a\n           ``ToolExecutionException`` and the file is left untouched.\n        2. **Application**: edits are applied sequentially in the order given.\n           Each edit does a literal string replacement (first occurrence only,\n           or all occurrences when ``edit.replace_all`` is set).\n        3. **Conflict handling**: if an earlier edit removes text that a later\n           edit targets, the later edit is skipped and a warning is included in\n           the returned summary so the caller can take corrective action.\n\n        Returns:\n            Dict with ``content`` (summary with counts and any warnings) and\n            ``file_info``.\n\n        Raises:\n            ToolExecutionException: when one or more find strings are absent\n                from the original file content (no changes written).\n        \"\"\"\n        edits = input_data.edits\n        encoding = input_data.encoding or \"utf-8\"\n        path = input_data.file_path\n\n        content = self.file_store.retrieve(path).decode(encoding)\n\n        missing = [e.find for e in edits if e.find not in content]\n        if missing:\n            raise ToolExecutionException(\n                f\"Aborting edit: find string(s) not found in '{path}': \"\n                f\"{[repr(s[:80]) for s in missing]}. No changes were made.\",\n                recoverable=True,\n            )\n\n        total = 0\n        skipped: list[str] = []\n        for edit in edits:\n            occurrences = content.count(edit.find)\n            if occurrences == 0:\n                skipped.append(edit.find)\n                continue\n            count = occurrences if edit.replace_all else min(1, occurrences)\n            if edit.replace_all:\n                content = content.replace(edit.find, edit.replace)\n            else:\n                content = content.replace(edit.find, edit.replace, 1)\n            total += count\n\n        payload = content.encode(encoding)\n        content_type = input_data.content_type or mimetypes.guess_type(path)[0] or \"text/plain\"\n        file_info = self.file_store.store(\n            path, payload, content_type=content_type, metadata=input_data.metadata, overwrite=True\n        )\n        applied = len(edits) - len(skipped)\n        summary = f\"Applied {applied} of {len(edits)} edit(s) with {total} replacement(s) to {path}.\"\n        if skipped:\n            summary += (\n                f\" Warning: {len(skipped)} find string(s) were present in the original file \"\n                f\"but disappeared after a prior edit and were skipped: \"\n                f\"{[repr(s[:80]) for s in skipped]}.\"\n            )\n        logger.info(f\"Tool {self.name} - {self.id}: {summary}\")\n\n        return {\n            \"content\": f\"{summary} Use FileReadTool to view the updated file.\",\n            \"file_info\": file_info.model_dump(mode=\"json\"),\n        }\n\n    def _prepare_content_payload(self, input_data: FileWriteInputSchema) -&gt; tuple[bytes, str]:\n        \"\"\"Serialize the incoming payload based on the requested format.\"\"\"\n        encoding = input_data.encoding or \"utf-8\"\n        fmt = input_data.content_format\n        value = input_data.content\n\n        if fmt == \"auto\":\n            if isinstance(value, bytes):\n                fmt = \"binary\"\n            elif isinstance(value, (dict, list)):\n                fmt = \"json\"\n            else:\n                fmt = \"text\"\n\n        if fmt == \"json\":\n            if isinstance(value, str):\n                try:\n                    json.loads(value)\n                    serialized = value\n                except json.JSONDecodeError:\n                    serialized = json.dumps(value)\n            else:\n                serialized = json.dumps(value)\n            return serialized.encode(encoding), \"application/json\"\n\n        if fmt == \"binary\":\n            if isinstance(value, bytes):\n                return value, \"application/octet-stream\"\n            return str(value).encode(encoding), \"application/octet-stream\"\n\n        if isinstance(value, bytes):\n            payload = value\n        else:\n            payload = str(value).encode(encoding)\n        return payload, \"text/plain\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.FileWriteTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute a file write or edit operation.</p> <p>Dispatches to <code>_execute_write</code> or <code>_execute_edit</code> based on <code>input_data.action</code>.  Any non-tool exception is wrapped in a recoverable <code>ToolExecutionException</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>FileWriteInputSchema</code> <p>Validated input containing action, file path, and either content (write) or edits (edit).</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional runnable configuration with callbacks.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to callbacks.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with <code>content</code> (result message) and <code>file_info</code>.</p> <p>Raises:</p> Type Description <code>ToolExecutionException</code> <p>On file I/O errors or failed edit pre-checks.</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def execute(\n    self,\n    input_data: FileWriteInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, Any]:\n    \"\"\"Execute a file write or edit operation.\n\n    Dispatches to ``_execute_write`` or ``_execute_edit`` based on\n    ``input_data.action``.  Any non-tool exception is wrapped in a\n    recoverable ``ToolExecutionException``.\n\n    Args:\n        input_data: Validated input containing action, file path, and\n            either content (write) or edits (edit).\n        config: Optional runnable configuration with callbacks.\n        **kwargs: Additional keyword arguments forwarded to callbacks.\n\n    Returns:\n        Dict with ``content`` (result message) and ``file_info``.\n\n    Raises:\n        ToolExecutionException: On file I/O errors or failed edit pre-checks.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    if input_data.file_path.startswith(f\"{RESERVED_AGENT_PATH_PREFIX}/\"):\n        raise ToolExecutionException(\n            f\"Path '{input_data.file_path}' is reserved for internal agent use. \"\n            f\"Use the dedicated tool to manage files under '{RESERVED_AGENT_PATH_PREFIX}/'.\",\n            recoverable=True,\n        )\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    try:\n        if input_data.action == FileWriteAction.EDIT:\n            return self._execute_edit(input_data)\n        return self._execute_write(input_data)\n    except Exception as e:\n        if isinstance(e, ToolExecutionException):\n            raise\n        action = input_data.action.value\n        logger.error(f\"Tool {self.name} - {self.id}: failed to {action} file. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to {action} file. Error: {str(e)}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.detect_file_type","title":"<code>detect_file_type(file, filename)</code>","text":"<p>Detect the file type based on file extension.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>BytesIO</code> <p>The file object (BytesIO)</p> required <code>filename</code> <code>str</code> <p>The filename to extract extension from</p> required <p>Returns:</p> Name Type Description <code>FileType</code> <code>FileType | None</code> <p>The detected file type, or None if not found</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def detect_file_type(file: BytesIO, filename: str) -&gt; FileType | None:\n    \"\"\"\n    Detect the file type based on file extension.\n\n    Args:\n        file: The file object (BytesIO)\n        filename: The filename to extract extension from\n\n    Returns:\n        FileType: The detected file type, or None if not found\n    \"\"\"\n    try:\n        if not filename and hasattr(file, \"name\"):\n            filename = file.name\n\n        if not filename:\n            logger.warning(\"No filename provided for file type detection\")\n            return None\n\n        file_ext = os.path.splitext(filename)[1][1:] if filename else \"\"\n        file_ext = file_ext.lower()\n\n        if not file_ext:\n            logger.warning(f\"No file extension found in filename: {filename}\")\n            return None\n\n        for file_type, extensions in EXTENSION_MAP.items():\n            if file_ext in extensions:\n                logger.info(f\"Detected file type: {file_type} for file: {filename}\")\n                return file_type\n\n        logger.warning(f\"Unknown file extension: {file_ext} for file: {filename}\")\n        return None\n\n    except Exception as e:\n        logger.warning(f\"File type detection failed for {filename}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"dynamiq/nodes/tools/file_tools/#dynamiq.nodes.tools.file_tools.validate_file_path","title":"<code>validate_file_path(file_path, allow_absolute=False)</code>","text":"<p>Validate a file path to prevent path traversal attacks.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The file path to validate.</p> required <code>allow_absolute</code> <code>bool</code> <p>If True, absolute paths are permitted (e.g. when the tool is backed by a Sandbox and the LLM discovers absolute paths from shell output).</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The validated (normalized) file path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path contains path traversal sequences or is absolute (when allow_absolute is False).</p> Source code in <code>dynamiq/nodes/tools/file_tools.py</code> <pre><code>def validate_file_path(file_path: str, allow_absolute: bool = False) -&gt; str:\n    \"\"\"\n    Validate a file path to prevent path traversal attacks.\n\n    Args:\n        file_path: The file path to validate.\n        allow_absolute: If True, absolute paths are permitted (e.g. when the\n            tool is backed by a Sandbox and the LLM discovers absolute paths\n            from shell output).\n\n    Returns:\n        The validated (normalized) file path.\n\n    Raises:\n        ValueError: If the path contains path traversal sequences or is\n            absolute (when *allow_absolute* is False).\n    \"\"\"\n    if not file_path:\n        return file_path\n\n    normalized = os.path.normpath(file_path)\n\n    if os.path.isabs(normalized):\n        if not allow_absolute:\n            raise ValueError(f\"Absolute paths are not allowed: {file_path}\")\n        return normalized\n\n    path_parts = normalized.split(os.sep)\n    if \"..\" in path_parts:\n        raise ValueError(f\"Path traversal sequences are not allowed: {file_path}\")\n\n    # Also check for Windows-style absolute paths (e.g., C:\\)\n    if len(normalized) &gt;= 2 and normalized[1] == \":\":\n        if not allow_absolute:\n            raise ValueError(f\"Absolute paths are not allowed: {file_path}\")\n\n    return normalized\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/","title":"Firecrawl","text":""},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.Action","title":"<code>Action</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Action to perform before content extraction.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class Action(BaseModel):\n    \"\"\"Action to perform before content extraction.\"\"\"\n\n    type: str\n    milliseconds: int | None = None\n    selector: str | None = None\n    text: str | None = None\n    key: str | None = None\n    all: bool | None = None\n    full_page: bool | None = Field(default=None, alias=\"fullPage\")\n    quality: int | None = None\n    viewport: dict[str, int] | None = None\n    script: str | None = None\n    format: str | None = None\n    landscape: bool | None = None\n    scale: float | None = None\n\n    model_config = ConfigDict(extra=\"allow\", populate_by_name=True)\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.FirecrawlInputSchema","title":"<code>FirecrawlInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema exposed to agents using the tool.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class FirecrawlInputSchema(BaseModel):\n    \"\"\"Schema exposed to agents using the tool.\"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    url: str = Field(..., description=\"URL of the page to scrape.\")\n    formats: list[str | dict[str, Any]] | None = Field(\n        default=None,\n        description=(\n            \"Firecrawl output formats. Accepts plain strings (markdown/html/rawHtml/links/summary/\"\n            \"screenshot/json/changeTracking/branding) or objects with format-specific options.\"\n        ),\n    )\n    only_main_content: bool | None = Field(\n        default=None,\n        alias=\"onlyMainContent\",\n        description=\"True trims boilerplate (nav/footer) for article-style pages; False keeps the full DOM.\",\n    )\n    include_tags: list[str] | None = Field(\n        default=None,\n        alias=\"includeTags\",\n        description=\"List of HTML tag names to force-include in the output (e.g. ['table', 'img']).\",\n    )\n    exclude_tags: list[str] | None = Field(\n        default=None,\n        alias=\"excludeTags\",\n        description=\"HTML tag names that should be stripped from the response.\",\n    )\n    max_age: int | None = Field(\n        default=None,\n        alias=\"maxAge\",\n        description=\"Cache freshness window in ms (Firecrawl default is 172800000 = two days).\",\n    )\n    headers: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Custom HTTP headers (cookies, user-agent, auth tokens) to forward with the scrape.\",\n    )\n    wait_for: int | None = Field(\n        default=None,\n        alias=\"waitFor\",\n        description=\"Delay in ms before scraping to let dynamic content render (default 0).\",\n    )\n    mobile: bool | None = Field(\n        default=None,\n        description=\"True emulates a mobile device viewport + UA, useful for responsive layouts/screenshots.\",\n    )\n    skip_tls_verification: bool | None = Field(\n        default=None,\n        alias=\"skipTlsVerification\",\n        description=\"True disables TLS verification (Firecrawl default), set False for strict cert checks.\",\n    )\n    timeout: int | None = Field(default=None, description=\"Request timeout in ms for the upstream fetch.\")\n    parsers: list[str | dict[str, Any]] | None = Field(\n        default=None,\n        description=(\n            \"Controls file parsers. Firecrawl defaults to ['pdf']; pass [] to disable auto PDF parsing or \"\n            \"objects like {'type': 'pdf', 'maxPages': 5} to limit cost.\"\n        ),\n    )\n    actions: list[Action] | None = Field(\n        default=None,\n        description=(\n            \"Optional automation instructions executed before scraping. Supports wait/screenshot/click/write/\"\n            \"press/scroll/scrape/executeJavascript/pdf actions following Firecrawl's schema.\"\n        ),\n    )\n    location: LocationSettings | None = Field(\n        default=None,\n        description=\"Country + preferred languages for proxy/language emulation (defaults to US if omitted).\",\n    )\n    remove_base64_images: bool | None = Field(\n        default=None,\n        alias=\"removeBase64Images\",\n        description=\"True strips giant base64 &lt;img&gt; blobs and replaces them with placeholders (default True).\",\n    )\n    block_ads: bool | None = Field(\n        default=None,\n        alias=\"blockAds\",\n        description=\"Enable ad + cookie popup blocking (default True on Firecrawl).\",\n    )\n    proxy: Literal[\"basic\", \"stealth\", \"auto\"] | None = Field(\n        default=None,\n        description=(\n            \"Proxy tier: 'basic' (fast, low protection), 'stealth' (solves harder anti-bot, higher cost), or \"\n            \"'auto' (retry with stealth on failure; Firecrawl default).\"\n        ),\n    )\n    store_in_cache: bool | None = Field(\n        default=None,\n        alias=\"storeInCache\",\n        description=\"True lets Firecrawl index/cache the page \"\n        \"(default True, forced False when using sensitive params).\",\n    )\n    zero_data_retention: bool | None = Field(\n        default=None,\n        alias=\"zeroDataRetention\",\n        description=\"Opt-in compliance flag; True enables Firecrawl's zero data retention mode (requires approval).\",\n    )\n    brief: str = Field(\n        default=\"Scraping the web for information.\",\n        description=\"Very brief description of the action being performed. Example: 'Scrape the web for information.'.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.FirecrawlTool","title":"<code>FirecrawlTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for scraping web pages using the Firecrawl service.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class FirecrawlTool(ConnectionNode):\n    \"\"\"A tool for scraping web pages using the Firecrawl service.\"\"\"\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SCRAPE\n    name: str = \"Firecrawl Tool\"\n    description: str = DESCRIPTION_FIRECRAWL\n    is_parallel_execution_allowed: bool = True\n    connection: Firecrawl\n    url: str | None = None\n    input_schema: ClassVar[type[FirecrawlInputSchema]] = FirecrawlInputSchema\n\n    formats: list[str | dict[str, Any]] = Field(default_factory=lambda: [\"markdown\"])\n    only_main_content: bool = Field(default=True, alias=\"onlyMainContent\")\n    include_tags: list[str] | None = Field(default=None, alias=\"includeTags\")\n    exclude_tags: list[str] | None = Field(default=None, alias=\"excludeTags\")\n    max_age: int | None = Field(default=None, alias=\"maxAge\")\n    headers: dict[str, Any] | None = None\n    wait_for: int = Field(default=0, alias=\"waitFor\")\n    mobile: bool = False\n    skip_tls_verification: bool = Field(default=True, alias=\"skipTlsVerification\")\n    timeout: int = 30000\n    parsers: list[str | dict[str, Any]] | None = None\n    actions: list[Action] | None = None\n    location: LocationSettings | None = None\n    remove_base64_images: bool = Field(default=True, alias=\"removeBase64Images\")\n    block_ads: bool = Field(default=True, alias=\"blockAds\")\n    proxy: Literal[\"basic\", \"stealth\", \"auto\"] | None = None\n    store_in_cache: bool | None = Field(default=None, alias=\"storeInCache\")\n    zero_data_retention: bool | None = Field(default=None, alias=\"zeroDataRetention\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, populate_by_name=True)\n\n    def _build_scrape_data(self, url: str, overrides: FirecrawlInputSchema) -&gt; dict:\n        \"\"\"Build the request payload for the Firecrawl API.\"\"\"\n\n        def resolve(field_name: str) -&gt; Any:\n            if hasattr(overrides, field_name):\n                value = getattr(overrides, field_name)\n                if value is not None:\n                    return value\n            return deepcopy(getattr(self, field_name, None))\n\n        formats = resolve(\"formats\") or [\"markdown\"]\n        only_main_content = resolve(\"only_main_content\")\n        if only_main_content is None:\n            only_main_content = True\n\n        base_data = {\n            \"url\": url,\n            \"formats\": formats,\n            \"onlyMainContent\": only_main_content,\n        }\n\n        conditional_fields = {\n            \"includeTags\": resolve(\"include_tags\"),\n            \"excludeTags\": resolve(\"exclude_tags\"),\n            \"maxAge\": resolve(\"max_age\"),\n            \"headers\": resolve(\"headers\"),\n            \"waitFor\": resolve(\"wait_for\"),\n            \"mobile\": resolve(\"mobile\"),\n            \"skipTlsVerification\": resolve(\"skip_tls_verification\"),\n            \"timeout\": resolve(\"timeout\"),\n            \"parsers\": resolve(\"parsers\"),\n            \"removeBase64Images\": resolve(\"remove_base64_images\"),\n            \"blockAds\": resolve(\"block_ads\"),\n            \"proxy\": resolve(\"proxy\"),\n            \"storeInCache\": resolve(\"store_in_cache\"),\n            \"zeroDataRetention\": resolve(\"zero_data_retention\"),\n        }\n\n        actions = resolve(\"actions\")\n        if actions:\n            conditional_fields[\"actions\"] = [\n                action.model_dump(exclude_none=True, by_alias=True) if isinstance(action, Action) else action\n                for action in actions\n            ]\n\n        location = resolve(\"location\")\n        if location:\n            conditional_fields[\"location\"] = (\n                location.model_dump(exclude_none=True) if isinstance(location, BaseModel) else location\n            )\n\n        # Filter out None values and merge with base data\n        filtered_fields = {k: v for k, v in conditional_fields.items() if v is not None}\n        return {**base_data, **filtered_fields}\n\n    @staticmethod\n    def _json_section(title: str, payload: Any) -&gt; str:\n        return f\"## {title}\\n```json\\n{json.dumps(payload, indent=2, ensure_ascii=False)}\\n```\"\n\n    def _format_agent_response(self, url: str, response: dict[str, Any]) -&gt; str:\n        \"\"\"Format the response for agent consumption using Markdown.\"\"\"\n        data = response.get(\"data\", {}) or {}\n        sections = [\n            \"## Firecrawl Scrape Result\",\n            f\"- URL: {url}\",\n            f\"- Success: {response.get('success', False)}\",\n        ]\n\n        warning = data.get(\"warning\")\n        if warning:\n            sections.append(f\"- Warning: {warning}\")\n\n        content_fields = [\n            (\"markdown\", \"Markdown\"),\n            (\"summary\", \"Summary\"),\n            (\"html\", \"HTML\"),\n            (\"rawHtml\", \"Raw HTML\"),\n        ]\n\n        for key, label in content_fields:\n            value = data.get(key)\n            if value:\n                sections.append(f\"## {label}\\n{value}\")\n\n        if screenshot := data.get(\"screenshot\"):\n            sections.append(f\"## Screenshot URL\\n{screenshot}\")\n\n        if links := data.get(\"links\"):\n            links_section = \"\\n\".join(f\"- {link}\" for link in links)\n            sections.append(f\"## Links\\n{links_section}\")\n\n        actions = data.get(\"actions\")\n        if actions:\n            sections.append(self._json_section(\"Action Results\", actions))\n\n        metadata = data.get(\"metadata\")\n        if metadata:\n            sections.append(self._json_section(\"Metadata\", metadata))\n\n        change_tracking = data.get(\"changeTracking\")\n        if change_tracking:\n            sections.append(self._json_section(\"Change Tracking\", change_tracking))\n\n        branding = data.get(\"branding\")\n        if branding:\n            sections.append(self._json_section(\"Branding\", branding))\n\n        return \"\\n\\n\".join(sections)\n\n    def execute(\n        self, input_data: FirecrawlInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the scraping tool with the provided input data.\"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        url = input_data.url or self.url\n        if not url:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n            raise ValueError(\"URL is required for scraping\")\n\n        scrape_data = self._build_scrape_data(url, input_data)\n        connection_url = self.connection.url + \"scrape\"\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json=scrape_data,\n                headers=self.connection.headers,\n            )\n            response.raise_for_status()\n            scrape_result = response.json()\n        except Exception as e:\n            logger.error(\n                f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n            )\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to execute the requested action. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        if self.is_optimized_for_agents:\n            result = self._format_agent_response(url, scrape_result)\n            output = {\"content\": result, \"urls\": [url]}\n        else:\n            result = {\"success\": scrape_result.get(\"success\", False), \"url\": url, **(scrape_result.get(\"data\") or {})}\n            output = {\"content\": result}\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.FirecrawlTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the scraping tool with the provided input data.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>def execute(\n    self, input_data: FirecrawlInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the scraping tool with the provided input data.\"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    url = input_data.url or self.url\n    if not url:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n        raise ValueError(\"URL is required for scraping\")\n\n    scrape_data = self._build_scrape_data(url, input_data)\n    connection_url = self.connection.url + \"scrape\"\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json=scrape_data,\n            headers=self.connection.headers,\n        )\n        response.raise_for_status()\n        scrape_result = response.json()\n    except Exception as e:\n        logger.error(\n            f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n        )\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to execute the requested action. Error: {str(e)}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    if self.is_optimized_for_agents:\n        result = self._format_agent_response(url, scrape_result)\n        output = {\"content\": result, \"urls\": [url]}\n    else:\n        result = {\"success\": scrape_result.get(\"success\", False), \"url\": url, **(scrape_result.get(\"data\") or {})}\n        output = {\"content\": result}\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.LocationSettings","title":"<code>LocationSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Settings for location emulation.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class LocationSettings(BaseModel):\n    \"\"\"Settings for location emulation.\"\"\"\n\n    country: str = \"US\"\n    languages: list[str] | None = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl_search/","title":"Firecrawl search","text":""},{"location":"dynamiq/nodes/tools/firecrawl_search/#dynamiq.nodes.tools.firecrawl_search.FirecrawlSearchInput","title":"<code>FirecrawlSearchInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema exposed to agents using the Firecrawl search tool.</p> Source code in <code>dynamiq/nodes/tools/firecrawl_search.py</code> <pre><code>class FirecrawlSearchInput(BaseModel):\n    \"\"\"Schema exposed to agents using the Firecrawl search tool.\"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    query: str = Field(..., description=\"Search query to execute on Firecrawl.\")\n    limit: int | None = Field(default=None, ge=1, le=100, description=\"Maximum number of search results to return.\")\n    sources: list[SourceType] | None = Field(\n        default=None, description=\"Result types to fetch: web/news/images with optional per-source options.\"\n    )\n    categories: list[CategoryType] | None = Field(\n        default=None,\n        description=\"Optional category filters for GitHub/research/PDF focused searches.\",\n    )\n    tbs: str | None = Field(\n        default=None,\n        description=\"Time-based search filter (qdr:h/d/w/m/y or custom ranges like \"\n        \"cdr:1,cd_min:12/1/2024,cd_max:12/31/2024).\",\n    )\n    location: str | None = Field(\n        default=None,\n        description=\"Location string for search results (e.g., 'San Francisco,California,United States').\",\n    )\n    country: str | None = Field(default=None, description=\"ISO country code for geo-targeting (e.g., 'US').\")\n    timeout: int | None = Field(default=None, description=\"Request timeout in milliseconds.\")\n    ignore_invalid_urls: bool | None = Field(\n        default=None,\n        alias=\"ignoreInvalidURLs\",\n        description=\"Exclude invalid URLs from search results when piping into other endpoints.\",\n    )\n    brief: str = Field(\n        default=\"Searching the web for information.\",\n        description=\"Very brief description of the action being performed. Example: 'Search for AI research papers'.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl_search/#dynamiq.nodes.tools.firecrawl_search.FirecrawlSearchTool","title":"<code>FirecrawlSearchTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for performing Firecrawl searches.</p> Source code in <code>dynamiq/nodes/tools/firecrawl_search.py</code> <pre><code>class FirecrawlSearchTool(ConnectionNode):\n    \"\"\"A tool for performing Firecrawl searches.\"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SEARCH\n    name: str = \"Firecrawl Search Tool\"\n    description: str = DESCRIPTION_FIRECRAWL_SEARCH\n    is_parallel_execution_allowed: bool = True\n    connection: Firecrawl\n    query: str | None = None\n\n    limit: int = Field(default=5, ge=1, le=100, description=\"Number of results to request.\")\n    sources: list[SourceType] = Field(default_factory=lambda: [SourceWeb()], description=\"Result verticals to include.\")\n    categories: list[CategoryType] = Field(default_factory=list, description=\"Optional category filters.\")\n    tbs: str | None = Field(default=None, description=\"Time-based search filter passed to Firecrawl.\")\n    location: str | None = Field(default=None, description=\"Geographic bias for the search query.\")\n    country: str = Field(default=\"US\", description=\"ISO country code for geo-targeting.\")\n    timeout: int = Field(default=60000, description=\"Request timeout in milliseconds.\")\n    ignore_invalid_urls: bool = Field(\n        default=False,\n        alias=\"ignoreInvalidURLs\",\n        description=\"Exclude invalid URLs from results to reduce downstream errors.\",\n    )\n    input_schema: ClassVar[type[FirecrawlSearchInput]] = FirecrawlSearchInput\n    model_config = ConfigDict(arbitrary_types_allowed=True, populate_by_name=True)\n\n    MAX_DESCRIPTION_CHARS: ClassVar[int] = 300\n    MAX_CONTENT_CHARS: ClassVar[int] = 800\n\n    @staticmethod\n    def _truncate(text: str | None, limit: int) -&gt; str:\n        \"\"\"Trim text to the specified length.\"\"\"\n        if not text:\n            return \"\"\n        if len(text) &lt;= limit:\n            return text\n        return text[:limit].rsplit(\" \", 1)[0].rstrip(\"\\n\") + \"...\"\n\n    def _build_search_payload(self, query: str, overrides: FirecrawlSearchInput) -&gt; dict[str, Any]:\n        \"\"\"Construct the payload for the Firecrawl search API.\"\"\"\n\n        def resolve(field_name: str) -&gt; Any:\n            if hasattr(overrides, field_name):\n                value = getattr(overrides, field_name)\n                if value is not None:\n                    return value\n            return deepcopy(getattr(self, field_name, None))\n\n        limit = resolve(\"limit\") or 5\n        sources = resolve(\"sources\") or [SourceWeb()]\n\n        payload = {\n            \"query\": query,\n            \"limit\": limit,\n            \"sources\": [\n                source.model_dump(exclude_none=True, by_alias=True) if isinstance(source, BaseModel) else source\n                for source in sources\n            ],\n            \"categories\": resolve(\"categories\"),\n            \"location\": resolve(\"location\"),\n            \"tbs\": resolve(\"tbs\"),\n            \"country\": resolve(\"country\"),\n            \"timeout\": resolve(\"timeout\"),\n            \"ignoreInvalidURLs\": resolve(\"ignore_invalid_urls\"),\n        }\n\n        return {k: v for k, v in payload.items() if v is not None}\n\n    def _format_scraped_results(self, query: str, results: list[dict[str, Any]]) -&gt; str:\n        if not results:\n            return f'## Firecrawl Search Results for \"{query}\"\\nNo results returned.'\n\n        sections = [f'## Firecrawl Search Results for \"{query}\"']\n        for index, item in enumerate(results, start=1):\n            title = item.get(\"title\") or \"Untitled result\"\n            url = item.get(\"url\")\n            description = self._truncate(item.get(\"description\"), self.MAX_DESCRIPTION_CHARS)\n            sections.append(f\"### Result {index}: {title}\")\n            if url:\n                sections.append(f\"- URL: [{url}]({url})\")\n            if description:\n                sections.append(f\"- Description: {description}\")\n\n            for field, label in ((\"markdown\", \"Markdown\"), (\"summary\", \"Summary\"), (\"html\", \"HTML\")):\n                content = self._truncate(item.get(field), self.MAX_CONTENT_CHARS)\n                if content:\n                    sections.append(f\"- {label}: {content}\")\n\n            if links := item.get(\"links\"):\n                link_lines = \"\\n\".join(f\"  * {link}\" for link in links)\n                sections.append(f\"- Links:\\n{link_lines}\")\n\n            sections.append(\"\")\n\n        return \"\\n\".join(sections).strip()\n\n    def _format_result_list(self, label: str, results: list[dict[str, Any]]) -&gt; list[str]:\n        if not results:\n            return []\n\n        section = [f\"### {label}\"]\n        for index, item in enumerate(results, start=1):\n            title = item.get(\"title\") or \"Untitled result\"\n            description = self._truncate(item.get(\"description\") or item.get(\"snippet\"), self.MAX_DESCRIPTION_CHARS)\n            url = item.get(\"url\") or item.get(\"imageUrl\")\n\n            if url:\n                section.append(f\"- {index}. [{title}]({url})\")\n            else:\n                section.append(f\"- {index}. {title}\")\n            if description:\n                section.append(f\"  - {description}\")\n        return section\n\n    def _format_indexed_results(self, query: str, data: dict[str, Any]) -&gt; str:\n        sections = [f'## Firecrawl Search Results for \"{query}\"']\n        sections.extend(self._format_result_list(\"Web Results\", data.get(\"web\", [])))\n        sections.extend(self._format_result_list(\"News Results\", data.get(\"news\", [])))\n        sections.extend(self._format_result_list(\"Image Results\", data.get(\"images\", [])))\n\n        if len(sections) == 1:\n            sections.append(\"No results returned.\")\n\n        return \"\\n\".join(sections)\n\n    def _format_agent_response(self, query: str, data: Any) -&gt; str:\n        \"\"\"Format the response for agent consumption using Markdown.\"\"\"\n        if isinstance(data, list):\n            return self._format_scraped_results(query, data)\n        if isinstance(data, dict):\n            return self._format_indexed_results(query, data)\n        return f'## Firecrawl Search Results for \"{query}\"\\nNo results returned.'\n\n    def execute(\n        self, input_data: FirecrawlSearchInput, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the search tool with the provided input data.\"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query = input_data.query or self.query\n        if not query:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n            raise ValueError(\"Query is required for search\")\n\n        search_payload = self._build_search_payload(query, input_data)\n        connection_url = urljoin(self.connection.url, \"search\")\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json=search_payload,\n                headers=self.connection.headers,\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to execute the requested action. Error: {str(e)}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        data = search_result.get(\"data\")\n        if self.is_optimized_for_agents:\n            result = self._format_agent_response(query, data)\n        else:\n            result = {\"success\": search_result.get(\"success\", False), \"query\": query, \"data\": data}\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl_search/#dynamiq.nodes.tools.firecrawl_search.FirecrawlSearchTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the search tool with the provided input data.</p> Source code in <code>dynamiq/nodes/tools/firecrawl_search.py</code> <pre><code>def execute(\n    self, input_data: FirecrawlSearchInput, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the search tool with the provided input data.\"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query = input_data.query or self.query\n    if not query:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n        raise ValueError(\"Query is required for search\")\n\n    search_payload = self._build_search_payload(query, input_data)\n    connection_url = urljoin(self.connection.url, \"search\")\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json=search_payload,\n            headers=self.connection.headers,\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to execute the requested action. Error: {str(e)}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    data = search_result.get(\"data\")\n    if self.is_optimized_for_agents:\n        result = self._format_agent_response(query, data)\n    else:\n        result = {\"success\": search_result.get(\"success\", False), \"query\": query, \"data\": data}\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/","title":"Function tool","text":""},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool","title":"<code>FunctionTool</code>","text":"<p>               Bases: <code>Node</code>, <code>Generic[T]</code></p> <p>A tool node for executing a specified function with the given input data.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>class FunctionTool(Node, Generic[T]):\n    \"\"\"\n    A tool node for executing a specified function with the given input data.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Function Tool\"\n    description: str = Field(\n        default=\"\"\"Executes custom Python functions as workflow tools with automatic schema generation and parameter validation.\n\nKey Capabilities:\n- Automatic input schema generation from function signatures\n- Dynamic parameter validation using function type hints\n- Support for both sync and async function execution\n- Integration with workflow dependency systems\n\nUsage Strategy:\n- Wrap existing utility functions for workflow integration\n- Create custom tool implementations without full tool classes\n- Rapid prototyping of workflow components\n- Build reusable function libraries for specific domains\n\nParameter Guide:\n- Function parameters matching wrapped function signature\n- Automatic schema validation based on function type hints\n- Must return serializable data types for workflow compatibility\n\nExamples:\n- {\"x\": 10, \"y\": 5} (for math functions)\n- {\"text\": \"Hello World\", \"reverse\": true} (for string operations)\n- {\"data\": [1,2,3,4,5], \"operation\": \"sum\"} (for data processing)\"\"\"  # noqa: E501\n    )\n    error_handling: ErrorHandling = Field(\n        default_factory=lambda: ErrorHandling(timeout_seconds=600)\n    )\n\n    def run_func(self, **_: Any) -&gt; Any:\n        \"\"\"\n        Execute the function logic with provided arguments.\n\n        This method must be implemented by subclasses.\n\n        :param kwargs: Arguments to pass to the function.\n        :return: Result of the function execution.\n        \"\"\"\n        raise NotImplementedError(\"run_func must be implemented by subclasses\")\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the tool with the provided input data and configuration.\n\n        :param input_data: Dictionary of input data to be passed to the tool.\n        :param config: Optional configuration for the runnable instance.\n        :return: Dictionary with the execution result.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        result = self.run_func(input_data, config=config, **kwargs)\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n        return {\"content\": result}\n\n    def get_schema(self):\n        \"\"\"\n        Generate the schema for the input and output of the tool.\n\n        :return: Dictionary representing the input and output schema.\n        \"\"\"\n        cls = self.__class__\n        run_tool_method = self.run_func\n        if hasattr(cls, \"_original_func\"):\n            run_tool_method = cls._original_func\n\n        signature = inspect.signature(run_tool_method)\n        parameters = signature.parameters\n\n        fields = {}\n        for name, param in parameters.items():\n            if name == \"self\":\n                continue\n            annotation = (\n                param.annotation if param.annotation != inspect.Parameter.empty else Any\n            )\n            default = ... if param.default == inspect.Parameter.empty else param.default\n            fields[name] = (annotation, default)\n\n        input_model = create_model(f\"{cls.__name__}Input\", **fields)\n\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"input_schema\": input_model.model_json_schema(),\n            \"output_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"content\": {\"type\": \"any\"}},\n            },\n        }\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the tool with the provided input data and configuration.</p> <p>:param input_data: Dictionary of input data to be passed to the tool. :param config: Optional configuration for the runnable instance. :return: Dictionary with the execution result.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the tool with the provided input data and configuration.\n\n    :param input_data: Dictionary of input data to be passed to the tool.\n    :param config: Optional configuration for the runnable instance.\n    :return: Dictionary with the execution result.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    result = self.run_func(input_data, config=config, **kwargs)\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.get_schema","title":"<code>get_schema()</code>","text":"<p>Generate the schema for the input and output of the tool.</p> <p>:return: Dictionary representing the input and output schema.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def get_schema(self):\n    \"\"\"\n    Generate the schema for the input and output of the tool.\n\n    :return: Dictionary representing the input and output schema.\n    \"\"\"\n    cls = self.__class__\n    run_tool_method = self.run_func\n    if hasattr(cls, \"_original_func\"):\n        run_tool_method = cls._original_func\n\n    signature = inspect.signature(run_tool_method)\n    parameters = signature.parameters\n\n    fields = {}\n    for name, param in parameters.items():\n        if name == \"self\":\n            continue\n        annotation = (\n            param.annotation if param.annotation != inspect.Parameter.empty else Any\n        )\n        default = ... if param.default == inspect.Parameter.empty else param.default\n        fields[name] = (annotation, default)\n\n    input_model = create_model(f\"{cls.__name__}Input\", **fields)\n\n    return {\n        \"name\": self.name,\n        \"description\": self.description,\n        \"input_schema\": input_model.model_json_schema(),\n        \"output_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\"content\": {\"type\": \"any\"}},\n        },\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.run_func","title":"<code>run_func(**_)</code>","text":"<p>Execute the function logic with provided arguments.</p> <p>This method must be implemented by subclasses.</p> <p>:param kwargs: Arguments to pass to the function. :return: Result of the function execution.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def run_func(self, **_: Any) -&gt; Any:\n    \"\"\"\n    Execute the function logic with provided arguments.\n\n    This method must be implemented by subclasses.\n\n    :param kwargs: Arguments to pass to the function.\n    :return: Result of the function execution.\n    \"\"\"\n    raise NotImplementedError(\"run_func must be implemented by subclasses\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.function_tool","title":"<code>function_tool(func)</code>","text":"<p>Decorator to convert a function into a FunctionTool subclass.</p> <p>:param func: Function to be converted into a tool. :return: A FunctionTool subclass that wraps the provided function.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def function_tool(func: Callable[..., T]) -&gt; type[FunctionTool[T]]:\n    \"\"\"\n    Decorator to convert a function into a FunctionTool subclass.\n\n    :param func: Function to be converted into a tool.\n    :return: A FunctionTool subclass that wraps the provided function.\n    \"\"\"\n\n    def create_input_schema(func) -&gt; type[BaseModel]:\n        signature = inspect.signature(func)\n\n        params_dict = {}\n\n        for param in signature.parameters.values():\n            if param.name == \"kwargs\" or param.name == \"config\":\n                continue\n            if param.default is inspect.Parameter.empty:\n                params_dict[param.name] = (param.annotation, ...)\n            else:\n                params_dict[param.name] = (param.annotation, param.default)\n\n        return create_model(\n            \"FunctionToolInputSchema\",\n            **params_dict,\n            __config__=ConfigDict(extra=\"allow\"),\n        )\n\n    class FunctionToolFromDecorator(FunctionTool[T]):\n        name: str = Field(default=func.__name__)\n        description: str = Field(\n            default=(func.__doc__ or \"\") + \"\\nFunction signature:\" + str(inspect.signature(func))\n            or f\"A tool for executing the {func.__name__} function with signature: {str(inspect.signature(func))}\"\n        )\n        _original_func = staticmethod(func)\n        input_schema: ClassVar[type[BaseModel]] = create_input_schema(func)\n\n        def run_func(self, input_data: BaseModel, **kwargs) -&gt; T:\n            return func(**input_data.model_dump(), **kwargs)\n\n    FunctionToolFromDecorator.__name__ = func.__name__\n    FunctionToolFromDecorator.__qualname__ = (\n        f\"FunctionToolFromDecorator({func.__name__})\"\n    )\n    FunctionToolFromDecorator.__module__ = func.__module__\n\n    return FunctionToolFromDecorator\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/","title":"Http api call","text":""},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCall","title":"<code>HttpApiCall</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for sending API requests using requests library.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group the node belongs to.</p> <code>connection</code> <code>Http | None</code> <p>The connection based on sending http requests.A new connection is created if none is provided.</p> <code>success_codes(list[int])</code> <code>Http | None</code> <p>The list of codes when request is successful.</p> <code>timeout</code> <code>float</code> <p>The timeout in seconds.</p> <code>data(dict[str,Any])</code> <code>float</code> <p>The data to send as body of request.</p> <code>headers(dict[str,Any])</code> <code>float</code> <p>The headers of request.</p> <code>payload_type</code> <code>dict[str, Any]</code> <p>Parameter to specify the type of payload data.</p> <code>params(dict[str,Any])</code> <code>dict[str, Any]</code> <p>The additional query params of request.</p> <code>url(str)</code> <code>dict[str, Any]</code> <p>The endpoint url for sending request</p> <code>method(str)</code> <code>dict[str, Any]</code> <p>The HTTP method for sending request.</p> <code>response_type(ResponseType|str)</code> <code>dict[str, Any]</code> <p>The type of response content.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>class HttpApiCall(ConnectionNode):\n    \"\"\"\n    A component for sending API requests using requests library.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group the node belongs to.\n        connection (HttpConnection | None): The connection based on sending http requests.A new connection\n            is created if none is provided.\n        success_codes(list[int]): The list of codes when request is successful.\n        timeout (float): The timeout in seconds.\n        data(dict[str,Any]): The data to send as body of request.\n        headers(dict[str,Any]): The headers of request.\n        payload_type (dict[str, Any]): Parameter to specify the type of payload data.\n        params(dict[str,Any]): The additional query params of request.\n        url(str): The endpoint url for sending request\n        method(str): The HTTP method for sending request.\n        response_type(ResponseType|str): The type of response content.\n    \"\"\"\n\n    name: str = \"Api Call Tool\"\n    description: str = DESCRIPTION_HTTP\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    connection: HttpConnection\n    success_codes: list[int] = [200]\n    timeout: float = 30\n    payload_type: RequestPayloadType = RequestPayloadType.RAW\n    data: dict[str, Any] = Field(default_factory=dict)\n    headers: dict[str, Any] = Field(default_factory=dict)\n    params: dict[str, Any] = Field(default_factory=dict)\n    url: str = \"\"\n    method: HTTPMethod | None = None\n    response_type: ResponseType | str | None = ResponseType.RAW\n    is_files_allowed: bool = True\n    input_schema: ClassVar[type[HttpApiCallInputSchema]] = HttpApiCallInputSchema\n\n    def execute(self, input_data: HttpApiCallInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"Execute the API call.\n\n        This method takes input data and returns content of API call response.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing(optionally) data, headers, payload_type,\n                params for request.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following keys:\n                - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request.\n                - \"status_code\" (int): The status code of the request.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n\" f\"{input_data.model_dump()}\")\n\n        data = self.connection.data | self.data | input_data.data\n        payload_type = input_data.payload_type or self.payload_type\n        files = {param: file_io.getvalue() for param, file_io in input_data.files.items()}\n\n        extras = {\"data\": data} if payload_type == RequestPayloadType.RAW else {\"json\": data}\n        url = input_data.url or self.url or self.connection.url\n        if not url:\n            raise ValueError(\"No url provided.\")\n        headers = input_data.headers\n        params = input_data.params\n        method = input_data.method or self.method or self.connection.method\n\n        try:\n            response = self.client.request(\n                method=method,\n                url=url,\n                headers=self.connection.headers | self.headers | headers,\n                params=self.connection.params | self.params | params,\n                timeout=self.timeout,\n                files=files,\n                **extras,\n            )\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Request failed with error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        if response.status_code not in self.success_codes:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results.\")\n            raise ToolExecutionException(\n                f\"Request failed with unexpected status code: {response.status_code} and response: {response.text}. \"\n                f\"Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        response_type = self.response_type\n        if \"response_type\" not in self.model_fields_set and response.headers.get(\"content-type\") == \"application/json\":\n            response_type = ResponseType.JSON\n\n        if response_type == ResponseType.TEXT:\n            content = response.text\n        elif response_type == ResponseType.RAW:\n            content = response.content\n        elif response_type == ResponseType.JSON:\n            content = response.json()\n        else:\n            allowed_types = [item.value for item in ResponseType]\n            raise ValueError(\n                f\"Response type must be one of the following: {', '.join(allowed_types)}\"\n            )\n        logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n\" f\"{str(content)[:200]}...\")\n        return {\"content\": content, \"status_code\": response.status_code}\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCall.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the API call.</p> <p>This method takes input data and returns content of API call response.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing(optionally) data, headers, payload_type, params for request.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the following keys: - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request. - \"status_code\" (int): The status code of the request.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>def execute(self, input_data: HttpApiCallInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"Execute the API call.\n\n    This method takes input data and returns content of API call response.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing(optionally) data, headers, payload_type,\n            params for request.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following keys:\n            - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request.\n            - \"status_code\" (int): The status code of the request.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n\" f\"{input_data.model_dump()}\")\n\n    data = self.connection.data | self.data | input_data.data\n    payload_type = input_data.payload_type or self.payload_type\n    files = {param: file_io.getvalue() for param, file_io in input_data.files.items()}\n\n    extras = {\"data\": data} if payload_type == RequestPayloadType.RAW else {\"json\": data}\n    url = input_data.url or self.url or self.connection.url\n    if not url:\n        raise ValueError(\"No url provided.\")\n    headers = input_data.headers\n    params = input_data.params\n    method = input_data.method or self.method or self.connection.method\n\n    try:\n        response = self.client.request(\n            method=method,\n            url=url,\n            headers=self.connection.headers | self.headers | headers,\n            params=self.connection.params | self.params | params,\n            timeout=self.timeout,\n            files=files,\n            **extras,\n        )\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Request failed with error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    if response.status_code not in self.success_codes:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results.\")\n        raise ToolExecutionException(\n            f\"Request failed with unexpected status code: {response.status_code} and response: {response.text}. \"\n            f\"Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    response_type = self.response_type\n    if \"response_type\" not in self.model_fields_set and response.headers.get(\"content-type\") == \"application/json\":\n        response_type = ResponseType.JSON\n\n    if response_type == ResponseType.TEXT:\n        content = response.text\n    elif response_type == ResponseType.RAW:\n        content = response.content\n    elif response_type == ResponseType.JSON:\n        content = response.json()\n    else:\n        allowed_types = [item.value for item in ResponseType]\n        raise ValueError(\n            f\"Response type must be one of the following: {', '.join(allowed_types)}\"\n        )\n    logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n\" f\"{str(content)[:200]}...\")\n    return {\"content\": content, \"status_code\": response.status_code}\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCallInputSchema","title":"<code>HttpApiCallInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>class HttpApiCallInputSchema(BaseModel):\n    data: dict = Field(default={}, description=\"Parameter to provide payload.\")\n    url: str = Field(default=\"\", description=\"Parameter to provide endpoint url.\")\n    payload_type: RequestPayloadType = Field(default=None, description=\"Parameter to specify the type of payload data.\")\n    headers: dict = Field(default={}, description=\"Parameter to provide headers to the request.\")\n    params: dict = Field(default={}, description=\"Parameter to provide GET parameters in URL.\")\n    method: HTTPMethod | None = Field(default=None, description=\"Parameter to specify HTTP method.\")\n    files: dict[str, io.BytesIO] = Field(\n        default={},\n        description=\"Parameter to provide files to the request. Maps parameter names to file paths for file uploads. \"\n        \"Provide strings for file IDs from files.\",\n        map_from_storage=True,\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"data\", \"headers\", \"params\", mode=\"before\")\n    @classmethod\n    def validate_dict_fields(cls, value: Any, field: str) -&gt; Any:\n        if isinstance(value, str):\n            try:\n                return json.loads(value or \"{}\")\n            except json.JSONDecodeError as e:\n                raise ActionParsingException(f\"Invalid JSON string provided for '{field}'. Error: {e}\")\n        elif isinstance(value, dict):\n            return value\n        else:\n            raise ActionParsingException(f\"Expected a dictionary or a JSON string for '{field}'.\")\n\n    @field_validator(\"files\", mode=\"before\")\n    @classmethod\n    def files_validator(cls, input_data: dict[str, str | bytes] | FileMappedInput):\n        \"\"\"Validate and process files.\"\"\"\n        return handle_file_upload(input_data)\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCallInputSchema.files_validator","title":"<code>files_validator(input_data)</code>  <code>classmethod</code>","text":"<p>Validate and process files.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>@field_validator(\"files\", mode=\"before\")\n@classmethod\ndef files_validator(cls, input_data: dict[str, str | bytes] | FileMappedInput):\n    \"\"\"Validate and process files.\"\"\"\n    return handle_file_upload(input_data)\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.handle_file_upload","title":"<code>handle_file_upload(input_data)</code>","text":"<p>Handles file uploading and converts all inputs to BytesIO objects.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, str | bytes] | FileMappedInput</code> <p>Dictionary mapping parameter names to file objects to upload or FileMappedInput object.</p> required <p>Returns:</p> Type Description <code>dict[str, BytesIO]</code> <p>dict[str, io.BytesIO]: Dictionary mapping parameter names to BytesIO objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid file data type is provided.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>def handle_file_upload(input_data: dict[str, str | bytes] | FileMappedInput) -&gt; dict[str, io.BytesIO]:\n    \"\"\"\n    Handles file uploading and converts all inputs to BytesIO objects.\n\n    Args:\n        input_data: Dictionary mapping parameter names to file objects to upload or FileMappedInput object.\n\n    Returns:\n        dict[str, io.BytesIO]: Dictionary mapping parameter names to BytesIO objects.\n\n    Raises:\n        ValueError: If invalid file data type is provided.\n    \"\"\"\n    files_data = {}\n    if isinstance(input_data, FileMappedInput):\n        files = input_data.input\n        files_map = {getattr(f, \"name\", f\"file_{id(f)}\"): f for f in input_data.files}\n    else:\n        files = input_data\n        files_map = {}\n\n    for param_name, file in files.items():\n        if isinstance(file, bytes):\n            bytes_io = io.BytesIO(file)\n            bytes_io.name = param_name\n            files_data[param_name] = bytes_io\n        elif isinstance(file, io.BytesIO):\n            files_data[param_name] = file\n        elif isinstance(file, FileInfo):\n            bytes_io = io.BytesIO(file.content)\n            bytes_io.name = file.name\n            files_data[param_name] = bytes_io\n        elif isinstance(file, str):\n            if files_map:\n                if file in files_map:\n                    bytes_io = io.BytesIO(files_map[file].getvalue())\n                    bytes_io.name = files_map[file].name\n                    files_data[param_name] = bytes_io\n                else:\n                    raise ValueError(f\"File {file} not found in files.\")\n            else:\n                raise ValueError(\n                    f\"Error: Invalid file data type: {type(file)}. \"\n                    \"If you want to use file path from files, provide FileMappedInput object.\"\n                )\n        else:\n            raise ValueError(f\"Error: Invalid file data type: {type(file)}. Expected bytes, BytesIO, or FileInfo.\")\n\n    return files_data\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/","title":"Human feedback","text":""},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackAction","title":"<code>HumanFeedbackAction</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Actions available for the HumanFeedbackTool.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class HumanFeedbackAction(str, Enum):\n    \"\"\"Actions available for the HumanFeedbackTool.\"\"\"\n\n    ASK = \"ask\"  # Request input from user\n    INFO = \"info\"  # Send info message without waiting for response\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackInputSchema","title":"<code>HumanFeedbackInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for HumanFeedbackTool.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class HumanFeedbackInputSchema(BaseModel):\n    \"\"\"Input schema for HumanFeedbackTool.\"\"\"\n\n    action: HumanFeedbackAction = Field(\n        default=HumanFeedbackAction.ASK,\n        description=\"Action to perform: 'ask' to request input from user, 'info' to just send a message.\",\n    )\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool","title":"<code>HumanFeedbackTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A unified tool for human interaction - both gathering feedback and sending messages.</p> <p>This tool can either prompt the user for input (action=\"ask\") or send an info message without waiting for response (action=\"info\").</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool's purpose.</p> <code>msg_template</code> <code>str</code> <p>Template of message to send.</p> <code>input_method</code> <code>FeedbackMethod | InputMethodCallable</code> <p>The method used to gather user input.</p> <code>output_method</code> <code>FeedbackMethod | OutputMethodCallable</code> <p>The method used to send messages.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class HumanFeedbackTool(Node):\n    \"\"\"\n    A unified tool for human interaction - both gathering feedback and sending messages.\n\n    This tool can either prompt the user for input (action=\"ask\") or send an info message\n    without waiting for response (action=\"info\").\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group the node belongs to.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool's purpose.\n        msg_template (str): Template of message to send.\n        input_method (FeedbackMethod | InputMethodCallable): The method used to gather user input.\n        output_method (FeedbackMethod | OutputMethodCallable): The method used to send messages.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"message_sender\"\n    description: str = \"\"\"A tool for gathering approval, confirmation, clarification, or information from user and\n  sending status updates.\n\nUse 'ask' action to request input - workflow WAITS for user response before continuing.\nUse 'info' action to send notification - workflow continues immediately without waiting.\n\nExamples:\n- {\"action\": \"ask\", \"input\": \"Should I proceed? (yes/no)\"}\n- {\"action\": \"info\", \"input\": \"Task completed.\"}\n\nImportant:\n- Use 'ask' for approval, confirmation, clarification, or information.\n- The user can only provide text responses - they can not perform actions.\n- This tool should be used to gather information from user and send status updates during agent execution.\n\"\"\"\n    input_method: FeedbackMethod | InputMethodCallable = FeedbackMethod.CONSOLE\n    output_method: FeedbackMethod | OutputMethodCallable = FeedbackMethod.CONSOLE\n    action: HumanFeedbackAction | None = Field(\n        default=None,\n        description=\"If set, this action is always used, ignoring input. Useful for workflow nodes.\",\n    )\n    input_schema: ClassVar[type[HumanFeedbackInputSchema]] = HumanFeedbackInputSchema\n    msg_template: str = \"{{input}}\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def update_description(self):\n        msg_template = self.msg_template\n        self.description += (\n            f\"\\nMessage template: '{msg_template}'.\" \" Parameters will be substituted based on the provided input data.\"\n        )\n        return self\n\n    def input_method_console(self, prompt: str) -&gt; str:\n        \"\"\"\n        Get input from the user using the console input method.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        return input(prompt)\n\n    def input_method_streaming(self, prompt: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Get input from the user using the queue streaming input method.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n            config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with prompt {prompt}\")\n\n        streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n\n        event = HFStreamingOutputEventMessage(\n            wf_run_id=config.run_id,\n            entity_id=self.id,\n            data=HFStreamingOutputEventMessageData(prompt=prompt, action=HumanFeedbackAction.ASK),\n            event=streaming.event,\n            source=StreamingEntitySource(\n                id=self.id,\n                name=self.name,\n                group=self.group,\n                type=self.type,\n            ),\n        )\n        logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n        self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n        event = self.get_input_streaming_event(\n            event_msg_type=HFStreamingInputEventMessage,\n            event=streaming.event,\n            config=config,\n        )\n        logger.debug(f\"Tool {self.name} - {self.id}: received input event {event}\")\n\n        return event.data.content\n\n    def output_method_console(self, message: str) -&gt; None:\n        \"\"\"\n        Sends message to console.\n\n        Args:\n            message (str): The message to display to the user.\n        \"\"\"\n        print(message)\n\n    def output_method_streaming(self, message: str, config: RunnableConfig, **kwargs) -&gt; None:\n        \"\"\"\n        Sends message using streaming method.\n\n        Args:\n            message (str): The message to display to the user.\n            config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n        \"\"\"\n        streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n\n        event = HFStreamingOutputEventMessage(\n            wf_run_id=config.run_id,\n            entity_id=self.id,\n            data=HFStreamingOutputEventMessageData(prompt=message, action=HumanFeedbackAction.INFO),\n            event=streaming.event,\n            source=StreamingEntitySource(\n                id=self.id,\n                name=self.name,\n                group=self.group,\n                type=self.type,\n            ),\n        )\n        logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n        self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n\n    def _execute_ask(self, input_text: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Execute the 'ask' action - get input from user.\"\"\"\n        if isinstance(self.input_method, FeedbackMethod):\n            if self.input_method == FeedbackMethod.CONSOLE:\n                return self.input_method_console(input_text)\n            elif self.input_method == FeedbackMethod.STREAM:\n                streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n                if not streaming.input_streaming_enabled:\n                    raise ValueError(\n                        f\"'{FeedbackMethod.STREAM}' input method requires enabled input and output streaming.\"\n                    )\n                return self.input_method_streaming(prompt=input_text, config=config, **kwargs)\n            else:\n                raise ValueError(f\"Unsupported input method: {self.input_method}\")\n        else:\n            return self.input_method.get_input(input_text)\n\n    def _execute_send(self, input_text: str, config: RunnableConfig, **kwargs) -&gt; None:\n        \"\"\"Execute the 'info' action - send info message to user.\"\"\"\n        if isinstance(self.output_method, FeedbackMethod):\n            if self.output_method == FeedbackMethod.CONSOLE:\n                self.output_method_console(input_text)\n            elif self.output_method == FeedbackMethod.STREAM:\n                self.output_method_streaming(message=input_text, config=config, **kwargs)\n            else:\n                raise ValueError(f\"Unsupported output method: {self.output_method}\")\n        else:\n            self.output_method.send_message(input_text)\n\n    def execute(\n        self, input_data: HumanFeedbackInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the tool with the provided input data and configuration.\n\n        Based on the 'action' parameter:\n        - \"ask\": Prompts the user for input and returns their response\n        - \"info\": Sends an info message to the user without waiting for response\n\n        Args:\n            input_data (HumanFeedbackInputSchema): The input data containing action and message.\n            config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n            **kwargs: Additional keyword arguments to be passed to the node execute run.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the result under the 'content' key.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_text = Template(self.msg_template).render(input_data.model_dump())\n        action = self.action if self.action is not None else input_data.action\n\n        if action == HumanFeedbackAction.ASK:\n            result = self._execute_ask(input_text, config, **kwargs)\n            logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n            return {\"content\": result}\n        elif action == HumanFeedbackAction.INFO:\n            self._execute_send(input_text, config, **kwargs)\n            logger.debug(f\"Tool {self.name} - {self.id}: message sent\")\n            return {\"content\": f\"Message sent: {input_text}\"}\n        else:\n            raise ValueError(\n                f\"Unsupported action: {action}. Use '{HumanFeedbackAction.ASK}' or '{HumanFeedbackAction.INFO}'.\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the tool with the provided input data and configuration.</p> <p>Based on the 'action' parameter: - \"ask\": Prompts the user for input and returns their response - \"info\": Sends an info message to the user without waiting for response</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>HumanFeedbackInputSchema</code> <p>The input data containing action and message.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the runnable. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the node execute run.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the result under the 'content' key.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def execute(\n    self, input_data: HumanFeedbackInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the tool with the provided input data and configuration.\n\n    Based on the 'action' parameter:\n    - \"ask\": Prompts the user for input and returns their response\n    - \"info\": Sends an info message to the user without waiting for response\n\n    Args:\n        input_data (HumanFeedbackInputSchema): The input data containing action and message.\n        config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to the node execute run.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the result under the 'content' key.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_text = Template(self.msg_template).render(input_data.model_dump())\n    action = self.action if self.action is not None else input_data.action\n\n    if action == HumanFeedbackAction.ASK:\n        result = self._execute_ask(input_text, config, **kwargs)\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n        return {\"content\": result}\n    elif action == HumanFeedbackAction.INFO:\n        self._execute_send(input_text, config, **kwargs)\n        logger.debug(f\"Tool {self.name} - {self.id}: message sent\")\n        return {\"content\": f\"Message sent: {input_text}\"}\n    else:\n        raise ValueError(\n            f\"Unsupported action: {action}. Use '{HumanFeedbackAction.ASK}' or '{HumanFeedbackAction.INFO}'.\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.input_method_console","title":"<code>input_method_console(prompt)</code>","text":"<p>Get input from the user using the console input method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def input_method_console(self, prompt: str) -&gt; str:\n    \"\"\"\n    Get input from the user using the console input method.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    return input(prompt)\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.input_method_streaming","title":"<code>input_method_streaming(prompt, config, **kwargs)</code>","text":"<p>Get input from the user using the queue streaming input method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the runnable. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def input_method_streaming(self, prompt: str, config: RunnableConfig, **kwargs) -&gt; str:\n    \"\"\"\n    Get input from the user using the queue streaming input method.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n        config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with prompt {prompt}\")\n\n    streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n\n    event = HFStreamingOutputEventMessage(\n        wf_run_id=config.run_id,\n        entity_id=self.id,\n        data=HFStreamingOutputEventMessageData(prompt=prompt, action=HumanFeedbackAction.ASK),\n        event=streaming.event,\n        source=StreamingEntitySource(\n            id=self.id,\n            name=self.name,\n            group=self.group,\n            type=self.type,\n        ),\n    )\n    logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n    self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n    event = self.get_input_streaming_event(\n        event_msg_type=HFStreamingInputEventMessage,\n        event=streaming.event,\n        config=config,\n    )\n    logger.debug(f\"Tool {self.name} - {self.id}: received input event {event}\")\n\n    return event.data.content\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.output_method_console","title":"<code>output_method_console(message)</code>","text":"<p>Sends message to console.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display to the user.</p> required Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def output_method_console(self, message: str) -&gt; None:\n    \"\"\"\n    Sends message to console.\n\n    Args:\n        message (str): The message to display to the user.\n    \"\"\"\n    print(message)\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.output_method_streaming","title":"<code>output_method_streaming(message, config, **kwargs)</code>","text":"<p>Sends message using streaming method.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to display to the user.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the runnable. Defaults to None.</p> required Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def output_method_streaming(self, message: str, config: RunnableConfig, **kwargs) -&gt; None:\n    \"\"\"\n    Sends message using streaming method.\n\n    Args:\n        message (str): The message to display to the user.\n        config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n    \"\"\"\n    streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n\n    event = HFStreamingOutputEventMessage(\n        wf_run_id=config.run_id,\n        entity_id=self.id,\n        data=HFStreamingOutputEventMessageData(prompt=message, action=HumanFeedbackAction.INFO),\n        event=streaming.event,\n        source=StreamingEntitySource(\n            id=self.id,\n            name=self.name,\n            group=self.group,\n            type=self.type,\n        ),\n    )\n    logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n    self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.InputMethodCallable","title":"<code>InputMethodCallable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for input methods.</p> <p>This class defines the interface for various input methods that can be used to gather user input in the HumanFeedbackTool.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class InputMethodCallable(ABC):\n    \"\"\"\n    Abstract base class for input methods.\n\n    This class defines the interface for various input methods that can be used\n    to gather user input in the HumanFeedbackTool.\n    \"\"\"\n\n    @abstractmethod\n    def get_input(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"\n        Get input from the user.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.InputMethodCallable.get_input","title":"<code>get_input(prompt, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Get input from the user.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>@abstractmethod\ndef get_input(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"\n    Get input from the user.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.OutputMethodCallable","title":"<code>OutputMethodCallable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sending message.</p> <p>This class defines the interface for various output methods that can be used to send messages in the HumanFeedbackTool (action='info').</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class OutputMethodCallable(ABC):\n    \"\"\"\n    Abstract base class for sending message.\n\n    This class defines the interface for various output methods that can be used\n    to send messages in the HumanFeedbackTool (action='info').\n    \"\"\"\n\n    @abstractmethod\n    def send_message(self, message: str, **kwargs) -&gt; None:\n        \"\"\"\n        Sends message to the user\n\n        Args:\n            message (str): The message to send to the user.\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.OutputMethodCallable.send_message","title":"<code>send_message(message, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Sends message to the user</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to send to the user.</p> required Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>@abstractmethod\ndef send_message(self, message: str, **kwargs) -&gt; None:\n    \"\"\"\n    Sends message to the user\n\n    Args:\n        message (str): The message to send to the user.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/jina/","title":"Jina","text":""},{"location":"dynamiq/nodes/tools/jina/#dynamiq.nodes.tools.jina.JinaScrapeTool","title":"<code>JinaScrapeTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for scraping web pages, powered by Jina Reader API.</p> <p>This class provides comprehensive web scraping capabilities with advanced customization options for content extraction and filtering.</p> Source code in <code>dynamiq/nodes/tools/jina.py</code> <pre><code>class JinaScrapeTool(ConnectionNode):\n    \"\"\"\n    A tool for scraping web pages, powered by Jina Reader API.\n\n    This class provides comprehensive web scraping capabilities with advanced\n    customization options for content extraction and filtering.\n    \"\"\"\n\n    SCRAPE_PATH: ClassVar[str] = \"https://r.jina.ai/\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SCRAPE\n    name: str = \"Jina Scraper Tool\"\n    description: str = DESCRIPTION_SCRAPE\n    response_format: JinaResponseFormat = JinaResponseFormat.MARKDOWN\n    connection: Jina\n    timeout: int = 60\n    url: str | None = Field(None, description=\"URL to scrape\")\n\n    # Advanced options\n    target_selector: str | None = Field(None, description=\"CSS selector to focus on specific elements\")\n    remove_selector: str | None = Field(None, description=\"CSS selector to exclude elements\")\n    include_links: bool = Field(default=False, description=\"Include links summary\")\n    include_images: bool = Field(default=False, description=\"Include images summary\")\n    generate_alt_text: bool = Field(default=True, description=\"Generate alt text for images\")\n    engine: str = Field(default=\"direct\", description=\"Engine: 'browser' or 'direct'\")\n    no_cache: bool = Field(default=False, description=\"Bypass cache\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[JinaScrapeInputSchema]] = JinaScrapeInputSchema\n\n    def _build_headers(self, input_data: JinaScrapeInputSchema) -&gt; dict[str, str]:\n        \"\"\"Build request headers based on configuration and input parameters.\"\"\"\n        headers = {\n            **self.connection.headers,\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"X-Timeout\": str(self.timeout),\n        }\n\n        if self.response_format != JinaResponseFormat.DEFAULT:\n            headers[\"X-Return-Format\"] = self.response_format.value\n\n        engine = input_data.engine or self.engine\n        if engine in [\"browser\", \"direct\", \"cf-browser-rendering\"]:\n            headers[\"X-Engine\"] = engine\n\n        target_selector = input_data.target_selector or self.target_selector\n        if target_selector:\n            headers[\"X-Target-Selector\"] = target_selector\n\n        remove_selector = input_data.remove_selector or self.remove_selector\n        if remove_selector:\n            headers[\"X-Remove-Selector\"] = remove_selector\n\n        include_links = input_data.include_links if input_data.include_links is not None else self.include_links\n        if include_links:\n            headers[\"X-With-Links-Summary\"] = \"true\"\n\n        include_images = input_data.include_images if input_data.include_images is not None else self.include_images\n        if include_images:\n            headers[\"X-With-Images-Summary\"] = \"true\"\n\n        generate_alt = (\n            input_data.generate_alt_text if input_data.generate_alt_text is not None else self.generate_alt_text\n        )\n        if generate_alt:\n            headers[\"X-With-Generated-Alt\"] = \"true\"\n\n        if self.no_cache:\n            headers[\"X-No-Cache\"] = \"true\"\n\n        return headers\n\n    def _build_request_body(self, input_data: JinaScrapeInputSchema) -&gt; dict[str, Any]:\n        \"\"\"Build request body according to Jina Reader API specification.\"\"\"\n        url = input_data.url or self.url\n        if not url:\n            raise ToolExecutionException(\n                \"No URL provided. Please provide a URL either during node initialization or execution.\",\n                recoverable=True,\n            )\n\n        return {\"url\": url}\n\n    def _parse_response(self, response_data: dict) -&gt; tuple[str, dict, dict]:\n        \"\"\"Parse Jina API response and extract content, links, and images.\"\"\"\n        if \"data\" not in response_data:\n            raise ToolExecutionException(\n                \"Invalid response format from Jina API - missing 'data' field\",\n                recoverable=True,\n            )\n\n        data = response_data[\"data\"]\n        content = data.get(\"content\", \"\")\n        links = data.get(\"links\", {})\n        images = data.get(\"images\", {})\n\n        return content, links, images\n\n    def execute(self, input_data: JinaScrapeInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the web scraping process using the Jina Reader API.\n\n        Args:\n            input_data (JinaScrapeInputSchema): Input data for the tool\n            config (RunnableConfig, optional): Configuration for the runnable\n            **kwargs: Additional arguments\n\n        Returns:\n            dict[str, Any]: Dictionary containing the scraped content and metadata\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        headers = self._build_headers(input_data)\n        request_body = self._build_request_body(input_data)\n\n        try:\n            response = self.client.request(\n                method=\"POST\",\n                url=self.SCRAPE_PATH,\n                headers=headers,\n                json=request_body,\n            )\n            response.raise_for_status()\n\n            if self.response_format in [JinaResponseFormat.PAGESHOT, JinaResponseFormat.SCREENSHOT]:\n                scrape_result = response.content\n                links, images = {}, {}\n            else:\n                response_data = response.json()\n                scrape_result, links, images = self._parse_response(response_data)\n\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to execute the requested action. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        url = request_body[\"url\"]\n\n        if self.is_optimized_for_agents:\n            result_parts = [f\"## Source URL\\n{url}\", f\"## Scraped Content\\n\\n{scrape_result}\"]\n\n            if links:\n                links_list = [f\"- [{text}]({url})\" for text, url in links.items()]\n                result_parts.append(\"## Links Found\\n\" + \"\\n\".join(links_list))\n\n            if images:\n                images_list = [f\"- {desc}: {url}\" for desc, url in images.items()]\n                result_parts.append(\"## Images Found\\n\" + \"\\n\".join(images_list))\n\n            result = \"\\n\\n\".join(result_parts)\n        else:\n            result = {\n                \"url\": url,\n                \"content\": scrape_result,\n                \"links\": links,\n                \"images\": images,\n                \"metadata\": {\n                    \"response_format\": self.response_format.value,\n                    \"engine_used\": headers.get(\"X-Engine\", \"direct\"),\n                    \"target_selector\": headers.get(\"X-Target-Selector\"),\n                    \"remove_selector\": headers.get(\"X-Remove-Selector\"),\n                },\n            }\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/jina/#dynamiq.nodes.tools.jina.JinaScrapeTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the web scraping process using the Jina Reader API.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>JinaScrapeInputSchema</code> <p>Input data for the tool</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing the scraped content and metadata</p> Source code in <code>dynamiq/nodes/tools/jina.py</code> <pre><code>def execute(self, input_data: JinaScrapeInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the web scraping process using the Jina Reader API.\n\n    Args:\n        input_data (JinaScrapeInputSchema): Input data for the tool\n        config (RunnableConfig, optional): Configuration for the runnable\n        **kwargs: Additional arguments\n\n    Returns:\n        dict[str, Any]: Dictionary containing the scraped content and metadata\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    headers = self._build_headers(input_data)\n    request_body = self._build_request_body(input_data)\n\n    try:\n        response = self.client.request(\n            method=\"POST\",\n            url=self.SCRAPE_PATH,\n            headers=headers,\n            json=request_body,\n        )\n        response.raise_for_status()\n\n        if self.response_format in [JinaResponseFormat.PAGESHOT, JinaResponseFormat.SCREENSHOT]:\n            scrape_result = response.content\n            links, images = {}, {}\n        else:\n            response_data = response.json()\n            scrape_result, links, images = self._parse_response(response_data)\n\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to execute the requested action. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    url = request_body[\"url\"]\n\n    if self.is_optimized_for_agents:\n        result_parts = [f\"## Source URL\\n{url}\", f\"## Scraped Content\\n\\n{scrape_result}\"]\n\n        if links:\n            links_list = [f\"- [{text}]({url})\" for text, url in links.items()]\n            result_parts.append(\"## Links Found\\n\" + \"\\n\".join(links_list))\n\n        if images:\n            images_list = [f\"- {desc}: {url}\" for desc, url in images.items()]\n            result_parts.append(\"## Images Found\\n\" + \"\\n\".join(images_list))\n\n        result = \"\\n\\n\".join(result_parts)\n    else:\n        result = {\n            \"url\": url,\n            \"content\": scrape_result,\n            \"links\": links,\n            \"images\": images,\n            \"metadata\": {\n                \"response_format\": self.response_format.value,\n                \"engine_used\": headers.get(\"X-Engine\", \"direct\"),\n                \"target_selector\": headers.get(\"X-Target-Selector\"),\n                \"remove_selector\": headers.get(\"X-Remove-Selector\"),\n            },\n        }\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/jina/#dynamiq.nodes.tools.jina.JinaSearchTool","title":"<code>JinaSearchTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for performing web searches using the Jina AI API.</p> <p>This tool accepts various search parameters and returns relevant search results.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>connection</code> <code>Jina</code> <p>The connection instance for the Jina API.</p> <code>query</code> <code>Optional[str]</code> <p>The search query, can be set during initialization.</p> <code>max_results</code> <code>int</code> <p>Maximum number of results to return.</p> <code>country</code> <code>Optional[str]</code> <p>Two-letter country code for search region.</p> <code>location</code> <code>Optional[str]</code> <p>Geographic location for search origin.</p> <code>language</code> <code>Optional[str]</code> <p>Two-letter language code for results.</p> <code>page</code> <code>int</code> <p>Page offset for pagination.</p> <code>site</code> <code>Optional[str]</code> <p>Domain to limit search to.</p> <code>return_format</code> <code>JinaResponseFormat</code> <p>Response format preference.</p> <code>include_images</code> <code>bool</code> <p>Include images in search results.</p> <code>include_links</code> <code>bool | Literal['all']</code> <p>Include link summaries or all links.</p> <code>include_favicons</code> <code>bool</code> <p>Include SERP favicons.</p> <code>include_favicon</code> <code>bool</code> <p>Include individual page favicon.</p> <code>include_full_content</code> <code>bool</code> <p>Include full content of search results.</p> <code>retain_images</code> <code>Literal['none'] | None</code> <p>Override X-Retain-Images header.</p> <code>respond_with</code> <code>Optional[str]</code> <p>Explicit X-Respond-With header (e.g., \"no-content\").</p> <code>engine</code> <code>Optional[str]</code> <p>Override engine (\"browser\", \"direct\", or \"cf-browser-rendering\").</p> <code>no_cache</code> <code>bool</code> <p>Bypass cache for real-time data.</p> <code>generate_alt_text</code> <code>bool</code> <p>Generate alt text for images.</p> <code>timeout</code> <code>Optional[int]</code> <p>Request timeout in seconds.</p> <code>locale</code> <code>Optional[str]</code> <p>Browser locale setting.</p> <code>cookies</code> <code>Optional[str]</code> <p>Custom cookie settings.</p> <code>proxy_url</code> <code>Optional[str]</code> <p>Proxy URL for requests.</p> Source code in <code>dynamiq/nodes/tools/jina.py</code> <pre><code>class JinaSearchTool(ConnectionNode):\n    \"\"\"\n      A tool for performing web searches using the Jina AI API.\n\n      This tool accepts various search parameters and returns relevant search results.\n\n    Attributes:\n          group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n          name (str): The name of the tool.\n          description (str): A brief description of the tool.\n          connection (Jina): The connection instance for the Jina API.\n          query (Optional[str]): The search query, can be set during initialization.\n          max_results (int): Maximum number of results to return.\n          country (Optional[str]): Two-letter country code for search region.\n          location (Optional[str]): Geographic location for search origin.\n          language (Optional[str]): Two-letter language code for results.\n          page (int): Page offset for pagination.\n          site (Optional[str]): Domain to limit search to.\n          return_format (JinaResponseFormat): Response format preference.\n          include_images (bool): Include images in search results.\n          include_links (bool | Literal[\"all\"]): Include link summaries or all links.\n          include_favicons (bool): Include SERP favicons.\n          include_favicon (bool): Include individual page favicon.\n          include_full_content (bool): Include full content of search results.\n          retain_images (Literal[\"none\"] | None): Override X-Retain-Images header.\n          respond_with (Optional[str]): Explicit X-Respond-With header (e.g., \"no-content\").\n          engine (Optional[str]): Override engine (\"browser\", \"direct\", or \"cf-browser-rendering\").\n          no_cache (bool): Bypass cache for real-time data.\n          generate_alt_text (bool): Generate alt text for images.\n          timeout (Optional[int]): Request timeout in seconds.\n          locale (Optional[str]): Browser locale setting.\n          cookies (Optional[str]): Custom cookie settings.\n          proxy_url (Optional[str]): Proxy URL for requests.\n    \"\"\"\n\n    SEARCH_PATH: ClassVar[str] = \"https://s.jina.ai/\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SEARCH\n    name: str = \"Jina Search Tool\"\n    description: str = DESCRIPTION_SEARCH\n    connection: Jina\n    query: str | None = Field(None, description=\"Search query\")\n    max_results: int = Field(default=5, ge=1, le=100, description=\"Maximum number of search results\")\n\n    country: str | None = Field(None, description=\"Two-letter country code (e.g., 'US', 'GB')\")\n    location: str | None = Field(None, description=\"Geographic location for search origin\")\n    language: str | None = Field(None, description=\"Two-letter language code (e.g., 'en', 'es')\")\n\n    page: int = Field(default=0, ge=0, description=\"Page offset for pagination\")\n\n    site: str | None = Field(None, description=\"Domain to limit search to\")\n    return_format: JinaResponseFormat = Field(default=JinaResponseFormat.DEFAULT, description=\"Response format\")\n\n    include_images: SummaryPreference = Field(\n        default=False, description=\"Include image summaries (True) or 'all' images per SERP entry\"\n    )\n    include_links: SummaryPreference = Field(\n        default=False, description=\"Include link summaries (True) or 'all' links per SERP entry\"\n    )\n    include_favicons: bool = Field(default=False, description=\"Include SERP favicons\")\n    include_favicon: bool = Field(default=False, description=\"Include individual page favicon\")\n    include_full_content: bool = Field(default=False, description=\"Include full content of results\")\n    retain_images: RetainImagesPreference | None = Field(\n        default=None, description=\"Controls X-Retain-Images header (e.g., 'none' to strip images)\"\n    )\n    respond_with: str | None = Field(default=None, description=\"Explicit X-Respond-With header value\")\n    engine: EnginePreference | None = Field(\n        default=None, description=\"Override engine ('browser', 'direct', or 'cf-browser-rendering')\"\n    )\n\n    no_cache: bool = Field(default=False, description=\"Bypass cache for real-time data\")\n    generate_alt_text: bool = Field(default=False, description=\"Generate alt text for images\")\n    timeout: int | None = Field(None, description=\"Request timeout in seconds\")\n    locale: str | None = Field(None, description=\"Browser locale setting\")\n    cookies: str | None = Field(None, description=\"Custom cookie settings\")\n    proxy_url: str | None = Field(None, description=\"Proxy URL for requests\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[JinaSearchInputSchema]] = JinaSearchInputSchema\n\n    @staticmethod\n    def _summary_header_value(value: SummaryPreference | None) -&gt; str | None:\n        \"\"\"Map boolean or literal summary preferences to header values.\"\"\"\n        if isinstance(value, str):\n            normalized = value.strip().lower()\n            if normalized == \"all\":\n                return \"all\"\n            if normalized in {\"true\", \"unique\"}:\n                return \"true\"\n            return None\n        if value:\n            return \"true\"\n        return None\n\n    def _build_headers(self, input_data: JinaSearchInputSchema, include_full: bool) -&gt; dict[str, str]:\n        \"\"\"Build request headers based on configuration and input parameters.\"\"\"\n        headers = {\n            **self.connection.headers,\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n\n        site = input_data.site or self.site\n        if site:\n            headers[\"X-Site\"] = site\n\n        include_links_value = input_data.include_links if input_data.include_links is not None else self.include_links\n        links_header = self._summary_header_value(include_links_value)\n        if links_header:\n            headers[\"X-With-Links-Summary\"] = links_header\n\n        include_images_value = (\n            input_data.include_images if input_data.include_images is not None else self.include_images\n        )\n        images_header = self._summary_header_value(include_images_value)\n        if images_header:\n            headers[\"X-With-Images-Summary\"] = images_header\n\n        retain_images_value = input_data.retain_images or self.retain_images\n        if retain_images_value:\n            headers[\"X-Retain-Images\"] = retain_images_value\n        elif not images_header:\n            headers[\"X-Retain-Images\"] = \"none\"\n\n        no_cache = input_data.no_cache if input_data.no_cache is not None else self.no_cache\n        if no_cache:\n            headers[\"X-No-Cache\"] = \"true\"\n\n        generate_alt = (\n            input_data.generate_alt_text if input_data.generate_alt_text is not None else self.generate_alt_text\n        )\n        if generate_alt:\n            headers[\"X-With-Generated-Alt\"] = \"true\"\n\n        respond_with = input_data.respond_with or self.respond_with\n        if respond_with:\n            headers[\"X-Respond-With\"] = respond_with\n        elif not include_full:\n            headers[\"X-Respond-With\"] = \"no-content\"\n\n        include_favicon = input_data.include_favicon if input_data.include_favicon is not None else self.include_favicon\n        if include_favicon:\n            headers[\"X-With-Favicon\"] = \"true\"\n\n        return_format = input_data.return_format or self.return_format\n        if return_format != JinaResponseFormat.DEFAULT:\n            headers[\"X-Return-Format\"] = return_format.value\n\n        engine = input_data.engine or self.engine\n        if engine:\n            headers[\"X-Engine\"] = engine\n        else:\n            headers[\"X-Engine\"] = \"browser\" if include_full else \"direct\"\n\n        include_favicons = (\n            input_data.include_favicons if input_data.include_favicons is not None else self.include_favicons\n        )\n        if include_favicons:\n            headers[\"X-With-Favicons\"] = \"true\"\n\n        timeout = input_data.timeout or self.timeout\n        if timeout:\n            headers[\"X-Timeout\"] = str(timeout)\n\n        cookies = input_data.cookies or self.cookies\n        if cookies:\n            headers[\"X-Set-Cookie\"] = cookies\n\n        proxy_url = input_data.proxy_url or self.proxy_url\n        if proxy_url:\n            headers[\"X-Proxy-Url\"] = proxy_url\n\n        locale = input_data.locale or self.locale\n        if locale:\n            headers[\"X-Locale\"] = locale\n\n        return headers\n\n    def _build_request_body(self, input_data: JinaSearchInputSchema) -&gt; dict[str, Any]:\n        \"\"\"Build request body according to Jina API specification.\"\"\"\n        query = input_data.query or self.query\n        if not query:\n            raise ToolExecutionException(\n                \"No query provided. Please provide a query either during node initialization or execution.\",\n                recoverable=True,\n            )\n\n        body = {\"q\": query}\n\n        max_results = input_data.max_results or self.max_results\n        if max_results:\n            body[\"num\"] = max_results\n\n        country = input_data.country or self.country\n        if country:\n            body[\"gl\"] = country\n\n        location = input_data.location or self.location\n        if location:\n            body[\"location\"] = location\n\n        language = input_data.language or self.language\n        if language:\n            body[\"hl\"] = language\n\n        page = input_data.page if input_data.page is not None else self.page\n        if page &gt; 0:\n            body[\"page\"] = page\n\n        return body\n\n    def _format_search_results(self, results: dict[str, Any], include_full: bool) -&gt; str:\n        \"\"\"Format the search results into a readable string format.\"\"\"\n        formatted_results = []\n        data = results.get(\"data\", [])\n        for idx, result in enumerate(data, start=1):\n            title = result.get(\"title\") or result.get(\"url\") or f\"Result {idx}\"\n            formatted_results.append(f\"### Result {idx}: {title}\")\n            if result.get(\"url\"):\n                formatted_results.append(f\"- URL: {result.get('url')}\")\n            if result.get(\"description\"):\n                formatted_results.append(f\"- Description: {result.get('description')}\")\n            if include_full and result.get(\"content\"):\n                formatted_results.append(f\"- Content: {result.get('content')}\")\n            if result.get(\"images\"):\n                image_lines = [f\"    \u2022 {label}: {img_url}\" for label, img_url in result[\"images\"].items()]\n                formatted_results.append(\"- Images:\\n\" + \"\\n\".join(image_lines))\n            formatted_results.append(\"\")\n        return \"\\n\".join(formatted_results).strip()\n\n    def execute(self, input_data: JinaSearchInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the web search process with full API parameter support.\n\n        Args:\n            input_data (JinaSearchInputSchema): Input data with search parameters.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional arguments passed to the execution context.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the search results.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        include_full = (\n            input_data.include_full_content\n            if input_data.include_full_content is not None\n            else self.include_full_content\n        )\n\n        headers = self._build_headers(input_data, include_full)\n        request_body = self._build_request_body(input_data)\n\n        try:\n            response = self.client.request(\n                method=\"POST\",\n                url=self.SEARCH_PATH,\n                headers=headers,\n                json=request_body,\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to retrieve search results. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        formatted_results = self._format_search_results(search_result, include_full)\n        sources_with_url = [\n            f\"[{result.get('title') or result.get('url')}]({result.get('url')})\"\n            for result in search_result.get(\"data\", [])\n            if result.get(\"url\")\n        ]\n\n        if self.is_optimized_for_agents:\n            result_sections = [\n                f\"## Jina Search Results for '{request_body['q']}'\",\n                \"\",\n            ]\n            if sources_with_url:\n                result_sections.extend([\"### Sources\", *sources_with_url, \"\"])\n            if formatted_results:\n                result_sections.append(formatted_results)\n            else:\n                result_sections.append(\"No results were returned by Jina Search.\")\n            result = \"\\n\".join(result_sections).strip()\n        else:\n            images = {}\n            for d in search_result.get(\"data\", []):\n                images.update(d.get(\"images\", {}))\n\n            result = {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"raw_response\": search_result,\n                \"images\": images,\n                \"query\": request_body[\"q\"],\n                \"request_body\": request_body,\n                \"headers_used\": {k: v for k, v in headers.items() if k.startswith(\"X-\")},\n                \"include_full_content\": include_full,\n            }\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/jina/#dynamiq.nodes.tools.jina.JinaSearchTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the web search process with full API parameter support.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>JinaSearchInputSchema</code> <p>Input data with search parameters.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the execution context.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the search results.</p> Source code in <code>dynamiq/nodes/tools/jina.py</code> <pre><code>def execute(self, input_data: JinaSearchInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the web search process with full API parameter support.\n\n    Args:\n        input_data (JinaSearchInputSchema): Input data with search parameters.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional arguments passed to the execution context.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the search results.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    include_full = (\n        input_data.include_full_content\n        if input_data.include_full_content is not None\n        else self.include_full_content\n    )\n\n    headers = self._build_headers(input_data, include_full)\n    request_body = self._build_request_body(input_data)\n\n    try:\n        response = self.client.request(\n            method=\"POST\",\n            url=self.SEARCH_PATH,\n            headers=headers,\n            json=request_body,\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to retrieve search results. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    formatted_results = self._format_search_results(search_result, include_full)\n    sources_with_url = [\n        f\"[{result.get('title') or result.get('url')}]({result.get('url')})\"\n        for result in search_result.get(\"data\", [])\n        if result.get(\"url\")\n    ]\n\n    if self.is_optimized_for_agents:\n        result_sections = [\n            f\"## Jina Search Results for '{request_body['q']}'\",\n            \"\",\n        ]\n        if sources_with_url:\n            result_sections.extend([\"### Sources\", *sources_with_url, \"\"])\n        if formatted_results:\n            result_sections.append(formatted_results)\n        else:\n            result_sections.append(\"No results were returned by Jina Search.\")\n        result = \"\\n\".join(result_sections).strip()\n    else:\n        images = {}\n        for d in search_result.get(\"data\", []):\n            images.update(d.get(\"images\", {}))\n\n        result = {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"raw_response\": search_result,\n            \"images\": images,\n            \"query\": request_body[\"q\"],\n            \"request_body\": request_body,\n            \"headers_used\": {k: v for k, v in headers.items() if k.startswith(\"X-\")},\n            \"include_full_content\": include_full,\n        }\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/","title":"Llm summarizer","text":""},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool","title":"<code>SummarizerTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for summarizing and cleaning up text extracted from HTML.</p> <p>This tool processes input text, typically extracted from HTML, by removing unnecessary content, cleaning up the remaining text, and formatting it into a coherent and well-organized summary.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A description of the tool's functionality.</p> <code>llm</code> <code>Node</code> <p>The language model node used for text processing.</p> <code>chunk_size</code> <code>int</code> <p>The maximum number of words in each chunk for processing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Configuration for error handling.</p> <code>prompt_template</code> <code>str</code> <p>The prompt template used for text summarization.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>class SummarizerTool(Node):\n    \"\"\"\n    A tool for summarizing and cleaning up text extracted from HTML.\n\n    This tool processes input text, typically extracted from HTML, by removing unnecessary content,\n    cleaning up the remaining text, and formatting it into a coherent and well-organized summary.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group this node belongs to.\n        name (str): The name of the tool.\n        description (str): A description of the tool's functionality.\n        llm (Node): The language model node used for text processing.\n        chunk_size (int): The maximum number of words in each chunk for processing.\n        error_handling (ErrorHandling): Configuration for error handling.\n        prompt_template (str): The prompt template used for text summarization.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Summarizer Tool\"\n    description: str = \"\"\"Summarizes and cleans up text content using LLM with automatic chunking and noise removal.\n\nKey Capabilities:\n- Intelligent text cleanup removing navigation, buttons, footers\n- Automatic chunking for large documents (4000 words default)\n- LLM-powered summarization with customizable prompts\n- Content structuring with proper headings and formatting\n\nUsage Strategy:\n- Process scraped web content for analysis\n- Clean up extracted HTML text for readability\n- Generate structured summaries from unformatted content\n- Remove noise and irrelevant elements from text data\n\nParameter Guide:\n- input: Raw text content to be summarized and cleaned\n\"\"\"\n    llm: Node\n    chunk_size: int = Field(default=4000, description=\"The maximum number of words in each chunk\")\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    prompt_template: str = Field(\n        default=PROMPT_TEMPLATE_SUMMARIZER,\n        description=\"The prompt template for the summarizer\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[SummarizerInputSchema]] = SummarizerInputSchema\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize the components of the tool.\n\n        Args:\n            connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Convert the tool to a dictionary representation.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary representation of the tool.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def _process_chunk(self, chunk: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Process a single chunk of text using the language model.\n\n        Args:\n            chunk (str): The text chunk to process.\n            config (RunnableConfig): The configuration for running the model.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The processed text chunk.\n\n        Raises:\n            ValueError: If the language model execution fails.\n        \"\"\"\n        prompt = self.prompt_template.format(input=chunk)\n        result = self.llm.run(\n            input_data={},\n            prompt=Prompt(messages=[Message(role=\"user\", content=prompt)]),\n            config=config,\n            **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}),\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict(for_tracing=True)]\n        if result.status != RunnableStatus.SUCCESS:\n            raise ValueError(\"LLM execution failed\")\n        return result.output[\"content\"]\n\n    def execute(\n        self, input_data: SummarizerInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the summarization tool on the input data.\n\n        This method processes the input text, potentially breaking it into chunks if it exceeds\n        the specified chunk size, and then summarizes each chunk using the language model.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input text under the 'input' key.\n            config (RunnableConfig, optional): The configuration for running the tool.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the summarized content under the 'content' key.\n\n        Raises:\n            ValueError: If the input_data does not contain an 'input' key.\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_text = input_data.input\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: started with input text length: {len(input_text)}, \"\n            f\"word count: {len(input_text.split())}\"\n        )\n\n        words = input_text.split()\n        if len(words) &gt; self.chunk_size:\n            content_chunks = [\n                \" \".join(words[i : i + self.chunk_size])\n                for i in range(0, len(words), self.chunk_size)\n            ]\n            summaries = [self._process_chunk(chunk, config, **kwargs) for chunk in content_chunks]\n            summary = \"\\n\".join(summaries)\n        else:\n            summary = self._process_chunk(input_text, config, **kwargs)\n\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: finished with result length: {len(summary)}, \"\n            f\"word count: {len(summary.split())}\"\n        )\n        return {\"content\": summary}\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the summarization tool on the input data.</p> <p>This method processes the input text, potentially breaking it into chunks if it exceeds the specified chunk size, and then summarizes each chunk using the language model.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input text under the 'input' key.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for running the tool.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the summarized content under the 'content' key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input_data does not contain an 'input' key.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def execute(\n    self, input_data: SummarizerInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the summarization tool on the input data.\n\n    This method processes the input text, potentially breaking it into chunks if it exceeds\n    the specified chunk size, and then summarizes each chunk using the language model.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input text under the 'input' key.\n        config (RunnableConfig, optional): The configuration for running the tool.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the summarized content under the 'content' key.\n\n    Raises:\n        ValueError: If the input_data does not contain an 'input' key.\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_text = input_data.input\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: started with input text length: {len(input_text)}, \"\n        f\"word count: {len(input_text.split())}\"\n    )\n\n    words = input_text.split()\n    if len(words) &gt; self.chunk_size:\n        content_chunks = [\n            \" \".join(words[i : i + self.chunk_size])\n            for i in range(0, len(words), self.chunk_size)\n        ]\n        summaries = [self._process_chunk(chunk, config, **kwargs) for chunk in content_chunks]\n        summary = \"\\n\".join(summaries)\n    else:\n        summary = self._process_chunk(input_text, config, **kwargs)\n\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: finished with result length: {len(summary)}, \"\n        f\"word count: {len(summary.split())}\"\n    )\n    return {\"content\": summary}\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize the components of the tool.\n\n    Args:\n        connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the tool to a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tool.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the tool to a dictionary representation.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary representation of the tool.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/","title":"Mcp","text":""},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPServer","title":"<code>MCPServer</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool that manages connections to MCP servers and initializes MCP tools.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>Node group.</p> <code>name</code> <code>str</code> <p>Node name.</p> <code>description</code> <code>str</code> <p>Node description.</p> <code>connection</code> <code>MCPSse | MCPStdio | MCPStreamableHTTP</code> <p>Connection module for the MCP server.</p> <code>include_tools</code> <code>list[str]</code> <p>Names of tools to include. If empty, all tools are included.</p> <code>exclude_tools</code> <code>list[str]</code> <p>Names of tools to exclude.</p> <code>_mcp_tools</code> <code>dict[str, MCPTool]</code> <p>Internal dict of initialized MCP tools.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>class MCPServer(ConnectionNode):\n    \"\"\"\n    A tool that manages connections to MCP servers and initializes MCP tools.\n\n    Attributes:\n      group (Literal[NodeGroup.TOOLS]): Node group.\n      name (str): Node name.\n      description (str): Node description.\n      connection (MCPSse | MCPStdio | MCPStreamableHTTP): Connection module for the MCP server.\n      include_tools (list[str]): Names of tools to include. If empty, all tools are included.\n      exclude_tools (list[str]): Names of tools to exclude.\n      _mcp_tools (dict[str, MCPTool]): Internal dict of initialized MCP tools.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"MCP Tool\"\n    description: str = \"\"\"Model Context Server integration for\n    dynamic tool discovery and external service connectivity.\n\nKey Capabilities:\n- Dynamic tool discovery and initialization from MCP servers\n- Support for multiple connection types (SSE, stdio, HTTP streaming)\n- Automatic schema generation from MCP tool definitions\n- Tool filtering and selection with include/exclude lists\n\nUsage Strategy:\n- Integrate with external services via MCP protocol\n- Access remote tools and APIs through standardized interface\n- Build distributed tool ecosystems across services\n- Extend workflow capabilities with external service tools\n\"\"\"  # noqa: E501\n    connection: MCPSse | MCPStdio | MCPStreamableHTTP\n\n    include_tools: list[str] = field(default_factory=list)\n    exclude_tools: list[str] = field(default_factory=list)\n    _mcp_tools: dict[str, MCPTool] = PrivateAttr(default_factory=dict)\n\n    async def initialize_tools(self):\n        \"\"\"\n        Initializes the MCP tool list from the client session.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            async with self.connection.connect() as result:\n                read, write = result[:2]\n                async with ClientSession(read, write) as session:\n                    await session.initialize()\n                    tools = await session.list_tools()\n                    for tool in tools.tools:\n                        self._mcp_tools[tool.name] = MCPTool(\n                            name=tool.name,\n                            description=tool.description or \"MCP Tool\",\n                            json_input_schema=tool.inputSchema,\n                            connection=self.connection,\n                            server_metadata=ServerMetadata(id=self.id, name=self.name, description=self.description),\n                        )\n\n            logger.info(f\"Tool {self.name}: {len(self._mcp_tools)} MCP tools initialized from a server.\")\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to initialize session. Error: {str(e)}\")\n            raise ToolExecutionException(f\"Tool {self.name} - {self.id}: failed to initialize session. Error: {str(e)}\")\n\n    def get_mcp_tools(self, select_all: bool = False) -&gt; list[MCPTool]:\n        \"\"\"\n        Synchronously fetches and initializes MCP tools if not already available.\n\n        Args:\n            select_all (bool): If True, returns all tools regardless of filtering.\n\n        Returns:\n            list[MCPTool]: A list of initialized MCPTool instances.\n        \"\"\"\n        if is_called_from_async_context():\n            with ContextAwareThreadPoolExecutor() as executor:\n                future = executor.submit(lambda: asyncio.run(self.get_mcp_tools_async(select_all=select_all)))\n                return future.result()\n        return asyncio.run(self.get_mcp_tools_async(select_all=select_all))\n\n    async def get_mcp_tools_async(self, select_all: bool = False) -&gt; list[MCPTool]:\n        \"\"\"\n        Asynchronously fetches and initializes MCP tools if not already available.\n\n        Args:\n            select_all (bool): If True, returns all tools regardless of filtering.\n\n        Returns:\n            list[MCPTool]: A list of initialized MCPTool instances.\n        \"\"\"\n        if not self._mcp_tools:\n            await self.initialize_tools()\n\n        if select_all or not self.include_tools and not self.exclude_tools:\n            return list(self._mcp_tools.values())\n\n        if self.include_tools:\n            return [v for k, v in self._mcp_tools.items() if k in self.include_tools and k not in self.exclude_tools]\n\n        return [v for k, v in self._mcp_tools.items() if k not in self.exclude_tools]\n\n    def execute(self, **kwargs):\n        \"\"\"\n        Disabled for the MCP server. Use `get_mcp_tools()` to access individual tools.\n\n        Raises:\n            NotImplementedError: Always, because this method is not supported.\n        \"\"\"\n        raise NotImplementedError(\"Use `get_mcp_tools()` to access individual tool instances.\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPServer.execute","title":"<code>execute(**kwargs)</code>","text":"<p>Disabled for the MCP server. Use <code>get_mcp_tools()</code> to access individual tools.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always, because this method is not supported.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>def execute(self, **kwargs):\n    \"\"\"\n    Disabled for the MCP server. Use `get_mcp_tools()` to access individual tools.\n\n    Raises:\n        NotImplementedError: Always, because this method is not supported.\n    \"\"\"\n    raise NotImplementedError(\"Use `get_mcp_tools()` to access individual tool instances.\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPServer.get_mcp_tools","title":"<code>get_mcp_tools(select_all=False)</code>","text":"<p>Synchronously fetches and initializes MCP tools if not already available.</p> <p>Parameters:</p> Name Type Description Default <code>select_all</code> <code>bool</code> <p>If True, returns all tools regardless of filtering.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[MCPTool]</code> <p>list[MCPTool]: A list of initialized MCPTool instances.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>def get_mcp_tools(self, select_all: bool = False) -&gt; list[MCPTool]:\n    \"\"\"\n    Synchronously fetches and initializes MCP tools if not already available.\n\n    Args:\n        select_all (bool): If True, returns all tools regardless of filtering.\n\n    Returns:\n        list[MCPTool]: A list of initialized MCPTool instances.\n    \"\"\"\n    if is_called_from_async_context():\n        with ContextAwareThreadPoolExecutor() as executor:\n            future = executor.submit(lambda: asyncio.run(self.get_mcp_tools_async(select_all=select_all)))\n            return future.result()\n    return asyncio.run(self.get_mcp_tools_async(select_all=select_all))\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPServer.get_mcp_tools_async","title":"<code>get_mcp_tools_async(select_all=False)</code>  <code>async</code>","text":"<p>Asynchronously fetches and initializes MCP tools if not already available.</p> <p>Parameters:</p> Name Type Description Default <code>select_all</code> <code>bool</code> <p>If True, returns all tools regardless of filtering.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[MCPTool]</code> <p>list[MCPTool]: A list of initialized MCPTool instances.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>async def get_mcp_tools_async(self, select_all: bool = False) -&gt; list[MCPTool]:\n    \"\"\"\n    Asynchronously fetches and initializes MCP tools if not already available.\n\n    Args:\n        select_all (bool): If True, returns all tools regardless of filtering.\n\n    Returns:\n        list[MCPTool]: A list of initialized MCPTool instances.\n    \"\"\"\n    if not self._mcp_tools:\n        await self.initialize_tools()\n\n    if select_all or not self.include_tools and not self.exclude_tools:\n        return list(self._mcp_tools.values())\n\n    if self.include_tools:\n        return [v for k, v in self._mcp_tools.items() if k in self.include_tools and k not in self.exclude_tools]\n\n    return [v for k, v in self._mcp_tools.items() if k not in self.exclude_tools]\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPServer.initialize_tools","title":"<code>initialize_tools()</code>  <code>async</code>","text":"<p>Initializes the MCP tool list from the client session.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>async def initialize_tools(self):\n    \"\"\"\n    Initializes the MCP tool list from the client session.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        async with self.connection.connect() as result:\n            read, write = result[:2]\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                tools = await session.list_tools()\n                for tool in tools.tools:\n                    self._mcp_tools[tool.name] = MCPTool(\n                        name=tool.name,\n                        description=tool.description or \"MCP Tool\",\n                        json_input_schema=tool.inputSchema,\n                        connection=self.connection,\n                        server_metadata=ServerMetadata(id=self.id, name=self.name, description=self.description),\n                    )\n\n        logger.info(f\"Tool {self.name}: {len(self._mcp_tools)} MCP tools initialized from a server.\")\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to initialize session. Error: {str(e)}\")\n        raise ToolExecutionException(f\"Tool {self.name} - {self.id}: failed to initialize session. Error: {str(e)}\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPTool","title":"<code>MCPTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool that interacts with the MCP server, enabling execution of specific server-side functions.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>Node group.</p> <code>name</code> <code>str</code> <p>Node name.</p> <code>description</code> <code>str</code> <p>Node description.</p> <code>input_schema</code> <code>ClassVar[type[BaseModel]]</code> <p>The schema that defines the expected structure of tool's input.</p> <code>connection</code> <code>MCPSse | MCPStdio | MCPStreamableHTTP</code> <p>Connection module for the MCP server.</p> <code>server_metadata</code> <code>ServerMetadata</code> <p>Server metadata for tracing.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>class MCPTool(ConnectionNode):\n    \"\"\"\n    A tool that interacts with the MCP server, enabling execution of specific server-side functions.\n\n    Attributes:\n      group (Literal[NodeGroup.TOOLS]): Node group.\n      name (str): Node name.\n      description (str): Node description.\n      input_schema (ClassVar[type[BaseModel]]): The schema that defines the expected structure of tool's input.\n      connection (MCPSse | MCPStdio | MCPStreamableHTTP): Connection module for the MCP server.\n      server_metadata (ServerMetadata): Server metadata for tracing.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str\n    description: str\n    input_schema: type[BaseModel]\n    json_input_schema: dict[str, Any]\n    connection: MCPSse | MCPStdio | MCPStreamableHTTP\n    server_metadata: ServerMetadata = field(default_factory=dict)\n\n    def __init__(self, json_input_schema: dict[str, Any], **kwargs):\n        \"\"\"\n        Initializes the MCP tool with a given input schema and additional parameters.\n\n        Args:\n            input_schema (dict[str, Any]): JSON schema to define input fields.\n            **kwargs\n        \"\"\"\n        input_schema = MCPTool.get_input_schema(json_input_schema)\n        json_input_schema = rename_keys_recursive(json_input_schema, {\"type\": \"type_\"})\n        super().__init__(input_schema=input_schema, json_input_schema=json_input_schema, **kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        parent_dict = super().to_dict_exclude_params.copy()\n        parent_dict.update(\n            {\n                \"input_schema\": True,\n            }\n        )\n        return parent_dict\n\n    @staticmethod\n    def get_input_schema(schema_dict) -&gt; type[BaseModel]:\n        \"\"\"\n        Creates an input schema based on provided MCP schema.\n\n        Args:\n            schema_dict (dict[str, Any]): A JSON schema dictionary describing the tool's expected input.\n        \"\"\"\n        schema_dict = rename_keys_recursive(schema_dict, {\"type_\": \"type\"})\n\n        for _, props in schema_dict.get(\"properties\", {}).items():\n            enum_values = props.pop(\"enum\", None)\n            if enum_values:\n                description = props.get(\"description\", \"\")\n                enum_description = f\" Allowed values: {', '.join(map(str, enum_values))}.\"\n                props[\"description\"] = description.rstrip() + enum_description\n\n        input_schema = json.dumps(schema_dict)\n\n        with TemporaryDirectory() as tmpdir:\n            out_path = Path(tmpdir) / \"model.py\"\n\n            generate(\n                input_schema,\n                input_file_type=InputFileType.JsonSchema,\n                output=out_path,\n                output_model_type=DataModelType.PydanticV2BaseModel,\n            )\n\n            module_name = \"dynamiq.nodes.tools.MCPTool.mcp_schema\"\n            spec = importlib.util.spec_from_file_location(module_name, out_path)\n            generated_module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = generated_module\n            spec.loader.exec_module(generated_module)\n            generated_classes = [\n                cls\n                for _, cls in inspect.getmembers(generated_module, inspect.isclass)\n                if cls.__module__ == generated_module.__name__\n            ]\n\n            model_cls = generated_classes[0]\n            model_cls.model_rebuild()\n            return model_cls\n\n    def execute(self, input_data: BaseModel, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the MCP tool synchronously with the provided input.\n\n        Args:\n            input_data (BaseModel): Input data for the tool execution.\n            config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n            **kwargs: Additional arguments passed to the execution context.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the tool's output.\n        \"\"\"\n        return asyncio.run(self.execute_async(input_data, config, **kwargs))\n\n    async def execute_async(\n        self, input_data: BaseModel, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the MCP tool asynchronously.\n\n        Args:\n            input_data (BaseModel): Input data for the tool execution.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the tool's output.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_dict = input_data.model_dump()\n\n        try:\n            async with self.connection.connect() as result:\n                read, write = result[:2]\n                async with ClientSession(read, write) as session:\n                    await session.initialize()\n                    result = await session.call_tool(self.name, input_dict)\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to call tool from the MCP server.\"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPTool.__init__","title":"<code>__init__(json_input_schema, **kwargs)</code>","text":"<p>Initializes the MCP tool with a given input schema and additional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>input_schema</code> <code>dict[str, Any]</code> <p>JSON schema to define input fields.</p> required Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>def __init__(self, json_input_schema: dict[str, Any], **kwargs):\n    \"\"\"\n    Initializes the MCP tool with a given input schema and additional parameters.\n\n    Args:\n        input_schema (dict[str, Any]): JSON schema to define input fields.\n        **kwargs\n    \"\"\"\n    input_schema = MCPTool.get_input_schema(json_input_schema)\n    json_input_schema = rename_keys_recursive(json_input_schema, {\"type\": \"type_\"})\n    super().__init__(input_schema=input_schema, json_input_schema=json_input_schema, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the MCP tool synchronously with the provided input.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>BaseModel</code> <p>Input data for the tool execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable, including callbacks.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the execution context.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the tool's output.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>def execute(self, input_data: BaseModel, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the MCP tool synchronously with the provided input.\n\n    Args:\n        input_data (BaseModel): Input data for the tool execution.\n        config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n        **kwargs: Additional arguments passed to the execution context.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the tool's output.\n    \"\"\"\n    return asyncio.run(self.execute_async(input_data, config, **kwargs))\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPTool.execute_async","title":"<code>execute_async(input_data, config=None, **kwargs)</code>  <code>async</code>","text":"<p>Executes the MCP tool asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>BaseModel</code> <p>Input data for the tool execution.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the tool's output.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>async def execute_async(\n    self, input_data: BaseModel, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the MCP tool asynchronously.\n\n    Args:\n        input_data (BaseModel): Input data for the tool execution.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the tool's output.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_dict = input_data.model_dump()\n\n    try:\n        async with self.connection.connect() as result:\n            read, write = result[:2]\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                result = await session.call_tool(self.name, input_dict)\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to call tool from the MCP server.\"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.MCPTool.get_input_schema","title":"<code>get_input_schema(schema_dict)</code>  <code>staticmethod</code>","text":"<p>Creates an input schema based on provided MCP schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_dict</code> <code>dict[str, Any]</code> <p>A JSON schema dictionary describing the tool's expected input.</p> required Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>@staticmethod\ndef get_input_schema(schema_dict) -&gt; type[BaseModel]:\n    \"\"\"\n    Creates an input schema based on provided MCP schema.\n\n    Args:\n        schema_dict (dict[str, Any]): A JSON schema dictionary describing the tool's expected input.\n    \"\"\"\n    schema_dict = rename_keys_recursive(schema_dict, {\"type_\": \"type\"})\n\n    for _, props in schema_dict.get(\"properties\", {}).items():\n        enum_values = props.pop(\"enum\", None)\n        if enum_values:\n            description = props.get(\"description\", \"\")\n            enum_description = f\" Allowed values: {', '.join(map(str, enum_values))}.\"\n            props[\"description\"] = description.rstrip() + enum_description\n\n    input_schema = json.dumps(schema_dict)\n\n    with TemporaryDirectory() as tmpdir:\n        out_path = Path(tmpdir) / \"model.py\"\n\n        generate(\n            input_schema,\n            input_file_type=InputFileType.JsonSchema,\n            output=out_path,\n            output_model_type=DataModelType.PydanticV2BaseModel,\n        )\n\n        module_name = \"dynamiq.nodes.tools.MCPTool.mcp_schema\"\n        spec = importlib.util.spec_from_file_location(module_name, out_path)\n        generated_module = importlib.util.module_from_spec(spec)\n        sys.modules[module_name] = generated_module\n        spec.loader.exec_module(generated_module)\n        generated_classes = [\n            cls\n            for _, cls in inspect.getmembers(generated_module, inspect.isclass)\n            if cls.__module__ == generated_module.__name__\n        ]\n\n        model_cls = generated_classes[0]\n        model_cls.model_rebuild()\n        return model_cls\n</code></pre>"},{"location":"dynamiq/nodes/tools/mcp/#dynamiq.nodes.tools.mcp.rename_keys_recursive","title":"<code>rename_keys_recursive(data, key_map)</code>","text":"<p>Recursively renames keys in a nested dictionary based on a provided key mapping.</p> Source code in <code>dynamiq/nodes/tools/mcp.py</code> <pre><code>def rename_keys_recursive(data: dict[str, Any] | list[str], key_map: dict[str, str]) -&gt; Any:\n    \"\"\"\n    Recursively renames keys in a nested dictionary based on a provided key mapping.\n    \"\"\"\n    if isinstance(data, dict):\n        return {key_map.get(key, key): rename_keys_recursive(value, key_map) for key, value in data.items()}\n    elif isinstance(data, list):\n        return [rename_keys_recursive(item, key_map) for item in data]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/parallel_tool_calls/","title":"Parallel tool calls","text":"<p>Meta-tool that enables parallel tool execution capability for agents.</p>"},{"location":"dynamiq/nodes/tools/parallel_tool_calls/#dynamiq.nodes.tools.parallel_tool_calls.ParallelToolCallsInputSchema","title":"<code>ParallelToolCallsInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for ParallelToolCallsTool.</p> Source code in <code>dynamiq/nodes/tools/parallel_tool_calls.py</code> <pre><code>class ParallelToolCallsInputSchema(BaseModel):\n    \"\"\"Input schema for ParallelToolCallsTool.\"\"\"\n\n    tools: list[ToolCallItem] = Field(\n        ...,\n        description=\"List of tools to execute in parallel\",\n        min_length=1,\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/parallel_tool_calls/#dynamiq.nodes.tools.parallel_tool_calls.ParallelToolCallsTool","title":"<code>ParallelToolCallsTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A meta-tool that signals the agent can execute multiple tools in parallel.</p> <p>This tool does not execute tools itself - it provides a schema that allows the agent to specify multiple tools to run in parallel. The agent's internal parallel execution logic handles the actual execution.</p> <p>When the agent uses this tool, it passes a list of tool calls which are then executed in parallel by the agent's execution engine.</p> Source code in <code>dynamiq/nodes/tools/parallel_tool_calls.py</code> <pre><code>class ParallelToolCallsTool(Node):\n    \"\"\"\n    A meta-tool that signals the agent can execute multiple tools in parallel.\n\n    This tool does not execute tools itself - it provides a schema that allows\n    the agent to specify multiple tools to run in parallel. The agent's internal\n    parallel execution logic handles the actual execution.\n\n    When the agent uses this tool, it passes a list of tool calls which are\n    then executed in parallel by the agent's execution engine.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: Literal[\"run_parallel\"] = PARALLEL_TOOL_NAME  # Frozen - cannot be overridden\n    description: str = \"Tool that enables running multiple other tools simultaneously in parallel execution.\"\n\n    input_schema: ClassVar[type[ParallelToolCallsInputSchema]] = ParallelToolCallsInputSchema\n\n    def execute(self, input_data: ParallelToolCallsInputSchema, config=None, **kwargs):\n        \"\"\"Agent intercepts this tool - execute is never called directly.\"\"\"\n        return None\n</code></pre>"},{"location":"dynamiq/nodes/tools/parallel_tool_calls/#dynamiq.nodes.tools.parallel_tool_calls.ParallelToolCallsTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Agent intercepts this tool - execute is never called directly.</p> Source code in <code>dynamiq/nodes/tools/parallel_tool_calls.py</code> <pre><code>def execute(self, input_data: ParallelToolCallsInputSchema, config=None, **kwargs):\n    \"\"\"Agent intercepts this tool - execute is never called directly.\"\"\"\n    return None\n</code></pre>"},{"location":"dynamiq/nodes/tools/parallel_tool_calls/#dynamiq.nodes.tools.parallel_tool_calls.ToolCallItem","title":"<code>ToolCallItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a single tool call within parallel execution.</p> Source code in <code>dynamiq/nodes/tools/parallel_tool_calls.py</code> <pre><code>class ToolCallItem(BaseModel):\n    \"\"\"Schema for a single tool call within parallel execution.\"\"\"\n\n    name: str = Field(\n        ...,\n        description=\"Name of the tool to execute (must match an available tool name)\",\n    )\n    input: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Input parameters for the tool as key-value pairs\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/preprocess_tool/","title":"Preprocess tool","text":""},{"location":"dynamiq/nodes/tools/preprocess_tool/#dynamiq.nodes.tools.preprocess_tool.PreprocessTool","title":"<code>PreprocessTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for preprocessing text by splitting it into smaller parts.</p> Source code in <code>dynamiq/nodes/tools/preprocess_tool.py</code> <pre><code>class PreprocessTool(Node):\n    \"\"\"\n    A tool for preprocessing text by splitting it into smaller parts.\n    \"\"\"\n\n    group: Literal[NodeGroup.SPLITTERS] = NodeGroup.SPLITTERS\n    name: str = \"PreprocessTool\"\n    description: str = PREPROCESS_TOOL_DESCRIPTION\n    split_by: DocumentSplitBy = DocumentSplitBy.SENTENCE\n    split_length: int = 10\n    split_overlap: int = 0\n    input_schema: ClassVar[type[PreprocessToolInputSchema]] = PreprocessToolInputSchema\n\n    def execute(self, input_data: PreprocessToolInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Splits the documents into chunks based on the provided parameters.\n\n        Args:\n            input_data (PreprocessToolInputSchema): The input data containing the documents to split.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n\n        split_by = input_data.split_by or self.split_by\n        split_length = input_data.split_length if input_data.split_length is not None else self.split_length\n        split_overlap = input_data.split_overlap if input_data.split_overlap is not None else self.split_overlap\n\n        logger.debug(\n            f\"Splitting {len(documents)} documents with parameters: split_by={split_by}, \"\n            f\"split_length={split_length}, split_overlap={split_overlap}\"\n        )\n\n        splitter = DocumentSplitterComponent(\n            split_by=split_by,\n            split_length=split_length,\n            split_overlap=split_overlap,\n        )\n\n        output = splitter.run(documents=documents)\n\n        split_documents = output[\"documents\"]\n        logger.debug(f\"Split {len(documents)} documents into {len(split_documents)} parts\")\n\n        return {\n            \"content\": split_documents,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/tools/preprocess_tool/#dynamiq.nodes.tools.preprocess_tool.PreprocessTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Splits the documents into chunks based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>PreprocessToolInputSchema</code> <p>The input data containing the documents to split.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the split documents under the key \"documents\".</p> Source code in <code>dynamiq/nodes/tools/preprocess_tool.py</code> <pre><code>def execute(self, input_data: PreprocessToolInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Splits the documents into chunks based on the provided parameters.\n\n    Args:\n        input_data (PreprocessToolInputSchema): The input data containing the documents to split.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n\n    split_by = input_data.split_by or self.split_by\n    split_length = input_data.split_length if input_data.split_length is not None else self.split_length\n    split_overlap = input_data.split_overlap if input_data.split_overlap is not None else self.split_overlap\n\n    logger.debug(\n        f\"Splitting {len(documents)} documents with parameters: split_by={split_by}, \"\n        f\"split_length={split_length}, split_overlap={split_overlap}\"\n    )\n\n    splitter = DocumentSplitterComponent(\n        split_by=split_by,\n        split_length=split_length,\n        split_overlap=split_overlap,\n    )\n\n    output = splitter.run(documents=documents)\n\n    split_documents = output[\"documents\"]\n    logger.debug(f\"Split {len(documents)} documents into {len(split_documents)} parts\")\n\n    return {\n        \"content\": split_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/python/","title":"Python","text":"<p>thon</p>"},{"location":"dynamiq/nodes/tools/python_code_executor/","title":"Python code executor","text":"<p>thon_code_executor</p>"},{"location":"dynamiq/nodes/tools/scale_serp/","title":"Scale serp","text":""},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpInputSchema","title":"<code>ScaleSerpInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>class ScaleSerpInputSchema(BaseModel):\n    query: str | None = Field(default=None, description=\"Parameter to provide a search query.\")\n    url: str | None = Field(default=None, description=\"Parameter to provide a search url.\")\n    limit: int | None = Field(default=None, description=\"Parameter to specify the number of results to return.\")\n    search_type: SearchType = Field(\n        default=SearchType.WEB, description=\"Type of search to perform (web, news, images, videos)\"\n    )\n    output: str | None = Field(\n        default=None, description=\"Output format for the results (json, html, csv). Defaults to json if not specified.\"\n    )\n    include_html: bool = Field(\n        default=False, description=\"Whether to include HTML content in the results. Defaults to False.\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_query_url(self):\n        \"\"\"Validate that either query or url is specified if both are provided\"\"\"\n        if self.url and self.query:\n            raise ValueError(\"Cannot specify both 'query' and 'url' at the same time.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpInputSchema.validate_query_url","title":"<code>validate_query_url()</code>","text":"<p>Validate that either query or url is specified if both are provided</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_query_url(self):\n    \"\"\"Validate that either query or url is specified if both are provided\"\"\"\n    if self.url and self.query:\n        raise ValueError(\"Cannot specify both 'query' and 'url' at the same time.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpTool","title":"<code>ScaleSerpTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for performing web searches using the Scale SERP API.</p> <p>This tool accepts a query or URL and returns search results based on the specified search type (organic, news, images, videos). The results include titles, links, and snippets.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>connection</code> <code>ScaleSerp</code> <p>The connection instance for the Scale SERP API.</p> <code>query</code> <code>str</code> <p>The default search query to use.</p> <code>url</code> <code>str</code> <p>The default URL to search.</p> <code>limit</code> <code>int</code> <p>The default number of search results to return.</p> <code>search_type</code> <code>SearchType</code> <p>The type of search to perform.</p> <code>output</code> <code>str | None</code> <p>The output format for the results (json, html, csv).</p> <code>include_html</code> <code>bool</code> <p>Whether to include HTML content in the results.</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>class ScaleSerpTool(ConnectionNode):\n    \"\"\"\n    A tool for performing web searches using the Scale SERP API.\n\n    This tool accepts a query or URL and returns search results based on the specified\n    search type (organic, news, images, videos). The results include titles, links, and snippets.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        connection (ScaleSerp): The connection instance for the Scale SERP API.\n        query (str): The default search query to use.\n        url (str): The default URL to search.\n        limit (int): The default number of search results to return.\n        search_type (SearchType): The type of search to perform.\n        output (str | None): The output format for the results (json, html, csv).\n        include_html (bool): Whether to include HTML content in the results.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SEARCH\n    name: str = \"Scale Serp Search Tool\"\n    description: str = DESCRIPTION_SERP\n    connection: ScaleSerp\n\n    query: str = Field(default=\"\", description=\"The default search query to use\")\n    url: str = Field(default=\"\", description=\"The default URL to search\")\n    limit: int = Field(default=10, ge=1, le=1000, description=\"The default number of search results to return\")\n    search_type: SearchType = Field(default=SearchType.WEB, description=\"The type of search to perform\")\n    output: str | None = Field(\n        default=None, description=\"Output format for the results (json, html, csv). Defaults to json if not specified.\"\n    )\n    include_html: bool = Field(\n        default=False, description=\"Whether to include HTML content in the results. Defaults to False.\"\n    )\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[ScaleSerpInputSchema]] = ScaleSerpInputSchema\n\n    def _format_search_results(self, results: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Formats the search results into a human-readable string.\n        \"\"\"\n        content_results = results.get(self.search_type.result_key, [])\n\n        formatted_results = []\n        for result in content_results:\n            formatted_results.extend(\n                [\n                    f\"Title: {result.get('title')}\",\n                    f\"Link: {result.get('link')}\",\n                    f\"Snippet: {result.get('snippet', 'N/A')}\",\n                    \"\",\n                ]\n            )\n\n        return \"\\n\".join(formatted_results).strip()\n\n    def execute(\n        self, input_data: ScaleSerpInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the search using the Scale SERP API and returns the formatted results.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query = input_data.query or self.query\n        url = input_data.url or self.url\n        limit = input_data.limit or self.limit\n        search_type = input_data.search_type or self.search_type\n        output_format = input_data.output or self.output\n        include_html = input_data.include_html if input_data.include_html is not None else self.include_html\n\n        if not query and not url:\n            raise ToolExecutionException(\n                \"Either 'query' or 'url' must be provided in input data or node parameters.\", recoverable=True\n            )\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=urljoin(self.connection.url, \"/search\"),\n                params=self.get_params(\n                    query=query,\n                    url=url,\n                    num=limit,\n                    search_type=search_type,\n                    output=output_format,\n                    include_html=include_html,\n                ),\n            )\n            search_result = response.json()\n            if response.status_code &gt;= 400:\n                error_message = search_result.get(\"request_info\", {}).get(\"message\")\n                logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(error_message)}\")\n                raise ToolExecutionException(\n                    f\"Tool '{self.name}' failed to retrieve search results. \"\n                    f\"Error: {str(error_message)}. Please analyze the error and take appropriate action.\",\n                    recoverable=True,\n                )\n        except ToolExecutionException:\n            raise\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' encountered an unexpected error. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        formatted_results = self._format_search_results(search_result)\n        content_results = search_result.get(search_type.result_key, [])\n\n        sources_with_url = [f\"[{result.get('title')}]({result.get('link')})\" for result in content_results]\n\n        if self.is_optimized_for_agents:\n            search_term = query or url\n            return {\n                \"content\": (\n                    \"## Sources with URLs\\n\"\n                    + \"\\n\".join(sources_with_url)\n                    + f\"\\n\\n## Search results for '{search_term}'\\n\"\n                    + formatted_results\n                )\n            }\n\n        return {\n            \"content\": {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"urls\": [result.get(\"link\") for result in content_results],\n                \"raw_response\": search_result,\n            }\n        }\n\n    def get_params(\n        self,\n        query: str | None = None,\n        url: str | None = None,\n        search_type: SearchType | None = None,\n        output: str | None = None,\n        include_html: bool | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Prepare the parameters for the API request.\n        \"\"\"\n        params = {\"api_key\": self.connection.api_key, **kwargs}\n\n        current_search_type = search_type or self.search_type\n\n        if current_search_type != SearchType.WEB:\n            params[\"search_type\"] = current_search_type\n\n        if query:\n            params[\"q\"] = query\n        elif url:\n            params[\"url\"] = url\n\n        if output:\n            params[\"output\"] = output\n\n        if include_html is not None:\n            params[\"include_html\"] = include_html\n\n        return {k: v for k, v in params.items() if v is not None}\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the search using the Scale SERP API and returns the formatted results.</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>def execute(\n    self, input_data: ScaleSerpInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the search using the Scale SERP API and returns the formatted results.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query = input_data.query or self.query\n    url = input_data.url or self.url\n    limit = input_data.limit or self.limit\n    search_type = input_data.search_type or self.search_type\n    output_format = input_data.output or self.output\n    include_html = input_data.include_html if input_data.include_html is not None else self.include_html\n\n    if not query and not url:\n        raise ToolExecutionException(\n            \"Either 'query' or 'url' must be provided in input data or node parameters.\", recoverable=True\n        )\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=urljoin(self.connection.url, \"/search\"),\n            params=self.get_params(\n                query=query,\n                url=url,\n                num=limit,\n                search_type=search_type,\n                output=output_format,\n                include_html=include_html,\n            ),\n        )\n        search_result = response.json()\n        if response.status_code &gt;= 400:\n            error_message = search_result.get(\"request_info\", {}).get(\"message\")\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(error_message)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to retrieve search results. \"\n                f\"Error: {str(error_message)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n    except ToolExecutionException:\n        raise\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' encountered an unexpected error. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    formatted_results = self._format_search_results(search_result)\n    content_results = search_result.get(search_type.result_key, [])\n\n    sources_with_url = [f\"[{result.get('title')}]({result.get('link')})\" for result in content_results]\n\n    if self.is_optimized_for_agents:\n        search_term = query or url\n        return {\n            \"content\": (\n                \"## Sources with URLs\\n\"\n                + \"\\n\".join(sources_with_url)\n                + f\"\\n\\n## Search results for '{search_term}'\\n\"\n                + formatted_results\n            )\n        }\n\n    return {\n        \"content\": {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"urls\": [result.get(\"link\") for result in content_results],\n            \"raw_response\": search_result,\n        }\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpTool.get_params","title":"<code>get_params(query=None, url=None, search_type=None, output=None, include_html=None, **kwargs)</code>","text":"<p>Prepare the parameters for the API request.</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>def get_params(\n    self,\n    query: str | None = None,\n    url: str | None = None,\n    search_type: SearchType | None = None,\n    output: str | None = None,\n    include_html: bool | None = None,\n    **kwargs,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare the parameters for the API request.\n    \"\"\"\n    params = {\"api_key\": self.connection.api_key, **kwargs}\n\n    current_search_type = search_type or self.search_type\n\n    if current_search_type != SearchType.WEB:\n        params[\"search_type\"] = current_search_type\n\n    if query:\n        params[\"q\"] = query\n    elif url:\n        params[\"url\"] = url\n\n    if output:\n        params[\"output\"] = output\n\n    if include_html is not None:\n        params[\"include_html\"] = include_html\n\n    return {k: v for k, v in params.items() if v is not None}\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.SearchType","title":"<code>SearchType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>class SearchType(str, enum.Enum):\n    WEB = \"web\"\n    NEWS = \"news\"\n    IMAGES = \"images\"\n    VIDEOS = \"videos\"\n\n    @property\n    def result_key(self) -&gt; str:\n        \"\"\"Returns the corresponding result key for the search type\"\"\"\n        return {\n            SearchType.WEB: \"organic_results\",\n            SearchType.NEWS: \"news_results\",\n            SearchType.IMAGES: \"image_results\",\n            SearchType.VIDEOS: \"video_results\",\n        }[self]\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.SearchType.result_key","title":"<code>result_key: str</code>  <code>property</code>","text":"<p>Returns the corresponding result key for the search type</p>"},{"location":"dynamiq/nodes/tools/skills_tool/","title":"Skills tool","text":""},{"location":"dynamiq/nodes/tools/skills_tool/#dynamiq.nodes.tools.skills_tool.SkillsTool","title":"<code>SkillsTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>Tool for skills: discover and get content from a skill registry (Dynamiq or FileSystem).</p> <p>After get, apply the skill's instructions yourself and provide the result in your final answer.</p> Source code in <code>dynamiq/nodes/tools/skills_tool.py</code> <pre><code>class SkillsTool(Node):\n    \"\"\"Tool for skills: discover and get content from a skill registry (Dynamiq or FileSystem).\n\n    After get, apply the skill's instructions yourself and provide the result in your final answer.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"SkillsTool\"\n    description: str = (\n        \"Manages skills (instructions and optional scripts). Use this tool to:\\n\"\n        \"- List available skills: action='list' (use this to discover skill names and descriptions).\\n\"\n        \"- Get skill content: action='get', skill_name='...' \"\n        \"\u2014 use only when skills are NOT available in the sandbox. \"\n        \"When a sandbox is available and skills have been \"\n        \"ingested (e.g. under /home/user/skills/), prefer reading \"\n        \"skill content from the sandbox via SandboxShellTool \"\n        \"(e.g. cat /home/user/skills/&lt;name&gt;/SKILL.md or grep for a section) \"\n        \"instead of calling get. For large skills use \"\n        \"section='Section title' or line_start/line_end to read only a part.\\n\\n\"\n        \"When get returns a 'scripts_path', or when using the \"\n        \"sandbox, scripts are under &lt;skill_dir&gt;/scripts/: \"\n        \"run them via the sandbox (cd &lt;path&gt; then run scripts).\\n\\n\"\n        \"After reading skill content (from sandbox or get), apply\"\n        \" the instructions yourself and provide the result \"\n        \"in your final answer. Do not call the tool again with\"\n        \" user content to transform; the tool only provides \"\n        \"instructions; you produce the output.\"\n    )\n\n    skill_registry: BaseSkillRegistry = Field(\n        ...,\n        description=\"Registry providing skills (Dynamiq or FileSystem).\",\n    )\n    input_schema: ClassVar[type[SkillsToolInputSchema]] = SkillsToolInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\n            \"skill_registry\": True,\n        }\n\n    def execute(\n        self, input_data: SkillsToolInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        action = input_data.action\n        logger.info(\"SkillsTool - action=%s\", action.value)\n\n        if action == SkillsToolAction.LIST:\n            return self._list_skills()\n        if action == SkillsToolAction.GET:\n            if not input_data.skill_name:\n                raise ToolExecutionException(\"skill_name required for get\", recoverable=True)\n            return self._get_skill(\n                input_data.skill_name,\n                section=input_data.section,\n                line_start=input_data.line_start,\n                line_end=input_data.line_end,\n            )\n        raise ToolExecutionException(f\"Unknown action: {action.value}\", recoverable=True)\n\n    def _list_skills(self) -&gt; dict[str, Any]:\n        metadata_list = self.skill_registry.get_skills_metadata()\n        base = normalize_sandbox_skills_base_path(getattr(self.skill_registry, \"sandbox_skills_base_path\", None))\n        skills_info = []\n        for m in metadata_list:\n            entry: dict[str, Any] = {\"name\": m.name, \"description\": m.description}\n            if base:\n                entry[\"sandbox_path\"] = f\"{base}/{m.name}/SKILL.md\"\n            scripts_path = self.skill_registry.get_skill_scripts_path(m.name)\n            if scripts_path:\n                entry[\"scripts_path\"] = scripts_path\n            skills_info.append(entry)\n        names = [m.name for m in metadata_list]\n        logger.info(\"SkillsTool - list: %d skill(s) %s\", len(metadata_list), names)\n        return {\n            \"content\": {\n                \"available_skills\": skills_info,\n                \"total\": len(metadata_list),\n            }\n        }\n\n    def _get_skill(\n        self,\n        skill_name: str,\n        section: str | None = None,\n        line_start: int | None = None,\n        line_end: int | None = None,\n    ) -&gt; dict[str, Any]:\n        try:\n            instructions = self.skill_registry.get_skill_instructions(skill_name)\n        except Exception as e:\n            raise ToolExecutionException(f\"Failed to get skill '{skill_name}': {e}\", recoverable=True) from e\n\n        def _content_dict(sliced_instructions: str, section_used: str | None = None) -&gt; dict[str, Any]:\n            out: dict[str, Any] = {\n                \"name\": instructions.name,\n                \"description\": instructions.description,\n                \"instructions\": sliced_instructions,\n            }\n            if section_used is not None:\n                out[\"section_used\"] = section_used\n            if instructions.metadata:\n                out[\"metadata\"] = instructions.metadata\n            path = self.skill_registry.get_skill_scripts_path(skill_name)\n            if path:\n                out[\"scripts_path\"] = path\n            return out\n\n        if section is not None or line_start is not None or line_end is not None:\n            sliced, section_used = extract_skill_content_slice(\n                instructions.instructions,\n                section=section,\n                line_start=line_start,\n                line_end=line_end,\n            )\n            if section is not None and section_used is None:\n                raise ToolExecutionException(\n                    f\"Section '{section}' not found in skill '{skill_name}'.\",\n                    recoverable=True,\n                )\n            one_line = sliced.replace(\"\\n\", \" \").strip()\n            preview = (one_line[:50] + \"...\") if len(one_line) &gt; 50 else one_line\n            logger.info(\n                \"SkillsTool - get: skill=%s (section=%s, lines=%s-%s) -&gt; content received (%d chars), preview: %s\",\n                skill_name,\n                section,\n                line_start,\n                line_end,\n                len(sliced),\n                preview,\n            )\n            return {\"content\": _content_dict(sliced, section_used)}\n\n        one_line = instructions.instructions.replace(\"\\n\", \" \").strip()\n        preview = (one_line[:50] + \"...\") if len(one_line) &gt; 50 else one_line\n        logger.info(\n            \"SkillsTool - get: skill=%s -&gt; content received (%d chars), preview: %s\",\n            skill_name,\n            len(instructions.instructions),\n            preview,\n        )\n        return {\"content\": _content_dict(instructions.instructions)}\n</code></pre>"},{"location":"dynamiq/nodes/tools/skills_tool/#dynamiq.nodes.tools.skills_tool.SkillsToolAction","title":"<code>SkillsToolAction</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Action for the Skills tool.</p> Source code in <code>dynamiq/nodes/tools/skills_tool.py</code> <pre><code>class SkillsToolAction(str, Enum):\n    \"\"\"Action for the Skills tool.\"\"\"\n\n    LIST = \"list\"\n    GET = \"get\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/skills_tool/#dynamiq.nodes.tools.skills_tool.SkillsToolInputSchema","title":"<code>SkillsToolInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for Skills tool. Actions: list (discover), get (full or partial content).</p> Source code in <code>dynamiq/nodes/tools/skills_tool.py</code> <pre><code>class SkillsToolInputSchema(BaseModel):\n    \"\"\"Input schema for Skills tool. Actions: list (discover), get (full or partial content).\"\"\"\n\n    action: SkillsToolAction = Field(\n        ...,\n        description=\"Action: 'list' discover skills, 'get' full or partial skill content.\",\n    )\n    skill_name: str | None = Field(default=None, description=\"Skill name (required for get)\")\n    section: str | None = Field(\n        default=None,\n        description=\"For get: return only this markdown section (e.g. 'Welcome messages')\",\n    )\n    line_start: int | None = Field(default=None, description=\"For get: 1-based start line (body only)\")\n    line_end: int | None = Field(default=None, description=\"For get: 1-based end line (inclusive)\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/sql_executor/","title":"Sql executor","text":""},{"location":"dynamiq/nodes/tools/sql_executor/#dynamiq.nodes.tools.sql_executor.SQLExecutor","title":"<code>SQLExecutor</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for SQL query execution.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>connection</code> <code>PostgreSQL | MySQL | Snowflake | AWSRedshift | DatabricksSQL</code> <p>The connection instance for the</p> <code>query</code> <code>Optional[str]</code> <p>The SQL statement to execute.</p> <code>input_schema</code> <code>SQLInputSchema</code> <p>The input schema for the tool.</p> Source code in <code>dynamiq/nodes/tools/sql_executor.py</code> <pre><code>class SQLExecutor(ConnectionNode):\n    \"\"\"\n    A tool for SQL query execution.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        connection (PostgreSQL|MySQL|Snowflake|AWSRedshift|DatabricksSQL): The connection instance for the\n        specified storage.\n        query (Optional[str]): The SQL statement to execute.\n        input_schema (SQLInputSchema): The input schema for the tool.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.DATABASE_QUERY\n    name: str = \"SQL Executor Tool\"\n    description: str = DESCRIPTION_SQL\n    connection: PostgreSQL | MySQL | Snowflake | AWSRedshift | DatabricksSQL\n    query: str | None = None\n    parameters: dict[str, Any] | list[Any] | None = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[SQLInputSchema]] = SQLInputSchema\n\n    def format_results(self, results: list[dict[str, Any]], query: str) -&gt; str:\n        \"\"\"Format the retrieved results.\n\n        Args:\n            query (str): The executed SQL statement.\n            results (list[dict[str,Any]]): List of execution results.\n\n        Returns:\n            str: Formatted content of the query result.\n        \"\"\"\n        formatted_results = []\n        if not results:\n            return f'Query \"{query}\" executed successfully. No results returned.'\n        for i, result in enumerate(results):\n            formatted_result = f\"Row {i + 1}\\n\"\n            formatted_result += \"\\n\".join(f\"{key}: {value}\" for key, value in result.items())\n            formatted_results.append(formatted_result)\n        return \"\\n\\n\".join(formatted_results)\n\n    def execute(self, input_data: SQLInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query = input_data.query or self.query\n        params = input_data.parameters if input_data.parameters is not None else self.parameters\n        try:\n            if not query:\n                raise ValueError(\"Query cannot be empty\")\n            cursor = self.client.cursor(\n                **(\n                    self.connection.cursor_params\n                    if not isinstance(self.connection, (PostgreSQL, AWSRedshift, DatabricksSQL))\n                    else {}\n                )\n            )\n            if params is not None:\n                cursor.execute(query, params)\n            else:\n                cursor.execute(query)\n            output = cursor.fetchall() if cursor.description is not None else []\n            if isinstance(self.connection, DatabricksSQL):\n                output = [row.asDict(True) for row in output]\n            cursor.close()\n            if self.is_optimized_for_agents:\n                output = self.format_results(output, query)\n            return {\"content\": output}\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool {self.name} failed to execute query {query}. Error: {e}\", recoverable=True\n            )\n</code></pre>"},{"location":"dynamiq/nodes/tools/sql_executor/#dynamiq.nodes.tools.sql_executor.SQLExecutor.format_results","title":"<code>format_results(results, query)</code>","text":"<p>Format the retrieved results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The executed SQL statement.</p> required <code>results</code> <code>list[dict[str, Any]]</code> <p>List of execution results.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted content of the query result.</p> Source code in <code>dynamiq/nodes/tools/sql_executor.py</code> <pre><code>def format_results(self, results: list[dict[str, Any]], query: str) -&gt; str:\n    \"\"\"Format the retrieved results.\n\n    Args:\n        query (str): The executed SQL statement.\n        results (list[dict[str,Any]]): List of execution results.\n\n    Returns:\n        str: Formatted content of the query result.\n    \"\"\"\n    formatted_results = []\n    if not results:\n        return f'Query \"{query}\" executed successfully. No results returned.'\n    for i, result in enumerate(results):\n        formatted_result = f\"Row {i + 1}\\n\"\n        formatted_result += \"\\n\".join(f\"{key}: {value}\" for key, value in result.items())\n        formatted_results.append(formatted_result)\n    return \"\\n\\n\".join(formatted_results)\n</code></pre>"},{"location":"dynamiq/nodes/tools/tavily/","title":"Tavily","text":""},{"location":"dynamiq/nodes/tools/tavily/#dynamiq.nodes.tools.tavily.TavilyTool","title":"<code>TavilyTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>TavilyTool is a ConnectionNode that interfaces with the Tavily search service.</p> <p>All parameters can be set during initialization and optionally overridden during execution.</p> Source code in <code>dynamiq/nodes/tools/tavily.py</code> <pre><code>class TavilyTool(ConnectionNode):\n    \"\"\"\n    TavilyTool is a ConnectionNode that interfaces with the Tavily search service.\n\n    All parameters can be set during initialization and optionally overridden during execution.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SEARCH\n    name: str = \"Tavily Search Tool\"\n    description: str = DESCRIPTION_TAVILY\n    connection: Tavily\n\n    auto_parameters: bool = Field(\n        default=False,\n        description=\"Automatically let Tavily tune parameters (costs 2 credits per request).\",\n    )\n    search_depth: Literal[\"basic\", \"advanced\"] = Field(\n        default=\"basic\",\n        description=\"`basic` for faster results, `advanced` for higher recall and chunk snippets.\",\n    )\n    topic: Literal[\"general\", \"news\", \"finance\"] = Field(\n        default=\"general\",\n        description=\"Content focus for the search query.\",\n    )\n    max_results: int = Field(\n        default=5,\n        ge=1,\n        le=20,\n        description=\"Maximum number of search results to return.\",\n    )\n    chunks_per_source: int | None = Field(\n        default=3,\n        ge=1,\n        le=3,\n        description=\"The number of chunks to return per source (default: 3, range: 1-3).\",\n    )\n    time_range: Literal[\"day\", \"week\", \"month\", \"year\", \"d\", \"w\", \"m\", \"y\"] | None = Field(\n        default=None,\n        description=\"The time range back from the current date to filter results. \"\n        \"Useful when looking for sources that have published data. \"\n        \"Available options are: `day`, `week`, `month`, `year`, `d`, `w`, `m`, `y`.\",\n    )\n    start_date: date | None = Field(default=None, description=\"Only return results published on/after this date.\")\n    end_date: date | None = Field(default=None, description=\"Only return results published on/before this date.\")\n    include_images: bool = Field(default=False, description=\"Include images in search results.\")\n    include_image_descriptions: bool = Field(\n        default=False,\n        description=\"Include short descriptions for each returned image (requires include_images).\",\n    )\n    include_favicon: bool = Field(default=False, description=\"Include favicon URLs for each search result.\")\n    include_answer: bool | Literal[\"basic\", \"advanced\"] = Field(\n        default=False,\n        description=\"Include an LLM-generated summary (`basic` or `advanced`).\",\n    )\n    include_raw_content: bool | Literal[\"markdown\", \"text\"] = Field(\n        default=False,\n        description=\"Include cleaned page content (`markdown` or `text`).\",\n    )\n    include_domains: list[str] = Field(default_factory=list, description=\"The domains to include in search results.\")\n    exclude_domains: list[str] = Field(default_factory=list, description=\"The domains to exclude from search results.\")\n    country: str | None = Field(\n        default=None,\n        description=\"Boost results originating from the provided country (topic must be `general`).\",\n    )\n    use_cache: bool = Field(\n        default=True,\n        description=\"Use cache for search results.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[TavilyInputSchema]] = TavilyInputSchema\n\n    def _format_search_results(self, results: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Formats the search results into a readable string format.\n\n        Args:\n            results (dict[str, Any]): The raw search results from Tavily.\n\n        Returns:\n            str: The formatted search results as a string.\n        \"\"\"\n        formatted_results = []\n        for result in results.get(\"results\", []):\n            formatted_results.append(f\"Source: {result.get('url')}\")\n            formatted_results.append(f\"Title: {result.get('title')}\")\n            formatted_results.append(f\"Content: {result.get('content')}\")\n            if result.get(\"raw_content\"):\n                formatted_results.append(f\"Full Content: {result.get('raw_content')}\")\n            if result.get(\"favicon\"):\n                formatted_results.append(f\"Favicon: {result.get('favicon')}\")\n            formatted_results.append(f\"Relevance Score: {result.get('score')}\")\n            formatted_results.append(\"\")  # Blank line between results\n\n        return \"\\n\".join(formatted_results).strip()\n\n    def execute(self, input_data: TavilyInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the search operation using the provided input data.\n        Parameters from input_data override the node's default parameters if provided.\n\n        Args:\n            input_data (TavilyInputSchema): The input data containing the search query and optional parameters.\n            config (RunnableConfig | None): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the search operation.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        search_data = {\n            \"query\": input_data.query,\n            \"auto_parameters\": self.auto_parameters,\n            \"search_depth\": self.search_depth,\n            \"topic\": self.topic,\n            \"max_results\": self.max_results,\n            \"include_images\": self.include_images,\n            \"include_image_descriptions\": self.include_image_descriptions,\n            \"include_favicon\": self.include_favicon,\n            \"include_answer\": self.include_answer,\n            \"include_raw_content\": self.include_raw_content,\n            \"include_domains\": self.include_domains,\n            \"exclude_domains\": self.exclude_domains,\n            \"country\": self.country,\n            \"use_cache\": self.use_cache,\n            \"chunks_per_source\": self.chunks_per_source,\n            \"time_range\": self.time_range,\n            \"start_date\": self.start_date,\n            \"end_date\": self.end_date,\n        }\n\n        input_dict = input_data.model_dump(exclude_unset=True)\n        for key, value in input_dict.items():\n            if value is not None:\n                search_data[key] = value\n\n        date_fields = (\"start_date\", \"end_date\")\n        for date_field in date_fields:\n            if isinstance(search_data.get(date_field), date):\n                search_data[date_field] = search_data[date_field].isoformat()\n\n        if search_data.get(\"search_depth\") != \"advanced\":\n            search_data.pop(\"chunks_per_source\", None)\n\n        search_data = {key: value for key, value in search_data.items() if value is not None}\n        request_payload = dict(search_data)\n\n        connection_url = urljoin(self.connection.url, \"/search\")\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json={**self.connection.data, **request_payload},\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to retrieve search results. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        formatted_results = self._format_search_results(search_result)\n        sources_with_url = []\n        for result in search_result.get(\"results\", []):\n            title, url = result.get(\"title\"), result.get(\"url\")\n            if title and url:\n                entry = f\"[{title}]({url})\"\n            else:\n                entry = url or title or \"\"\n            if result.get(\"favicon\"):\n                entry += f\" (favicon: {result.get('favicon')})\"\n            if entry:\n                sources_with_url.append(entry)\n\n        images_info = []\n        for image in search_result.get(\"images\", []):\n            url = image.get(\"url\")\n            if not url:\n                continue\n            description = image.get(\"description\") or \"Image\"\n            images_info.append(f\"- {description}: {url}\")\n\n        metadata_lines: list[str] = []\n        if search_result.get(\"response_time\") is not None:\n            metadata_lines.append(f\"- Response Time: {search_result.get('response_time')}s\")\n        if search_result.get(\"request_id\"):\n            metadata_lines.append(f\"- Request ID: {search_result.get('request_id')}\")\n        auto_parameters = search_result.get(\"auto_parameters\")\n        if auto_parameters:\n            auto_params_str = \", \".join(f\"{key}={value}\" for key, value in auto_parameters.items())\n            metadata_lines.append(f\"- Auto Parameters: {auto_params_str}\")\n\n        if self.is_optimized_for_agents:\n            result = (\n                \"## Sources with URLs\\n\"\n                + \"\\n\".join([f\"- {source}\" for source in sources_with_url])\n                + \"\\n\\n## Search results for: \"\n                + f\"'{search_result.get('query', input_data.query)}'\\n\\n\"\n                + formatted_results\n            )\n            if search_result.get(\"answer\", \"\"):\n                result += f\"\\n\\n## Summary Answer\\n\\n{search_result.get('answer')}\"\n            if images_info:\n                result += \"\\n\\n## Images\\n\" + \"\\n\".join(images_info)\n            if metadata_lines:\n                result += \"\\n\\n## Metadata\\n\" + \"\\n\".join(metadata_lines)\n            if request_payload:\n                result += \"\\n\\n## Request Parameters\\n```json\\n\" + json.dumps(request_payload, indent=2) + \"\\n```\"\n            result += \"\\n\\n## Raw Response\\n```json\\n\" + json.dumps(search_result, indent=2) + \"\\n```\"\n        else:\n            result = {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"raw_response\": search_result,\n                \"images\": search_result.get(\"images\", []),\n                \"answer\": search_result.get(\"answer\", \"\"),\n                \"query\": search_result.get(\"query\", \"\"),\n                \"response_time\": search_result.get(\"response_time\", 0),\n                \"request_id\": search_result.get(\"request_id\"),\n                \"auto_parameters\": auto_parameters or {},\n                \"search_parameters\": request_payload,\n            }\n\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/tavily/#dynamiq.nodes.tools.tavily.TavilyTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the search operation using the provided input data. Parameters from input_data override the node's default parameters if provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TavilyInputSchema</code> <p>The input data containing the search query and optional parameters.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The result of the search operation.</p> Source code in <code>dynamiq/nodes/tools/tavily.py</code> <pre><code>def execute(self, input_data: TavilyInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the search operation using the provided input data.\n    Parameters from input_data override the node's default parameters if provided.\n\n    Args:\n        input_data (TavilyInputSchema): The input data containing the search query and optional parameters.\n        config (RunnableConfig | None): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the search operation.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    search_data = {\n        \"query\": input_data.query,\n        \"auto_parameters\": self.auto_parameters,\n        \"search_depth\": self.search_depth,\n        \"topic\": self.topic,\n        \"max_results\": self.max_results,\n        \"include_images\": self.include_images,\n        \"include_image_descriptions\": self.include_image_descriptions,\n        \"include_favicon\": self.include_favicon,\n        \"include_answer\": self.include_answer,\n        \"include_raw_content\": self.include_raw_content,\n        \"include_domains\": self.include_domains,\n        \"exclude_domains\": self.exclude_domains,\n        \"country\": self.country,\n        \"use_cache\": self.use_cache,\n        \"chunks_per_source\": self.chunks_per_source,\n        \"time_range\": self.time_range,\n        \"start_date\": self.start_date,\n        \"end_date\": self.end_date,\n    }\n\n    input_dict = input_data.model_dump(exclude_unset=True)\n    for key, value in input_dict.items():\n        if value is not None:\n            search_data[key] = value\n\n    date_fields = (\"start_date\", \"end_date\")\n    for date_field in date_fields:\n        if isinstance(search_data.get(date_field), date):\n            search_data[date_field] = search_data[date_field].isoformat()\n\n    if search_data.get(\"search_depth\") != \"advanced\":\n        search_data.pop(\"chunks_per_source\", None)\n\n    search_data = {key: value for key, value in search_data.items() if value is not None}\n    request_payload = dict(search_data)\n\n    connection_url = urljoin(self.connection.url, \"/search\")\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json={**self.connection.data, **request_payload},\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to retrieve search results. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    formatted_results = self._format_search_results(search_result)\n    sources_with_url = []\n    for result in search_result.get(\"results\", []):\n        title, url = result.get(\"title\"), result.get(\"url\")\n        if title and url:\n            entry = f\"[{title}]({url})\"\n        else:\n            entry = url or title or \"\"\n        if result.get(\"favicon\"):\n            entry += f\" (favicon: {result.get('favicon')})\"\n        if entry:\n            sources_with_url.append(entry)\n\n    images_info = []\n    for image in search_result.get(\"images\", []):\n        url = image.get(\"url\")\n        if not url:\n            continue\n        description = image.get(\"description\") or \"Image\"\n        images_info.append(f\"- {description}: {url}\")\n\n    metadata_lines: list[str] = []\n    if search_result.get(\"response_time\") is not None:\n        metadata_lines.append(f\"- Response Time: {search_result.get('response_time')}s\")\n    if search_result.get(\"request_id\"):\n        metadata_lines.append(f\"- Request ID: {search_result.get('request_id')}\")\n    auto_parameters = search_result.get(\"auto_parameters\")\n    if auto_parameters:\n        auto_params_str = \", \".join(f\"{key}={value}\" for key, value in auto_parameters.items())\n        metadata_lines.append(f\"- Auto Parameters: {auto_params_str}\")\n\n    if self.is_optimized_for_agents:\n        result = (\n            \"## Sources with URLs\\n\"\n            + \"\\n\".join([f\"- {source}\" for source in sources_with_url])\n            + \"\\n\\n## Search results for: \"\n            + f\"'{search_result.get('query', input_data.query)}'\\n\\n\"\n            + formatted_results\n        )\n        if search_result.get(\"answer\", \"\"):\n            result += f\"\\n\\n## Summary Answer\\n\\n{search_result.get('answer')}\"\n        if images_info:\n            result += \"\\n\\n## Images\\n\" + \"\\n\".join(images_info)\n        if metadata_lines:\n            result += \"\\n\\n## Metadata\\n\" + \"\\n\".join(metadata_lines)\n        if request_payload:\n            result += \"\\n\\n## Request Parameters\\n```json\\n\" + json.dumps(request_payload, indent=2) + \"\\n```\"\n        result += \"\\n\\n## Raw Response\\n```json\\n\" + json.dumps(search_result, indent=2) + \"\\n```\"\n    else:\n        result = {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"raw_response\": search_result,\n            \"images\": search_result.get(\"images\", []),\n            \"answer\": search_result.get(\"answer\", \"\"),\n            \"query\": search_result.get(\"query\", \"\"),\n            \"response_time\": search_result.get(\"response_time\", 0),\n            \"request_id\": search_result.get(\"request_id\"),\n            \"auto_parameters\": auto_parameters or {},\n            \"search_parameters\": request_payload,\n        }\n\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/","title":"Thinking tool","text":""},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool","title":"<code>ThinkingTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for structured thinking and reasoning processes.</p> <p>This tool helps agents process thoughts in a structured way, providing a cognitive scratchpad for complex reasoning, planning, and analysis. The agent's LLM will be used automatically when this tool is called by an agent.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A description of the tool's functionality.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM to use for processing thoughts.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Configuration for error handling.</p> <code>prompt_template</code> <code>str</code> <p>The prompt template used for thinking processes.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>class ThinkingTool(Node):\n    \"\"\"\n    A tool for structured thinking and reasoning processes.\n\n    This tool helps agents process thoughts in a structured way, providing a cognitive\n    scratchpad for complex reasoning, planning, and analysis. The agent's LLM will be\n    used automatically when this tool is called by an agent.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group this node belongs to.\n        name (str): The name of the tool.\n        description (str): A description of the tool's functionality.\n        llm (BaseLLM): The LLM to use for processing thoughts.\n        error_handling (ErrorHandling): Configuration for error handling.\n        prompt_template (str): The prompt template used for thinking processes.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Thinking Tool\"\n    description: str = \"\"\"Analyzes thoughts and reasoning processes to improve decision-making clarity and identify gaps in logic.\n\nKey Capabilities:\n- Structured analysis of complex thoughts and problems\n- Component breakdown with insights and next steps\n- Context-aware analysis with optional memory support\n- Sequential reasoning validation and action planning\n\nUsage Strategy:\n- Use before making important decisions or actions\n- Analyze complex multi-step problems systematically\n- Validate reasoning logic and identify assumptions\n- Plan next steps in complex workflows with clarity\n\nParameter Guide:\n- thought: The idea, reasoning, or problem to analyze (required)\n- context: Background information or constraints\n- focus: Analysis area (planning, problem-solving, decision-making)\n- memory_enabled: Maintain history of previous thoughts\n\nExamples:\n- {\"thought\": \"Should we implement feature X?\", \"focus\": \"decision-making\"}\n- {\"thought\": \"API integration failed with 401 error\", \"context\": \"OAuth2 auth\"}\n- {\"thought\": \"Choose database solutions\", \"context\": \"100k users, scaling\"}\"\"\"  # noqa E501\n\n    llm: BaseLLM = Field(..., description=\"LLM to use for thinking processes\")\n\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n\n    prompt_template: str = Field(\n        default=THINKING_PROMPT_TEMPLATE, description=\"The prompt template for the thinking process\"\n    )\n\n    memory_enabled: bool = Field(\n        default=False, description=\"Whether to maintain memory of previous thoughts in this session\"\n    )\n    max_thoughts_in_memory: int = Field(\n        default=3,\n        description=\"Number of recent thoughts to keep in memory when memory is enabled\",\n    )\n    max_thought_chars: int = Field(\n        default=300,\n        description=\"Maximum characters of each thought to display in memory when memory is enabled\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[ThinkingInputSchema]] = ThinkingInputSchema\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._thought_history: list[dict] = []\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"Initialize the components of the tool.\"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def reset_run_state(self):\n        \"\"\"Reset the intermediate steps (run_depends) of the node.\"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        \"\"\"Property to define which parameters should be excluded when converting to dictionary.\"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert the tool to a dictionary representation.\"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def _build_context_section(self, context: str, focus: str) -&gt; str:\n        \"\"\"Build the context section for the prompt.\"\"\"\n        sections = []\n\n        if context:\n            sections.append(f\"Additional context:\\n{context}\")\n\n        if focus and focus != \"general\":\n            sections.append(f\"Focus area: {focus}\")\n\n        if self.memory_enabled and self._thought_history:\n            recent_thoughts = self._thought_history[-self.max_thoughts_in_memory :]\n            history_text = \"\\n\".join(\n                [\n                    f\"- {i + 1}. {thought['thought'][:self.max_thought_chars]}{'...' if len(thought['thought']) &gt; self.max_thought_chars else ''}\"  # noqa E501\n                    for i, thought in enumerate(recent_thoughts)\n                ]\n            )\n            sections.append(f\"Recent thinking history:\\n{history_text}\")\n\n        return \"\\n\\n\".join(sections) if sections else \"\"\n\n    def execute(\n        self, input_data: ThinkingInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the thinking tool on the input data.\n\n        This method processes a thought through structured analysis, helping to clarify,\n        organize, and develop the reasoning around the given input.\n\n        Args:\n            input_data (ThinkingInputSchema): Input containing thought, context, and focus\n            config (RunnableConfig, optional): The configuration for running the tool\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            dict[str, Any]: A dictionary containing the analysis, original thought, and metadata\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        thought = input_data.thought\n        context = input_data.context\n        focus = input_data.focus\n\n        logger.debug(f\"Tool {self.name} - {self.id}: started thinking process for thought: '{thought[:100]}...'\")\n\n        context_section = self._build_context_section(context, focus)\n\n        prompt_content = self.prompt_template.format(thought=thought, context_section=context_section)\n\n        logger.debug(f\"Tool {self.name} - {self.id}: prompt content:\\n{prompt_content}\")\n\n        result = self.llm.run(\n            input_data={},\n            prompt=Prompt(messages=[Message(role=\"user\", content=prompt_content, static=True)]),\n            config=config,\n            **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}),\n        )\n\n        logger.debug(f\"Tool {self.name} - {self.id}: result status: {result.output}\")\n\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n        if result.status != RunnableStatus.SUCCESS:\n            raise ValueError(\"LLM execution failed during thinking process\")\n\n        analysis = result.output[\"content\"]\n\n        if self.memory_enabled:\n            self._thought_history.append(\n                {\n                    \"thought\": thought,\n                    \"context\": context,\n                    \"focus\": focus,\n                    \"analysis\": analysis,\n                    \"timestamp\": kwargs.get(\"run_id\", \"unknown\"),\n                }\n            )\n\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: completed thinking process, \" f\"analysis length: {len(analysis)} characters\"\n        )\n\n        return {\n            \"content\": analysis,\n            \"original_thought\": thought,\n            \"context_used\": context,\n            \"focus_area\": focus,\n            \"thinking_session_count\": len(self._thought_history) if self.memory_enabled else None,\n        }\n\n    def clear_memory(self) -&gt; None:\n        \"\"\"Clear the thinking history memory.\"\"\"\n        self._thought_history.clear()\n        logger.debug(f\"Tool {self.name} - {self.id}: cleared thinking history memory\")\n\n    def get_thought_history(self) -&gt; list[dict]:\n        \"\"\"Get the current thought history.\"\"\"\n        return deepcopy(self._thought_history) if self.memory_enabled else []\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting to dictionary.</p>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.clear_memory","title":"<code>clear_memory()</code>","text":"<p>Clear the thinking history memory.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def clear_memory(self) -&gt; None:\n    \"\"\"Clear the thinking history memory.\"\"\"\n    self._thought_history.clear()\n    logger.debug(f\"Tool {self.name} - {self.id}: cleared thinking history memory\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the thinking tool on the input data.</p> <p>This method processes a thought through structured analysis, helping to clarify, organize, and develop the reasoning around the given input.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ThinkingInputSchema</code> <p>Input containing thought, context, and focus</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for running the tool</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the analysis, original thought, and metadata</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def execute(\n    self, input_data: ThinkingInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the thinking tool on the input data.\n\n    This method processes a thought through structured analysis, helping to clarify,\n    organize, and develop the reasoning around the given input.\n\n    Args:\n        input_data (ThinkingInputSchema): Input containing thought, context, and focus\n        config (RunnableConfig, optional): The configuration for running the tool\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        dict[str, Any]: A dictionary containing the analysis, original thought, and metadata\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    thought = input_data.thought\n    context = input_data.context\n    focus = input_data.focus\n\n    logger.debug(f\"Tool {self.name} - {self.id}: started thinking process for thought: '{thought[:100]}...'\")\n\n    context_section = self._build_context_section(context, focus)\n\n    prompt_content = self.prompt_template.format(thought=thought, context_section=context_section)\n\n    logger.debug(f\"Tool {self.name} - {self.id}: prompt content:\\n{prompt_content}\")\n\n    result = self.llm.run(\n        input_data={},\n        prompt=Prompt(messages=[Message(role=\"user\", content=prompt_content, static=True)]),\n        config=config,\n        **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}),\n    )\n\n    logger.debug(f\"Tool {self.name} - {self.id}: result status: {result.output}\")\n\n    self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n    if result.status != RunnableStatus.SUCCESS:\n        raise ValueError(\"LLM execution failed during thinking process\")\n\n    analysis = result.output[\"content\"]\n\n    if self.memory_enabled:\n        self._thought_history.append(\n            {\n                \"thought\": thought,\n                \"context\": context,\n                \"focus\": focus,\n                \"analysis\": analysis,\n                \"timestamp\": kwargs.get(\"run_id\", \"unknown\"),\n            }\n        )\n\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: completed thinking process, \" f\"analysis length: {len(analysis)} characters\"\n    )\n\n    return {\n        \"content\": analysis,\n        \"original_thought\": thought,\n        \"context_used\": context,\n        \"focus_area\": focus,\n        \"thinking_session_count\": len(self._thought_history) if self.memory_enabled else None,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.get_thought_history","title":"<code>get_thought_history()</code>","text":"<p>Get the current thought history.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def get_thought_history(self) -&gt; list[dict]:\n    \"\"\"Get the current thought history.\"\"\"\n    return deepcopy(self._thought_history) if self.memory_enabled else []\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the tool.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"Initialize the components of the tool.\"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Reset the intermediate steps (run_depends) of the node.\"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/tools/thinking_tool/#dynamiq.nodes.tools.thinking_tool.ThinkingTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the tool to a dictionary representation.</p> Source code in <code>dynamiq/nodes/tools/thinking_tool.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert the tool to a dictionary representation.\"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/todo_tools/","title":"Todo tools","text":"<p>Todo Management Tools for Agents</p>"},{"location":"dynamiq/nodes/tools/todo_tools/#dynamiq.nodes.tools.todo_tools.TodoItem","title":"<code>TodoItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single todo item.</p> Source code in <code>dynamiq/nodes/tools/todo_tools.py</code> <pre><code>class TodoItem(BaseModel):\n    \"\"\"A single todo item.\"\"\"\n\n    id: str\n    content: str\n    status: TodoStatus = TodoStatus.PENDING\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    def to_display_string(self) -&gt; str:\n        \"\"\"Format todo item for display with status icon.\"\"\"\n        icon = {\n            TodoStatus.PENDING: \"[ ]\",\n            TodoStatus.IN_PROGRESS: \"[~]\",\n            TodoStatus.COMPLETED: \"[+]\",\n        }.get(self.status, \"[ ]\")\n        return f\"{icon} {self.id}: {self.content}\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/todo_tools/#dynamiq.nodes.tools.todo_tools.TodoItem.to_display_string","title":"<code>to_display_string()</code>","text":"<p>Format todo item for display with status icon.</p> Source code in <code>dynamiq/nodes/tools/todo_tools.py</code> <pre><code>def to_display_string(self) -&gt; str:\n    \"\"\"Format todo item for display with status icon.\"\"\"\n    icon = {\n        TodoStatus.PENDING: \"[ ]\",\n        TodoStatus.IN_PROGRESS: \"[~]\",\n        TodoStatus.COMPLETED: \"[+]\",\n    }.get(self.status, \"[ ]\")\n    return f\"{icon} {self.id}: {self.content}\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/todo_tools/#dynamiq.nodes.tools.todo_tools.TodoStatus","title":"<code>TodoStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a todo item.</p> Source code in <code>dynamiq/nodes/tools/todo_tools.py</code> <pre><code>class TodoStatus(str, Enum):\n    \"\"\"Status of a todo item.\"\"\"\n\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/todo_tools/#dynamiq.nodes.tools.todo_tools.TodoWriteInputSchema","title":"<code>TodoWriteInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for writing todos.</p> Source code in <code>dynamiq/nodes/tools/todo_tools.py</code> <pre><code>class TodoWriteInputSchema(BaseModel):\n    \"\"\"Input schema for writing todos.\"\"\"\n\n    todos: list[TodoItem] = Field(\n        ...,\n        description=(\n            \"List of todo items. Each item MUST have 'id', 'content', and 'status'. \"\n            \"When updating (merge=true), content is required but ignored \u2014 the original content is preserved.\"\n        ),\n    )\n    merge: bool = Field(\n        default=True,\n        description=\"If true, update status of existing todos by id (content you send is ignored, \"\n        \"original is preserved). \"\n        \"If false, replace all todos with the provided list.\",\n    )\n</code></pre>"},{"location":"dynamiq/nodes/tools/todo_tools/#dynamiq.nodes.tools.todo_tools.TodoWriteTool","title":"<code>TodoWriteTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>Write/update the todo list in storage.</p> <p>Saves the provided list of todos, either merging with existing or replacing all.</p> Source code in <code>dynamiq/nodes/tools/todo_tools.py</code> <pre><code>class TodoWriteTool(Node):\n    \"\"\"\n    Write/update the todo list in storage.\n\n    Saves the provided list of todos, either merging with existing or replacing all.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"todo-write\"\n    description: str = \"\"\"Save or update the todo list. Every item requires 'id', 'content', and 'status'.\n\nTwo modes:\n\nCREATE (merge=false): Build the full todo list.\n  {\"todos\": [{\"id\": \"1\", \"content\": \"Implement auth\", \"status\": \"in_progress\"},\n  {\"id\": \"2\", \"content\": \"Add tests\", \"status\": \"pending\"}], \"merge\": false}\n\nUPDATE (merge=true, default): Change status only. Content is required but ignored \u2014 the original content is preserved.\n  {\"todos\": [{\"id\": \"1\", \"content\": \"ignored\", \"status\": \"completed\"},\n  {\"id\": \"2\", \"content\": \"ignored\", \"status\": \"in_progress\"}], \"merge\": true}\n\nRULES:\n- Use merge=false ONLY for initial list creation. First task should be \"in_progress\", rest \"pending\".\n- Use merge=true for ALL subsequent updates \u2014 only status is applied, content stays unchanged.\n- Do NOT restructure, reword, or reorder todos when updating status.\n\"\"\"\n\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=30))\n    file_store: FileStore | Sandbox = Field(..., description=\"File storage for todos\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[TodoWriteInputSchema]] = TodoWriteInputSchema\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n\n    def reset_run_state(self):\n        self._run_depends = []\n\n    def _load_todos(self) -&gt; list[dict]:\n        \"\"\"Load todos from file store.\"\"\"\n        try:\n            if self.file_store.exists(TODOS_FILE_PATH):\n                content = self.file_store.retrieve(TODOS_FILE_PATH)\n                data = json.loads(content.decode(\"utf-8\"))\n                todos = data.get(\"todos\")\n                if not isinstance(todos, list):\n                    logger.warning(f\"TodoWriteTool: Invalid todos format (expected list, got {type(todos).__name__})\")\n                    return []\n                validated = []\n                for t in todos:\n                    try:\n                        validated.append(TodoItem.model_validate(t).model_dump())\n                    except Exception as e:\n                        logger.warning(f\"TodoWriteTool: Skipping invalid todo item: {e}\")\n                return validated\n        except Exception as e:\n            logger.warning(f\"TodoWriteTool: Failed to load todos: {e}\")\n        return []\n\n    def _save_todos(self, todos: list[dict]) -&gt; None:\n        \"\"\"Save todos to file store or sandbox.\"\"\"\n        content = json.dumps({\"todos\": todos}, indent=2)\n        if isinstance(self.file_store, Sandbox):\n            self.file_store.upload_file(\n                TODOS_FILE_PATH,\n                content.encode(\"utf-8\"),\n            )\n        else:\n            self.file_store.store(\n                file_path=TODOS_FILE_PATH,\n                content=content,\n                content_type=\"application/json\",\n                overwrite=True,\n            )\n\n    def execute(\n        self, input_data: TodoWriteInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        # Convert TodoItem objects to dicts for storage\n        new_todos = [todo.model_dump() for todo in input_data.todos]\n\n        if input_data.merge:\n            existing = self._load_todos()\n            existing_by_id = {t.get(\"id\"): t for t in existing if t.get(\"id\")}\n\n            unknown_ids = [t[\"id\"] for t in new_todos if t[\"id\"] not in existing_by_id]\n            if unknown_ids:\n                raise ToolExecutionException(\n                    f\"Todo ids not found: {unknown_ids}. Existing ids: {list(existing_by_id.keys())}\",\n                    recoverable=True,\n                )\n\n            for todo in new_todos:\n                existing_by_id[todo[\"id\"]][\"status\"] = todo[\"status\"]\n\n            final_todos = list(existing_by_id.values())\n        else:\n            # Replace all\n            final_todos = new_todos\n\n        self._save_todos(final_todos)\n\n        # Calculate stats\n        stats = {\n            \"total\": len(final_todos),\n            TodoStatus.PENDING.value: sum(1 for t in final_todos if t.get(\"status\") == TodoStatus.PENDING.value),\n            TodoStatus.IN_PROGRESS.value: sum(\n                1 for t in final_todos if t.get(\"status\") == TodoStatus.IN_PROGRESS.value\n            ),\n            TodoStatus.COMPLETED.value: sum(1 for t in final_todos if t.get(\"status\") == TodoStatus.COMPLETED.value),\n        }\n\n        status_icons = {\n            TodoStatus.PENDING.value: \"\u23f3\",\n            TodoStatus.IN_PROGRESS.value: \"\ud83d\udd04\",\n            TodoStatus.COMPLETED.value: \"\u2705\",\n        }\n\n        lines = [\"\u2705 Todos saved successfully!\"]\n        lines.append(\"\")\n        lines.append(\"\ud83d\udccb Current Todo List:\")\n        for t in final_todos:\n            icon = status_icons.get(t.get(\"status\", \"\"), \"\u2753\")\n            lines.append(f\"  {icon} [{t.get('id')}] {t.get('content')} ({t.get('status')})\")\n\n        lines.append(\"\")\n        pending = TodoStatus.PENDING.value\n        in_progress = TodoStatus.IN_PROGRESS.value\n        completed = TodoStatus.COMPLETED.value\n        lines.append(\n            f\"\ud83d\udcca Stats: {stats['total']} total | \u23f3 {stats[pending]} pending |\"\n            f\" \ud83d\udd04 {stats[in_progress]} in progress | \u2705 {stats[completed]} completed\"\n        )\n\n        return {\"content\": \"\\n\".join(lines)}\n</code></pre>"},{"location":"dynamiq/nodes/tools/zenrows/","title":"Zenrows","text":""},{"location":"dynamiq/nodes/tools/zenrows/#dynamiq.nodes.tools.zenrows.ZenRowsTool","title":"<code>ZenRowsTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for scraping web pages, powered by ZenRows.</p> <p>This class is responsible for scraping the content of a web page using ZenRows.</p> Source code in <code>dynamiq/nodes/tools/zenrows.py</code> <pre><code>class ZenRowsTool(ConnectionNode):\n    \"\"\"\n    A tool for scraping web pages, powered by ZenRows.\n\n    This class is responsible for scraping the content of a web page using ZenRows.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    action_type: ActionType = ActionType.WEB_SCRAPE\n    name: str = \"Zenrows Scraper Tool\"\n    description: str = DESCRIPTION_ZENROWS\n    connection: ZenRows\n    url: str | None = None\n    markdown_response: bool = Field(\n        default=True,\n        description=\"If True, the content will be parsed as Markdown instead of HTML.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[ZenRowsInputSchema]] = ZenRowsInputSchema\n\n    def execute(self, input_data: ZenRowsInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the web scraping process.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing 'input' key with the URL to scrape.\n            config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n            kwargs: Additional arguments passed to the execution context.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the URL and the scraped content.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n        # Ensure the config is set up correctly\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        params = {\n            \"url\": input_data.url,\n            \"markdown_response\": str(self.markdown_response).lower(),\n        }\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=self.connection.url,\n                params={**self.connection.params, **params},\n            )\n            if response.status_code &gt;= 400:\n                error = response.json().get(\"detail\")\n                logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {error}\")\n                raise ToolExecutionException(\n                    f\"Tool '{self.name}' failed to execute the requested action. \"\n                    f\"Error: {error}. Please analyze the error and take appropriate action.\",\n                    recoverable=True,\n                )\n            scrape_result = response.text\n        except ToolExecutionException:\n            raise\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' encountered an unexpected error. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n        if self.is_optimized_for_agents:\n            result = f\"## Source URL\\n{input_data.url}\\n\\n## Scraped Result\\n\\n{scrape_result}\\n\"\n        else:\n            result = {\"url\": input_data.url, \"content\": scrape_result}\n        logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/zenrows/#dynamiq.nodes.tools.zenrows.ZenRowsTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the web scraping process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing 'input' key with the URL to scrape.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable, including callbacks.</p> <code>None</code> <code>kwargs</code> <p>Additional arguments passed to the execution context.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the URL and the scraped content.</p> Source code in <code>dynamiq/nodes/tools/zenrows.py</code> <pre><code>def execute(self, input_data: ZenRowsInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the web scraping process.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing 'input' key with the URL to scrape.\n        config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n        kwargs: Additional arguments passed to the execution context.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the URL and the scraped content.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: started with input:\\n{input_data.model_dump()}\")\n\n    # Ensure the config is set up correctly\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    params = {\n        \"url\": input_data.url,\n        \"markdown_response\": str(self.markdown_response).lower(),\n    }\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=self.connection.url,\n            params={**self.connection.params, **params},\n        )\n        if response.status_code &gt;= 400:\n            error = response.json().get(\"detail\")\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {error}\")\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to execute the requested action. \"\n                f\"Error: {error}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n        scrape_result = response.text\n    except ToolExecutionException:\n        raise\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: unexpected error occurred. Error: {str(e)}\")\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' encountered an unexpected error. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n\n    if self.is_optimized_for_agents:\n        result = f\"## Source URL\\n{input_data.url}\\n\\n## Scraped Result\\n\\n{scrape_result}\\n\"\n    else:\n        result = {\"url\": input_data.url, \"content\": scrape_result}\n    logger.info(f\"Tool {self.name} - {self.id}: finished with result:\\n{str(result)[:200]}...\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/utils/utils/","title":"Utils","text":""},{"location":"dynamiq/nodes/utils/utils/#dynamiq.nodes.utils.utils.Input","title":"<code>Input</code>","text":"<p>               Bases: <code>Pass</code></p> <p>A utility node representing the input of workflow.</p> <p>This class inherits from the Pass operator and is used to mark the beginning of a sequence of operations. It is typically used in workflow definitions or process models.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[UTILS]</code> <p>The group the node belongs to, set to UTILS.</p> <code>schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the input data.</p> Source code in <code>dynamiq/nodes/utils/utils.py</code> <pre><code>class Input(Pass):\n    \"\"\"\n    A utility node representing the input of workflow.\n\n    This class inherits from the Pass operator and is used to mark the beginning of a sequence of\n    operations. It is typically used in workflow definitions or process models.\n\n    Attributes:\n        group (Literal[NodeGroup.UTILS]): The group the node belongs to, set to UTILS.\n        schema (dict[str, Any] | None): The JSON schema for the input data.\n    \"\"\"\n\n    name: str | None = \"Start\"\n    group: Literal[NodeGroup.UTILS] = NodeGroup.UTILS\n    json_schema: dict[str, Any] | None = Field(\n        default=None,\n        alias=\"schema\",\n        description=\"\"\"Determines input parameters of workflow.\n        Provide it in the properties field format. Example:\n        \"properties\": {\n            \"query\": {\n                \"type\": \"Any\"\n            },\n            \"files\": {\n                \"type\": \"list[files]\"\n            }\n        }\n    \"\"\",\n    )\n    _json_schema_fields: ClassVar[list[str]] = [\"json_schema\"]\n</code></pre>"},{"location":"dynamiq/nodes/utils/utils/#dynamiq.nodes.utils.utils.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>Pass</code></p> <p>A utility node representing the output of workflow.</p> <p>This class inherits from the Pass operator and is used to mark the conclusion of a sequence of operations. It is typically used in workflow definitions or process models.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[UTILS]</code> <p>The group the node belongs to, set to UTILS.</p> <code>schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the output data.</p> Source code in <code>dynamiq/nodes/utils/utils.py</code> <pre><code>class Output(Pass):\n    \"\"\"\n    A utility node representing the output of workflow.\n\n    This class inherits from the Pass operator and is used to mark the conclusion of a sequence of\n    operations. It is typically used in workflow definitions or process models.\n\n    Attributes:\n        group (Literal[NodeGroup.UTILS]): The group the node belongs to, set to UTILS.\n        schema (dict[str, Any] | None): The JSON schema for the output data.\n    \"\"\"\n\n    name: str | None = \"End\"\n    group: Literal[NodeGroup.UTILS] = NodeGroup.UTILS\n    json_schema: dict[str, Any] | None = Field(\n        default=None,\n        alias=\"schema\",\n        description=\"\"\"Determines output parameters of workflow.\n        Provide it in the properties field format. Example:\n        \"properties\": {\n            \"query\": {\n                \"type\": \"Any\"\n            }\n        }\n    \"\"\",\n    )\n    _json_schema_fields: ClassVar[list[str]] = [\"json_schema\"]\n</code></pre>"},{"location":"dynamiq/nodes/validators/base/","title":"Base","text":""},{"location":"dynamiq/nodes/validators/base/#dynamiq.nodes.validators.base.BaseValidator","title":"<code>BaseValidator</code>","text":"<p>               Bases: <code>Node</code></p> Source code in <code>dynamiq/nodes/validators/base.py</code> <pre><code>class BaseValidator(Node):\n    group: Literal[NodeGroup.VALIDATORS] = NodeGroup.VALIDATORS\n    name: str | None = \"Validator\"\n    behavior: Behavior | None = Behavior.RETURN\n\n    input_schema: ClassVar[type[ValidatorInputSchema]] = ValidatorInputSchema\n\n    def execute(self, input_data: ValidatorInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"Executes the validation process for a given value.\n\n        Args:\n            input_data (ValidatorInputSchema): The input data containing the value to check.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            input_data: A dictionary with the following key if behavior is return:\n                - \"valid\" (bool): boolean indicating if the value is valid.\n                - \"content\" (Any): passed value if everything is correct.\n            bool\n\n        Raises:\n            ValueError: If the value is not valid and behavior equal raise type.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        try:\n            self.validate(input_data.content)\n        except Exception as error:\n            if self.behavior == Behavior.RETURN:\n                return {\"valid\": False, \"content\": input_data.content}\n            raise ValueError(str(error))\n        return {\"valid\": True, \"content\": input_data.content}\n\n    @abstractmethod\n    def validate(self, content):\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/validators/base/#dynamiq.nodes.validators.base.BaseValidator.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the validation process for a given value.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ValidatorInputSchema</code> <p>The input data containing the value to check.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>input_data</code> <p>A dictionary with the following key if behavior is return: - \"valid\" (bool): boolean indicating if the value is valid. - \"content\" (Any): passed value if everything is correct.</p> <p>bool</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not valid and behavior equal raise type.</p> Source code in <code>dynamiq/nodes/validators/base.py</code> <pre><code>def execute(self, input_data: ValidatorInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"Executes the validation process for a given value.\n\n    Args:\n        input_data (ValidatorInputSchema): The input data containing the value to check.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        input_data: A dictionary with the following key if behavior is return:\n            - \"valid\" (bool): boolean indicating if the value is valid.\n            - \"content\" (Any): passed value if everything is correct.\n        bool\n\n    Raises:\n        ValueError: If the value is not valid and behavior equal raise type.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    try:\n        self.validate(input_data.content)\n    except Exception as error:\n        if self.behavior == Behavior.RETURN:\n            return {\"valid\": False, \"content\": input_data.content}\n        raise ValueError(str(error))\n    return {\"valid\": True, \"content\": input_data.content}\n</code></pre>"},{"location":"dynamiq/nodes/validators/regex_match/","title":"Regex match","text":""},{"location":"dynamiq/nodes/validators/regex_match/#dynamiq.nodes.validators.regex_match.RegexMatch","title":"<code>RegexMatch</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Validates that a value matches a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <p>A regular expression pattern.</p> required <code>match_type</code> <p>Match type to check input value for a regex search or full-match option.</p> required Source code in <code>dynamiq/nodes/validators/regex_match.py</code> <pre><code>class RegexMatch(BaseValidator):\n    \"\"\"\n    Validates that a value matches a regular expression.\n\n    Args:\n        regex: A regular expression pattern.\n        match_type: Match type to check input value for a regex search or full-match option.\n    \"\"\"\n\n    regex: str\n    match_type: MatchType | None = MatchType.FULL_MATCH\n\n    def validate(self, content: str):\n        \"\"\"\n        Validates if the provided value matches the given regular expression pattern.\n\n        Args:\n            content (str): The value to validate.\n\n        Raises:\n            ValueError: If the provided value does not match the given pattern.\n        \"\"\"\n        compiled_pattern = re.compile(self.regex)\n        match_method = getattr(compiled_pattern, self.match_type)\n        if not match_method(content):\n            raise ValueError(\n                f\"Value does not match the valid pattern. Value: '{content}'. Pattern: '{self.regex}'\",\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/regex_match/#dynamiq.nodes.validators.regex_match.RegexMatch.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided value matches the given regular expression pattern.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided value does not match the given pattern.</p> Source code in <code>dynamiq/nodes/validators/regex_match.py</code> <pre><code>def validate(self, content: str):\n    \"\"\"\n    Validates if the provided value matches the given regular expression pattern.\n\n    Args:\n        content (str): The value to validate.\n\n    Raises:\n        ValueError: If the provided value does not match the given pattern.\n    \"\"\"\n    compiled_pattern = re.compile(self.regex)\n    match_method = getattr(compiled_pattern, self.match_type)\n    if not match_method(content):\n        raise ValueError(\n            f\"Value does not match the valid pattern. Value: '{content}'. Pattern: '{self.regex}'\",\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_choices/","title":"Valid choices","text":""},{"location":"dynamiq/nodes/validators/valid_choices/#dynamiq.nodes.validators.valid_choices.ValidChoices","title":"<code>ValidChoices</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if the provided value is within the list of valid choices.</p> <p>Parameters:</p> Name Type Description Default <code>choices(List[Any])</code> <p>A list of values representing the acceptable choices.</p> required Source code in <code>dynamiq/nodes/validators/valid_choices.py</code> <pre><code>class ValidChoices(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if the provided value is within the list of valid choices.\n\n    Args:\n        choices(List[Any]): A list of values representing the acceptable choices.\n\n    \"\"\"\n\n    choices: list[Any] = None\n\n    def validate(self, content: Any):\n        \"\"\"\n        Validates if the provided value is among the acceptable choices.\n\n        Args:\n            content(Any): The value to validate.\n\n        Raises:\n            ValueError: If the provided value is not in valid choices.\n        \"\"\"\n        if isinstance(content, str):\n            content = content.strip()\n\n        if content not in self.choices:\n            raise ValueError(\n                f\"Value is not in valid choices. Value: '{content}'. Choices: '{self.choices}'.\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_choices/#dynamiq.nodes.validators.valid_choices.ValidChoices.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided value is among the acceptable choices.</p> <p>Parameters:</p> Name Type Description Default <code>content(Any)</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided value is not in valid choices.</p> Source code in <code>dynamiq/nodes/validators/valid_choices.py</code> <pre><code>def validate(self, content: Any):\n    \"\"\"\n    Validates if the provided value is among the acceptable choices.\n\n    Args:\n        content(Any): The value to validate.\n\n    Raises:\n        ValueError: If the provided value is not in valid choices.\n    \"\"\"\n    if isinstance(content, str):\n        content = content.strip()\n\n    if content not in self.choices:\n        raise ValueError(\n            f\"Value is not in valid choices. Value: '{content}'. Choices: '{self.choices}'.\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_json/","title":"Valid json","text":""},{"location":"dynamiq/nodes/validators/valid_json/#dynamiq.nodes.validators.valid_json.ValidJSON","title":"<code>ValidJSON</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if a value matches a basic JSON structure.</p> Source code in <code>dynamiq/nodes/validators/valid_json.py</code> <pre><code>class ValidJSON(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if a value matches a basic JSON structure.\n    \"\"\"\n\n    def validate(self, content: str | dict):\n        \"\"\"\n        Validates if the provided string is a properly formatted JSON.\n\n        Args:\n            content(str): The value to check.\n\n        Raises:\n            ValueError: If the value is not a properly formatted JSON.\n\n        \"\"\"\n        try:\n            if not isinstance(content, str):\n                content = json.dumps(content)\n\n            json.loads(content)\n        except (json.decoder.JSONDecodeError, TypeError) as error:\n            raise ValueError(\n                f\"Value is not valid JSON. Value: '{content}'. Error details: {str(error)}\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_json/#dynamiq.nodes.validators.valid_json.ValidJSON.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided string is a properly formatted JSON.</p> <p>Parameters:</p> Name Type Description Default <code>content(str)</code> <p>The value to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not a properly formatted JSON.</p> Source code in <code>dynamiq/nodes/validators/valid_json.py</code> <pre><code>def validate(self, content: str | dict):\n    \"\"\"\n    Validates if the provided string is a properly formatted JSON.\n\n    Args:\n        content(str): The value to check.\n\n    Raises:\n        ValueError: If the value is not a properly formatted JSON.\n\n    \"\"\"\n    try:\n        if not isinstance(content, str):\n            content = json.dumps(content)\n\n        json.loads(content)\n    except (json.decoder.JSONDecodeError, TypeError) as error:\n        raise ValueError(\n            f\"Value is not valid JSON. Value: '{content}'. Error details: {str(error)}\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_python/","title":"Valid python","text":""},{"location":"dynamiq/nodes/validators/valid_python/#dynamiq.nodes.validators.valid_python.ValidPython","title":"<code>ValidPython</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if a value matches a basic Python code standards.</p> Source code in <code>dynamiq/nodes/validators/valid_python.py</code> <pre><code>class ValidPython(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if a value matches a basic Python code standards.\n    \"\"\"\n\n    def validate(self, content: str):\n        \"\"\"\n        Validates the provided Python code to determine if it is syntactically correct.\n\n        Args:\n            content (str): The Python code to validate.\n\n        Raises:\n            ValueError: Raised if the provided value is not syntactically correct Python code.\n        \"\"\"\n        try:\n            ast.parse(content)\n        except SyntaxError as e:\n            raise ValueError(\n                f\"Value is not valid python code. Value: '{content}'. Error details: {e.msg}\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_python/#dynamiq.nodes.validators.valid_python.ValidPython.validate","title":"<code>validate(content)</code>","text":"<p>Validates the provided Python code to determine if it is syntactically correct.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The Python code to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if the provided value is not syntactically correct Python code.</p> Source code in <code>dynamiq/nodes/validators/valid_python.py</code> <pre><code>def validate(self, content: str):\n    \"\"\"\n    Validates the provided Python code to determine if it is syntactically correct.\n\n    Args:\n        content (str): The Python code to validate.\n\n    Raises:\n        ValueError: Raised if the provided value is not syntactically correct Python code.\n    \"\"\"\n    try:\n        ast.parse(content)\n    except SyntaxError as e:\n        raise ValueError(\n            f\"Value is not valid python code. Value: '{content}'. Error details: {e.msg}\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/writers/base/","title":"Base","text":""},{"location":"dynamiq/nodes/writers/base/#dynamiq.nodes.writers.base.Writer","title":"<code>Writer</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>ABC</code></p> Source code in <code>dynamiq/nodes/writers/base.py</code> <pre><code>class Writer(VectorStoreNode, ABC):\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    input_schema: ClassVar[type[WriterInputSchema]] = WriterInputSchema\n\n    def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n        \"\"\"Clean up resources created during dry run.\"\"\"\n\n        self.vector_store.dry_run_cleanup(dry_run_config)\n</code></pre>"},{"location":"dynamiq/nodes/writers/base/#dynamiq.nodes.writers.base.Writer.dry_run_cleanup","title":"<code>dry_run_cleanup(dry_run_config=None)</code>","text":"<p>Clean up resources created during dry run.</p> Source code in <code>dynamiq/nodes/writers/base.py</code> <pre><code>def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n    \"\"\"Clean up resources created during dry run.\"\"\"\n\n    self.vector_store.dry_run_cleanup(dry_run_config)\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter","title":"<code>ChromaDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Document Writer Node using Chroma Vector Store.</p> <p>This class represents a node for writing documents to a Chroma Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Chroma | None</code> <p>The connection to the Chroma Vector Store.</p> <code>vector_store</code> <code>ChromaVectorStore | None</code> <p>The Chroma Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>class ChromaDocumentWriter(Writer, BaseWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Chroma Vector Store.\n\n    This class represents a node for writing documents to a Chroma Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Chroma | None): The connection to the Chroma Vector Store.\n        vector_store (ChromaVectorStore | None): The Chroma Vector Store instance.\n    \"\"\"\n\n    name: str = \"ChromaDocumentWriter\"\n    connection: Chroma | None = None\n    vector_store: ChromaVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ChromaDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Chroma()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ChromaVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include={\"index_name\", \"create_if_not_exist\"}) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the documents provided in the input_data to the Chroma Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): An instance containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n\n        output = self.vector_store.write_documents(documents)\n        return {\n            \"upserted_count\": output,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ChromaDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ChromaDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Chroma()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the documents provided in the input_data to the Chroma Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>An instance containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the documents provided in the input_data to the Chroma Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): An instance containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n\n    output = self.vector_store.write_documents(documents)\n    return {\n        \"upserted_count\": output,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/elasticsearch/","title":"Elasticsearch","text":""},{"location":"dynamiq/nodes/writers/elasticsearch/#dynamiq.nodes.writers.elasticsearch.ElasticsearchDocumentWriter","title":"<code>ElasticsearchDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>ElasticsearchVectorStoreWriterParams</code></p> <p>Document Writer Node using Elasticsearch Vector Store.</p> <p>This class represents a node for writing documents to an Elasticsearch Vector Store. It supports vector search, BM25 text search, and hybrid search capabilities.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Optional[Elasticsearch]</code> <p>The Elasticsearch connection.</p> <code>vector_store</code> <code>Optional[ElasticsearchVectorStore]</code> <p>The Elasticsearch Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/elasticsearch.py</code> <pre><code>class ElasticsearchDocumentWriter(Writer, ElasticsearchVectorStoreWriterParams):\n    \"\"\"\n    Document Writer Node using Elasticsearch Vector Store.\n\n    This class represents a node for writing documents to an Elasticsearch Vector Store.\n    It supports vector search, BM25 text search, and hybrid search capabilities.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Optional[Elasticsearch]): The Elasticsearch connection.\n        vector_store (Optional[ElasticsearchVectorStore]): The Elasticsearch Vector Store instance.\n    \"\"\"\n\n    name: str = \"ElasticsearchDocumentWriter\"\n    connection: Elasticsearch | str | None = None\n    vector_store: ElasticsearchVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ElasticsearchDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Elasticsearch connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Elasticsearch()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ElasticsearchVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(ElasticsearchVectorStoreWriterParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(\n        self,\n        input_data: WriterInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, int]:\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the Elasticsearch Vector Store.\n        It supports:\n        - Vector embeddings for similarity search\n        - Text content for BM25 search\n        - Metadata for filtering and custom ranking\n        - Custom mappings and analyzers\n        - Index settings and templates\n        - Duplicate handling with configurable policy\n        - Batch operations with configurable size\n\n        Args:\n            input_data (WriterInputSchema): Input data containing the documents to be written.\n            config (Optional[RunnableConfig]): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, int]: A dictionary containing the count of written documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        upserted_count = self.vector_store.write_documents(\n            documents=input_data.documents,\n            policy=DuplicatePolicy.FAIL,\n            content_key=input_data.content_key,\n            embedding_key=input_data.embedding_key,\n        )\n        logger.debug(f\"Upserted {upserted_count} documents to Elasticsearch Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/elasticsearch/#dynamiq.nodes.writers.elasticsearch.ElasticsearchDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElasticsearchDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Elasticsearch connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/elasticsearch.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ElasticsearchDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Elasticsearch connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Elasticsearch()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/elasticsearch/#dynamiq.nodes.writers.elasticsearch.ElasticsearchDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the Elasticsearch Vector Store. It supports: - Vector embeddings for similarity search - Text content for BM25 search - Metadata for filtering and custom ranking - Custom mappings and analyzers - Index settings and templates - Duplicate handling with configurable policy - Batch operations with configurable size</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: A dictionary containing the count of written documents.</p> Source code in <code>dynamiq/nodes/writers/elasticsearch.py</code> <pre><code>def execute(\n    self,\n    input_data: WriterInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, int]:\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the Elasticsearch Vector Store.\n    It supports:\n    - Vector embeddings for similarity search\n    - Text content for BM25 search\n    - Metadata for filtering and custom ranking\n    - Custom mappings and analyzers\n    - Index settings and templates\n    - Duplicate handling with configurable policy\n    - Batch operations with configurable size\n\n    Args:\n        input_data (WriterInputSchema): Input data containing the documents to be written.\n        config (Optional[RunnableConfig]): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, int]: A dictionary containing the count of written documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    upserted_count = self.vector_store.write_documents(\n        documents=input_data.documents,\n        policy=DuplicatePolicy.FAIL,\n        content_key=input_data.content_key,\n        embedding_key=input_data.embedding_key,\n    )\n    logger.debug(f\"Upserted {upserted_count} documents to Elasticsearch Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter","title":"<code>MilvusDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>MilvusWriterVectorStoreParams</code></p> <p>Document Writer Node using Milvus Vector Store.</p> <p>This class represents a node for writing documents to a Milvus Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Milvus | None</code> <p>The connection to the Milvus Vector Store.</p> <code>vector_store</code> <code>MilvusVectorStore | None</code> <p>The Milvus Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>class MilvusDocumentWriter(Writer, MilvusWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Milvus Vector Store.\n\n    This class represents a node for writing documents to a Milvus Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Milvus | None): The connection to the Milvus Vector Store.\n        vector_store (MilvusVectorStore | None): The Milvus Vector Store instance.\n    \"\"\"\n\n    name: str = \"MilvusDocumentWriter\"\n    connection: Milvus | None = None\n    vector_store: MilvusVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the MilvusDocumentWriter.\n\n        If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Milvus()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return MilvusVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(MilvusWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing process.\n\n        This method writes the input documents to the Milvus Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): An instance containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the number of upserted documents.\n\n        Raises:\n            Any exceptions raised by the vector store's write_documents method.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        content_key = input_data.content_key\n        embedding_key = input_data.embedding_key\n\n        # Write documents to Milvus\n        upserted_count = self.vector_store.write_documents(\n            documents, content_key=content_key, embedding_key=embedding_key\n        )\n        logger.debug(f\"Upserted {upserted_count} documents to Milvus Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MilvusDocumentWriter.</p> <p>If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the MilvusDocumentWriter.\n\n    If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Milvus()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing process.</p> <p>This method writes the input documents to the Milvus Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>An instance containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the number of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing process.\n\n    This method writes the input documents to the Milvus Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): An instance containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the number of upserted documents.\n\n    Raises:\n        Any exceptions raised by the vector store's write_documents method.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    content_key = input_data.content_key\n    embedding_key = input_data.embedding_key\n\n    # Write documents to Milvus\n    upserted_count = self.vector_store.write_documents(\n        documents, content_key=content_key, embedding_key=embedding_key\n    )\n    logger.debug(f\"Upserted {upserted_count} documents to Milvus Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/opensearch/","title":"Opensearch","text":""},{"location":"dynamiq/nodes/writers/opensearch/#dynamiq.nodes.writers.opensearch.OpenSearchDocumentWriter","title":"<code>OpenSearchDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>OpenSearchVectorStoreWriterParams</code></p> <p>Document Writer Node using AWS OpenSearch Vector Store.</p> <p>This class represents a node for writing documents to an AWS OpenSearch Vector Store. It supports vector search, BM25 text search, and hybrid search capabilities.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Optional[AWSOpenSearch]</code> <p>The AWS OpenSearch connection.</p> <code>vector_store</code> <code>Optional[OpenSearchVectorStore]</code> <p>The OpenSearch Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/opensearch.py</code> <pre><code>class OpenSearchDocumentWriter(Writer, OpenSearchVectorStoreWriterParams):\n    \"\"\"\n    Document Writer Node using AWS OpenSearch Vector Store.\n\n    This class represents a node for writing documents to an AWS OpenSearch Vector Store.\n    It supports vector search, BM25 text search, and hybrid search capabilities.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Optional[AWSOpenSearch]): The AWS OpenSearch connection.\n        vector_store (Optional[OpenSearchVectorStore]): The OpenSearch Vector Store instance.\n    \"\"\"\n\n    name: str = \"OpenSearchDocumentWriter\"\n    connection: AWSOpenSearch | str | None = None\n    vector_store: OpenSearchVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the OpenSearchDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default AWSOpenSearch connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AWSOpenSearch()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return OpenSearchVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(OpenSearchVectorStoreWriterParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(\n        self,\n        input_data: WriterInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, int]:\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the AWS OpenSearch Vector Store.\n        It supports:\n        - Vector embeddings for similarity search\n        - Text content for BM25 search\n        - Metadata for filtering and custom ranking\n        - Custom mappings and analyzers\n        - Index settings and templates\n        - Duplicate handling with configurable policy\n        - Batch operations with configurable size\n\n        Args:\n            input_data (WriterInputSchema): Input data containing the documents to be written.\n            config (Optional[RunnableConfig]): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, int]: A dictionary containing the count of written documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        upserted_count = self.vector_store.write_documents(\n            documents=input_data.documents,\n            policy=DuplicatePolicy.FAIL,\n            content_key=input_data.content_key,\n            embedding_key=input_data.embedding_key,\n        )\n        logger.debug(f\"Upserted {upserted_count} documents to OpenSearch Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/opensearch/#dynamiq.nodes.writers.opensearch.OpenSearchDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenSearchDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default AWSOpenSearch connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/opensearch.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the OpenSearchDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default AWSOpenSearch connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AWSOpenSearch()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/opensearch/#dynamiq.nodes.writers.opensearch.OpenSearchDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the AWS OpenSearch Vector Store. It supports: - Vector embeddings for similarity search - Text content for BM25 search - Metadata for filtering and custom ranking - Custom mappings and analyzers - Index settings and templates - Duplicate handling with configurable policy - Batch operations with configurable size</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: A dictionary containing the count of written documents.</p> Source code in <code>dynamiq/nodes/writers/opensearch.py</code> <pre><code>def execute(\n    self,\n    input_data: WriterInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, int]:\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the AWS OpenSearch Vector Store.\n    It supports:\n    - Vector embeddings for similarity search\n    - Text content for BM25 search\n    - Metadata for filtering and custom ranking\n    - Custom mappings and analyzers\n    - Index settings and templates\n    - Duplicate handling with configurable policy\n    - Batch operations with configurable size\n\n    Args:\n        input_data (WriterInputSchema): Input data containing the documents to be written.\n        config (Optional[RunnableConfig]): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, int]: A dictionary containing the count of written documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    upserted_count = self.vector_store.write_documents(\n        documents=input_data.documents,\n        policy=DuplicatePolicy.FAIL,\n        content_key=input_data.content_key,\n        embedding_key=input_data.embedding_key,\n    )\n    logger.debug(f\"Upserted {upserted_count} documents to OpenSearch Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pgvector/","title":"Pgvector","text":""},{"location":"dynamiq/nodes/writers/pgvector/#dynamiq.nodes.writers.pgvector.PGVectorDocumentWriter","title":"<code>PGVectorDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>PGVectorStoreWriterParams</code></p> <p>Document Writer Node using PGVector Vector Store.</p> <p>This class represents a node for writing documents to a PGVector Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>PostgreSQL | None</code> <p>The PostgreSQL connection.</p> <code>vector_store</code> <code>PGVectorStore | None</code> <p>The PGVector Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/pgvector.py</code> <pre><code>class PGVectorDocumentWriter(Writer, PGVectorStoreWriterParams):\n    \"\"\"\n    Document Writer Node using PGVector Vector Store.\n\n    This class represents a node for writing documents to a PGVector Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (PostgreSQL | None): The PostgreSQL connection.\n        vector_store (PGVectorStore | None): The PGVector Vector Store instance.\n    \"\"\"\n\n    name: str = \"PGVectorDocumentWriter\"\n    connection: PostgreSQL | str | None = None\n    vector_store: PGVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PGVectorDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = PostgreSQL()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return PGVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PGVectorStoreWriterParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the PGVector Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): Input data containing the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        content_key = input_data.content_key\n        embedding_key = input_data.embedding_key\n\n        upserted_count = self.vector_store.write_documents(\n            documents, content_key=content_key, embedding_key=embedding_key\n        )\n        logger.debug(f\"Upserted {upserted_count} documents to PGVector Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pgvector/#dynamiq.nodes.writers.pgvector.PGVectorDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PGVectorDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/pgvector.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PGVectorDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default PostgreSQL connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = PostgreSQL()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/pgvector/#dynamiq.nodes.writers.pgvector.PGVectorDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the PGVector Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/pgvector.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the PGVector Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): Input data containing the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    content_key = input_data.content_key\n    embedding_key = input_data.embedding_key\n\n    upserted_count = self.vector_store.write_documents(\n        documents, content_key=content_key, embedding_key=embedding_key\n    )\n    logger.debug(f\"Upserted {upserted_count} documents to PGVector Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter","title":"<code>PineconeDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>PineconeWriterVectorStoreParams</code></p> <p>Document Writer Node using Pinecone Vector Store.</p> <p>This class represents a node for writing documents to a Pinecone Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Pinecone | None</code> <p>The Pinecone connection object.</p> <code>vector_store</code> <code>PineconeVectorStore | None</code> <p>The Pinecone Vector Store object.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>class PineconeDocumentWriter(Writer, PineconeWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Pinecone Vector Store.\n\n    This class represents a node for writing documents to a Pinecone Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Pinecone | None): The Pinecone connection object.\n        vector_store (PineconeVectorStore | None): The Pinecone Vector Store object.\n    \"\"\"\n\n    name: str = \"PineconeDocumentWriter\"\n    connection: Pinecone | None = None\n    vector_store: PineconeVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PineconeDocumentWriter.\n\n        If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Pinecone()\n        super().__init__(**kwargs)\n\n    @model_validator(mode=\"after\")\n    def check_required_params(self) -&gt; \"PineconeDocumentWriter\":\n        \"\"\"\n        Validate required parameters\n\n        Returns:\n            self: The updated instance.\n        \"\"\"\n        if self.vector_store is None:\n            if self.create_if_not_exist and self.index_type is None:\n                raise ValueError(\"Index type 'pod' or 'serverless' must be specified when creating an index\")\n\n            if self.index_type == PineconeIndexType.POD and (self.environment is None or self.pod_type is None):\n                raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n            if self.index_type == PineconeIndexType.SERVERLESS and (self.cloud is None or self.region is None):\n                raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n\n        return self\n\n    @property\n    def vector_store_cls(self):\n        return PineconeVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PineconeWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing process.\n\n        This method writes the input documents to the Pinecone Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): An instance containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the number of upserted documents.\n\n        Raises:\n            Any exceptions raised by the vector store's write_documents method.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        content_key = input_data.content_key\n\n        upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n        logger.debug(f\"Upserted {upserted_count} documents to Pinecone Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PineconeDocumentWriter.</p> <p>If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PineconeDocumentWriter.\n\n    If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Pinecone()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.check_required_params","title":"<code>check_required_params()</code>","text":"<p>Validate required parameters</p> <p>Returns:</p> Name Type Description <code>self</code> <code>PineconeDocumentWriter</code> <p>The updated instance.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_required_params(self) -&gt; \"PineconeDocumentWriter\":\n    \"\"\"\n    Validate required parameters\n\n    Returns:\n        self: The updated instance.\n    \"\"\"\n    if self.vector_store is None:\n        if self.create_if_not_exist and self.index_type is None:\n            raise ValueError(\"Index type 'pod' or 'serverless' must be specified when creating an index\")\n\n        if self.index_type == PineconeIndexType.POD and (self.environment is None or self.pod_type is None):\n            raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n        if self.index_type == PineconeIndexType.SERVERLESS and (self.cloud is None or self.region is None):\n            raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing process.</p> <p>This method writes the input documents to the Pinecone Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>An instance containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the number of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing process.\n\n    This method writes the input documents to the Pinecone Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): An instance containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the number of upserted documents.\n\n    Raises:\n        Any exceptions raised by the vector store's write_documents method.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    content_key = input_data.content_key\n\n    upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n    logger.debug(f\"Upserted {upserted_count} documents to Pinecone Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter","title":"<code>QdrantDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>QdrantWriterVectorStoreParams</code></p> <p>Document Writer Node using Qdrant Vector Store.</p> <p>This class represents a node for writing documents to a Weaviate Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Qdrant | None</code> <p>The Qdrant connection.</p> <code>vector_store</code> <code>QdrantVectorStore | None</code> <p>The Qdrant Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>class QdrantDocumentWriter(Writer, QdrantWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Qdrant Vector Store.\n\n    This class represents a node for writing documents to a Weaviate Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Qdrant | None): The Qdrant connection.\n        vector_store (QdrantVectorStore | None): The Qdrant Vector Store instance.\n    \"\"\"\n\n    name: str = \"QdrantDocumentWriter\"\n    connection: QdrantConnection | None = None\n    vector_store: QdrantVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the QdrantDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = QdrantConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return QdrantVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(QdrantWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the Qdrant Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): Input data containing the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        content_key = input_data.content_key\n\n        upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n        logger.debug(f\"Upserted {upserted_count} documents to Qdrant Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the QdrantDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the QdrantDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = QdrantConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the Qdrant Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the Qdrant Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): Input data containing the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    content_key = input_data.content_key\n\n    upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n    logger.debug(f\"Upserted {upserted_count} documents to Qdrant Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter","title":"<code>WeaviateDocumentWriter</code>","text":"<p>               Bases: <code>Writer</code>, <code>WeaviateWriterVectorStoreParams</code></p> <p>Document Writer Node using Weaviate Vector Store.</p> <p>This class represents a node for writing documents to a Weaviate Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Weaviate | None</code> <p>The Weaviate connection.</p> <code>vector_store</code> <code>WeaviateVectorStore | None</code> <p>The Weaviate Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>class WeaviateDocumentWriter(Writer, WeaviateWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Weaviate Vector Store.\n\n    This class represents a node for writing documents to a Weaviate Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Weaviate | None): The Weaviate connection.\n        vector_store (WeaviateVectorStore | None): The Weaviate Vector Store instance.\n    \"\"\"\n\n    name: str = \"WeaviateDocumentWriter\"\n    connection: Weaviate | None = None\n    vector_store: WeaviateVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the WeaviateDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Weaviate()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return WeaviateVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(WeaviateWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the Weaviate Vector Store.\n\n        Args:\n            input_data (WriterInputSchema): Input data containing the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n        content_key = input_data.content_key\n\n        upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n        logger.debug(f\"Upserted {upserted_count} documents to Weaviate Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WeaviateDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the WeaviateDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Weaviate()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the Weaviate Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>WriterInputSchema</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>def execute(self, input_data: WriterInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the Weaviate Vector Store.\n\n    Args:\n        input_data (WriterInputSchema): Input data containing the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n    content_key = input_data.content_key\n\n    upserted_count = self.vector_store.write_documents(documents, content_key=content_key)\n    logger.debug(f\"Upserted {upserted_count} documents to Weaviate Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/","title":"Writer","text":""},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter","title":"<code>VectorStoreWriter</code>","text":"<p>               Bases: <code>Node</code></p> <p>Node for writing documents to a vector store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>Group for the node. Defaults to NodeGroup.TOOLS.</p> <code>name</code> <code>str</code> <p>Name of the tool. Defaults to \"VectorStore Writer\".</p> <code>description</code> <code>str</code> <p>Description of the tool.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>document_embedder</code> <code>DocumentEmbedder</code> <p>Document embedder node.</p> <code>document_writer</code> <code>Writer</code> <p>Document writer node.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>class VectorStoreWriter(Node):\n    \"\"\"Node for writing documents to a vector store.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): Group for the node. Defaults to NodeGroup.TOOLS.\n        name (str): Name of the tool. Defaults to \"VectorStore Writer\".\n        description (str): Description of the tool.\n        error_handling (ErrorHandling): Error handling configuration.\n        document_embedder (DocumentEmbedder): Document embedder node.\n        document_writer (Writer): Document writer node.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"VectorStore Writer\"\n    description: str = DESCRIPTION_VECTOR_STORE_WRITER\n    error_handling: ErrorHandling = Field(default_factory=lambda: ErrorHandling(timeout_seconds=600))\n    document_embedder: DocumentEmbedder\n    document_writer: Writer\n\n    input_schema: ClassVar[type[VectorStoreWriterInputSchema]] = VectorStoreWriterInputSchema\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the VectorStoreWriter with the given parameters.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n        \"\"\"\n        Initialize the components of the tool.\n\n        Args:\n            connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        connection_manager = connection_manager or ConnectionManager()\n        super().init_components(connection_manager)\n        if self.document_embedder.is_postponed_component_init:\n            self.document_embedder.init_components(connection_manager)\n        if self.document_writer.is_postponed_component_init:\n            self.document_writer.init_components(connection_manager)\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"document_embedder\": True, \"document_writer\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"document_embedder\"] = self.document_embedder.to_dict(**kwargs)\n        data[\"document_writer\"] = self.document_writer.to_dict(**kwargs)\n        return data\n\n    def execute(\n        self, input_data: VectorStoreWriterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the vector store writer tool.\n\n        Args:\n            input_data (VectorStoreWriterInputSchema): Input data for the tool.\n            config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Result of the writing operation.\n        \"\"\"\n\n        logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data.documents\n\n        try:\n            kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n            kwargs.pop(\"run_depends\", None)\n\n            document_embedder_output = self.document_embedder.run(\n                input_data={\"documents\": documents}, run_depends=self._run_depends, config=config, **kwargs\n            )\n            self._run_depends = [NodeDependency(node=self.document_embedder).to_dict(for_tracing=True)]\n            embedded_documents = document_embedder_output.output.get(\"documents\", [])\n            logger.debug(f\"Tool {self.name} - {self.id}: embedded {len(embedded_documents)} documents\")\n\n            document_writer_output = self.document_writer.run(\n                input_data={\"documents\": embedded_documents},\n                run_depends=self._run_depends,\n                config=config,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.document_writer).to_dict(for_tracing=True)]\n            upserted_count = document_writer_output.output.get(\"upserted_count\", 0)\n            logger.debug(f\"Tool {self.name} - {self.id}: wrote {upserted_count} documents to vector store\")\n\n            result = {\"upserted_count\": upserted_count}\n            logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n            return result\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n            raise ToolExecutionException(\n                f\"Tool '{self.name}' failed to write documents to the vector store. \"\n                f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n                recoverable=True,\n            )\n\n    def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n        \"\"\"Clean up resources created during dry run.\"\"\"\n        if self.document_writer:\n            self.document_writer.dry_run_cleanup(dry_run_config)\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the VectorStoreWriter with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the VectorStoreWriter with the given parameters.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.dry_run_cleanup","title":"<code>dry_run_cleanup(dry_run_config=None)</code>","text":"<p>Clean up resources created during dry run.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def dry_run_cleanup(self, dry_run_config: DryRunConfig | None = None) -&gt; None:\n    \"\"\"Clean up resources created during dry run.\"\"\"\n    if self.document_writer:\n        self.document_writer.dry_run_cleanup(dry_run_config)\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the vector store writer tool.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>VectorStoreWriterInputSchema</code> <p>Input data for the tool.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable, including callbacks.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Result of the writing operation.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def execute(\n    self, input_data: VectorStoreWriterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the vector store writer tool.\n\n    Args:\n        input_data (VectorStoreWriterInputSchema): Input data for the tool.\n        config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Result of the writing operation.\n    \"\"\"\n\n    logger.info(f\"Tool {self.name} - {self.id}: started with INPUT DATA:\\n{input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data.documents\n\n    try:\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n\n        document_embedder_output = self.document_embedder.run(\n            input_data={\"documents\": documents}, run_depends=self._run_depends, config=config, **kwargs\n        )\n        self._run_depends = [NodeDependency(node=self.document_embedder).to_dict(for_tracing=True)]\n        embedded_documents = document_embedder_output.output.get(\"documents\", [])\n        logger.debug(f\"Tool {self.name} - {self.id}: embedded {len(embedded_documents)} documents\")\n\n        document_writer_output = self.document_writer.run(\n            input_data={\"documents\": embedded_documents},\n            run_depends=self._run_depends,\n            config=config,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.document_writer).to_dict(for_tracing=True)]\n        upserted_count = document_writer_output.output.get(\"upserted_count\", 0)\n        logger.debug(f\"Tool {self.name} - {self.id}: wrote {upserted_count} documents to vector store\")\n\n        result = {\"upserted_count\": upserted_count}\n        logger.info(f\"Tool {self.name} - {self.id}: finished with RESULT:\\n{str(result)[:200]}...\")\n\n        return result\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n        raise ToolExecutionException(\n            f\"Tool '{self.name}' failed to write documents to the vector store. \"\n            f\"Error: {str(e)}. Please analyze the error and take appropriate action.\",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.init_components","title":"<code>init_components(connection_manager=None)</code>","text":"<p>Initialize the components of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>connection manager. Defaults to ConnectionManager.</p> <code>None</code> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager | None = None) -&gt; None:\n    \"\"\"\n    Initialize the components of the tool.\n\n    Args:\n        connection_manager (ConnectionManager, optional): connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    connection_manager = connection_manager or ConnectionManager()\n    super().init_components(connection_manager)\n    if self.document_embedder.is_postponed_component_init:\n        self.document_embedder.init_components(connection_manager)\n    if self.document_writer.is_postponed_component_init:\n        self.document_writer.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriter.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"document_embedder\"] = self.document_embedder.to_dict(**kwargs)\n    data[\"document_writer\"] = self.document_writer.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriterInputSchema","title":"<code>VectorStoreWriterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>class VectorStoreWriterInputSchema(BaseModel):\n    documents: list[Document] | list[dict] = Field(\n        ...,\n        description=\"Parameter to provide documents to write to the vector store.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_input_documents(self):\n        \"\"\"\n        Validate the input documents by converting list of dictionaries\n        to Documents (when using inside an agent) and ensuring metadata is never None.\n        \"\"\"\n        if self.documents:\n            if isinstance(self.documents[0], dict):\n                converted_docs = []\n                for doc_dict in self.documents:\n                    if not doc_dict.get(\"content\", \"\"):\n                        raise ValueError(\"Document dict must contain 'content' field\")\n                    if not doc_dict.get(\"metadata\", {}):\n                        doc_dict[\"metadata\"] = {}\n                    converted_docs.append(Document(**doc_dict))\n                self.documents = converted_docs\n            elif isinstance(self.documents[0], Document):\n                for doc in self.documents:\n                    doc.metadata = doc.metadata or {}\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/writers/writer/#dynamiq.nodes.writers.writer.VectorStoreWriterInputSchema.validate_input_documents","title":"<code>validate_input_documents()</code>","text":"<p>Validate the input documents by converting list of dictionaries to Documents (when using inside an agent) and ensuring metadata is never None.</p> Source code in <code>dynamiq/nodes/writers/writer.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_input_documents(self):\n    \"\"\"\n    Validate the input documents by converting list of dictionaries\n    to Documents (when using inside an agent) and ensuring metadata is never None.\n    \"\"\"\n    if self.documents:\n        if isinstance(self.documents[0], dict):\n            converted_docs = []\n            for doc_dict in self.documents:\n                if not doc_dict.get(\"content\", \"\"):\n                    raise ValueError(\"Document dict must contain 'content' field\")\n                if not doc_dict.get(\"metadata\", {}):\n                    doc_dict[\"metadata\"] = {}\n                converted_docs.append(Document(**doc_dict))\n            self.documents = converted_docs\n        elif isinstance(self.documents[0], Document):\n            for doc in self.documents:\n                doc.metadata = doc.metadata or {}\n    return self\n</code></pre>"},{"location":"dynamiq/prompts/prompts/","title":"Prompts","text":""},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for prompts.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the prompt, generated using generate_uuid by default.</p> <code>version</code> <code>str | None</code> <p>Version of the prompt, optional.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class BasePrompt(ABC, BaseModel):\n    \"\"\"\n    Abstract base class for prompts.\n\n    Attributes:\n        id (str): Unique identifier for the prompt, generated using generate_uuid by default.\n        version (str | None): Version of the prompt, optional.\n    \"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n    version: str | None = None\n\n    @abstractmethod\n    def format_messages(self, **kwargs) -&gt; list[dict]:\n        \"\"\"\n        Abstract method to format messages.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            list[dict]: A list of formatted messages as dictionaries.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_tools(self, **kwargs) -&gt; list[dict] | None:\n        \"\"\"\n        Abstract method to format tools.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            list[dict]: A list of formatted tools as dictionaries.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt.format_messages","title":"<code>format_messages(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format messages.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of formatted messages as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>@abstractmethod\ndef format_messages(self, **kwargs) -&gt; list[dict]:\n    \"\"\"\n    Abstract method to format messages.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        list[dict]: A list of formatted messages as dictionaries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt.format_tools","title":"<code>format_tools(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format tools.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict] | None</code> <p>list[dict]: A list of formatted tools as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>@abstractmethod\ndef format_tools(self, **kwargs) -&gt; list[dict] | None:\n    \"\"\"\n    Abstract method to format tools.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        list[dict]: A list of formatted tools as dictionaries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a message in a conversation. Attributes:     content (str): The content of the message.     role (MessageRole): The role of the message sender.     metadata (dict | None): Additional metadata for the message, default is None.     static (bool): Determines whether it is possible to pass parameters via this message.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    Represents a message in a conversation.\n    Attributes:\n        content (str): The content of the message.\n        role (MessageRole): The role of the message sender.\n        metadata (dict | None): Additional metadata for the message, default is None.\n        static (bool): Determines whether it is possible to pass parameters via this message.\n    \"\"\"\n    content: str\n    role: MessageRole = MessageRole.USER\n    metadata: dict | None = None\n    static: bool = Field(default=False, exclude=True)\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        # Import and initialize Jinja2 Template here\n        from jinja2 import Template\n\n        self._Template = Template\n\n    def format_message(self, **kwargs) -&gt; \"Message\":\n        \"Returns formated copy of message\"\n        return Message(\n            role=self.role,\n            content=self._Template(self.content).render(**kwargs),\n        )\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Message.format_message","title":"<code>format_message(**kwargs)</code>","text":"<p>Returns formated copy of message</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def format_message(self, **kwargs) -&gt; \"Message\":\n    \"Returns formated copy of message\"\n    return Message(\n        role=self.role,\n        content=self._Template(self.content).render(**kwargs),\n    )\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BasePrompt</code></p> <p>Concrete implementation of BasePrompt for handling both text and vision messages.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[Message | VisionMessage]</code> <p>List of Message or VisionMessage objects</p> <code>tools</code> <code>list[Tool]</code> <p>List of functions for which the model may generate JSON inputs.</p> <code>response_format</code> <code>dict[str, Any]</code> <p>JSON schema that specifies the structure of the llm's output.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class Prompt(BasePrompt):\n    \"\"\"\n    Concrete implementation of BasePrompt for handling both text and vision messages.\n\n    Attributes:\n        messages (list[Message | VisionMessage]): List of Message or VisionMessage objects\n        representing the prompt.\n        tools (list[Tool]): List of functions for which the model may generate JSON inputs.\n        response_format (dict[str, Any]): JSON schema that specifies the structure of the llm's output.\n    \"\"\"\n\n    messages: list[Message | VisionMessage]\n    tools: list[Tool | dict] | None = None\n    response_format: dict[str, Any] | None = None\n    _Template: Any = PrivateAttr()\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n\n    def __init__(self, **data):\n        super().__init__(**data)\n\n    def count_tokens(self, model: str) -&gt; int:\n        \"\"\"\n        Counts number of tokens in prompt based on the model name.\n\n        Args:\n            * model (str): Model name.\n\n        Returns:\n            int: Number of tokens.\n        \"\"\"\n        return token_counter(\n            model=model, messages=[message.model_dump(exclude={\"metadata\"}) for message in self.messages]\n        )\n\n    def get_required_parameters(self) -&gt; set[str]:\n        \"\"\"Extracts set of parameters required for messages.\n\n        Returns:\n            set[str]: Set of parameter names.\n        \"\"\"\n        parameters = set()\n\n        env = Environment(autoescape=True)\n        for msg in self.messages:\n            if isinstance(msg, Message):\n                if not msg.static:\n                    parameters |= get_parameters_for_template(msg.content, env=env)\n            elif isinstance(msg, VisionMessage):\n                for content in msg.content:\n                    if isinstance(content, VisionMessageTextContent):\n                        if not msg.static:\n                            parameters |= get_parameters_for_template(content.text, env=env)\n                    elif isinstance(content, VisionMessageImageContent):\n                        parameters |= get_parameters_for_template(content.image_url.url, env=env)\n                    elif isinstance(content, VisionMessageFileContent):\n                        parameters |= get_parameters_for_template(content.file.file_data, env=env)\n                    else:\n                        raise ValueError(f\"Invalid content type: {content.type}\")\n            else:\n                raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n        return parameters\n\n    def format_messages(self, **kwargs) -&gt; list[dict]:\n        \"\"\"\n        Formats the messages in the prompt, rendering any templates.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments used for template rendering.\n\n        Returns:\n            list[dict]: A list of formatted messages as dictionaries.\n        \"\"\"\n        out: list[dict] = []\n        for msg in self.messages:\n            if isinstance(msg, Message):\n                if not msg.static:\n                    msg = msg.format_message(**kwargs)\n                out.append(msg.model_dump(exclude={\"metadata\"}))\n            elif isinstance(msg, VisionMessage):\n                out.append(msg.format_message(**kwargs).model_dump(exclude={\"metadata\"}))\n            else:\n                raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n        return out\n\n    def format_tools(self, **kwargs) -&gt; list[dict] | None:\n        out = None\n        if self.tools:\n            out = [tool.model_dump() for tool in self.tools]\n        return out\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt.count_tokens","title":"<code>count_tokens(model)</code>","text":"<p>Counts number of tokens in prompt based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>model (str</code> <p>Model name.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of tokens.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def count_tokens(self, model: str) -&gt; int:\n    \"\"\"\n    Counts number of tokens in prompt based on the model name.\n\n    Args:\n        * model (str): Model name.\n\n    Returns:\n        int: Number of tokens.\n    \"\"\"\n    return token_counter(\n        model=model, messages=[message.model_dump(exclude={\"metadata\"}) for message in self.messages]\n    )\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt.format_messages","title":"<code>format_messages(**kwargs)</code>","text":"<p>Formats the messages in the prompt, rendering any templates.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments used for template rendering.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of formatted messages as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; list[dict]:\n    \"\"\"\n    Formats the messages in the prompt, rendering any templates.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments used for template rendering.\n\n    Returns:\n        list[dict]: A list of formatted messages as dictionaries.\n    \"\"\"\n    out: list[dict] = []\n    for msg in self.messages:\n        if isinstance(msg, Message):\n            if not msg.static:\n                msg = msg.format_message(**kwargs)\n            out.append(msg.model_dump(exclude={\"metadata\"}))\n        elif isinstance(msg, VisionMessage):\n            out.append(msg.format_message(**kwargs).model_dump(exclude={\"metadata\"}))\n        else:\n            raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n    return out\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt.get_required_parameters","title":"<code>get_required_parameters()</code>","text":"<p>Extracts set of parameters required for messages.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: Set of parameter names.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def get_required_parameters(self) -&gt; set[str]:\n    \"\"\"Extracts set of parameters required for messages.\n\n    Returns:\n        set[str]: Set of parameter names.\n    \"\"\"\n    parameters = set()\n\n    env = Environment(autoescape=True)\n    for msg in self.messages:\n        if isinstance(msg, Message):\n            if not msg.static:\n                parameters |= get_parameters_for_template(msg.content, env=env)\n        elif isinstance(msg, VisionMessage):\n            for content in msg.content:\n                if isinstance(content, VisionMessageTextContent):\n                    if not msg.static:\n                        parameters |= get_parameters_for_template(content.text, env=env)\n                elif isinstance(content, VisionMessageImageContent):\n                    parameters |= get_parameters_for_template(content.image_url.url, env=env)\n                elif isinstance(content, VisionMessageFileContent):\n                    parameters |= get_parameters_for_template(content.file.file_data, env=env)\n                else:\n                    raise ValueError(f\"Invalid content type: {content.type}\")\n        else:\n            raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n    return parameters\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage","title":"<code>VisionMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a vision message in a conversation.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>list[VisionTextMessage | VisionImageMessage]</code> <p>The content of the message.</p> <code>role</code> <code>MessageRole</code> <p>The role of the message sender.</p> <code>static</code> <code>bool</code> <p>Determines whether it is possible to pass parameters via this message.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessage(BaseModel):\n    \"\"\"\n    Represents a vision message in a conversation.\n\n    Attributes:\n        content (list[VisionTextMessage | VisionImageMessage]): The content of the message.\n        role (MessageRole): The role of the message sender.\n        static (bool): Determines whether it is possible to pass parameters via this message.\n    \"\"\"\n\n    content: list[VisionMessageTextContent | VisionMessageImageContent | VisionMessageFileContent]\n    role: MessageRole = MessageRole.USER\n    static: bool = Field(default=False, exclude=True)\n\n    def parse_bytes_to_base64(self, file_bytes: bytes) -&gt; str:\n        \"\"\"\n        Parses file bytes in base64 format.\n\n        Args:\n            file_bytes (bytes): File bytes.\n\n        Returns:\n            str: Base64 encoded file.\n        \"\"\"\n        extension = filetype.guess_extension(file_bytes)\n        if not extension:\n            extension = \"txt\"\n\n        encoded_str = base64.b64encode(file_bytes).decode(\"utf-8\")\n\n        mime_type, _ = mimetypes.guess_type(f\"file.{extension}\")\n\n        if mime_type is None:\n            mime_type = \"text/plain\"\n\n        return f\"data:{mime_type};base64,{encoded_str}\"\n\n    def parse_image_url_parameters(self, url_template: str, kwargs: dict) -&gt; None:\n        \"\"\"\n        Converts image URL parameters in kwargs to Base64-encoded Data URLs if they contain image data.\n\n        Args:\n            url_template (str): Jinja template for the image URL.\n            kwargs (dict): Dictionary of parameters to be used with the template.\n\n        Raises:\n            KeyError: If a required parameter is missing in kwargs.\n            ValueError: If the file type cannot be determined or unsupported data type is provided.\n        \"\"\"\n        template_params = get_parameters_for_template(url_template)\n\n        for param in template_params:\n            if param not in kwargs:\n                raise KeyError(f\"Missing required parameter: '{param}'\")\n\n            value = kwargs[param]\n\n            # Initialize as unchanged; will be modified if image data is detected\n            processed_value = value\n\n            if isinstance(value, io.BytesIO):\n                image_bytes = value.getvalue()\n                processed_value = self.parse_bytes_to_base64(image_bytes)\n\n            elif isinstance(value, bytes):\n                processed_value = self.parse_bytes_to_base64(value)\n\n            elif isinstance(value, str):\n                pass  # No action needed; assuming it's a regular URL or already a Data URL\n\n            else:\n                # Unsupported data type for image parameter\n                raise ValueError(f\"Unsupported data type for parameter '{param}': {type(value)}\")\n\n            # Update the parameter with the processed value\n            kwargs[param] = processed_value\n\n    def format_message(self, **kwargs):\n        out_msg_content = []\n        for content in self.content:\n            if isinstance(content, VisionMessageTextContent):\n                if not self.static:\n                    out_msg_content.append(\n                        VisionMessageTextContent(\n                            text=self._Template(content.text).render(**kwargs),\n                        )\n                    )\n                else:\n                    out_msg_content.append(content)\n            elif isinstance(content, VisionMessageImageContent):\n                self.parse_image_url_parameters(content.image_url.url, kwargs)\n                out_msg_content.append(\n                    VisionMessageImageContent(\n                        image_url=VisionMessageImageURL(\n                            url=self._Template(content.image_url.url).render(**kwargs),\n                            detail=content.image_url.detail,\n                        )\n                    )\n                )\n            elif isinstance(content, VisionMessageFileContent):\n                self.parse_image_url_parameters(content.file.file_data, kwargs)\n                out_msg_content.append(\n                    VisionMessageFileContent(\n                        file=VisionMessageFileData(\n                            file_data=self._Template(content.file.file_data).render(**kwargs),\n                        )\n                    )\n                )\n            else:\n                raise ValueError(f\"Invalid content type: {content.type}\")\n\n        if len(out_msg_content) == 1 and out_msg_content[0].type == VisionMessageType.TEXT:\n            return Message(role=self.role, content=out_msg_content[0].text)\n\n        return VisionMessage(role=self.role, content=out_msg_content)\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        # Import and initialize Jinja2 Template here\n        from jinja2 import Template\n\n        self._Template = Template\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Converts the message to a dictionary.\n\n        Returns:\n            dict: The message as a dictionary.\n        \"\"\"\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage.parse_bytes_to_base64","title":"<code>parse_bytes_to_base64(file_bytes)</code>","text":"<p>Parses file bytes in base64 format.</p> <p>Parameters:</p> Name Type Description Default <code>file_bytes</code> <code>bytes</code> <p>File bytes.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Base64 encoded file.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def parse_bytes_to_base64(self, file_bytes: bytes) -&gt; str:\n    \"\"\"\n    Parses file bytes in base64 format.\n\n    Args:\n        file_bytes (bytes): File bytes.\n\n    Returns:\n        str: Base64 encoded file.\n    \"\"\"\n    extension = filetype.guess_extension(file_bytes)\n    if not extension:\n        extension = \"txt\"\n\n    encoded_str = base64.b64encode(file_bytes).decode(\"utf-8\")\n\n    mime_type, _ = mimetypes.guess_type(f\"file.{extension}\")\n\n    if mime_type is None:\n        mime_type = \"text/plain\"\n\n    return f\"data:{mime_type};base64,{encoded_str}\"\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage.parse_image_url_parameters","title":"<code>parse_image_url_parameters(url_template, kwargs)</code>","text":"<p>Converts image URL parameters in kwargs to Base64-encoded Data URLs if they contain image data.</p> <p>Parameters:</p> Name Type Description Default <code>url_template</code> <code>str</code> <p>Jinja template for the image URL.</p> required <code>kwargs</code> <code>dict</code> <p>Dictionary of parameters to be used with the template.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If a required parameter is missing in kwargs.</p> <code>ValueError</code> <p>If the file type cannot be determined or unsupported data type is provided.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def parse_image_url_parameters(self, url_template: str, kwargs: dict) -&gt; None:\n    \"\"\"\n    Converts image URL parameters in kwargs to Base64-encoded Data URLs if they contain image data.\n\n    Args:\n        url_template (str): Jinja template for the image URL.\n        kwargs (dict): Dictionary of parameters to be used with the template.\n\n    Raises:\n        KeyError: If a required parameter is missing in kwargs.\n        ValueError: If the file type cannot be determined or unsupported data type is provided.\n    \"\"\"\n    template_params = get_parameters_for_template(url_template)\n\n    for param in template_params:\n        if param not in kwargs:\n            raise KeyError(f\"Missing required parameter: '{param}'\")\n\n        value = kwargs[param]\n\n        # Initialize as unchanged; will be modified if image data is detected\n        processed_value = value\n\n        if isinstance(value, io.BytesIO):\n            image_bytes = value.getvalue()\n            processed_value = self.parse_bytes_to_base64(image_bytes)\n\n        elif isinstance(value, bytes):\n            processed_value = self.parse_bytes_to_base64(value)\n\n        elif isinstance(value, str):\n            pass  # No action needed; assuming it's a regular URL or already a Data URL\n\n        else:\n            # Unsupported data type for image parameter\n            raise ValueError(f\"Unsupported data type for parameter '{param}': {type(value)}\")\n\n        # Update the parameter with the processed value\n        kwargs[param] = processed_value\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the message to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The message as a dictionary.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Converts the message to a dictionary.\n\n    Returns:\n        dict: The message as a dictionary.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageFileContent","title":"<code>VisionMessageFileContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a file message in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>VisionMessageType</code> <p>The type of the message, default is \"image_url\".</p> <code>file</code> <code>VisionMessageFileData</code> <p>The file data class.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageFileContent(BaseModel):\n    \"\"\"\n    Represents a file message in a vision conversation.\n\n    Attributes:\n        type (VisionMessageType): The type of the message, default is \"image_url\".\n        file (VisionMessageFileData): The file data class.\n    \"\"\"\n\n    type: VisionMessageType = VisionMessageType.FILE\n    file: VisionMessageFileData\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageFileData","title":"<code>VisionMessageFileData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a file data in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>file_data</code> <code>str</code> <p>The base64 formatted data of the file.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageFileData(BaseModel):\n    \"\"\"\n    Represents a file data in a vision conversation.\n\n    Attributes:\n        file_data (str): The base64 formatted data of the file.\n    \"\"\"\n\n    file_data: str\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageImageContent","title":"<code>VisionMessageImageContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an image message in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>VisionMessageType</code> <p>The type of the message, default is \"image_url\".</p> <code>image_url</code> <code>VisionMessageImageURL</code> <p>The image URL class.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageImageContent(BaseModel):\n    \"\"\"\n    Represents an image message in a vision conversation.\n\n    Attributes:\n        type (VisionMessageType): The type of the message, default is \"image_url\".\n        image_url (VisionMessageImageURL): The image URL class.\n    \"\"\"\n    type: VisionMessageType = VisionMessageType.IMAGE_URL\n    image_url: VisionMessageImageURL\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageImageURL","title":"<code>VisionMessageImageURL</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an image URL in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the image.</p> <code>detail</code> <code>VisionDetail</code> <p>The detail level of the image, default is \"auto\".</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageImageURL(BaseModel):\n    \"\"\"\n    Represents an image URL in a vision conversation.\n\n    Attributes:\n        url (str): The URL of the image.\n        detail (VisionDetail): The detail level of the image, default is \"auto\".\n    \"\"\"\n\n    url: str\n    detail: VisionDetail = VisionDetail.AUTO\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageTextContent","title":"<code>VisionMessageTextContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a text message in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>VisionMessageType</code> <p>The type of the message, default is \"text\".</p> <code>text</code> <code>str</code> <p>The text content of the message.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageTextContent(BaseModel):\n    \"\"\"\n    Represents a text message in a vision conversation.\n\n    Attributes:\n        type (VisionMessageType): The type of the message, default is \"text\".\n        text (str): The text content of the message.\n    \"\"\"\n\n    type: VisionMessageType = VisionMessageType.TEXT\n    text: str\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.get_parameters_for_template","title":"<code>get_parameters_for_template(template, env=None)</code>","text":"<p>Extracts set of parameters for template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Template to find parameters for.</p> required <code>env</code> <code>Environment | None</code> <p>(Environment, optional): jinja Environment object.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>Set of required parameters.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def get_parameters_for_template(template: str, env: Environment | None = None) -&gt; set[str]:\n    \"\"\"\n    Extracts set of parameters for template.\n\n    Args:\n        template (str): Template to find parameters for.\n        env: (Environment, optional): jinja Environment object.\n\n    Returns:\n        set: Set of required parameters.\n    \"\"\"\n    if not env:\n        env = Environment(autoescape=True)\n    # Parse the template to get its Abstract Syntax Tree\n    ast = env.parse(template)\n\n    # Find and return set of undeclared variables in the template\n    return meta.find_undeclared_variables(ast)\n</code></pre>"},{"location":"dynamiq/runnables/base/","title":"Base","text":""},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable","title":"<code>Runnable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for runnable objects.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class Runnable(ABC):\n    \"\"\"\n    Abstract base class for runnable objects.\n    \"\"\"\n\n    def run(\n        self, input_data: Any, config: RunnableConfig = None, is_async: bool | None = None, **kwargs\n    ) -&gt; RunnableResult | Awaitable[RunnableResult]:\n        \"\"\"Run the workflow with given input data and configuration.\n\n        This method acts as a dispatcher based on whether it's called from an async context:\n        - In synchronous contexts, it calls run_sync\n        - In asynchronous contexts, it calls run_async when awaited\n        - The mode can be explicitly specified with the is_async parameter\n\n        For direct control, use run_sync() or run_async() methods.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            is_async (bool, optional): Force synchronous or asynchronous execution.\n                If None, tries to detect based on calling context.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Union[RunnableResult, Awaitable[RunnableResult]]: Result of the workflow execution\n                or awaitable coroutine leading to the result.\n        \"\"\"\n        if is_async is None:\n            is_async = is_called_from_async_context()\n\n        if is_async:\n            return self.run_async(input_data, config, **kwargs)\n        else:\n            return self.run_sync(input_data, config, **kwargs)\n\n    @abstractmethod\n    def run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Abstract method to run the Runnable object synchronously.\n\n        Args:\n            input_data (Any): The input data for the execution.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: The result of the execution.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Abstract method to run the Runnable object asynchronously.\n\n        Args:\n            input_data (Any): The input data for the execution.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: The result of the execution.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable.run","title":"<code>run(input_data, config=None, is_async=None, **kwargs)</code>","text":"<p>Run the workflow with given input data and configuration.</p> <p>This method acts as a dispatcher based on whether it's called from an async context: - In synchronous contexts, it calls run_sync - In asynchronous contexts, it calls run_async when awaited - The mode can be explicitly specified with the is_async parameter</p> <p>For direct control, use run_sync() or run_async() methods.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>is_async</code> <code>bool</code> <p>Force synchronous or asynchronous execution. If None, tries to detect based on calling context.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>RunnableResult | Awaitable[RunnableResult]</code> <p>Union[RunnableResult, Awaitable[RunnableResult]]: Result of the workflow execution or awaitable coroutine leading to the result.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def run(\n    self, input_data: Any, config: RunnableConfig = None, is_async: bool | None = None, **kwargs\n) -&gt; RunnableResult | Awaitable[RunnableResult]:\n    \"\"\"Run the workflow with given input data and configuration.\n\n    This method acts as a dispatcher based on whether it's called from an async context:\n    - In synchronous contexts, it calls run_sync\n    - In asynchronous contexts, it calls run_async when awaited\n    - The mode can be explicitly specified with the is_async parameter\n\n    For direct control, use run_sync() or run_async() methods.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        is_async (bool, optional): Force synchronous or asynchronous execution.\n            If None, tries to detect based on calling context.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Union[RunnableResult, Awaitable[RunnableResult]]: Result of the workflow execution\n            or awaitable coroutine leading to the result.\n    \"\"\"\n    if is_async is None:\n        is_async = is_called_from_async_context()\n\n    if is_async:\n        return self.run_async(input_data, config, **kwargs)\n    else:\n        return self.run_sync(input_data, config, **kwargs)\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable.run_async","title":"<code>run_async(input_data, config=None, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to run the Runnable object asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data for the execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>The result of the execution.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>@abstractmethod\nasync def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"\n    Abstract method to run the Runnable object asynchronously.\n\n    Args:\n        input_data (Any): The input data for the execution.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: The result of the execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable.run_sync","title":"<code>run_sync(input_data, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to run the Runnable object synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data for the execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>The result of the execution.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>@abstractmethod\ndef run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"\n    Abstract method to run the Runnable object synchronously.\n\n    Args:\n        input_data (Any): The input data for the execution.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: The result of the execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableConfig","title":"<code>RunnableConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for Runnable objects.</p> <p>Attributes:</p> Name Type Description <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> <code>cache</code> <code>CacheConfig | None</code> <p>Cache configuration.</p> <code>max_node_workers</code> <code>int | None</code> <p>Maximum number of node workers.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableConfig(BaseModel):\n    \"\"\"\n    Configuration class for Runnable objects.\n\n    Attributes:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        cache (CacheConfig | None): Cache configuration.\n        max_node_workers (int | None): Maximum number of node workers.\n    \"\"\"\n\n    run_id: str | None = Field(default_factory=generate_uuid)\n    callbacks: list[BaseCallbackHandler] = []\n    cache: CacheConfig | None = None\n    max_node_workers: int | None = None\n    nodes_override: dict[str, NodeRunnableConfig] = {}\n    dry_run: DryRunConfig | None = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableFailedNodeInfo","title":"<code>RunnableFailedNodeInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a failed node with RAISE error behavior.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Node ID.</p> <code>name</code> <code>str | None</code> <p>Node name.</p> <code>error_message</code> <code>str | None</code> <p>Error message from the node.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableFailedNodeInfo(BaseModel):\n    \"\"\"Information about a failed node with RAISE error behavior.\n\n    Attributes:\n        id (str): Node ID.\n        name (str | None): Node name.\n        error_message (str | None): Error message from the node.\n    \"\"\"\n\n    id: str\n    name: str | None = None\n    error_message: str | None = None\n\n    def to_dict(self) -&gt; dict:\n        return self.model_dump()\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult","title":"<code>RunnableResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass representing the result of a Runnable execution.</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>RunnableStatus</code> <p>The status of the execution.</p> <code>input</code> <code>Any</code> <p>The input data of the execution.</p> <code>output</code> <code>Any</code> <p>The output data of the execution.</p> <code>error</code> <code>RunnableResultError | None</code> <p>The error of the execution.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableResult(BaseModel):\n    \"\"\"\n    Dataclass representing the result of a Runnable execution.\n\n    Attributes:\n        status (RunnableStatus): The status of the execution.\n        input (Any): The input data of the execution.\n        output (Any): The output data of the execution.\n        error (RunnableResultError | None): The error of the execution.\n    \"\"\"\n\n    status: RunnableStatus\n    input: Any = None\n    output: Any = None\n    error: RunnableResultError | None = None\n\n    def to_depend_dict(\n        self,\n        skip_format_types: set | None = None,\n        force_format_types: set | None = None,\n        **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary that used as dependency context.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n        if skip_format_types is None:\n            skip_format_types = set()\n        skip_format_types.update({BytesIO, BaseModel, bytes})\n\n        if force_format_types is None:\n            force_format_types = set()\n        force_format_types.update({RunnableResult, RunnableResultError})\n\n        return self.to_dict(skip_format_types, force_format_types, **kwargs)\n\n    def to_tracing_depend_dict(\n        self, skip_format_types: set | None = None, force_format_types: set | None = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n\n        depend_dict = self.to_depend_dict(skip_format_types, force_format_types, **kwargs)\n        depend_dict.pop(\"input\", None)\n\n        return depend_dict\n\n    def to_dict(\n        self,\n        skip_format_types: set | None = None,\n        force_format_types: set | None = None,\n        **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n\n        data = {\n            \"status\": self.status.value,\n            \"input\": format_value(self.input, skip_format_types, force_format_types),\n            \"output\": format_value(self.output, skip_format_types, force_format_types),\n        }\n        if self.error:\n            data[\"error\"] = format_value(self.error, skip_format_types, force_format_types)\n\n        return data\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_depend_dict","title":"<code>to_depend_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary that used as dependency context.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_depend_dict(\n    self,\n    skip_format_types: set | None = None,\n    force_format_types: set | None = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary that used as dependency context.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n    if skip_format_types is None:\n        skip_format_types = set()\n    skip_format_types.update({BytesIO, BaseModel, bytes})\n\n    if force_format_types is None:\n        force_format_types = set()\n    force_format_types.update({RunnableResult, RunnableResultError})\n\n    return self.to_dict(skip_format_types, force_format_types, **kwargs)\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_dict","title":"<code>to_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_dict(\n    self,\n    skip_format_types: set | None = None,\n    force_format_types: set | None = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n\n    data = {\n        \"status\": self.status.value,\n        \"input\": format_value(self.input, skip_format_types, force_format_types),\n        \"output\": format_value(self.output, skip_format_types, force_format_types),\n    }\n    if self.error:\n        data[\"error\"] = format_value(self.error, skip_format_types, force_format_types)\n\n    return data\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_tracing_depend_dict","title":"<code>to_tracing_depend_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_tracing_depend_dict(\n    self, skip_format_types: set | None = None, force_format_types: set | None = None, **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n\n    depend_dict = self.to_depend_dict(skip_format_types, force_format_types, **kwargs)\n    depend_dict.pop(\"input\", None)\n\n    return depend_dict\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResultError","title":"<code>RunnableResultError</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableResultError(BaseModel):\n    type: type[Exception]\n    message: str\n    recoverable: bool = False\n    failed_nodes: list[RunnableFailedNodeInfo] = []\n\n    @classmethod\n    def from_exception(\n        cls, exception: Exception, recoverable: bool = False, failed_nodes: list[RunnableFailedNodeInfo] | None = None\n    ) -&gt; \"RunnableResultError\":\n        return cls(\n            type=type(exception),\n            message=str(exception),\n            recoverable=recoverable,\n            failed_nodes=failed_nodes or [],\n        )\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResultError instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResultError.\n        \"\"\"\n        data = self.model_dump(**kwargs)\n        data[\"type\"] = self.type.__name__\n        data[\"failed_nodes\"] = [node.to_dict() for node in self.failed_nodes]\n        return data\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResultError.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the RunnableResultError instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResultError.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResultError instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResultError.\n    \"\"\"\n    data = self.model_dump(**kwargs)\n    data[\"type\"] = self.type.__name__\n    data[\"failed_nodes\"] = [node.to_dict() for node in self.failed_nodes]\n    return data\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableStatus","title":"<code>RunnableStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of possible statuses for a Runnable object.</p> <p>Attributes:</p> Name Type Description <code>UNDEFINED</code> <p>Undefined status.</p> <code>FAILURE</code> <p>Failure status.</p> <code>SUCCESS</code> <p>Success status.</p> <code>SKIP</code> <p>Skip status.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableStatus(str, Enum):\n    \"\"\"\n    Enumeration of possible statuses for a Runnable object.\n\n    Attributes:\n        UNDEFINED: Undefined status.\n        FAILURE: Failure status.\n        SUCCESS: Success status.\n        SKIP: Skip status.\n    \"\"\"\n\n    UNDEFINED = \"undefined\"\n    FAILURE = \"failure\"\n    SUCCESS = \"success\"\n    SKIP = \"skip\"\n</code></pre>"},{"location":"dynamiq/sandboxes/base/","title":"Base","text":"<p>Base sandbox interface and common data structures.</p>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox","title":"<code>Sandbox</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for sandbox implementations.</p> <p>This interface provides a unified way to interact with different sandbox backends (in-memory, file system, E2B, Docker, etc.). Sandboxes provide file storage and can be extended to support code execution and other isolated environment capabilities.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>class Sandbox(abc.ABC, BaseModel):\n    \"\"\"Abstract base class for sandbox implementations.\n\n    This interface provides a unified way to interact with different\n    sandbox backends (in-memory, file system, E2B, Docker, etc.).\n    Sandboxes provide file storage and can be extended to support\n    code execution and other isolated environment capabilities.\n    \"\"\"\n\n    connection: BaseConnection | None = Field(default=None, description=\"Connection to the sandbox backend.\")\n    base_path: str = Field(default=\"/home/user\", description=\"Base path in the sandbox filesystem.\")\n    max_output_files: int = Field(\n        default=50, description=\"Maximum number of files to collect from the output directory.\"\n    )\n    _clone_shared: ClassVar[bool] = True\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        \"\"\"Returns the backend type as a string.\"\"\"\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {\"connection\": True}\n\n    def _resolve_path(self, file_path: str) -&gt; str:\n        \"\"\"Resolve relative file paths against sandbox base path.\"\"\"\n        if file_path.startswith(\"/\"):\n            return file_path\n        return f\"{self.base_path.rstrip('/')}/{file_path.lstrip('/')}\"\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Convert the Sandbox instance to a dictionary.\n\n        Args:\n            for_tracing: If True, exclude sensitive fields like connection credentials.\n\n        Returns:\n            dict: Dictionary representation of the Sandbox instance.\n        \"\"\"\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        kwargs.pop(\"include_secure_params\", None)\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n\n        has_connection = getattr(self, \"connection\", None) is not None\n        data = self.model_dump(exclude=exclude, **kwargs)\n        data[\"type\"] = self.type\n\n        if has_connection:\n            data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing, **kwargs)\n\n        return data\n\n    def run_command_shell(\n        self,\n        command: str,\n        timeout: int = 60,\n        run_in_background_enabled: bool = False,\n    ) -&gt; ShellCommandResult:\n        \"\"\"Execute a shell command in the sandbox.\n\n        This is an optional capability. Subclasses that support command execution\n        should override this method. The base implementation raises NotImplementedError.\n\n        Args:\n            command: Shell command or script to execute.\n            timeout: Timeout in seconds (default 60).\n            run_in_background_enabled: If True, run command in background (no output).\n\n        Returns:\n            ShellCommandResult with stdout, stderr, and exit_code.\n\n        Raises:\n            NotImplementedError: If the sandbox does not support command execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support command execution. \"\n            \"Use a sandbox backend that supports shell commands (e.g., E2BSandbox).\"\n        )\n\n    @abc.abstractmethod\n    def get_tools(self, llm: Any = None) -&gt; list[Node]:\n        \"\"\"Return tools this sandbox provides for agent use.\n\n        Subclasses must implement this method to return tools specific\n        to their sandbox type. Tools are configured via the `tools` field.\n\n        Args:\n            llm: Optional LLM instance passed to tools that require one (e.g. FileReadTool).\n\n        Returns:\n            List of tool instances (Node objects).\n        \"\"\"\n        ...\n\n    def upload_file(\n        self,\n        file_name: str,\n        content: bytes,\n        destination_path: str | None = None,\n        ensure_parent_dirs: bool = True,\n    ) -&gt; str:\n        \"\"\"Upload a file to the sandbox.\n\n        Args:\n            file_name: Name of the file.\n            content: File content as bytes.\n            destination_path: Optional destination path in sandbox. If None, uses base_path/file_name.\n            ensure_parent_dirs: When True, create parent directories with mkdir -p before upload\n                so existing directories do not cause errors (e.g. re-ingesting skills).\n\n        Returns:\n            The path where the file was uploaded in the sandbox.\n\n        Raises:\n            NotImplementedError: If the sandbox does not support file uploads.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support file uploads. \"\n            \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n        )\n\n    def list_files(self, target_dir: str | None = None) -&gt; list[str]:\n        \"\"\"List files in the sandbox directory.\n\n        Args:\n            target_dir: Directory to list. Defaults to the output directory.\n\n        Implementations should respect ``max_output_files`` when scanning\n        for files.\n\n        Returns:\n            List of absolute file paths found in the directory.\n\n        Raises:\n            NotImplementedError: If the sandbox does not support file listing.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support file listing. \"\n            \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n        )\n\n    def collect_files(self, target_dir: str | None = None, file_paths: list[str] | None = None) -&gt; list[io.BytesIO]:\n        \"\"\"Collect files from the sandbox directory as BytesIO objects.\n\n        Args:\n            target_dir: Directory to collect files from. Defaults to the base path.\n            file_paths: List of file paths to collect. If None, all files in the target directory are collected.\n\n        Returns:\n            List of BytesIO objects with name, description, and content_type attributes.\n\n        Raises:\n            FileNotFoundError: If explicit ``file_paths`` were requested and any\n                of them could not be retrieved.\n        \"\"\"\n        file_paths_requested = bool(file_paths)\n\n        if file_paths_requested:\n            resolved: list[str] = []\n            for file_path in file_paths:\n                if not file_path.startswith(\"/\"):\n                    file_path = f\"{self.base_path.rstrip('/')}/{file_path.lstrip('/')}\"\n                resolved.append(file_path)\n            file_paths = resolved\n\n        if not file_paths:\n            file_paths = self.list_files(target_dir=target_dir)\n\n        if not file_paths:\n            return []\n\n        result_files: list[io.BytesIO] = []\n        for file_path in file_paths:\n            file_name = file_path.rsplit(\"/\", 1)[-1] if \"/\" in file_path else file_path\n            try:\n                content = self.retrieve(file_path)\n                content_type = mimetypes.guess_type(file_name)[0] or \"application/octet-stream\"\n\n                file_bytesio = io.BytesIO(content)\n                file_bytesio.name = file_name\n                file_bytesio.description = f\"Generated file from sandbox: {file_path}\"\n                file_bytesio.content_type = content_type\n                file_bytesio.seek(0)\n\n                result_files.append(file_bytesio)\n            except Exception as e:\n                if file_paths_requested:\n                    raise FileNotFoundError(f\"Failed to download requested file '{file_path}': {e}\") from e\n                logging.getLogger(__name__).warning(f\"Failed to download file '{file_path}': {e}\")\n                continue\n\n        return result_files\n\n    def exists(self, file_path: str) -&gt; bool:\n        \"\"\"Check whether a file exists in the sandbox filesystem.\n\n        Required for FileReadTool compatibility.\n\n        Args:\n            file_path: Path to the file (relative or absolute).\n\n        Returns:\n            True if the file exists, False otherwise.\n\n        Raises:\n            NotImplementedError: If the sandbox does not support file existence checks.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support file existence checks. \"\n            \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n        )\n\n    def retrieve(self, file_path: str) -&gt; bytes:\n        \"\"\"Read file content from the sandbox filesystem.\n\n        Required for FileReadTool compatibility.\n\n        Args:\n            file_path: Path to the file (relative or absolute).\n\n        Returns:\n            The file content as bytes.\n\n        Raises:\n            NotImplementedError: If the sandbox does not support file retrieval.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support file retrieval. \"\n            \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n        )\n\n    def get_sandbox_info(self, port: int | None = None) -&gt; SandboxInfo:\n        \"\"\"Return sandbox metadata for the agent (e.g. base_path, optional public URL for a port).\n\n        Subclasses that support a public URL (e.g. E2B) may override and include\n        sandbox_id, public_host, and public_url when port is provided.\n\n        Args:\n            port: Optional port number; if provided and the backend supports it,\n                the returned schema may include public_host and public_url.\n\n        Returns:\n            SandboxInfo with at least base_path; backends may add\n            sandbox_id, public_host, public_url (when port is given), etc.\n        \"\"\"\n        return SandboxInfo(\n            base_path=self.base_path,\n        )\n\n    def store(\n        self,\n        file_path: str | Path,\n        content: str | bytes | BinaryIO,\n        content_type: str = None,\n        metadata: dict[str, Any] = None,\n        overwrite: bool = False,\n    ) -&gt; FileInfo:\n        \"\"\"Store a file in the sandbox filesystem.\n\n        Provides FileStore-compatible write interface so tools like\n        FileWriteTool work transparently with both backends.\n\n        Args:\n            file_path: Destination path (relative paths resolved against base_path).\n            content: File content as string, bytes, or file-like object.\n            content_type: MIME type of the file content.\n            metadata: Additional metadata to attach to the returned FileInfo.\n            overwrite: Ignored for sandbox (always overwrites).\n\n        Returns:\n            FileInfo with details about the stored file.\n        \"\"\"\n        resolved_path = self._resolve_path(str(file_path))\n\n        if isinstance(content, str):\n            raw = content.encode(\"utf-8\")\n        elif hasattr(content, \"read\"):\n            raw = content.read()\n            if isinstance(raw, str):\n                raw = raw.encode(\"utf-8\")\n        else:\n            raw = content\n\n        file_name = resolved_path.rsplit(\"/\", 1)[-1] if \"/\" in resolved_path else resolved_path\n        dest = self.upload_file(file_name, raw, destination_path=resolved_path)\n\n        return FileInfo(\n            name=file_name,\n            path=dest,\n            size=len(raw),\n            content_type=content_type or mimetypes.guess_type(file_name)[0] or \"application/octet-stream\",\n            metadata=metadata or {},\n            content=raw,\n        )\n\n    def close(self) -&gt; None:\n        \"\"\"Close the sandbox.\"\"\"\n        raise NotImplementedError(f\"Implementation of close() is not implemented for {self.__class__.__name__}\")\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.type","title":"<code>type: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the backend type as a string.</p>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.close","title":"<code>close()</code>","text":"<p>Close the sandbox.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the sandbox.\"\"\"\n    raise NotImplementedError(f\"Implementation of close() is not implemented for {self.__class__.__name__}\")\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.collect_files","title":"<code>collect_files(target_dir=None, file_paths=None)</code>","text":"<p>Collect files from the sandbox directory as BytesIO objects.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>str | None</code> <p>Directory to collect files from. Defaults to the base path.</p> <code>None</code> <code>file_paths</code> <code>list[str] | None</code> <p>List of file paths to collect. If None, all files in the target directory are collected.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[BytesIO]</code> <p>List of BytesIO objects with name, description, and content_type attributes.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If explicit <code>file_paths</code> were requested and any of them could not be retrieved.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def collect_files(self, target_dir: str | None = None, file_paths: list[str] | None = None) -&gt; list[io.BytesIO]:\n    \"\"\"Collect files from the sandbox directory as BytesIO objects.\n\n    Args:\n        target_dir: Directory to collect files from. Defaults to the base path.\n        file_paths: List of file paths to collect. If None, all files in the target directory are collected.\n\n    Returns:\n        List of BytesIO objects with name, description, and content_type attributes.\n\n    Raises:\n        FileNotFoundError: If explicit ``file_paths`` were requested and any\n            of them could not be retrieved.\n    \"\"\"\n    file_paths_requested = bool(file_paths)\n\n    if file_paths_requested:\n        resolved: list[str] = []\n        for file_path in file_paths:\n            if not file_path.startswith(\"/\"):\n                file_path = f\"{self.base_path.rstrip('/')}/{file_path.lstrip('/')}\"\n            resolved.append(file_path)\n        file_paths = resolved\n\n    if not file_paths:\n        file_paths = self.list_files(target_dir=target_dir)\n\n    if not file_paths:\n        return []\n\n    result_files: list[io.BytesIO] = []\n    for file_path in file_paths:\n        file_name = file_path.rsplit(\"/\", 1)[-1] if \"/\" in file_path else file_path\n        try:\n            content = self.retrieve(file_path)\n            content_type = mimetypes.guess_type(file_name)[0] or \"application/octet-stream\"\n\n            file_bytesio = io.BytesIO(content)\n            file_bytesio.name = file_name\n            file_bytesio.description = f\"Generated file from sandbox: {file_path}\"\n            file_bytesio.content_type = content_type\n            file_bytesio.seek(0)\n\n            result_files.append(file_bytesio)\n        except Exception as e:\n            if file_paths_requested:\n                raise FileNotFoundError(f\"Failed to download requested file '{file_path}': {e}\") from e\n            logging.getLogger(__name__).warning(f\"Failed to download file '{file_path}': {e}\")\n            continue\n\n    return result_files\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.exists","title":"<code>exists(file_path)</code>","text":"<p>Check whether a file exists in the sandbox filesystem.</p> <p>Required for FileReadTool compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file (relative or absolute).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, False otherwise.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the sandbox does not support file existence checks.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def exists(self, file_path: str) -&gt; bool:\n    \"\"\"Check whether a file exists in the sandbox filesystem.\n\n    Required for FileReadTool compatibility.\n\n    Args:\n        file_path: Path to the file (relative or absolute).\n\n    Returns:\n        True if the file exists, False otherwise.\n\n    Raises:\n        NotImplementedError: If the sandbox does not support file existence checks.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not support file existence checks. \"\n        \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.get_sandbox_info","title":"<code>get_sandbox_info(port=None)</code>","text":"<p>Return sandbox metadata for the agent (e.g. base_path, optional public URL for a port).</p> <p>Subclasses that support a public URL (e.g. E2B) may override and include sandbox_id, public_host, and public_url when port is provided.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int | None</code> <p>Optional port number; if provided and the backend supports it, the returned schema may include public_host and public_url.</p> <code>None</code> <p>Returns:</p> Type Description <code>SandboxInfo</code> <p>SandboxInfo with at least base_path; backends may add</p> <code>SandboxInfo</code> <p>sandbox_id, public_host, public_url (when port is given), etc.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def get_sandbox_info(self, port: int | None = None) -&gt; SandboxInfo:\n    \"\"\"Return sandbox metadata for the agent (e.g. base_path, optional public URL for a port).\n\n    Subclasses that support a public URL (e.g. E2B) may override and include\n    sandbox_id, public_host, and public_url when port is provided.\n\n    Args:\n        port: Optional port number; if provided and the backend supports it,\n            the returned schema may include public_host and public_url.\n\n    Returns:\n        SandboxInfo with at least base_path; backends may add\n        sandbox_id, public_host, public_url (when port is given), etc.\n    \"\"\"\n    return SandboxInfo(\n        base_path=self.base_path,\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.get_tools","title":"<code>get_tools(llm=None)</code>  <code>abstractmethod</code>","text":"<p>Return tools this sandbox provides for agent use.</p> <p>Subclasses must implement this method to return tools specific to their sandbox type. Tools are configured via the <code>tools</code> field.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Any</code> <p>Optional LLM instance passed to tools that require one (e.g. FileReadTool).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of tool instances (Node objects).</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>@abc.abstractmethod\ndef get_tools(self, llm: Any = None) -&gt; list[Node]:\n    \"\"\"Return tools this sandbox provides for agent use.\n\n    Subclasses must implement this method to return tools specific\n    to their sandbox type. Tools are configured via the `tools` field.\n\n    Args:\n        llm: Optional LLM instance passed to tools that require one (e.g. FileReadTool).\n\n    Returns:\n        List of tool instances (Node objects).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.list_files","title":"<code>list_files(target_dir=None)</code>","text":"<p>List files in the sandbox directory.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>str | None</code> <p>Directory to list. Defaults to the output directory.</p> <code>None</code> <p>Implementations should respect <code>max_output_files</code> when scanning for files.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of absolute file paths found in the directory.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the sandbox does not support file listing.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def list_files(self, target_dir: str | None = None) -&gt; list[str]:\n    \"\"\"List files in the sandbox directory.\n\n    Args:\n        target_dir: Directory to list. Defaults to the output directory.\n\n    Implementations should respect ``max_output_files`` when scanning\n    for files.\n\n    Returns:\n        List of absolute file paths found in the directory.\n\n    Raises:\n        NotImplementedError: If the sandbox does not support file listing.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not support file listing. \"\n        \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.retrieve","title":"<code>retrieve(file_path)</code>","text":"<p>Read file content from the sandbox filesystem.</p> <p>Required for FileReadTool compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file (relative or absolute).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The file content as bytes.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the sandbox does not support file retrieval.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def retrieve(self, file_path: str) -&gt; bytes:\n    \"\"\"Read file content from the sandbox filesystem.\n\n    Required for FileReadTool compatibility.\n\n    Args:\n        file_path: Path to the file (relative or absolute).\n\n    Returns:\n        The file content as bytes.\n\n    Raises:\n        NotImplementedError: If the sandbox does not support file retrieval.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not support file retrieval. \"\n        \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.run_command_shell","title":"<code>run_command_shell(command, timeout=60, run_in_background_enabled=False)</code>","text":"<p>Execute a shell command in the sandbox.</p> <p>This is an optional capability. Subclasses that support command execution should override this method. The base implementation raises NotImplementedError.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>Shell command or script to execute.</p> required <code>timeout</code> <code>int</code> <p>Timeout in seconds (default 60).</p> <code>60</code> <code>run_in_background_enabled</code> <code>bool</code> <p>If True, run command in background (no output).</p> <code>False</code> <p>Returns:</p> Type Description <code>ShellCommandResult</code> <p>ShellCommandResult with stdout, stderr, and exit_code.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the sandbox does not support command execution.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def run_command_shell(\n    self,\n    command: str,\n    timeout: int = 60,\n    run_in_background_enabled: bool = False,\n) -&gt; ShellCommandResult:\n    \"\"\"Execute a shell command in the sandbox.\n\n    This is an optional capability. Subclasses that support command execution\n    should override this method. The base implementation raises NotImplementedError.\n\n    Args:\n        command: Shell command or script to execute.\n        timeout: Timeout in seconds (default 60).\n        run_in_background_enabled: If True, run command in background (no output).\n\n    Returns:\n        ShellCommandResult with stdout, stderr, and exit_code.\n\n    Raises:\n        NotImplementedError: If the sandbox does not support command execution.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not support command execution. \"\n        \"Use a sandbox backend that supports shell commands (e.g., E2BSandbox).\"\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.store","title":"<code>store(file_path, content, content_type=None, metadata=None, overwrite=False)</code>","text":"<p>Store a file in the sandbox filesystem.</p> <p>Provides FileStore-compatible write interface so tools like FileWriteTool work transparently with both backends.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Destination path (relative paths resolved against base_path).</p> required <code>content</code> <code>str | bytes | BinaryIO</code> <p>File content as string, bytes, or file-like object.</p> required <code>content_type</code> <code>str</code> <p>MIME type of the file content.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata to attach to the returned FileInfo.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Ignored for sandbox (always overwrites).</p> <code>False</code> <p>Returns:</p> Type Description <code>FileInfo</code> <p>FileInfo with details about the stored file.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def store(\n    self,\n    file_path: str | Path,\n    content: str | bytes | BinaryIO,\n    content_type: str = None,\n    metadata: dict[str, Any] = None,\n    overwrite: bool = False,\n) -&gt; FileInfo:\n    \"\"\"Store a file in the sandbox filesystem.\n\n    Provides FileStore-compatible write interface so tools like\n    FileWriteTool work transparently with both backends.\n\n    Args:\n        file_path: Destination path (relative paths resolved against base_path).\n        content: File content as string, bytes, or file-like object.\n        content_type: MIME type of the file content.\n        metadata: Additional metadata to attach to the returned FileInfo.\n        overwrite: Ignored for sandbox (always overwrites).\n\n    Returns:\n        FileInfo with details about the stored file.\n    \"\"\"\n    resolved_path = self._resolve_path(str(file_path))\n\n    if isinstance(content, str):\n        raw = content.encode(\"utf-8\")\n    elif hasattr(content, \"read\"):\n        raw = content.read()\n        if isinstance(raw, str):\n            raw = raw.encode(\"utf-8\")\n    else:\n        raw = content\n\n    file_name = resolved_path.rsplit(\"/\", 1)[-1] if \"/\" in resolved_path else resolved_path\n    dest = self.upload_file(file_name, raw, destination_path=resolved_path)\n\n    return FileInfo(\n        name=file_name,\n        path=dest,\n        size=len(raw),\n        content_type=content_type or mimetypes.guess_type(file_name)[0] or \"application/octet-stream\",\n        metadata=metadata or {},\n        content=raw,\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the Sandbox instance to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>for_tracing</code> <p>If True, exclude sensitive fields like connection credentials.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary representation of the Sandbox instance.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Convert the Sandbox instance to a dictionary.\n\n    Args:\n        for_tracing: If True, exclude sensitive fields like connection credentials.\n\n    Returns:\n        dict: Dictionary representation of the Sandbox instance.\n    \"\"\"\n    for_tracing = kwargs.pop(\"for_tracing\", False)\n    kwargs.pop(\"include_secure_params\", None)\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n\n    has_connection = getattr(self, \"connection\", None) is not None\n    data = self.model_dump(exclude=exclude, **kwargs)\n    data[\"type\"] = self.type\n\n    if has_connection:\n        data[\"connection\"] = self.connection.to_dict(for_tracing=for_tracing, **kwargs)\n\n    return data\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.Sandbox.upload_file","title":"<code>upload_file(file_name, content, destination_path=None, ensure_parent_dirs=True)</code>","text":"<p>Upload a file to the sandbox.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file.</p> required <code>content</code> <code>bytes</code> <p>File content as bytes.</p> required <code>destination_path</code> <code>str | None</code> <p>Optional destination path in sandbox. If None, uses base_path/file_name.</p> <code>None</code> <code>ensure_parent_dirs</code> <code>bool</code> <p>When True, create parent directories with mkdir -p before upload so existing directories do not cause errors (e.g. re-ingesting skills).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the file was uploaded in the sandbox.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the sandbox does not support file uploads.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def upload_file(\n    self,\n    file_name: str,\n    content: bytes,\n    destination_path: str | None = None,\n    ensure_parent_dirs: bool = True,\n) -&gt; str:\n    \"\"\"Upload a file to the sandbox.\n\n    Args:\n        file_name: Name of the file.\n        content: File content as bytes.\n        destination_path: Optional destination path in sandbox. If None, uses base_path/file_name.\n        ensure_parent_dirs: When True, create parent directories with mkdir -p before upload\n            so existing directories do not cause errors (e.g. re-ingesting skills).\n\n    Returns:\n        The path where the file was uploaded in the sandbox.\n\n    Raises:\n        NotImplementedError: If the sandbox does not support file uploads.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} does not support file uploads. \"\n        \"Use a sandbox backend that supports file operations (e.g., E2BSandbox).\"\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.SandboxConfig","title":"<code>SandboxConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for sandbox and related features.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether sandbox is enabled.</p> <code>backend</code> <code>Sandbox</code> <p>The sandbox backend to use.</p> <code>config</code> <code>dict[str, Any]</code> <p>Additional configuration options.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>class SandboxConfig(BaseModel):\n    \"\"\"Configuration for sandbox and related features.\n\n    Attributes:\n        enabled: Whether sandbox is enabled.\n        backend: The sandbox backend to use.\n        config: Additional configuration options.\n    \"\"\"\n\n    enabled: bool = False\n    backend: Sandbox = Field(..., description=\"Sandbox backend to use.\")\n    config: dict[str, Any] = Field(default_factory=dict)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {\"backend\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Convert the SandboxConfig instance to a dictionary.\"\"\"\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        kwargs.pop(\"include_secure_params\", None)\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n        config_data = self.model_dump(exclude=exclude, **kwargs)\n        config_data[\"backend\"] = self.backend.to_dict(for_tracing=for_tracing, **kwargs)\n        return config_data\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.SandboxConfig.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.SandboxConfig.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the SandboxConfig instance to a dictionary.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Convert the SandboxConfig instance to a dictionary.\"\"\"\n    for_tracing = kwargs.pop(\"for_tracing\", False)\n    kwargs.pop(\"include_secure_params\", None)\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n    config_data = self.model_dump(exclude=exclude, **kwargs)\n    config_data[\"backend\"] = self.backend.to_dict(for_tracing=for_tracing, **kwargs)\n    return config_data\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.SandboxInfo","title":"<code>SandboxInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for sandbox metadata returned by get_sandbox_info().</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>class SandboxInfo(BaseModel):\n    \"\"\"Schema for sandbox metadata returned by get_sandbox_info().\"\"\"\n\n    base_path: str\n    sandbox_id: str | None = None\n    public_host: str | None = None\n    public_url: str | None = None\n    public_url_error: str | None = None\n\n    model_config = ConfigDict(extra=\"allow\")\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.SandboxTool","title":"<code>SandboxTool</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for sandbox tool types.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>class SandboxTool(str, Enum):\n    \"\"\"Enum for sandbox tool types.\"\"\"\n\n    SHELL = \"shell\"\n</code></pre>"},{"location":"dynamiq/sandboxes/base/#dynamiq.sandboxes.base.ShellCommandResult","title":"<code>ShellCommandResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of a shell command execution.</p> Source code in <code>dynamiq/sandboxes/base.py</code> <pre><code>class ShellCommandResult(BaseModel):\n    \"\"\"Result of a shell command execution.\"\"\"\n\n    stdout: str\n    stderr: str\n    exit_code: int | None\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/","title":"E2b","text":"<p>E2B sandbox implementation.</p>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox","title":"<code>E2BSandbox</code>","text":"<p>               Bases: <code>Sandbox</code></p> <p>E2B sandbox implementation.</p> <p>This implementation stores files in E2B remote sandbox filesystem. Files persist for the lifetime of the sandbox session.</p> <p>Supports reconnecting to existing sandboxes by providing sandbox_id.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>class E2BSandbox(Sandbox):\n    \"\"\"E2B sandbox implementation.\n\n    This implementation stores files in E2B remote sandbox filesystem.\n    Files persist for the lifetime of the sandbox session.\n\n    Supports reconnecting to existing sandboxes by providing sandbox_id.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    connection: E2B\n    timeout: int = 3600\n\n    template: str | None = Field(\n        default=None, description=\"Template to use for sandbox creation. \" \"If None, the default template is used.\"\n    )\n    sandbox_id: str | None = Field(\n        default=None,\n        description=\"Existing sandbox ID to reconnect to. If None, a new sandbox is created.\",\n    )\n    creation_error_handling: SandboxCreationErrorHandling = Field(\n        default_factory=SandboxCreationErrorHandling,\n        description=\"Retry and backoff config for sandbox creation and reconnection (rate-limit and transient errors).\",\n    )\n    _sandbox: E2BDesktopSandbox | None = PrivateAttr(default=None)\n    _sandbox_lock: threading.Lock = PrivateAttr(default_factory=threading.Lock)\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the E2B sandbox storage.\"\"\"\n        super().__init__(**kwargs)\n\n    @property\n    def current_sandbox_id(self) -&gt; str | None:\n        \"\"\"Get the current sandbox ID (for saving/reconnecting later).\"\"\"\n        return self.sandbox_id\n\n    def get_public_host(self, port: int) -&gt; str:\n        \"\"\"Return the public host for a given port so the sandbox can be reached at https://{host}.\n\n        E2B exposes each sandbox at a URL like https://{port}-{sandbox_id}.e2b.app.\n        Use this when the agent starts a server in the sandbox (e.g. dev server on port 5173)\n        so it can report the URL to the user.\n\n        Args:\n            port: Port number the service listens on inside the sandbox (e.g. 3000, 5173).\n\n        Returns:\n            Host string (e.g. 3000-abc123.e2b.app). Full URL is https://{host}.\n        \"\"\"\n        self._ensure_sandbox()\n        raw = self._sandbox\n        get_host = getattr(raw, \"get_host\", None)\n        if callable(get_host):\n            try:\n                return get_host(port)\n            except Exception as e:\n                logger.debug(\"E2B get_host(port) failed, using URL pattern: %s\", e)\n        domain = getattr(self.connection, \"domain\", None) or \"e2b.app\"\n        return f\"{port}-{self.sandbox_id}.{domain}\"\n\n    def get_sandbox_info(self, port: int | None = None) -&gt; SandboxInfo:\n        \"\"\"Return sandbox metadata including optional public URL for a port.\"\"\"\n        public_host: str | None = None\n        public_url: str | None = None\n        public_url_error: str | None = None\n        if port is not None:\n            try:\n                public_host = self.get_public_host(port)  # may trigger _ensure_sandbox() and set self.sandbox_id\n                public_url = f\"https://{public_host}\"\n            except Exception as e:\n                logger.debug(\"get_public_host failed: %s\", e)\n                public_url_error = str(e)\n        return SandboxInfo(\n            base_path=self.base_path,\n            sandbox_id=self.sandbox_id,\n            public_host=public_host,\n            public_url=public_url,\n            public_url_error=public_url_error,\n        )\n\n    def _ensure_sandbox(self) -&gt; E2BDesktopSandbox:\n        \"\"\"Lazily initialize or reconnect to E2B sandbox, with retries on rate-limit and transient errors.\n\n        Uses double-checked locking so concurrent threads (e.g. parallel tool\n        calls via ThreadPoolExecutor) never create duplicate sandboxes.\n        \"\"\"\n        if self._sandbox is not None:\n            return self._sandbox\n\n        with self._sandbox_lock:\n            # Double-check after acquiring the lock\n            if self._sandbox is not None:\n                return self._sandbox\n\n            if self.sandbox_id:\n                try:\n                    self._sandbox = self._reconnect_with_retry()\n                    logger.debug(f\"E2B sandbox reconnected: {self.sandbox_id}\")\n                    self._ensure_directories()\n                    return self._sandbox\n                except Exception as e:\n                    raise SandboxConnectionError(self.sandbox_id, cause=e) from e\n\n            # Create new sandbox (no sandbox_id)\n            self._sandbox = self._create_with_retry()\n            self.sandbox_id = self._sandbox.sandbox_id\n            logger.debug(f\"E2B sandbox created: {self.sandbox_id}\")\n            self._ensure_directories()\n            return self._sandbox\n\n    def _ensure_directories(self) -&gt; None:\n        \"\"\"Create the base directory inside the sandbox if it does not exist.\"\"\"\n        if self._sandbox is None:\n            return\n        try:\n            self._sandbox.commands.run(f\"mkdir -p {shlex.quote(self.base_path)}\")\n            logger.debug(f\"E2BSandbox ensured directory exists: {self.base_path}\")\n        except Exception as e:\n            logger.warning(f\"E2BSandbox failed to create directory: {e}\")\n\n    def _reconnect_with_retry(self) -&gt; E2BDesktopSandbox:\n        \"\"\"Reconnect to existing sandbox with exponential backoff on rate-limit.\"\"\"\n        cfg = self.creation_error_handling\n\n        @retry(\n            retry=retry_if_exception_type(E2BRateLimitException),\n            stop=stop_after_attempt(cfg.max_retries),\n            wait=wait_exponential_jitter(\n                initial=cfg.initial_wait_seconds,\n                max=cfg.max_wait_seconds,\n                exp_base=cfg.exponential_base,\n                jitter=cfg.jitter,\n            ),\n            reraise=True,\n        )\n        def connect() -&gt; E2BDesktopSandbox:\n            logger.debug(f\"Reconnecting to E2B sandbox: {self.sandbox_id}\")\n            return E2BDesktopSandbox.connect(\n                sandbox_id=self.sandbox_id,\n                api_key=self.connection.api_key,\n                domain=getattr(self.connection, \"domain\", None),\n            )\n\n        return connect()\n\n    def _create_with_retry(self) -&gt; E2BDesktopSandbox:\n        \"\"\"Create a new sandbox with exponential backoff on rate-limit.\"\"\"\n        cfg = self.creation_error_handling\n\n        @retry(\n            retry=retry_if_exception_type(E2BRateLimitException),\n            stop=stop_after_attempt(cfg.max_retries),\n            wait=wait_exponential_jitter(\n                initial=cfg.initial_wait_seconds,\n                max=cfg.max_wait_seconds,\n                exp_base=cfg.exponential_base,\n                jitter=cfg.jitter,\n            ),\n            reraise=True,\n        )\n        def create() -&gt; E2BDesktopSandbox:\n            try:\n                return E2BDesktopSandbox.create(\n                    template=self.template,\n                    api_key=self.connection.api_key,\n                    timeout=self.timeout,\n                    domain=getattr(self.connection, \"domain\", None),\n                )\n            except E2BRateLimitException:\n                logger.warning(\"E2B sandbox creation rate-limited. Retrying with exponential backoff.\")\n                raise\n\n        return create()\n\n    def run_command_shell(\n        self,\n        command: str,\n        timeout: int = 60,\n        run_in_background_enabled: bool = False,\n    ) -&gt; ShellCommandResult:\n        \"\"\"Execute a shell command in the E2B sandbox.\n\n        Args:\n            command: Shell command or script to execute.\n            timeout: Timeout in seconds (default 60).\n            background: If True, run command in background (no output).\n\n        Returns:\n            ShellCommandResult with stdout, stderr, and exit_code.\n        \"\"\"\n        sandbox = self._ensure_sandbox()\n        logger.debug(f\"E2BSandbox running command: {command[:100]}...\")\n\n        try:\n            if run_in_background_enabled:\n                sandbox.commands.run(command, background=True)\n                return ShellCommandResult(\n                    stdout=f\"Command started in background: {command}\",\n                    stderr=\"\",\n                    exit_code=0,\n                )\n\n            result = sandbox.commands.run(command, timeout=timeout)\n            return ShellCommandResult(\n                stdout=result.stdout or \"\",\n                stderr=result.stderr or \"\",\n                exit_code=getattr(result, \"exit_code\", None),\n            )\n        except Exception as e:\n            logger.error(f\"Command execution failed: {e}\")\n            return ShellCommandResult(\n                stdout=\"\",\n                stderr=str(e),\n                exit_code=1,\n            )\n\n    def upload_file(\n        self,\n        file_name: str,\n        content: bytes,\n        destination_path: str | None = None,\n        ensure_parent_dirs: bool = True,\n    ) -&gt; str:\n        \"\"\"Upload a file to the E2B sandbox.\n\n        When ensure_parent_dirs is True, parent directories are created with ``mkdir -p``\n        before writing so that existing directories (e.g. from a previous skill ingestion)\n        do not cause 500 errors from backends that use non-idempotent mkdir.\n\n        Args:\n            file_name: Name of the file.\n            content: File content as bytes.\n            destination_path: Optional destination path in sandbox. If None, uses base_path/file_name.\n            ensure_parent_dirs: When True, run mkdir -p for the destination's parent before write.\n\n        Returns:\n            The path where the file was uploaded in the sandbox.\n        \"\"\"\n        sandbox = self._ensure_sandbox()\n\n        if destination_path is None:\n            destination_path = f\"{self.base_path}/{file_name}\"\n\n        if ensure_parent_dirs and destination_path:\n            parent = destination_path.rsplit(\"/\", 1)[0]\n            if parent and parent != destination_path:\n                try:\n                    sandbox.commands.run(f\"mkdir -p {shlex.quote(parent)}\")\n                    logger.debug(f\"E2BSandbox ensured parent dir: {parent}\")\n                except Exception as e:\n                    logger.warning(f\"E2BSandbox mkdir -p for {parent!r} failed (continuing): {e}\")\n\n        try:\n            sandbox.files.write(destination_path, content)\n            logger.debug(f\"E2BSandbox uploaded file: {destination_path}\")\n            return destination_path\n        except Exception as e:\n            logger.error(f\"Failed to upload file {file_name}: {e}\")\n            raise\n\n    def list_files(self, target_dir: str | None = None) -&gt; list[str]:\n        \"\"\"List files in the E2B sandbox directory.\n\n        Searches for files in the given directory (defaults to output directory),\n        returning at most ``max_output_files`` file paths.\n\n        Returns:\n            List of absolute file paths found in the directory.\n        \"\"\"\n        sandbox = self._ensure_sandbox()\n        if target_dir is None:\n            target_dir = self.base_path\n\n        try:\n            # Check if the directory exists\n            check_cmd = f\"test -d {shlex.quote(target_dir)} &amp;&amp; echo exists\"\n            check_result = sandbox.commands.run(check_cmd)\n            if check_result.exit_code != 0 or \"exists\" not in (check_result.stdout or \"\"):\n                return []\n\n            # List files recursively (configurable limit)\n            cmd = f\"find {shlex.quote(target_dir)} -type f 2&gt;/dev/null | head -{self.max_output_files}\"\n            result = sandbox.commands.run(cmd)\n            if result.exit_code != 0 or not (result.stdout or \"\").strip():\n                return []\n\n            return [f.strip() for f in result.stdout.splitlines() if f.strip()]\n\n        except Exception as e:\n            logger.warning(f\"E2BSandbox list_files failed for {target_dir}: {e}\")\n            return []\n\n    def exists(self, file_path: str) -&gt; bool:\n        \"\"\"Return True when file exists in sandbox filesystem.\"\"\"\n        try:\n            sandbox = self._ensure_sandbox()\n            resolved_path = self._resolve_path(file_path)\n            check_cmd = f\"test -f {shlex.quote(resolved_path)}\"\n            result = sandbox.commands.run(check_cmd)\n            return getattr(result, \"exit_code\", 1) == 0\n        except Exception as e:\n            logger.debug(f\"E2BSandbox exists({file_path}) failed (treating as missing): {e}\")\n            return False\n\n    def retrieve(self, file_path: str) -&gt; bytes:\n        \"\"\"Read file bytes from sandbox filesystem.\"\"\"\n        sandbox = self._ensure_sandbox()\n        resolved_path = self._resolve_path(file_path)\n        return sandbox.files.read(resolved_path, \"bytes\")\n\n    def get_tools(self, llm: Any = None) -&gt; list[Node]:\n        \"\"\"Return tools this sandbox provides for agent use.\n\n        Creates tools based on tools config and TOOL_REGISTRY.\n        Each tool is enabled by default unless explicitly disabled.\n\n        Args:\n            llm: Optional LLM instance passed to tools that require one (e.g. FileReadTool).\n\n        Returns:\n            List of tool instances (Node objects).\n        \"\"\"\n        from dynamiq.nodes.tools.file_tools import FileReadTool, FileWriteTool\n        from dynamiq.nodes.tools.todo_tools import TodoWriteTool\n        from dynamiq.sandboxes.tools.sandbox_info import SandboxInfoTool\n        from dynamiq.sandboxes.tools.shell import SandboxShellTool\n\n        tools = [\n            SandboxShellTool(sandbox=self),\n            FileWriteTool(file_store=self, absolute_file_paths_allowed=True),\n            TodoWriteTool(file_store=self),\n            SandboxInfoTool(sandbox=self),\n        ]\n        if llm is not None:\n            tools.append(FileReadTool(file_store=self, llm=llm, absolute_file_paths_allowed=True))\n\n        return tools\n\n    def close(self, kill: bool = False) -&gt; None:\n        \"\"\"Close the E2B sandbox connection.\n\n        Args:\n            kill: If False (default), just disconnects\n                  but keeps the sandbox alive for reconnection using sandbox_id. If True, kills the sandbox.\n        \"\"\"\n        if self._sandbox:\n            try:\n                if kill:\n                    self._sandbox.kill()\n                    logger.debug(f\"E2B sandbox killed: {self.sandbox_id}\")\n                    self.sandbox_id = None\n                else:\n                    logger.debug(f\"E2B sandbox disconnected (kept alive): {self.sandbox_id}\")\n            except Exception as e:\n                logger.warning(f\"E2BSandbox close() failed: {e}\")\n            finally:\n                self._sandbox = None\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self._ensure_sandbox()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n\n    def __del__(self):\n        \"\"\"Destructor - attempt to close sandbox on garbage collection.\"\"\"\n        try:\n            self.close()\n        except Exception:\n            # Cannot reliably log in __del__, just suppress\n            ...  # noqa: E701\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.current_sandbox_id","title":"<code>current_sandbox_id: str | None</code>  <code>property</code>","text":"<p>Get the current sandbox ID (for saving/reconnecting later).</p>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.__del__","title":"<code>__del__()</code>","text":"<p>Destructor - attempt to close sandbox on garbage collection.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor - attempt to close sandbox on garbage collection.\"\"\"\n    try:\n        self.close()\n    except Exception:\n        # Cannot reliably log in __del__, just suppress\n        ...  # noqa: E701\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    self._ensure_sandbox()\n    return self\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.close()\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the E2B sandbox storage.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the E2B sandbox storage.\"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.close","title":"<code>close(kill=False)</code>","text":"<p>Close the E2B sandbox connection.</p> <p>Parameters:</p> Name Type Description Default <code>kill</code> <code>bool</code> <p>If False (default), just disconnects   but keeps the sandbox alive for reconnection using sandbox_id. If True, kills the sandbox.</p> <code>False</code> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def close(self, kill: bool = False) -&gt; None:\n    \"\"\"Close the E2B sandbox connection.\n\n    Args:\n        kill: If False (default), just disconnects\n              but keeps the sandbox alive for reconnection using sandbox_id. If True, kills the sandbox.\n    \"\"\"\n    if self._sandbox:\n        try:\n            if kill:\n                self._sandbox.kill()\n                logger.debug(f\"E2B sandbox killed: {self.sandbox_id}\")\n                self.sandbox_id = None\n            else:\n                logger.debug(f\"E2B sandbox disconnected (kept alive): {self.sandbox_id}\")\n        except Exception as e:\n            logger.warning(f\"E2BSandbox close() failed: {e}\")\n        finally:\n            self._sandbox = None\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.exists","title":"<code>exists(file_path)</code>","text":"<p>Return True when file exists in sandbox filesystem.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def exists(self, file_path: str) -&gt; bool:\n    \"\"\"Return True when file exists in sandbox filesystem.\"\"\"\n    try:\n        sandbox = self._ensure_sandbox()\n        resolved_path = self._resolve_path(file_path)\n        check_cmd = f\"test -f {shlex.quote(resolved_path)}\"\n        result = sandbox.commands.run(check_cmd)\n        return getattr(result, \"exit_code\", 1) == 0\n    except Exception as e:\n        logger.debug(f\"E2BSandbox exists({file_path}) failed (treating as missing): {e}\")\n        return False\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.get_public_host","title":"<code>get_public_host(port)</code>","text":"<p>Return the public host for a given port so the sandbox can be reached at https://{host}.</p> <p>E2B exposes each sandbox at a URL like https://{port}-{sandbox_id}.e2b.app. Use this when the agent starts a server in the sandbox (e.g. dev server on port 5173) so it can report the URL to the user.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port number the service listens on inside the sandbox (e.g. 3000, 5173).</p> required <p>Returns:</p> Type Description <code>str</code> <p>Host string (e.g. 3000-abc123.e2b.app). Full URL is https://{host}.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def get_public_host(self, port: int) -&gt; str:\n    \"\"\"Return the public host for a given port so the sandbox can be reached at https://{host}.\n\n    E2B exposes each sandbox at a URL like https://{port}-{sandbox_id}.e2b.app.\n    Use this when the agent starts a server in the sandbox (e.g. dev server on port 5173)\n    so it can report the URL to the user.\n\n    Args:\n        port: Port number the service listens on inside the sandbox (e.g. 3000, 5173).\n\n    Returns:\n        Host string (e.g. 3000-abc123.e2b.app). Full URL is https://{host}.\n    \"\"\"\n    self._ensure_sandbox()\n    raw = self._sandbox\n    get_host = getattr(raw, \"get_host\", None)\n    if callable(get_host):\n        try:\n            return get_host(port)\n        except Exception as e:\n            logger.debug(\"E2B get_host(port) failed, using URL pattern: %s\", e)\n    domain = getattr(self.connection, \"domain\", None) or \"e2b.app\"\n    return f\"{port}-{self.sandbox_id}.{domain}\"\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.get_sandbox_info","title":"<code>get_sandbox_info(port=None)</code>","text":"<p>Return sandbox metadata including optional public URL for a port.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def get_sandbox_info(self, port: int | None = None) -&gt; SandboxInfo:\n    \"\"\"Return sandbox metadata including optional public URL for a port.\"\"\"\n    public_host: str | None = None\n    public_url: str | None = None\n    public_url_error: str | None = None\n    if port is not None:\n        try:\n            public_host = self.get_public_host(port)  # may trigger _ensure_sandbox() and set self.sandbox_id\n            public_url = f\"https://{public_host}\"\n        except Exception as e:\n            logger.debug(\"get_public_host failed: %s\", e)\n            public_url_error = str(e)\n    return SandboxInfo(\n        base_path=self.base_path,\n        sandbox_id=self.sandbox_id,\n        public_host=public_host,\n        public_url=public_url,\n        public_url_error=public_url_error,\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.get_tools","title":"<code>get_tools(llm=None)</code>","text":"<p>Return tools this sandbox provides for agent use.</p> <p>Creates tools based on tools config and TOOL_REGISTRY. Each tool is enabled by default unless explicitly disabled.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Any</code> <p>Optional LLM instance passed to tools that require one (e.g. FileReadTool).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>List of tool instances (Node objects).</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def get_tools(self, llm: Any = None) -&gt; list[Node]:\n    \"\"\"Return tools this sandbox provides for agent use.\n\n    Creates tools based on tools config and TOOL_REGISTRY.\n    Each tool is enabled by default unless explicitly disabled.\n\n    Args:\n        llm: Optional LLM instance passed to tools that require one (e.g. FileReadTool).\n\n    Returns:\n        List of tool instances (Node objects).\n    \"\"\"\n    from dynamiq.nodes.tools.file_tools import FileReadTool, FileWriteTool\n    from dynamiq.nodes.tools.todo_tools import TodoWriteTool\n    from dynamiq.sandboxes.tools.sandbox_info import SandboxInfoTool\n    from dynamiq.sandboxes.tools.shell import SandboxShellTool\n\n    tools = [\n        SandboxShellTool(sandbox=self),\n        FileWriteTool(file_store=self, absolute_file_paths_allowed=True),\n        TodoWriteTool(file_store=self),\n        SandboxInfoTool(sandbox=self),\n    ]\n    if llm is not None:\n        tools.append(FileReadTool(file_store=self, llm=llm, absolute_file_paths_allowed=True))\n\n    return tools\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.list_files","title":"<code>list_files(target_dir=None)</code>","text":"<p>List files in the E2B sandbox directory.</p> <p>Searches for files in the given directory (defaults to output directory), returning at most <code>max_output_files</code> file paths.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of absolute file paths found in the directory.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def list_files(self, target_dir: str | None = None) -&gt; list[str]:\n    \"\"\"List files in the E2B sandbox directory.\n\n    Searches for files in the given directory (defaults to output directory),\n    returning at most ``max_output_files`` file paths.\n\n    Returns:\n        List of absolute file paths found in the directory.\n    \"\"\"\n    sandbox = self._ensure_sandbox()\n    if target_dir is None:\n        target_dir = self.base_path\n\n    try:\n        # Check if the directory exists\n        check_cmd = f\"test -d {shlex.quote(target_dir)} &amp;&amp; echo exists\"\n        check_result = sandbox.commands.run(check_cmd)\n        if check_result.exit_code != 0 or \"exists\" not in (check_result.stdout or \"\"):\n            return []\n\n        # List files recursively (configurable limit)\n        cmd = f\"find {shlex.quote(target_dir)} -type f 2&gt;/dev/null | head -{self.max_output_files}\"\n        result = sandbox.commands.run(cmd)\n        if result.exit_code != 0 or not (result.stdout or \"\").strip():\n            return []\n\n        return [f.strip() for f in result.stdout.splitlines() if f.strip()]\n\n    except Exception as e:\n        logger.warning(f\"E2BSandbox list_files failed for {target_dir}: {e}\")\n        return []\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.retrieve","title":"<code>retrieve(file_path)</code>","text":"<p>Read file bytes from sandbox filesystem.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def retrieve(self, file_path: str) -&gt; bytes:\n    \"\"\"Read file bytes from sandbox filesystem.\"\"\"\n    sandbox = self._ensure_sandbox()\n    resolved_path = self._resolve_path(file_path)\n    return sandbox.files.read(resolved_path, \"bytes\")\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.run_command_shell","title":"<code>run_command_shell(command, timeout=60, run_in_background_enabled=False)</code>","text":"<p>Execute a shell command in the E2B sandbox.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>Shell command or script to execute.</p> required <code>timeout</code> <code>int</code> <p>Timeout in seconds (default 60).</p> <code>60</code> <code>background</code> <p>If True, run command in background (no output).</p> required <p>Returns:</p> Type Description <code>ShellCommandResult</code> <p>ShellCommandResult with stdout, stderr, and exit_code.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def run_command_shell(\n    self,\n    command: str,\n    timeout: int = 60,\n    run_in_background_enabled: bool = False,\n) -&gt; ShellCommandResult:\n    \"\"\"Execute a shell command in the E2B sandbox.\n\n    Args:\n        command: Shell command or script to execute.\n        timeout: Timeout in seconds (default 60).\n        background: If True, run command in background (no output).\n\n    Returns:\n        ShellCommandResult with stdout, stderr, and exit_code.\n    \"\"\"\n    sandbox = self._ensure_sandbox()\n    logger.debug(f\"E2BSandbox running command: {command[:100]}...\")\n\n    try:\n        if run_in_background_enabled:\n            sandbox.commands.run(command, background=True)\n            return ShellCommandResult(\n                stdout=f\"Command started in background: {command}\",\n                stderr=\"\",\n                exit_code=0,\n            )\n\n        result = sandbox.commands.run(command, timeout=timeout)\n        return ShellCommandResult(\n            stdout=result.stdout or \"\",\n            stderr=result.stderr or \"\",\n            exit_code=getattr(result, \"exit_code\", None),\n        )\n    except Exception as e:\n        logger.error(f\"Command execution failed: {e}\")\n        return ShellCommandResult(\n            stdout=\"\",\n            stderr=str(e),\n            exit_code=1,\n        )\n</code></pre>"},{"location":"dynamiq/sandboxes/e2b/#dynamiq.sandboxes.e2b.E2BSandbox.upload_file","title":"<code>upload_file(file_name, content, destination_path=None, ensure_parent_dirs=True)</code>","text":"<p>Upload a file to the E2B sandbox.</p> <p>When ensure_parent_dirs is True, parent directories are created with <code>mkdir -p</code> before writing so that existing directories (e.g. from a previous skill ingestion) do not cause 500 errors from backends that use non-idempotent mkdir.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file.</p> required <code>content</code> <code>bytes</code> <p>File content as bytes.</p> required <code>destination_path</code> <code>str | None</code> <p>Optional destination path in sandbox. If None, uses base_path/file_name.</p> <code>None</code> <code>ensure_parent_dirs</code> <code>bool</code> <p>When True, run mkdir -p for the destination's parent before write.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the file was uploaded in the sandbox.</p> Source code in <code>dynamiq/sandboxes/e2b.py</code> <pre><code>def upload_file(\n    self,\n    file_name: str,\n    content: bytes,\n    destination_path: str | None = None,\n    ensure_parent_dirs: bool = True,\n) -&gt; str:\n    \"\"\"Upload a file to the E2B sandbox.\n\n    When ensure_parent_dirs is True, parent directories are created with ``mkdir -p``\n    before writing so that existing directories (e.g. from a previous skill ingestion)\n    do not cause 500 errors from backends that use non-idempotent mkdir.\n\n    Args:\n        file_name: Name of the file.\n        content: File content as bytes.\n        destination_path: Optional destination path in sandbox. If None, uses base_path/file_name.\n        ensure_parent_dirs: When True, run mkdir -p for the destination's parent before write.\n\n    Returns:\n        The path where the file was uploaded in the sandbox.\n    \"\"\"\n    sandbox = self._ensure_sandbox()\n\n    if destination_path is None:\n        destination_path = f\"{self.base_path}/{file_name}\"\n\n    if ensure_parent_dirs and destination_path:\n        parent = destination_path.rsplit(\"/\", 1)[0]\n        if parent and parent != destination_path:\n            try:\n                sandbox.commands.run(f\"mkdir -p {shlex.quote(parent)}\")\n                logger.debug(f\"E2BSandbox ensured parent dir: {parent}\")\n            except Exception as e:\n                logger.warning(f\"E2BSandbox mkdir -p for {parent!r} failed (continuing): {e}\")\n\n    try:\n        sandbox.files.write(destination_path, content)\n        logger.debug(f\"E2BSandbox uploaded file: {destination_path}\")\n        return destination_path\n    except Exception as e:\n        logger.error(f\"Failed to upload file {file_name}: {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/sandboxes/exceptions/","title":"Exceptions","text":"<p>Sandbox-related exceptions.</p>"},{"location":"dynamiq/sandboxes/exceptions/#dynamiq.sandboxes.exceptions.SandboxConnectionError","title":"<code>SandboxConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when connecting to an existing sandbox fails after retries.</p> Source code in <code>dynamiq/sandboxes/exceptions.py</code> <pre><code>class SandboxConnectionError(Exception):\n    \"\"\"Raised when connecting to an existing sandbox fails after retries.\"\"\"\n\n    def __init__(self, sandbox_id: str, cause: Exception | None = None):\n        self.sandbox_id = sandbox_id\n        self.cause = cause\n        super().__init__(\n            f\"Failed to connect to E2B sandbox {sandbox_id}. \"\n            \"The sandbox may have been killed or expired.\" + (f\" Cause: {cause}\" if cause else \"\")\n        )\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/sandbox_info/","title":"Sandbox info","text":""},{"location":"dynamiq/sandboxes/tools/sandbox_info/#dynamiq.sandboxes.tools.sandbox_info.SandboxInfoInputSchema","title":"<code>SandboxInfoInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for SandboxInfoTool.</p> Source code in <code>dynamiq/sandboxes/tools/sandbox_info.py</code> <pre><code>class SandboxInfoInputSchema(BaseModel):\n    \"\"\"Input schema for SandboxInfoTool.\"\"\"\n\n    port: int | None = Field(\n        default=None,\n        description=\"Optional port number. If provided and the sandbox supports it, \"\n        \"the response includes the public URL to access a service running on this port (e.g. dev server).\",\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/sandbox_info/#dynamiq.sandboxes.tools.sandbox_info.SandboxInfoTool","title":"<code>SandboxInfoTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for the agent to get sandbox metadata and, when needed, the public URL for a port.</p> <p>Use this when the agent starts a server in the sandbox (e.g. <code>npm run dev</code> on port 5173) so it can report the shareable URL to the user. The sandbox backend (e.g. E2B) exposes services at a public URL; this tool returns that URL for a given port.</p> Source code in <code>dynamiq/sandboxes/tools/sandbox_info.py</code> <pre><code>class SandboxInfoTool(Node):\n    \"\"\"A tool for the agent to get sandbox metadata and, when needed, the public URL for a port.\n\n    Use this when the agent starts a server in the sandbox (e.g. `npm run dev` on port 5173)\n    so it can report the shareable URL to the user. The sandbox backend (e.g. E2B) exposes\n    services at a public URL; this tool returns that URL for a given port.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"SandboxInfoTool\"\n    description: str = (\n        \"Get information about the current sandbox (paths, sandbox id, and optional public URL).\\n\\n\"\n        \"Call this when you need to report a URL to the user for a service you started in the sandbox \"\n        \"(e.g. a dev server). Pass the port number the service actually listens on (check the server output: \"\n        \"e.g. Vite -&gt; 5173, Next.js -&gt; 3000, python -m http.server -&gt; 8000). The URL will only work if a \"\n        \"process is listening on that port in the sandbox.\\n\\n\"\n        \"Parameters:\\n\"\n        \"- port (int, optional): Port the service listens on. If provided, response includes \"\n        \"public_host and public_url (https) so the user can open the app in a browser.\\n\\n\"\n        \"Examples:\\n\"\n        '- {\"port\": 5173}  \u2192 get sandbox info and public URL for port 5173 (Vite default)\\n'\n        \"- {}  \u2192 get base_path, sandbox_id only\"\n    )\n\n    sandbox: Sandbox = Field(..., description=\"Sandbox backend to query.\")\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[SandboxInfoInputSchema]] = SandboxInfoInputSchema\n\n    @property\n    def to_dict_exclude_params(self) -&gt; set[str]:\n        return super().to_dict_exclude_params | {\"sandbox\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        data = super().to_dict(for_tracing=for_tracing, **kwargs)\n        data[\"sandbox\"] = self.sandbox.to_dict(for_tracing=for_tracing, **kwargs) if self.sandbox else None\n        return data\n\n    def execute(\n        self,\n        input_data: SandboxInfoInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Return sandbox info and optional public URL for the given port.\"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        info: SandboxInfo = self.sandbox.get_sandbox_info(port=input_data.port)\n\n        if input_data.port is not None:\n            if info.public_url:\n                logger.info(\n                    \"SandboxInfoTool: port=%s -&gt; public_url=%s \"\n                    \"(ensure the server is running on this port in the sandbox)\",\n                    input_data.port,\n                    info.public_url,\n                )\n            elif info.public_url_error:\n                logger.warning(\n                    \"SandboxInfoTool: port=%s -&gt; error: %s\",\n                    input_data.port,\n                    info.public_url_error,\n                )\n        else:\n            logger.info(\n                \"SandboxInfoTool: sandbox_id=%s base_path=%s\",\n                info.sandbox_id,\n                info.base_path,\n            )\n\n        lines = [\n            f\"base_path: {info.base_path}\",\n        ]\n        if info.sandbox_id:\n            lines.append(f\"sandbox_id: {info.sandbox_id}\")\n        if input_data.port is not None:\n            if info.public_url:\n                lines.append(f\"public_url: {info.public_url}\")\n                lines.append(\"Share this URL with the user to open the app in a browser.\")\n            elif info.public_url_error:\n                lines.append(f\"public_url_error: {info.public_url_error}\")\n\n        content = \"\\n\".join(lines)\n        return {\"content\": content, \"sandbox_info\": info.model_dump()}\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/sandbox_info/#dynamiq.sandboxes.tools.sandbox_info.SandboxInfoTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Return sandbox info and optional public URL for the given port.</p> Source code in <code>dynamiq/sandboxes/tools/sandbox_info.py</code> <pre><code>def execute(\n    self,\n    input_data: SandboxInfoInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Return sandbox info and optional public URL for the given port.\"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    info: SandboxInfo = self.sandbox.get_sandbox_info(port=input_data.port)\n\n    if input_data.port is not None:\n        if info.public_url:\n            logger.info(\n                \"SandboxInfoTool: port=%s -&gt; public_url=%s \"\n                \"(ensure the server is running on this port in the sandbox)\",\n                input_data.port,\n                info.public_url,\n            )\n        elif info.public_url_error:\n            logger.warning(\n                \"SandboxInfoTool: port=%s -&gt; error: %s\",\n                input_data.port,\n                info.public_url_error,\n            )\n    else:\n        logger.info(\n            \"SandboxInfoTool: sandbox_id=%s base_path=%s\",\n            info.sandbox_id,\n            info.base_path,\n        )\n\n    lines = [\n        f\"base_path: {info.base_path}\",\n    ]\n    if info.sandbox_id:\n        lines.append(f\"sandbox_id: {info.sandbox_id}\")\n    if input_data.port is not None:\n        if info.public_url:\n            lines.append(f\"public_url: {info.public_url}\")\n            lines.append(\"Share this URL with the user to open the app in a browser.\")\n        elif info.public_url_error:\n            lines.append(f\"public_url_error: {info.public_url_error}\")\n\n    content = \"\\n\".join(lines)\n    return {\"content\": content, \"sandbox_info\": info.model_dump()}\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/shell/","title":"Shell","text":"<p>Sandbox shell tools for command execution and file operations.</p>"},{"location":"dynamiq/sandboxes/tools/shell/#dynamiq.sandboxes.tools.shell.SandboxShellInputSchema","title":"<code>SandboxShellInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input schema for SandboxShellTool.</p> Source code in <code>dynamiq/sandboxes/tools/shell.py</code> <pre><code>class SandboxShellInputSchema(BaseModel):\n    \"\"\"Input schema for SandboxShellTool.\"\"\"\n\n    command: str = Field(..., description=\"Shell command to execute in the sandbox.\")\n    timeout: int = Field(\n        default=60,\n        description=\"Timeout in seconds for command execution.\",\n    )\n    run_in_background_enabled: bool = Field(\n        default=False,\n        description=\"If True, run the command in background without waiting for output.\",\n    )\n    brief: str = Field(\n        default=\"Executing a shell command\",\n        description=\"Very brief description of the action being performed. \"\n        \"Example: 'List all files in the current directory'.\",\n    )\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/shell/#dynamiq.sandboxes.tools.shell.SandboxShellTool","title":"<code>SandboxShellTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for executing shell commands in a sandbox environment.</p> <p>This tool delegates command execution to the sandbox backend, allowing the sandbox to determine how commands are executed (e.g., locally, in a container, or in a remote E2B environment).</p> <p>Attributes:</p> Name Type Description <code>sandbox</code> <code>Sandbox</code> <p>The sandbox backend to execute commands in.</p> <code>blocked_commands</code> <code>list[str] | None</code> <p>Optional list of blocked substrings; command is blocked if it contains any (case-insensitive).</p> Source code in <code>dynamiq/sandboxes/tools/shell.py</code> <pre><code>class SandboxShellTool(Node):\n    \"\"\"A tool for executing shell commands in a sandbox environment.\n\n    This tool delegates command execution to the sandbox backend,\n    allowing the sandbox to determine how commands are executed\n    (e.g., locally, in a container, or in a remote E2B environment).\n\n    Attributes:\n        sandbox: The sandbox backend to execute commands in.\n        blocked_commands: Optional list of blocked substrings; command is blocked if it contains any (case-insensitive).\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"SandboxShellTool\"\n    description: str = (\n        \"Execute shell commands in an isolated sandbox environment.\\n\\n\"\n        \"Parameters:\\n\"\n        \"- command (str, required): The shell command to execute.\\n\"\n        \"- timeout (int, default 60): Max seconds to wait for completion.\\n\"\n        \"- run_in_background_enabled (bool, default false): Run without waiting for output.\\n\\n\"\n        \"Paths: Input files are under /home/user/input. Write outputs to /home/user/output; \"\n        \"files there are automatically collected and returned after the run. \"\n        \"If you start a dev server (e.g. npm run dev), \"\n        \"use SandboxInfoTool with the server port to get the public URL to share with the user.\\n\\n\"\n        \"Examples:\\n\"\n        '- {\"command\": \"ls -la /home/user/input\"}\\n'\n        '- {\"command\": \"python3 /home/user/input/script.py /home/user/input/data.txt /home/user/output\"}\\n'\n        '- {\"command\": \"cp result.csv /home/user/output/\"}\\n'\n        '- {\"command\": \"cat &lt;&lt;\\'EOF\\' &gt; script.py &amp;&amp; python3 script.py\\\\nimport csv\\\\n'\n        \"with open('data.csv') as f:\\\\n\"\n        \"    reader = csv.reader(f)\\\\n\"\n        \"    print(list(reader))\\\\n\"\n        \"print('Done')\\\\n\"\n        'EOF\"}'\n    )\n\n    sandbox: Sandbox = Field(..., description=\"Sandbox backend to execute commands in.\")\n    blocked_commands: list[str] | None = Field(\n        default=None,\n        description=\"Optional list of blocked substrings. A command is blocked if\"\n        \" it contains any of these strings (case-insensitive).\",\n    )\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[SandboxShellInputSchema]] = SandboxShellInputSchema\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"sandbox\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        # Pop for_tracing to avoid passing it twice (explicitly and in **kwargs).\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        data = super().to_dict(for_tracing=for_tracing, **kwargs)\n        data[\"sandbox\"] = self.sandbox.to_dict(for_tracing=for_tracing, **kwargs) if self.sandbox else None\n        return data\n\n    def _validate_command(self, command: str) -&gt; None:\n        \"\"\"Validate command against blocked list. Blocked entries are matched as substrings\n        (case-insensitive): if the command contains any blocked string, it is rejected.\"\"\"\n        cmd_lower = command.strip().lower()\n\n        # Check blocked commands\n        if self.blocked_commands:\n            for blocked in self.blocked_commands:\n                if blocked.lower() in cmd_lower:\n                    raise ToolExecutionException(\n                        f\"Command '{command}' is blocked for security reasons.\",\n                        recoverable=True,\n                    )\n\n    def execute(\n        self,\n        input_data: SandboxShellInputSchema,\n        config: RunnableConfig | None = None,\n        **kwargs,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute a shell command in the sandbox.\n\n        Args:\n            input_data: Input containing the command and options.\n            config: Runtime configuration.\n            **kwargs: Additional arguments.\n\n        Returns:\n            Dictionary with stdout, stderr, and exit_code from command execution.\n        \"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: executing command: {input_data.command[:100]}...\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        try:\n            # Validate command against security rules\n            self._validate_command(input_data.command)\n\n            # Execute command via sandbox\n            result = self.sandbox.run_command_shell(\n                command=input_data.command,\n                timeout=input_data.timeout,\n                run_in_background_enabled=input_data.run_in_background_enabled,\n            )\n\n            # Handle None exit_code: treat as success unless stderr indicates error\n            is_success = result.exit_code == 0 or (result.exit_code is None and not result.stderr)\n            output = {\n                \"content\": result.stdout if result.stdout else \"(no output)\",\n                \"stdout\": result.stdout,\n                \"stderr\": result.stderr,\n                \"exit_code\": result.exit_code,\n                \"success\": is_success,\n            }\n\n            if not is_success:\n                output[\"content\"] = (\n                    f\"Command failed with exit code {result.exit_code}.\\n\"\n                    f\"stdout: {result.stdout}\\n\"\n                    f\"stderr: {result.stderr}\"\n                )\n\n            logger.info(f\"Tool {self.name} - {self.id}: command completed with exit code {result.exit_code}\")\n            return output\n\n        except ToolExecutionException:\n            raise\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: command execution failed: {e}\")\n            raise ToolExecutionException(\n                f\"Failed to execute command: {e}\",\n                recoverable=True,\n            )\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/shell/#dynamiq.sandboxes.tools.shell.SandboxShellTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute a shell command in the sandbox.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>SandboxShellInputSchema</code> <p>Input containing the command and options.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Runtime configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with stdout, stderr, and exit_code from command execution.</p> Source code in <code>dynamiq/sandboxes/tools/shell.py</code> <pre><code>def execute(\n    self,\n    input_data: SandboxShellInputSchema,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; dict[str, Any]:\n    \"\"\"Execute a shell command in the sandbox.\n\n    Args:\n        input_data: Input containing the command and options.\n        config: Runtime configuration.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Dictionary with stdout, stderr, and exit_code from command execution.\n    \"\"\"\n    logger.info(f\"Tool {self.name} - {self.id}: executing command: {input_data.command[:100]}...\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    try:\n        # Validate command against security rules\n        self._validate_command(input_data.command)\n\n        # Execute command via sandbox\n        result = self.sandbox.run_command_shell(\n            command=input_data.command,\n            timeout=input_data.timeout,\n            run_in_background_enabled=input_data.run_in_background_enabled,\n        )\n\n        # Handle None exit_code: treat as success unless stderr indicates error\n        is_success = result.exit_code == 0 or (result.exit_code is None and not result.stderr)\n        output = {\n            \"content\": result.stdout if result.stdout else \"(no output)\",\n            \"stdout\": result.stdout,\n            \"stderr\": result.stderr,\n            \"exit_code\": result.exit_code,\n            \"success\": is_success,\n        }\n\n        if not is_success:\n            output[\"content\"] = (\n                f\"Command failed with exit code {result.exit_code}.\\n\"\n                f\"stdout: {result.stdout}\\n\"\n                f\"stderr: {result.stderr}\"\n            )\n\n        logger.info(f\"Tool {self.name} - {self.id}: command completed with exit code {result.exit_code}\")\n        return output\n\n    except ToolExecutionException:\n        raise\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: command execution failed: {e}\")\n        raise ToolExecutionException(\n            f\"Failed to execute command: {e}\",\n            recoverable=True,\n        )\n</code></pre>"},{"location":"dynamiq/sandboxes/tools/shell/#dynamiq.sandboxes.tools.shell.SandboxShellTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/sandboxes/tools/shell.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    # Pop for_tracing to avoid passing it twice (explicitly and in **kwargs).\n    for_tracing = kwargs.pop(\"for_tracing\", False)\n    data = super().to_dict(for_tracing=for_tracing, **kwargs)\n    data[\"sandbox\"] = self.sandbox.to_dict(for_tracing=for_tracing, **kwargs) if self.sandbox else None\n    return data\n</code></pre>"},{"location":"dynamiq/serializers/types/","title":"Types","text":""},{"location":"dynamiq/serializers/types/#dynamiq.serializers.types.RequirementData","title":"<code>RequirementData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a dict/object that requires external resolution.</p> <p>This model tracks any dict in YAML that has both $type and $id fields, which indicates it needs to be resolved via an external API before workflow initialization.</p> <p>Optionally, <code>value_path</code> specifies a JSONPath expression to extract a specific value from the resolved requirement data (e.g., \"$.account_id\").</p> Source code in <code>dynamiq/serializers/types.py</code> <pre><code>class RequirementData(BaseModel):\n    \"\"\"Information about a dict/object that requires external resolution.\n\n    This model tracks any dict in YAML that has both $type and $id fields,\n    which indicates it needs to be resolved via an external API before workflow initialization.\n\n    Optionally, `value_path` specifies a JSONPath expression to extract a specific\n    value from the resolved requirement data (e.g., \"$.account_id\").\n    \"\"\"\n\n    type: str = Field(..., alias=\"$type\", description=\"The $type field value\")\n    id: str = Field(..., alias=\"$id\", description=\"The $id field - unique identifier for external resolution\")\n    value_path: str | None = Field(\n        None, description=\"Optional JSONPath expression to extract a specific value from the resolved requirement\"\n    )\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"RequirementData | None\":\n        \"\"\"Create RequirementData from dict if it has $type and $id keys.\"\"\"\n        try:\n            return cls.model_validate(data)\n        except ValidationError:\n            return None\n</code></pre>"},{"location":"dynamiq/serializers/types/#dynamiq.serializers.types.RequirementData.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create RequirementData from dict if it has $type and $id keys.</p> Source code in <code>dynamiq/serializers/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"RequirementData | None\":\n    \"\"\"Create RequirementData from dict if it has $type and $id keys.\"\"\"\n    try:\n        return cls.model_validate(data)\n    except ValidationError:\n        return None\n</code></pre>"},{"location":"dynamiq/serializers/types/#dynamiq.serializers.types.WorkflowYamlData","title":"<code>WorkflowYamlData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data model for the Workflow YAML.</p> Source code in <code>dynamiq/serializers/types.py</code> <pre><code>class WorkflowYamlData(BaseModel):\n    \"\"\"Data model for the Workflow YAML.\"\"\"\n\n    connections: dict[str, BaseConnection]\n    nodes: dict[str, Node]\n    flows: dict[str, Flow]\n    workflows: dict[str, Workflow]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/","title":"Yaml","text":""},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper","title":"<code>WorkflowYAMLDumper</code>","text":"<p>Dumper class for parsing workflow components and save them to YAML.</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>class WorkflowYAMLDumper:\n    \"\"\"Dumper class for parsing workflow components and save them to YAML.\"\"\"\n\n    @classmethod\n    def get_updated_node_data(\n        cls,\n        node_data: dict,\n        connections_data: dict[str, dict],\n        skip_nullable: bool = False,\n    ):\n        \"\"\"\n        Get node init data with and connections components recursively (llms, agents, etc)\n\n        Args:\n            node_data: Dictionary containing node data.\n            connections_data: Dictionary containing connections shared data.\n            skip_nullable: Skip nullable fields.\n\n        Returns:\n            A dictionary of newly created nodes with dependencies.\n        \"\"\"\n        updated_node_init_data = {}\n        for param_name, param_data in node_data.items():\n            if param_name == \"depends\":\n                updated_node_init_data[param_name] = [\n                    {\"node\": dep[\"node\"][\"id\"], \"option\": dep[\"option\"]} for dep in param_data\n                ]\n\n            elif param_name == \"connection\":\n                param_id = None\n                connections_data[param_data[\"id\"]] = cls.get_updated_node_data(\n                    node_data={param_id: param_data}, connections_data=connections_data, skip_nullable=True\n                )[param_id]\n                updated_node_init_data[param_name] = param_data[\"id\"]\n\n            elif isinstance(param_data, dict):\n                updated_param_data = {}\n                for param_name_inner, param_data_inner in param_data.items():\n                    if isinstance(param_data_inner, (dict, list)):\n                        param_data_inner = cls.get_updated_node_data(\n                            node_data={param_name_inner: param_data_inner}, connections_data=connections_data\n                        )[param_name_inner]\n                    elif param_data_inner is None and skip_nullable:\n                        continue\n                    else:\n                        param_data_inner = encode(param_data_inner)\n\n                    updated_param_data[param_name_inner] = param_data_inner\n\n                updated_node_init_data[param_name] = updated_param_data\n\n            elif isinstance(param_data, list):\n                updated_items = []\n                for item in param_data:\n                    if isinstance(item, (dict, list)):\n                        param_id = None\n                        item = cls.get_updated_node_data(node_data={param_id: item}, connections_data=connections_data)[\n                            param_id\n                        ]\n                    elif item is None and skip_nullable:\n                        continue\n                    else:\n                        item = encode(item)\n                    updated_items.append(item)\n                updated_node_init_data[param_name] = updated_items\n\n            else:\n                if param_data is None and skip_nullable:\n                    continue\n                else:\n                    param_data = encode(param_data)\n\n                updated_node_init_data[param_name] = param_data\n\n        return updated_node_init_data\n\n    @classmethod\n    def get_nodes_data_and_connections(\n        cls, nodes: dict[str, Node], connections: dict[str, BaseConnection]\n    ) -&gt; tuple[dict, dict]:\n        \"\"\"\n        Get nodes data prepared for Yaml and connections from the given data.\n\n        Args:\n            nodes: Existing nodes dictionary.\n            connections: Existing connections dictionary.\n\n        Returns:\n            A tuple of parsed nodes and connections\n        \"\"\"\n        nodes_data = {}\n        connections_data = {conn_id: conn.to_dict() for conn_id, conn in connections.items()}\n        for node_id, node in nodes.items():\n            node_data = cls.get_updated_node_data(\n                node_data=node.to_dict(include_secure_params=True, by_alias=True), connections_data=connections_data\n            )\n            nodes_data[node_id] = node_data\n\n        return nodes_data, connections_data\n\n    @classmethod\n    def get_flows_data(cls, flows: dict[str, Flow]):\n        \"\"\"\n        Get flows data prepared for Yaml from the given data.\n\n        Args:\n            flows: Existing flows dictionary.\n\n        Returns:\n            A dictionary of newly created flows\n        \"\"\"\n        flows_data = {}\n        for flow_id, flow in flows.items():\n            flow_data = flow.to_dict(exclude={\"nodes\", \"executor\", \"connection_manager\"})\n            flow_data[\"nodes\"] = [node.id for node in flow.nodes]\n            flows_data[flow_id] = flow_data\n\n        return flows_data\n\n    @classmethod\n    def dump(\n        cls,\n        file_path: str | PathLike,\n        data: WorkflowYamlData,\n    ):\n        \"\"\"\n        Parse data from a WorkflowYamlData and save it to YAML file.\n\n        Args:\n            file_path: Path to the YAML file.\n            data: WorkflowYamlData object.\n        \"\"\"\n        data = cls.parse(data=data)\n        cls.dumps(file_path=file_path, data=data)\n\n    @classmethod\n    def dumps(cls, file_path: str | PathLike | IO[Any], data: dict):\n        \"\"\"\n        Load data from a YAML file.\n\n        Args:\n            file_path: Path to the YAML file.\n            data: Data to dump to the YAML file.\n\n        Raises:\n            WorkflowYAMLDumperException: If the file is not found.\n        \"\"\"\n        from omegaconf import OmegaConf\n\n        try:\n            conf = OmegaConf.create(data)\n            logger.debug(\"Dumped data to config\")\n\n            OmegaConf.save(config=conf, f=file_path)\n        except FileNotFoundError:\n            raise WorkflowYAMLDumperException(f\"File '{file_path}' not found\")\n\n    @classmethod\n    def parse(cls, data: WorkflowYamlData) -&gt; dict:\n        \"\"\"\n        Parse dynamiq workflow data.\n\n        Args:\n            data: WorkflowYamlData object.\n\n        Returns:\n            Parsed dict object.\n\n        Raises:\n            WorkflowYAMLDumperException: If parsing fails.\n        \"\"\"\n\n        try:\n            nodes, connections = cls.get_nodes_data_and_connections(data.nodes, data.connections)\n            flows = cls.get_flows_data(data.flows)\n            wf_data = {\n                \"connections\": connections,\n                \"nodes\": nodes,\n                \"flows\": flows,\n                \"workflows\": {\n                    wf_id: wf.to_dict(exclude={\"flow\"}) | {\"flow\": wf.flow.id} for wf_id, wf in data.workflows.items()\n                },\n            }\n        except Exception as e:\n            logger.exception(\"Failed to parse WorkflowYamlData object with unexpected error\")\n            raise WorkflowYAMLDumperException from e\n\n        return wf_data\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.dump","title":"<code>dump(file_path, data)</code>  <code>classmethod</code>","text":"<p>Parse data from a WorkflowYamlData and save it to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike</code> <p>Path to the YAML file.</p> required <code>data</code> <code>WorkflowYamlData</code> <p>WorkflowYamlData object.</p> required Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef dump(\n    cls,\n    file_path: str | PathLike,\n    data: WorkflowYamlData,\n):\n    \"\"\"\n    Parse data from a WorkflowYamlData and save it to YAML file.\n\n    Args:\n        file_path: Path to the YAML file.\n        data: WorkflowYamlData object.\n    \"\"\"\n    data = cls.parse(data=data)\n    cls.dumps(file_path=file_path, data=data)\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.dumps","title":"<code>dumps(file_path, data)</code>  <code>classmethod</code>","text":"<p>Load data from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike | IO[Any]</code> <p>Path to the YAML file.</p> required <code>data</code> <code>dict</code> <p>Data to dump to the YAML file.</p> required <p>Raises:</p> Type Description <code>WorkflowYAMLDumperException</code> <p>If the file is not found.</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef dumps(cls, file_path: str | PathLike | IO[Any], data: dict):\n    \"\"\"\n    Load data from a YAML file.\n\n    Args:\n        file_path: Path to the YAML file.\n        data: Data to dump to the YAML file.\n\n    Raises:\n        WorkflowYAMLDumperException: If the file is not found.\n    \"\"\"\n    from omegaconf import OmegaConf\n\n    try:\n        conf = OmegaConf.create(data)\n        logger.debug(\"Dumped data to config\")\n\n        OmegaConf.save(config=conf, f=file_path)\n    except FileNotFoundError:\n        raise WorkflowYAMLDumperException(f\"File '{file_path}' not found\")\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.get_flows_data","title":"<code>get_flows_data(flows)</code>  <code>classmethod</code>","text":"<p>Get flows data prepared for Yaml from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <p>Returns:</p> Type Description <p>A dictionary of newly created flows</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef get_flows_data(cls, flows: dict[str, Flow]):\n    \"\"\"\n    Get flows data prepared for Yaml from the given data.\n\n    Args:\n        flows: Existing flows dictionary.\n\n    Returns:\n        A dictionary of newly created flows\n    \"\"\"\n    flows_data = {}\n    for flow_id, flow in flows.items():\n        flow_data = flow.to_dict(exclude={\"nodes\", \"executor\", \"connection_manager\"})\n        flow_data[\"nodes\"] = [node.id for node in flow.nodes]\n        flows_data[flow_id] = flow_data\n\n    return flows_data\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.get_nodes_data_and_connections","title":"<code>get_nodes_data_and_connections(nodes, connections)</code>  <code>classmethod</code>","text":"<p>Get nodes data prepared for Yaml and connections from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>A tuple of parsed nodes and connections</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef get_nodes_data_and_connections(\n    cls, nodes: dict[str, Node], connections: dict[str, BaseConnection]\n) -&gt; tuple[dict, dict]:\n    \"\"\"\n    Get nodes data prepared for Yaml and connections from the given data.\n\n    Args:\n        nodes: Existing nodes dictionary.\n        connections: Existing connections dictionary.\n\n    Returns:\n        A tuple of parsed nodes and connections\n    \"\"\"\n    nodes_data = {}\n    connections_data = {conn_id: conn.to_dict() for conn_id, conn in connections.items()}\n    for node_id, node in nodes.items():\n        node_data = cls.get_updated_node_data(\n            node_data=node.to_dict(include_secure_params=True, by_alias=True), connections_data=connections_data\n        )\n        nodes_data[node_id] = node_data\n\n    return nodes_data, connections_data\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.get_updated_node_data","title":"<code>get_updated_node_data(node_data, connections_data, skip_nullable=False)</code>  <code>classmethod</code>","text":"<p>Get node init data with and connections components recursively (llms, agents, etc)</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>connections_data</code> <code>dict[str, dict]</code> <p>Dictionary containing connections shared data.</p> required <code>skip_nullable</code> <code>bool</code> <p>Skip nullable fields.</p> <code>False</code> <p>Returns:</p> Type Description <p>A dictionary of newly created nodes with dependencies.</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef get_updated_node_data(\n    cls,\n    node_data: dict,\n    connections_data: dict[str, dict],\n    skip_nullable: bool = False,\n):\n    \"\"\"\n    Get node init data with and connections components recursively (llms, agents, etc)\n\n    Args:\n        node_data: Dictionary containing node data.\n        connections_data: Dictionary containing connections shared data.\n        skip_nullable: Skip nullable fields.\n\n    Returns:\n        A dictionary of newly created nodes with dependencies.\n    \"\"\"\n    updated_node_init_data = {}\n    for param_name, param_data in node_data.items():\n        if param_name == \"depends\":\n            updated_node_init_data[param_name] = [\n                {\"node\": dep[\"node\"][\"id\"], \"option\": dep[\"option\"]} for dep in param_data\n            ]\n\n        elif param_name == \"connection\":\n            param_id = None\n            connections_data[param_data[\"id\"]] = cls.get_updated_node_data(\n                node_data={param_id: param_data}, connections_data=connections_data, skip_nullable=True\n            )[param_id]\n            updated_node_init_data[param_name] = param_data[\"id\"]\n\n        elif isinstance(param_data, dict):\n            updated_param_data = {}\n            for param_name_inner, param_data_inner in param_data.items():\n                if isinstance(param_data_inner, (dict, list)):\n                    param_data_inner = cls.get_updated_node_data(\n                        node_data={param_name_inner: param_data_inner}, connections_data=connections_data\n                    )[param_name_inner]\n                elif param_data_inner is None and skip_nullable:\n                    continue\n                else:\n                    param_data_inner = encode(param_data_inner)\n\n                updated_param_data[param_name_inner] = param_data_inner\n\n            updated_node_init_data[param_name] = updated_param_data\n\n        elif isinstance(param_data, list):\n            updated_items = []\n            for item in param_data:\n                if isinstance(item, (dict, list)):\n                    param_id = None\n                    item = cls.get_updated_node_data(node_data={param_id: item}, connections_data=connections_data)[\n                        param_id\n                    ]\n                elif item is None and skip_nullable:\n                    continue\n                else:\n                    item = encode(item)\n                updated_items.append(item)\n            updated_node_init_data[param_name] = updated_items\n\n        else:\n            if param_data is None and skip_nullable:\n                continue\n            else:\n                param_data = encode(param_data)\n\n            updated_node_init_data[param_name] = param_data\n\n    return updated_node_init_data\n</code></pre>"},{"location":"dynamiq/serializers/dumpers/yaml/#dynamiq.serializers.dumpers.yaml.WorkflowYAMLDumper.parse","title":"<code>parse(data)</code>  <code>classmethod</code>","text":"<p>Parse dynamiq workflow data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>WorkflowYamlData</code> <p>WorkflowYamlData object.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Parsed dict object.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLDumperException</code> <p>If parsing fails.</p> Source code in <code>dynamiq/serializers/dumpers/yaml.py</code> <pre><code>@classmethod\ndef parse(cls, data: WorkflowYamlData) -&gt; dict:\n    \"\"\"\n    Parse dynamiq workflow data.\n\n    Args:\n        data: WorkflowYamlData object.\n\n    Returns:\n        Parsed dict object.\n\n    Raises:\n        WorkflowYAMLDumperException: If parsing fails.\n    \"\"\"\n\n    try:\n        nodes, connections = cls.get_nodes_data_and_connections(data.nodes, data.connections)\n        flows = cls.get_flows_data(data.flows)\n        wf_data = {\n            \"connections\": connections,\n            \"nodes\": nodes,\n            \"flows\": flows,\n            \"workflows\": {\n                wf_id: wf.to_dict(exclude={\"flow\"}) | {\"flow\": wf.flow.id} for wf_id, wf in data.workflows.items()\n            },\n        }\n    except Exception as e:\n        logger.exception(\"Failed to parse WorkflowYamlData object with unexpected error\")\n        raise WorkflowYAMLDumperException from e\n\n    return wf_data\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/","title":"Yaml","text":""},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader","title":"<code>WorkflowYAMLLoader</code>","text":"<p>Loader class for parsing YAML files and creating workflow components.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>class WorkflowYAMLLoader:\n    \"\"\"Loader class for parsing YAML files and creating workflow components.\"\"\"\n\n    @classmethod\n    def get_requirements(cls, data: dict) -&gt; list[RequirementData]:\n        \"\"\"\n        Recursively scan YAML data and extract all requirement dicts.\n\n        A requirement dict is identified by having both $type and $id fields.\n        This method does NOT initialize any components.\n\n        Args:\n            data: Dictionary containing workflow YAML data.\n\n        Returns:\n            List of RequirementData for all requirement dicts.\n        \"\"\"\n        requirements: list[RequirementData] = []\n        cls._get_requirements_from_dict(data, requirements)\n        return requirements\n\n    @classmethod\n    def _get_requirements_from_dict(cls, data: dict, requirements: list[RequirementData]) -&gt; None:\n        \"\"\"Recursively extract RequirementData from dicts with both $type and $id.\"\"\"\n        if requirement := RequirementData.from_dict(data):\n            requirements.append(requirement)\n            # Don't recurse into requirement dicts - they will be replaced entirely\n            return\n\n        for key, value in data.items():\n            # Skip fields that use 'type'/'object' keywords for different purposes (JSON Schema, prompts)\n            if key in (\"prompt\", \"schema\", \"response_format\"):\n                continue\n\n            if isinstance(value, dict):\n                cls._get_requirements_from_dict(value, requirements)\n            elif isinstance(value, list):\n                cls._get_requirements_from_list(value, requirements)\n\n    @classmethod\n    def _get_requirements_from_list(cls, items: list, requirements: list[RequirementData]) -&gt; None:\n        \"\"\"Recursively extract RequirementData from a list.\"\"\"\n        for item in items:\n            if isinstance(item, dict):\n                cls._get_requirements_from_dict(item, requirements)\n            elif isinstance(item, list):\n                cls._get_requirements_from_list(item, requirements)\n\n    @staticmethod\n    def is_node_type(type_value: str | None) -&gt; bool:\n        \"\"\"\n        Check if the type value represents a node type (dotted path like 'module.ClassName').\n\n        Node types use dotted path format (e.g., 'dynamiq.nodes.agents.Agent'),\n        while other types (e.g., JSON schema types like 'string', 'object') don't contain dots.\n\n        Args:\n            type_value: The type string to check.\n\n        Returns:\n            True if the type value is a node type (contains a dot), False otherwise.\n        \"\"\"\n        return bool(type_value and \".\" in type_value)\n\n    @classmethod\n    def apply_resolved_requirements(\n        cls,\n        data: dict,\n        resolved_requirements: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        Apply resolved requirement data to YAML dict in-place before initialization.\n\n        Replaces requirement dicts (identified by both $type and $id) with the corresponding\n        resolved data. The resolved value can be any type (dict, list, string, etc.).\n\n        Args:\n            data: Raw YAML data dictionary (will be mutated).\n            resolved_requirements: Dict mapping $id to resolved data (any type).\n\n        Raises:\n            WorkflowYAMLLoaderException: If a $id cannot be resolved.\n        \"\"\"\n        cls._apply_requirements_to_dict(data, resolved_requirements)\n\n    @staticmethod\n    def _apply_value_path(resolved_value: Any, value_path: str, requirement_id: str) -&gt; Any:\n        \"\"\"Apply JSONPath expression to resolved requirement value to extract a single field.\n\n        Args:\n            resolved_value: The full resolved value from the external API.\n            value_path: JSONPath expression (e.g., \"$.account_id\").\n            requirement_id: Requirement $id for error messages.\n\n        Returns:\n            Single extracted value after applying the JSONPath.\n\n        Raises:\n            WorkflowYAMLLoaderException: If value_path is an invalid JSONPath expression,\n                doesn't match, matches multiple values, or resolved_value is not a dict/list.\n        \"\"\"\n        if not isinstance(resolved_value, (dict, list)):\n            raise WorkflowYAMLLoaderException(\n                f\"Cannot apply value_path '{value_path}' to requirement '{requirement_id}': \"\n                f\"resolved value must be a dict or list, got {type(resolved_value).__name__}\"\n            )\n\n        try:\n            matches = jsonpath_filter_all(resolved_value, value_path)\n        except ValueError:\n            raise WorkflowYAMLLoaderException(f\"Invalid value_path '{value_path}' for requirement '{requirement_id}'\")\n\n        if len(matches) == 0:\n            raise WorkflowYAMLLoaderException(\n                f\"value_path '{value_path}' did not match any value in resolved requirement '{requirement_id}'\"\n            )\n        if len(matches) &gt; 1:\n            raise WorkflowYAMLLoaderException(\n                f\"value_path '{value_path}' matched multiple values in resolved requirement '{requirement_id}'. \"\n                f\"Expected a single value, got {len(matches)} matches.\"\n            )\n        return matches[0]\n\n    @classmethod\n    def _resolve_requirement_value(\n        cls,\n        requirement: RequirementData,\n        resolved_requirements: dict[str, Any],\n    ) -&gt; Any:\n        \"\"\"Look up and optionally apply value_path to a resolved requirement.\n\n        Args:\n            requirement: Parsed requirement with id and optional value_path.\n            resolved_requirements: Mapping of $id to resolved data.\n\n        Returns:\n            Final resolved value (with value_path applied if specified).\n\n        Raises:\n            WorkflowYAMLLoaderException: If $id is missing or value_path extraction fails.\n        \"\"\"\n        if requirement.id not in resolved_requirements:\n            raise WorkflowYAMLLoaderException(\n                f\"Cannot resolve $id '{requirement.id}': not found in resolved_requirements. \"\n                f\"Use get_requirements() to get requirements and resolve them before parsing.\"\n            )\n\n        resolved_value = resolved_requirements[requirement.id]\n\n        if requirement.value_path:\n            resolved_value = cls._apply_value_path(resolved_value, requirement.value_path, requirement.id)\n\n        return resolved_value\n\n    @classmethod\n    def _apply_requirements_to_dict(\n        cls,\n        data: dict,\n        resolved_requirements: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Recursively apply requirements to a dictionary and its nested structures.\n\n        When a requirement dict is found (has both $type and $id), it is completely\n        replaced with the resolved data at the parent level. If the requirement\n        specifies a value_path, the JSONPath is applied to extract a specific value.\n        \"\"\"\n        for key, value in list(data.items()):\n            # Skip fields that use 'type'/'object' keywords for different purposes (JSON Schema, prompts)\n            if key in (\"prompt\", \"schema\", \"response_format\"):\n                continue\n\n            if isinstance(value, dict):\n                if requirement := RequirementData.from_dict(value):\n                    data[key] = cls._resolve_requirement_value(requirement, resolved_requirements)\n                else:\n                    cls._apply_requirements_to_dict(value, resolved_requirements)\n            elif isinstance(value, list):\n                cls._apply_requirements_to_list(value, resolved_requirements)\n\n    @classmethod\n    def _apply_requirements_to_list(\n        cls,\n        items: list,\n        resolved_requirements: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Recursively apply requirements within a list.\"\"\"\n        for i, item in enumerate(items):\n            if isinstance(item, dict):\n                if requirement := RequirementData.from_dict(item):\n                    items[i] = cls._resolve_requirement_value(requirement, resolved_requirements)\n                else:\n                    cls._apply_requirements_to_dict(item, resolved_requirements)\n            elif isinstance(item, list):\n                cls._apply_requirements_to_list(item, resolved_requirements)\n\n    @classmethod\n    def get_entity_by_type(cls, entity_type: str, entity_registry: dict[str, Any] | None = None) -&gt; Any:\n        \"\"\"\n        Try to get entity by type and update mutable shared registry.\n\n        Args:\n            entity_type (str): The type of entity to retrieve.\n            entity_registry (dict[str, Any] | None): A registry of entities.\n\n        Returns:\n            Any: The retrieved entity.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the entity is not valid or cannot be found.\n        \"\"\"\n        if entity_registry is None:\n            entity_registry = {}\n\n        if entity := entity_registry.get(entity_type):\n            return entity\n\n        try:\n            entity = ConnectionManager.get_connection_by_type(entity_type)\n        except ValueError:\n            pass\n\n        if not entity:\n            try:\n                entity = NodeManager.get_node_by_type(entity_type)\n            except ValueError:\n                pass\n\n        if not entity:\n            raise WorkflowYAMLLoaderException(f\"Entity '{entity_type}' is not valid.\")\n\n        entity_registry[entity_type] = entity\n        return entity\n\n    @classmethod\n    def get_connections(\n        cls,\n        data: dict[str, dict],\n        registry: dict[str, Any],\n    ) -&gt; dict[str, BaseConnection]:\n        \"\"\"\n        Get connections from the provided data.\n\n        Args:\n            data: The data containing connection information.\n            registry: A registry of entities.\n\n        Returns:\n            A dictionary of connections.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in connection data or initialization.\n        \"\"\"\n        connections = {}\n\n        for conn_id, conn_data in data.get(\"connections\", {}).items():\n            if conn_id in connections:\n                raise WorkflowYAMLLoaderException(f\"Connection '{conn_id}' already exists\")\n            if not (conn_type := conn_data.get(\"type\")):\n                raise WorkflowYAMLLoaderException(f\"Value 'type' not found for connection '{conn_id}'\")\n\n            conn_cls = cls.get_entity_by_type(entity_type=conn_type, entity_registry=registry)\n            conn_init_data = conn_data | {\"id\": conn_id}\n            conn_init_data.pop(\"type\", None)\n            try:\n                connection = conn_cls(**conn_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(f\"Connection '{conn_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n            connections[conn_id] = connection\n\n        return connections\n\n    @classmethod\n    def get_inline_connection(\n        cls,\n        node_id: str,\n        conn_data: dict,\n        registry: dict[str, Any] | None = None,\n    ) -&gt; BaseConnection:\n        \"\"\"\n        Create an inline connection from node's connection data.\n\n        Args:\n            node_id: The ID of the node that uses this connection.\n            conn_data: The connection data dictionary.\n            registry: A registry of entities.\n\n        Returns:\n            The created connection.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the connection data is invalid.\n        \"\"\"\n        if not (conn_type := conn_data.get(\"type\")):\n            raise WorkflowYAMLLoaderException(f\"Value 'type' not found for inline connection in node '{node_id}'\")\n\n        conn_cls = cls.get_entity_by_type(entity_type=conn_type, entity_registry=registry)\n        conn_init_data = conn_data.copy()\n        conn_init_data.pop(\"type\", None)\n\n        # Generate UUID for connection id if not provided\n        if \"id\" not in conn_init_data:\n            conn_init_data[\"id\"] = generate_uuid()\n\n        try:\n            connection = conn_cls(**conn_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Inline connection for node '{node_id}' data is invalid. Error: {str(e) or repr(e)}\"\n            )\n\n        return connection\n\n    @classmethod\n    def init_prompt(cls, prompt_init_data: dict) -&gt; Prompt:\n        \"\"\"\n        Initialize a prompt from the provided data.\n\n        Args:\n            prompt_init_data (dict): The data for the prompt.\n\n        Returns:\n            Prompt: The initialized prompt.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified prompt is not found.\n        \"\"\"\n        try:\n            return Prompt(**prompt_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Prompt '{prompt_init_data.get('id')}' data is invalid. Error: {str(e) or repr(e)}\"\n            )\n\n    @classmethod\n    def get_prompts(cls, data: dict[str, dict]) -&gt; dict[str, Prompt]:\n        \"\"\"\n        Get prompts from the provided data.\n\n        Args:\n            data (dict[str, dict]): The data containing prompt information.\n\n        Returns:\n            dict[str, Prompt]: A dictionary of prompts.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in prompt data or initialization.\n        \"\"\"\n        prompts = {}\n        for prompt_id, prompt_data in data.get(\"prompts\", {}).items():\n            if prompt_id in prompts:\n                raise WorkflowYAMLLoaderException(f\"Prompt '{prompt_id}' already exists\")\n            prompts[prompt_id] = cls.init_prompt(prompt_data | {\"id\": prompt_id})\n\n        return prompts\n\n    @classmethod\n    def get_node_prompt(cls, node_id: str, node_data: dict, prompts: dict[id, Prompt]) -&gt; Prompt | None:\n        \"\"\"\n        Get the prompt for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            prompts (dict[id, Prompt]): A dictionary of available prompts.\n\n        Returns:\n            Prompt | None: The prompt for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified prompt is not found.\n        \"\"\"\n        prompt = None\n        if prompt_id := node_data.get(\"prompt\"):\n            prompt = prompts.get(prompt_id)\n            if not prompt:\n                raise WorkflowYAMLLoaderException(f\"Prompt '{prompt_id}' for node '{node_id}' not found\")\n        return prompt\n\n    @classmethod\n    def get_node_connection(\n        cls,\n        node_id: str,\n        node_data: dict,\n        connections: dict[str, BaseConnection],\n        registry: dict[str, Any] | None = None,\n    ) -&gt; BaseConnection | None:\n        \"\"\"\n        Get the connection for a node.\n\n        Supports both:\n        - Reference by ID (string): Looks up connection in the connections dictionary.\n        - Inline connection (dict): Creates a new connection from the provided data.\n\n        Args:\n            node_id: The ID of the node.\n            node_data: The data for the node.\n            connections: A dictionary of available connections.\n            registry: A registry of entities (required for inline connections).\n\n        Returns:\n            The connection for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified connection is not found or invalid.\n        \"\"\"\n        conn_value = node_data.get(\"connection\")\n        if conn_value is None:\n            return\n\n        if isinstance(conn_value, str):\n            conn = connections.get(conn_value)\n            if not conn:\n                raise WorkflowYAMLLoaderException(f\"Connection '{conn_value}' for node '{node_id}' not found\")\n            return conn\n\n        if isinstance(conn_value, dict):\n            connection = cls.get_inline_connection(\n                node_id=node_id,\n                conn_data=conn_value,\n                registry=registry,\n            )\n            connections[connection.id] = connection\n            return connection\n\n        raise WorkflowYAMLLoaderException(\n            f\"Invalid connection format for node '{node_id}'. Expected string (reference) or dict (inline).\"\n        )\n\n    @classmethod\n    def get_node_vector_store_connection(\n        cls,\n        node_id: str,\n        node_data: dict,\n        connections: dict[str, BaseConnection],\n        registry: dict[str, Any] | None = None,\n    ) -&gt; Any | None:\n        \"\"\"\n        Get the vector store connection for a node.\n\n        Args:\n            node_id: The ID of the node.\n            node_data: The data for the node.\n            connections: A dictionary of available connections.\n            registry: A registry of entities (required for inline connections).\n\n        Returns:\n            The vector store connection for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified vector store connection is not found or\n                                         does not support vector store initialization.\n        \"\"\"\n        conn = cls.get_node_connection(\n            node_id=node_id,\n            node_data=node_data,\n            connections=connections,\n            registry=registry,\n        )\n        if conn:\n            if not (conn_to_vs := getattr(conn, \"connect_to_vector_store\", None)) or not callable(conn_to_vs):\n                raise WorkflowYAMLLoaderException(\n                    f\"Vector store connection '{conn.id}' for node '{node_id}' not support vector store initialization\"\n                )\n        return conn\n\n    @classmethod\n    def get_node_flow(cls, node_id: str, node_data: dict, flows: dict[id, Flow]) -&gt; Flow | None:\n        \"\"\"\n        Get the flow for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            flows (dict[id, Flow]): A dictionary of available flows.\n\n        Returns:\n            Flow | None: The flow for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified flow is not found.\n        \"\"\"\n        flow = None\n        if flow_id := node_data.get(\"flow\"):\n            flow = flows.get(flow_id)\n            if not flow:\n                raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for node '{node_id}' not found\")\n        return flow\n\n    @classmethod\n    def get_node_flows(cls, node_id: str, node_data: dict, flows: dict[id, Flow]) -&gt; list[Flow]:\n        \"\"\"\n        Get the flows for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            flows (dict[id, Flow]): A dictionary of available flows.\n\n        Returns:\n            list[Flow]: A list of flows for the node.\n\n        Raises:\n            WorkflowYAMLLoaderException: If any specified flow is not found.\n        \"\"\"\n        node_flows = []\n        for flow_id in node_data.get(\"flows\", []):\n            node_flow = flows.get(flow_id)\n            if not node_flow:\n                raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for node '{node_id}' not found\")\n            node_flows.append(node_flow)\n        return node_flows\n\n    @classmethod\n    def get_node_dependencies(cls, node_id: str, node_data: dict, nodes: dict[str, Node]):\n        \"\"\"\n        Get the dependencies for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            nodes (dict[str, Node]): A dictionary of available nodes.\n\n        Returns:\n            list[NodeDependency]: A list of node dependencies.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in dependency data or initialization.\n        \"\"\"\n        node_depends = []\n        for dependency_data in node_data.get(\"depends\", []):\n            dependency_node = nodes.get(dependency_data.get(\"node\"))\n            dependency_init_data = dependency_data | {\"node\": dependency_node}\n            try:\n                dependency = NodeDependency(**dependency_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency '{dependency_data.get('node')}' data for node '{node_id}' \"\n                    f\"is invalid. Error: {str(e) or repr(e)}\"\n                )\n\n            if dependency.option:\n                if not (dep_options := getattr(dependency_node, \"options\", [])):\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency '{dependency.node.id}' with option '{dependency.option}' \"\n                        f\"for node '{node_id}' not found\"\n                    )\n\n                if not any(opt.id == dependency.option for opt in dep_options):\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency '{dependency.node.id}' with option '{dependency.option}' \"\n                        f\"for node '{node_id}' not found\"\n                    )\n\n            node_depends.append(dependency)\n        return node_depends\n\n    @classmethod\n    def get_updated_node_init_data_with_initialized_nodes(\n        cls,\n        node_init_data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ):\n        \"\"\"\n        Get node init data with initialized nodes components recursively (llms, agents, etc)\n\n        Args:\n            node_init_data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            A dictionary of newly created nodes with dependencies.\n        \"\"\"\n        updated_node_init_data = {}\n        kwargs = dict(\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n        for param_name, param_data in node_init_data.items():\n            # TODO: dummy fix, revisit this!\n            # We had to add this condition because some nodes have a `schema`/`response_format` params,\n            # that have a `type` field that contains types supported by JSON schema (e.g., string, object).\n            if param_name in (\"schema\", \"response_format\"):\n                updated_node_init_data[param_name] = param_data\n\n            elif isinstance(param_data, dict):\n                updated_param_data = {}\n                for param_name_inner, param_data_inner in param_data.items():\n                    if param_name_inner in (\"prompt\", \"schema\", \"response_format\"):\n                        updated_param_data[param_name_inner] = param_data_inner\n                    elif isinstance(param_data_inner, (dict, list)):\n                        param_id = None\n                        updated_param_data[param_name_inner] = cls.get_updated_node_init_data_with_initialized_nodes(\n                            {param_id: param_data_inner}, **kwargs\n                        )[param_id]\n                    else:\n                        updated_param_data[param_name_inner] = param_data_inner\n\n                if cls.is_node_type(updated_param_data.get(\"type\")):\n                    param_id = updated_param_data.get(\"id\")\n                    updated_param_data = cls.get_nodes_without_depends({param_id: updated_param_data}, **kwargs)[\n                        param_id\n                    ]\n\n                updated_node_init_data[param_name] = updated_param_data\n\n            elif isinstance(param_data, list):\n                updated_items = []\n                for item in param_data:\n                    if isinstance(item, (dict, list)):\n                        param_id = None\n                        updated_items.append(\n                            cls.get_updated_node_init_data_with_initialized_nodes(\n                                node_init_data={param_id: item}, **kwargs\n                            )[param_id]\n                        )\n                    else:\n                        updated_items.append(item)\n                updated_node_init_data[param_name] = updated_items\n\n            else:\n                updated_node_init_data[param_name] = param_data\n\n        return updated_node_init_data\n\n    @classmethod\n    def get_nodes_without_depends(\n        cls,\n        data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ) -&gt; dict[str, Node]:\n        \"\"\"\n        Create nodes without dependencies from the given data.\n        Automatically uses parallel processing for multiple nodes (&gt;1).\n\n        Args:\n            data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            A dictionary of newly created nodes without dependencies.\n\n        Raises:\n            WorkflowYAMLLoaderException: If node data is invalid or duplicates are found.\n        \"\"\"\n        new_nodes = {}\n        get_node_kwargs = dict(\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n\n        nodes_to_create = {}\n        for node_id, node_data in data.items():\n            if node_id in nodes:\n                continue\n            if node_id in new_nodes:\n                raise WorkflowYAMLLoaderException(f\"Node '{node_id}' already exists\")\n            nodes_to_create[node_id] = node_data\n\n        if len(nodes_to_create) &lt;= 1:\n            for node_id, node_data in nodes_to_create.items():\n                try:\n                    node_id, node = cls.get_node_without_depends(\n                        node_id=node_id,\n                        node_data=node_data,\n                        **get_node_kwargs,\n                    )\n                    new_nodes[node_id] = node\n                except WorkflowYAMLLoaderException:\n                    raise\n\n            return new_nodes\n\n        with ContextAwareThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_node_id = {\n                executor.submit(\n                    cls.get_node_without_depends,\n                    node_id=node_id,\n                    node_data=node_data,\n                    **get_node_kwargs,\n                ): node_id\n                for node_id, node_data in nodes_to_create.items()\n            }\n\n            for future in as_completed(future_to_node_id):\n                node_id = future_to_node_id[future]\n                try:\n                    result_node_id, node = future.result()\n                    new_nodes[result_node_id] = node\n                except WorkflowYAMLLoaderException:\n                    raise\n                except Exception as e:\n                    raise WorkflowYAMLLoaderException(f\"Node '{node_id}' processing failed. Error: {str(e) or repr(e)}\")\n\n        return new_nodes\n\n    @classmethod\n    def get_node_without_depends(\n        cls,\n        node_id: str,\n        node_data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ) -&gt; tuple[str, Node]:\n        \"\"\"\n        Create a single node without dependencies from the given data.\n\n        Args:\n            node_id: Node identifier.\n            node_data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            A tuple of (node_id, node).\n\n        Raises:\n            WorkflowYAMLLoaderException: If node data is invalid.\n        \"\"\"\n        if not (node_type := node_data.get(\"type\")):\n            raise WorkflowYAMLLoaderException(f\"Value 'type' for node '{node_id}' not found\")\n\n        node_cls = cls.get_entity_by_type(entity_type=node_type, entity_registry=registry)\n\n        # Init node params\n        node_init_data = node_data.copy()\n        if node_id:\n            node_init_data[\"id\"] = node_id\n        node_init_data.pop(\"type\", None)\n        node_init_data.pop(\"depends\", None)\n\n        if \"is_postponed_component_init\" not in node_init_data:\n            node_init_data[\"is_postponed_component_init\"] = True\n\n        if \"connection\" in node_init_data:\n            get_node_conn = (\n                cls.get_node_vector_store_connection\n                if isinstance(node_cls, ConnectionNode)\n                else cls.get_node_connection\n            )\n            node_init_data[\"connection\"] = get_node_conn(\n                node_id=node_id,\n                node_data=node_data,\n                connections=connections,\n                registry=registry,\n            )\n        if prompt_data := node_init_data.get(\"prompt\"):\n            node_init_data[\"prompt\"] = (\n                cls.get_node_prompt(node_id=node_id, node_data=node_data, prompts=prompts)\n                if isinstance(prompt_data, str)\n                else cls.init_prompt(prompt_data)\n            )\n        if \"flow\" in node_init_data:\n            node_init_data[\"flow\"] = cls.get_node_flow(node_id=node_id, node_data=node_data, flows=flows)\n        if \"flows\" in node_init_data:\n            node_init_data[\"flows\"] = cls.get_node_flows(node_id=node_id, node_data=node_data, flows=flows)\n\n        try:\n            node_init_data = cls.get_updated_node_init_data_with_initialized_nodes(\n                node_init_data=node_init_data,\n                nodes=nodes,\n                flows=flows,\n                connections=connections,\n                prompts=prompts,\n                registry=registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )\n\n            node = node_cls(**node_init_data)\n\n            if init_components and getattr(node, \"init_components\", False):\n                node.init_components(connection_manager=connection_manager)\n                node.is_postponed_component_init = False\n\n        except Exception as e:\n            logger.exception(f\"Node '{node_id}' data is invalid\")\n            raise WorkflowYAMLLoaderException(f\"Node '{node_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n        return node_id, node\n\n    @classmethod\n    def get_nodes(\n        cls,\n        nodes_data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ):\n        \"\"\"\n        Create nodes with dependencies from the given data.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            A dictionary of newly created nodes with dependencies.\n        \"\"\"\n\n        new_nodes = cls.get_nodes_without_depends(\n            data=nodes_data,\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n\n        all_nodes = nodes | new_nodes\n        for node_id, node in new_nodes.items():\n            node.depends = cls.get_node_dependencies(node_id=node_id, node_data=nodes_data[node_id], nodes=all_nodes)\n\n        return new_nodes\n\n    @classmethod\n    def get_dependant_nodes(\n        cls,\n        nodes_data: dict[str, dict],\n        flows_data: dict[str, dict],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ) -&gt; dict[str, Node]:\n        \"\"\"\n        Get nodes that are dependent on flows.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            flows_data: Dictionary containing flow data.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            A dictionary of nodes that are dependent on flows.\n        \"\"\"\n        dependant_nodes, dependant_nodes_data = {}, {}\n        dependant_flow_ids = []\n\n        for node_id, node_data in nodes_data.items():\n            if \"flow\" in node_data:\n                dependant_nodes_data[node_id] = node_data\n                dependant_flow_ids.append(node_data[\"flow\"])\n            if \"flows\" in node_data:\n                dependant_nodes_data[node_id] = node_data\n                dependant_flow_ids.extend(node_data[\"flows\"])\n\n        # Get nodes from dependant flows\n        if dependant_flow_ids:\n            dependant_flows_nodes_ids = []\n            for flow_id, flow_data in flows_data.items():\n                if flow_id in dependant_flow_ids:\n                    dependant_flows_nodes_ids.extend(flow_data.get(\"nodes\", []))\n\n            dependant_flows_nodes_data = {\n                node_id: node_data for node_id, node_data in nodes_data.items() if node_id in dependant_flows_nodes_ids\n            }\n\n            dependant_nodes = cls.get_nodes(\n                nodes_data=dependant_flows_nodes_data,\n                nodes={},\n                flows={},\n                connections=connections,\n                prompts=prompts,\n                registry=registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n                max_workers=max_workers,\n            )\n\n        return dependant_nodes\n\n    @classmethod\n    def get_flows(\n        cls,\n        data: dict,\n        flows: dict[str, Flow],\n        nodes: dict[str, Node],\n        connection_manager: ConnectionManager | None = None,\n    ) -&gt; dict[str, Flow]:\n        \"\"\"\n        Create flows from the given data.\n\n        Args:\n            data: Dictionary containing flow data.\n            flows: Existing flows dictionary.\n            nodes: Existing nodes dictionary.\n            connection_manager: Optional connection manager.\n\n        Returns:\n            A dictionary of newly created flows.\n\n        Raises:\n            WorkflowYAMLLoaderException: If flow data is invalid or duplicates are found.\n        \"\"\"\n        new_flows = {}\n        for flow_id, flow_data in data.items():\n            if flow_id in flows:\n                continue\n\n            if flow_id in new_flows:\n                raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' already exists\")\n\n            flow_node_ids = flow_data.get(\"nodes\", [])\n            flow_node_ids = set(flow_node_ids)\n            dep_node_ids = set()\n            for node_id in flow_node_ids:\n                if node_id not in nodes:\n                    raise WorkflowYAMLLoaderException(f\"Node '{node_id}' for flow '{flow_id}' not found\")\n\n                dep_node_ids.update({dep.node.id for dep in nodes[node_id].depends})\n\n            for node_id in dep_node_ids:\n                if node_id not in flow_node_ids:\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency node '{node_id}' in the flow '{flow_id}' node list not found\"\n                    )\n\n            flow_init_data = flow_data | {\n                \"id\": flow_id,\n                \"nodes\": [nodes[node_id] for node_id in flow_node_ids],\n            }\n            if connection_manager:\n                flow_init_data[\"connection_manager\"] = connection_manager\n\n            try:\n                flow = Flow(**flow_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n            new_flows[flow_id] = flow\n        return new_flows\n\n    @classmethod\n    def get_dependant_flows(\n        cls,\n        nodes_data: dict[str, dict],\n        flows_data: dict[str, dict],\n        dependant_nodes: dict[str, Node],\n        connection_manager: ConnectionManager | None = None,\n    ) -&gt; dict[str, Flow]:\n        \"\"\"\n        Get flows that are dependent on nodes.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            flows_data: Dictionary containing flow data.\n            dependant_nodes: Dictionary of dependent nodes.\n            connection_manager: Optional connection manager.\n\n        Returns:\n            A dictionary of flows that are dependent on nodes.\n        \"\"\"\n        dependant_flows = {}\n        dependant_flow_ids = []\n\n        for node_id, node_data in nodes_data.items():\n            if \"flow\" in node_data:\n                dependant_flow_ids.append(node_data[\"flow\"])\n            if \"flows\" in node_data:\n                dependant_flow_ids.extend(node_data[\"flows\"])\n\n        if dependant_flow_ids:\n            dependant_flows_data = {\n                flow_id: flow_data for flow_id, flow_data in flows_data.items() if flow_id in dependant_flow_ids\n            }\n            dependant_flows = cls.get_flows(\n                data=dependant_flows_data,\n                flows={},\n                nodes=dependant_nodes,\n                connection_manager=connection_manager,\n            )\n\n        return dependant_flows\n\n    @classmethod\n    def get_workflows(cls, data: dict, flows: dict[str, Flow]) -&gt; dict[str, Workflow]:\n        \"\"\"\n        Create workflows from the given data.\n\n        Args:\n            data: Dictionary containing workflow data.\n            flows: Existing flows dictionary.\n\n        Returns:\n            A dictionary of newly created workflows.\n\n        Raises:\n            WorkflowYAMLLoaderException: If workflow data is invalid.\n        \"\"\"\n        workflows = {}\n        for wf_id, wf_data in data.get(\"workflows\", {}).items():\n            if not (flow_id := wf_data.get(\"flow\")):\n                raise WorkflowYAMLLoaderException(f\"Value 'flow' for dynamiq '{wf_id}' not found \")\n            if not (flow := flows.get(flow_id)):\n                raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for dynamiq '{wf_id}' not found\")\n            if version := wf_data.get(\"version\"):\n                version = str(version)\n\n            try:\n                wf = Workflow(id=wf_id, flow=flow, version=version)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(f\"Workflow '{wf_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n            workflows[wf_id] = wf\n        return workflows\n\n    @classmethod\n    def load(\n        cls,\n        file_path: str | PathLike,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ) -&gt; WorkflowYamlData:\n        \"\"\"\n        Load data from a YAML file and parse it.\n\n        Args:\n            file_path: Path to the YAML file.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            Parsed WorkflowYamlData object.\n        \"\"\"\n        data = cls.loads(file_path)\n        return cls.parse(\n            data=data,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n\n    @classmethod\n    def loads(cls, file_path: str | PathLike):\n        \"\"\"\n        Load data from a YAML file.\n\n        Args:\n            file_path: Path to the YAML file.\n\n        Returns:\n            Parsed data from the YAML file.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the file is not found.\n        \"\"\"\n        from omegaconf import OmegaConf\n\n        try:\n            conf = OmegaConf.load(file_path)\n            logger.debug(f\"Loaded config from '{file_path}'\")\n\n            data = OmegaConf.to_container(conf, resolve=True)\n        except FileNotFoundError:\n            raise WorkflowYAMLLoaderException(f\"File '{file_path}' not found\")\n\n        return data\n\n    @classmethod\n    def parse(\n        cls,\n        data: dict,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n        max_workers: int | None = None,\n    ) -&gt; WorkflowYamlData:\n        \"\"\"\n        Parse dynamiq workflow data.\n\n        Args:\n            data: Dictionary containing workflow data.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n            max_workers: Maximum number of worker threads for node parallel processing.\n\n        Returns:\n            Parsed WorkflowYamlData object.\n\n        Raises:\n            WorkflowYAMLLoaderException: If parsing fails.\n\n        Usage:\n            # Step 1: Extract all requirements without initialization\n            requirements = WorkflowYAMLLoader.get_requirements(data)\n\n            # Step 2: Resolve requirements via external API\n            resolved = {req.id: api_fetch(req.id) for req in requirements}\n\n            # Step 3: Apply resolved requirements to YAML data (mutates in-place)\n            WorkflowYAMLLoader.apply_resolved_requirements(data, resolved)\n\n            # Step 4: Parse the data\n            result = WorkflowYAMLLoader.parse(data)\n        \"\"\"\n        nodes, flows = {}, {}\n        # Mutable shared registry that updates with each new entity.\n        node_registry, connection_registry = {}, {}\n\n        if init_components and connection_manager is None:\n            connection_manager = ConnectionManager()\n\n        try:\n            connections = cls.get_connections(\n                data=data,\n                registry=connection_registry,\n            )\n            prompts = cls.get_prompts(data)\n\n            nodes_data = data.get(\"nodes\", {})\n            flows_data = data.get(\"flows\", {})\n\n            dependant_nodes = cls.get_dependant_nodes(\n                nodes_data=nodes_data,\n                flows_data=flows_data,\n                connections=connections,\n                prompts=prompts,\n                registry=node_registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n                max_workers=max_workers,\n            )\n            nodes.update(dependant_nodes)\n\n            dependant_flows = cls.get_dependant_flows(\n                nodes_data=nodes_data,\n                flows_data=flows_data,\n                dependant_nodes=dependant_nodes,\n                connection_manager=connection_manager,\n            )\n            flows.update(dependant_flows)\n\n            non_dependant_nodes = cls.get_nodes(\n                nodes_data=nodes_data,\n                nodes=nodes,\n                flows=flows,\n                connections=connections,\n                prompts=prompts,\n                registry=node_registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n                max_workers=max_workers,\n            )\n            nodes.update(non_dependant_nodes)\n\n            non_dependant_flows = cls.get_flows(\n                data=flows_data,\n                flows=flows,\n                nodes=nodes,\n                connection_manager=connection_manager,\n            )\n            flows.update(non_dependant_flows)\n\n            workflows = cls.get_workflows(data, flows)\n        except WorkflowYAMLLoaderException:\n            raise\n        except Exception:\n            logger.exception(\"Failed to parse Yaml data with unexpected error\")\n            raise\n\n        return WorkflowYamlData(\n            connections=connections,\n            nodes=nodes,\n            flows=flows,\n            workflows=workflows,\n        )\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.apply_resolved_requirements","title":"<code>apply_resolved_requirements(data, resolved_requirements)</code>  <code>classmethod</code>","text":"<p>Apply resolved requirement data to YAML dict in-place before initialization.</p> <p>Replaces requirement dicts (identified by both $type and $id) with the corresponding resolved data. The resolved value can be any type (dict, list, string, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Raw YAML data dictionary (will be mutated).</p> required <code>resolved_requirements</code> <code>dict[str, Any]</code> <p>Dict mapping $id to resolved data (any type).</p> required <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If a $id cannot be resolved.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef apply_resolved_requirements(\n    cls,\n    data: dict,\n    resolved_requirements: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    Apply resolved requirement data to YAML dict in-place before initialization.\n\n    Replaces requirement dicts (identified by both $type and $id) with the corresponding\n    resolved data. The resolved value can be any type (dict, list, string, etc.).\n\n    Args:\n        data: Raw YAML data dictionary (will be mutated).\n        resolved_requirements: Dict mapping $id to resolved data (any type).\n\n    Raises:\n        WorkflowYAMLLoaderException: If a $id cannot be resolved.\n    \"\"\"\n    cls._apply_requirements_to_dict(data, resolved_requirements)\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_connections","title":"<code>get_connections(data, registry)</code>  <code>classmethod</code>","text":"<p>Get connections from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, dict]</code> <p>The data containing connection information.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>A registry of entities.</p> required <p>Returns:</p> Type Description <code>dict[str, BaseConnection]</code> <p>A dictionary of connections.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in connection data or initialization.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_connections(\n    cls,\n    data: dict[str, dict],\n    registry: dict[str, Any],\n) -&gt; dict[str, BaseConnection]:\n    \"\"\"\n    Get connections from the provided data.\n\n    Args:\n        data: The data containing connection information.\n        registry: A registry of entities.\n\n    Returns:\n        A dictionary of connections.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in connection data or initialization.\n    \"\"\"\n    connections = {}\n\n    for conn_id, conn_data in data.get(\"connections\", {}).items():\n        if conn_id in connections:\n            raise WorkflowYAMLLoaderException(f\"Connection '{conn_id}' already exists\")\n        if not (conn_type := conn_data.get(\"type\")):\n            raise WorkflowYAMLLoaderException(f\"Value 'type' not found for connection '{conn_id}'\")\n\n        conn_cls = cls.get_entity_by_type(entity_type=conn_type, entity_registry=registry)\n        conn_init_data = conn_data | {\"id\": conn_id}\n        conn_init_data.pop(\"type\", None)\n        try:\n            connection = conn_cls(**conn_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(f\"Connection '{conn_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n        connections[conn_id] = connection\n\n    return connections\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_dependant_flows","title":"<code>get_dependant_flows(nodes_data, flows_data, dependant_nodes, connection_manager=None)</code>  <code>classmethod</code>","text":"<p>Get flows that are dependent on nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict[str, dict]</code> <p>Dictionary containing node data.</p> required <code>flows_data</code> <code>dict[str, dict]</code> <p>Dictionary containing flow data.</p> required <code>dependant_nodes</code> <code>dict[str, Node]</code> <p>Dictionary of dependent nodes.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Flow]</code> <p>A dictionary of flows that are dependent on nodes.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_dependant_flows(\n    cls,\n    nodes_data: dict[str, dict],\n    flows_data: dict[str, dict],\n    dependant_nodes: dict[str, Node],\n    connection_manager: ConnectionManager | None = None,\n) -&gt; dict[str, Flow]:\n    \"\"\"\n    Get flows that are dependent on nodes.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        flows_data: Dictionary containing flow data.\n        dependant_nodes: Dictionary of dependent nodes.\n        connection_manager: Optional connection manager.\n\n    Returns:\n        A dictionary of flows that are dependent on nodes.\n    \"\"\"\n    dependant_flows = {}\n    dependant_flow_ids = []\n\n    for node_id, node_data in nodes_data.items():\n        if \"flow\" in node_data:\n            dependant_flow_ids.append(node_data[\"flow\"])\n        if \"flows\" in node_data:\n            dependant_flow_ids.extend(node_data[\"flows\"])\n\n    if dependant_flow_ids:\n        dependant_flows_data = {\n            flow_id: flow_data for flow_id, flow_data in flows_data.items() if flow_id in dependant_flow_ids\n        }\n        dependant_flows = cls.get_flows(\n            data=dependant_flows_data,\n            flows={},\n            nodes=dependant_nodes,\n            connection_manager=connection_manager,\n        )\n\n    return dependant_flows\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_dependant_nodes","title":"<code>get_dependant_nodes(nodes_data, flows_data, connections, prompts, registry, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Get nodes that are dependent on flows.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict[str, dict]</code> <p>Dictionary containing node data.</p> required <code>flows_data</code> <code>dict[str, dict]</code> <p>Dictionary containing flow data.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Node]</code> <p>A dictionary of nodes that are dependent on flows.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_dependant_nodes(\n    cls,\n    nodes_data: dict[str, dict],\n    flows_data: dict[str, dict],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n) -&gt; dict[str, Node]:\n    \"\"\"\n    Get nodes that are dependent on flows.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        flows_data: Dictionary containing flow data.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        A dictionary of nodes that are dependent on flows.\n    \"\"\"\n    dependant_nodes, dependant_nodes_data = {}, {}\n    dependant_flow_ids = []\n\n    for node_id, node_data in nodes_data.items():\n        if \"flow\" in node_data:\n            dependant_nodes_data[node_id] = node_data\n            dependant_flow_ids.append(node_data[\"flow\"])\n        if \"flows\" in node_data:\n            dependant_nodes_data[node_id] = node_data\n            dependant_flow_ids.extend(node_data[\"flows\"])\n\n    # Get nodes from dependant flows\n    if dependant_flow_ids:\n        dependant_flows_nodes_ids = []\n        for flow_id, flow_data in flows_data.items():\n            if flow_id in dependant_flow_ids:\n                dependant_flows_nodes_ids.extend(flow_data.get(\"nodes\", []))\n\n        dependant_flows_nodes_data = {\n            node_id: node_data for node_id, node_data in nodes_data.items() if node_id in dependant_flows_nodes_ids\n        }\n\n        dependant_nodes = cls.get_nodes(\n            nodes_data=dependant_flows_nodes_data,\n            nodes={},\n            flows={},\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n\n    return dependant_nodes\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_entity_by_type","title":"<code>get_entity_by_type(entity_type, entity_registry=None)</code>  <code>classmethod</code>","text":"<p>Try to get entity by type and update mutable shared registry.</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of entity to retrieve.</p> required <code>entity_registry</code> <code>dict[str, Any] | None</code> <p>A registry of entities.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The retrieved entity.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the entity is not valid or cannot be found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_entity_by_type(cls, entity_type: str, entity_registry: dict[str, Any] | None = None) -&gt; Any:\n    \"\"\"\n    Try to get entity by type and update mutable shared registry.\n\n    Args:\n        entity_type (str): The type of entity to retrieve.\n        entity_registry (dict[str, Any] | None): A registry of entities.\n\n    Returns:\n        Any: The retrieved entity.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the entity is not valid or cannot be found.\n    \"\"\"\n    if entity_registry is None:\n        entity_registry = {}\n\n    if entity := entity_registry.get(entity_type):\n        return entity\n\n    try:\n        entity = ConnectionManager.get_connection_by_type(entity_type)\n    except ValueError:\n        pass\n\n    if not entity:\n        try:\n            entity = NodeManager.get_node_by_type(entity_type)\n        except ValueError:\n            pass\n\n    if not entity:\n        raise WorkflowYAMLLoaderException(f\"Entity '{entity_type}' is not valid.\")\n\n    entity_registry[entity_type] = entity\n    return entity\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_flows","title":"<code>get_flows(data, flows, nodes, connection_manager=None)</code>  <code>classmethod</code>","text":"<p>Create flows from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing flow data.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Flow]</code> <p>A dictionary of newly created flows.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If flow data is invalid or duplicates are found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_flows(\n    cls,\n    data: dict,\n    flows: dict[str, Flow],\n    nodes: dict[str, Node],\n    connection_manager: ConnectionManager | None = None,\n) -&gt; dict[str, Flow]:\n    \"\"\"\n    Create flows from the given data.\n\n    Args:\n        data: Dictionary containing flow data.\n        flows: Existing flows dictionary.\n        nodes: Existing nodes dictionary.\n        connection_manager: Optional connection manager.\n\n    Returns:\n        A dictionary of newly created flows.\n\n    Raises:\n        WorkflowYAMLLoaderException: If flow data is invalid or duplicates are found.\n    \"\"\"\n    new_flows = {}\n    for flow_id, flow_data in data.items():\n        if flow_id in flows:\n            continue\n\n        if flow_id in new_flows:\n            raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' already exists\")\n\n        flow_node_ids = flow_data.get(\"nodes\", [])\n        flow_node_ids = set(flow_node_ids)\n        dep_node_ids = set()\n        for node_id in flow_node_ids:\n            if node_id not in nodes:\n                raise WorkflowYAMLLoaderException(f\"Node '{node_id}' for flow '{flow_id}' not found\")\n\n            dep_node_ids.update({dep.node.id for dep in nodes[node_id].depends})\n\n        for node_id in dep_node_ids:\n            if node_id not in flow_node_ids:\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency node '{node_id}' in the flow '{flow_id}' node list not found\"\n                )\n\n        flow_init_data = flow_data | {\n            \"id\": flow_id,\n            \"nodes\": [nodes[node_id] for node_id in flow_node_ids],\n        }\n        if connection_manager:\n            flow_init_data[\"connection_manager\"] = connection_manager\n\n        try:\n            flow = Flow(**flow_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n        new_flows[flow_id] = flow\n    return new_flows\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_inline_connection","title":"<code>get_inline_connection(node_id, conn_data, registry=None)</code>  <code>classmethod</code>","text":"<p>Create an inline connection from node's connection data.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node that uses this connection.</p> required <code>conn_data</code> <code>dict</code> <p>The connection data dictionary.</p> required <code>registry</code> <code>dict[str, Any] | None</code> <p>A registry of entities.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseConnection</code> <p>The created connection.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the connection data is invalid.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_inline_connection(\n    cls,\n    node_id: str,\n    conn_data: dict,\n    registry: dict[str, Any] | None = None,\n) -&gt; BaseConnection:\n    \"\"\"\n    Create an inline connection from node's connection data.\n\n    Args:\n        node_id: The ID of the node that uses this connection.\n        conn_data: The connection data dictionary.\n        registry: A registry of entities.\n\n    Returns:\n        The created connection.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the connection data is invalid.\n    \"\"\"\n    if not (conn_type := conn_data.get(\"type\")):\n        raise WorkflowYAMLLoaderException(f\"Value 'type' not found for inline connection in node '{node_id}'\")\n\n    conn_cls = cls.get_entity_by_type(entity_type=conn_type, entity_registry=registry)\n    conn_init_data = conn_data.copy()\n    conn_init_data.pop(\"type\", None)\n\n    # Generate UUID for connection id if not provided\n    if \"id\" not in conn_init_data:\n        conn_init_data[\"id\"] = generate_uuid()\n\n    try:\n        connection = conn_cls(**conn_init_data)\n    except Exception as e:\n        raise WorkflowYAMLLoaderException(\n            f\"Inline connection for node '{node_id}' data is invalid. Error: {str(e) or repr(e)}\"\n        )\n\n    return connection\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_connection","title":"<code>get_node_connection(node_id, node_data, connections, registry=None)</code>  <code>classmethod</code>","text":"<p>Get the connection for a node.</p> <p>Supports both: - Reference by ID (string): Looks up connection in the connections dictionary. - Inline connection (dict): Creates a new connection from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>A dictionary of available connections.</p> required <code>registry</code> <code>dict[str, Any] | None</code> <p>A registry of entities (required for inline connections).</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseConnection | None</code> <p>The connection for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified connection is not found or invalid.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_connection(\n    cls,\n    node_id: str,\n    node_data: dict,\n    connections: dict[str, BaseConnection],\n    registry: dict[str, Any] | None = None,\n) -&gt; BaseConnection | None:\n    \"\"\"\n    Get the connection for a node.\n\n    Supports both:\n    - Reference by ID (string): Looks up connection in the connections dictionary.\n    - Inline connection (dict): Creates a new connection from the provided data.\n\n    Args:\n        node_id: The ID of the node.\n        node_data: The data for the node.\n        connections: A dictionary of available connections.\n        registry: A registry of entities (required for inline connections).\n\n    Returns:\n        The connection for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified connection is not found or invalid.\n    \"\"\"\n    conn_value = node_data.get(\"connection\")\n    if conn_value is None:\n        return\n\n    if isinstance(conn_value, str):\n        conn = connections.get(conn_value)\n        if not conn:\n            raise WorkflowYAMLLoaderException(f\"Connection '{conn_value}' for node '{node_id}' not found\")\n        return conn\n\n    if isinstance(conn_value, dict):\n        connection = cls.get_inline_connection(\n            node_id=node_id,\n            conn_data=conn_value,\n            registry=registry,\n        )\n        connections[connection.id] = connection\n        return connection\n\n    raise WorkflowYAMLLoaderException(\n        f\"Invalid connection format for node '{node_id}'. Expected string (reference) or dict (inline).\"\n    )\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_dependencies","title":"<code>get_node_dependencies(node_id, node_data, nodes)</code>  <code>classmethod</code>","text":"<p>Get the dependencies for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>A dictionary of available nodes.</p> required <p>Returns:</p> Type Description <p>list[NodeDependency]: A list of node dependencies.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in dependency data or initialization.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_dependencies(cls, node_id: str, node_data: dict, nodes: dict[str, Node]):\n    \"\"\"\n    Get the dependencies for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        nodes (dict[str, Node]): A dictionary of available nodes.\n\n    Returns:\n        list[NodeDependency]: A list of node dependencies.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in dependency data or initialization.\n    \"\"\"\n    node_depends = []\n    for dependency_data in node_data.get(\"depends\", []):\n        dependency_node = nodes.get(dependency_data.get(\"node\"))\n        dependency_init_data = dependency_data | {\"node\": dependency_node}\n        try:\n            dependency = NodeDependency(**dependency_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Dependency '{dependency_data.get('node')}' data for node '{node_id}' \"\n                f\"is invalid. Error: {str(e) or repr(e)}\"\n            )\n\n        if dependency.option:\n            if not (dep_options := getattr(dependency_node, \"options\", [])):\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency '{dependency.node.id}' with option '{dependency.option}' \"\n                    f\"for node '{node_id}' not found\"\n                )\n\n            if not any(opt.id == dependency.option for opt in dep_options):\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency '{dependency.node.id}' with option '{dependency.option}' \"\n                    f\"for node '{node_id}' not found\"\n                )\n\n        node_depends.append(dependency)\n    return node_depends\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_flow","title":"<code>get_node_flow(node_id, node_data, flows)</code>  <code>classmethod</code>","text":"<p>Get the flow for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>flows</code> <code>dict[id, Flow]</code> <p>A dictionary of available flows.</p> required <p>Returns:</p> Type Description <code>Flow | None</code> <p>Flow | None: The flow for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified flow is not found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_flow(cls, node_id: str, node_data: dict, flows: dict[id, Flow]) -&gt; Flow | None:\n    \"\"\"\n    Get the flow for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        flows (dict[id, Flow]): A dictionary of available flows.\n\n    Returns:\n        Flow | None: The flow for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified flow is not found.\n    \"\"\"\n    flow = None\n    if flow_id := node_data.get(\"flow\"):\n        flow = flows.get(flow_id)\n        if not flow:\n            raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for node '{node_id}' not found\")\n    return flow\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_flows","title":"<code>get_node_flows(node_id, node_data, flows)</code>  <code>classmethod</code>","text":"<p>Get the flows for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>flows</code> <code>dict[id, Flow]</code> <p>A dictionary of available flows.</p> required <p>Returns:</p> Type Description <code>list[Flow]</code> <p>list[Flow]: A list of flows for the node.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If any specified flow is not found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_flows(cls, node_id: str, node_data: dict, flows: dict[id, Flow]) -&gt; list[Flow]:\n    \"\"\"\n    Get the flows for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        flows (dict[id, Flow]): A dictionary of available flows.\n\n    Returns:\n        list[Flow]: A list of flows for the node.\n\n    Raises:\n        WorkflowYAMLLoaderException: If any specified flow is not found.\n    \"\"\"\n    node_flows = []\n    for flow_id in node_data.get(\"flows\", []):\n        node_flow = flows.get(flow_id)\n        if not node_flow:\n            raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for node '{node_id}' not found\")\n        node_flows.append(node_flow)\n    return node_flows\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_prompt","title":"<code>get_node_prompt(node_id, node_data, prompts)</code>  <code>classmethod</code>","text":"<p>Get the prompt for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>prompts</code> <code>dict[id, Prompt]</code> <p>A dictionary of available prompts.</p> required <p>Returns:</p> Type Description <code>Prompt | None</code> <p>Prompt | None: The prompt for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified prompt is not found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_prompt(cls, node_id: str, node_data: dict, prompts: dict[id, Prompt]) -&gt; Prompt | None:\n    \"\"\"\n    Get the prompt for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        prompts (dict[id, Prompt]): A dictionary of available prompts.\n\n    Returns:\n        Prompt | None: The prompt for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified prompt is not found.\n    \"\"\"\n    prompt = None\n    if prompt_id := node_data.get(\"prompt\"):\n        prompt = prompts.get(prompt_id)\n        if not prompt:\n            raise WorkflowYAMLLoaderException(f\"Prompt '{prompt_id}' for node '{node_id}' not found\")\n    return prompt\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_vector_store_connection","title":"<code>get_node_vector_store_connection(node_id, node_data, connections, registry=None)</code>  <code>classmethod</code>","text":"<p>Get the vector store connection for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>A dictionary of available connections.</p> required <code>registry</code> <code>dict[str, Any] | None</code> <p>A registry of entities (required for inline connections).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>The vector store connection for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified vector store connection is not found or                          does not support vector store initialization.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_vector_store_connection(\n    cls,\n    node_id: str,\n    node_data: dict,\n    connections: dict[str, BaseConnection],\n    registry: dict[str, Any] | None = None,\n) -&gt; Any | None:\n    \"\"\"\n    Get the vector store connection for a node.\n\n    Args:\n        node_id: The ID of the node.\n        node_data: The data for the node.\n        connections: A dictionary of available connections.\n        registry: A registry of entities (required for inline connections).\n\n    Returns:\n        The vector store connection for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified vector store connection is not found or\n                                     does not support vector store initialization.\n    \"\"\"\n    conn = cls.get_node_connection(\n        node_id=node_id,\n        node_data=node_data,\n        connections=connections,\n        registry=registry,\n    )\n    if conn:\n        if not (conn_to_vs := getattr(conn, \"connect_to_vector_store\", None)) or not callable(conn_to_vs):\n            raise WorkflowYAMLLoaderException(\n                f\"Vector store connection '{conn.id}' for node '{node_id}' not support vector store initialization\"\n            )\n    return conn\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_node_without_depends","title":"<code>get_node_without_depends(node_id, node_data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Create a single node without dependencies from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>Node identifier.</p> required <code>node_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[str, Node]</code> <p>A tuple of (node_id, node).</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If node data is invalid.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_without_depends(\n    cls,\n    node_id: str,\n    node_data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n) -&gt; tuple[str, Node]:\n    \"\"\"\n    Create a single node without dependencies from the given data.\n\n    Args:\n        node_id: Node identifier.\n        node_data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        A tuple of (node_id, node).\n\n    Raises:\n        WorkflowYAMLLoaderException: If node data is invalid.\n    \"\"\"\n    if not (node_type := node_data.get(\"type\")):\n        raise WorkflowYAMLLoaderException(f\"Value 'type' for node '{node_id}' not found\")\n\n    node_cls = cls.get_entity_by_type(entity_type=node_type, entity_registry=registry)\n\n    # Init node params\n    node_init_data = node_data.copy()\n    if node_id:\n        node_init_data[\"id\"] = node_id\n    node_init_data.pop(\"type\", None)\n    node_init_data.pop(\"depends\", None)\n\n    if \"is_postponed_component_init\" not in node_init_data:\n        node_init_data[\"is_postponed_component_init\"] = True\n\n    if \"connection\" in node_init_data:\n        get_node_conn = (\n            cls.get_node_vector_store_connection\n            if isinstance(node_cls, ConnectionNode)\n            else cls.get_node_connection\n        )\n        node_init_data[\"connection\"] = get_node_conn(\n            node_id=node_id,\n            node_data=node_data,\n            connections=connections,\n            registry=registry,\n        )\n    if prompt_data := node_init_data.get(\"prompt\"):\n        node_init_data[\"prompt\"] = (\n            cls.get_node_prompt(node_id=node_id, node_data=node_data, prompts=prompts)\n            if isinstance(prompt_data, str)\n            else cls.init_prompt(prompt_data)\n        )\n    if \"flow\" in node_init_data:\n        node_init_data[\"flow\"] = cls.get_node_flow(node_id=node_id, node_data=node_data, flows=flows)\n    if \"flows\" in node_init_data:\n        node_init_data[\"flows\"] = cls.get_node_flows(node_id=node_id, node_data=node_data, flows=flows)\n\n    try:\n        node_init_data = cls.get_updated_node_init_data_with_initialized_nodes(\n            node_init_data=node_init_data,\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n\n        node = node_cls(**node_init_data)\n\n        if init_components and getattr(node, \"init_components\", False):\n            node.init_components(connection_manager=connection_manager)\n            node.is_postponed_component_init = False\n\n    except Exception as e:\n        logger.exception(f\"Node '{node_id}' data is invalid\")\n        raise WorkflowYAMLLoaderException(f\"Node '{node_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n    return node_id, node\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_nodes","title":"<code>get_nodes(nodes_data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Create nodes with dependencies from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of newly created nodes with dependencies.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_nodes(\n    cls,\n    nodes_data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n):\n    \"\"\"\n    Create nodes with dependencies from the given data.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        A dictionary of newly created nodes with dependencies.\n    \"\"\"\n\n    new_nodes = cls.get_nodes_without_depends(\n        data=nodes_data,\n        nodes=nodes,\n        flows=flows,\n        connections=connections,\n        prompts=prompts,\n        registry=registry,\n        connection_manager=connection_manager,\n        init_components=init_components,\n        max_workers=max_workers,\n    )\n\n    all_nodes = nodes | new_nodes\n    for node_id, node in new_nodes.items():\n        node.depends = cls.get_node_dependencies(node_id=node_id, node_data=nodes_data[node_id], nodes=all_nodes)\n\n    return new_nodes\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_nodes_without_depends","title":"<code>get_nodes_without_depends(data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Create nodes without dependencies from the given data. Automatically uses parallel processing for multiple nodes (&gt;1).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Node]</code> <p>A dictionary of newly created nodes without dependencies.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If node data is invalid or duplicates are found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_nodes_without_depends(\n    cls,\n    data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n) -&gt; dict[str, Node]:\n    \"\"\"\n    Create nodes without dependencies from the given data.\n    Automatically uses parallel processing for multiple nodes (&gt;1).\n\n    Args:\n        data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        A dictionary of newly created nodes without dependencies.\n\n    Raises:\n        WorkflowYAMLLoaderException: If node data is invalid or duplicates are found.\n    \"\"\"\n    new_nodes = {}\n    get_node_kwargs = dict(\n        nodes=nodes,\n        flows=flows,\n        connections=connections,\n        prompts=prompts,\n        registry=registry,\n        connection_manager=connection_manager,\n        init_components=init_components,\n    )\n\n    nodes_to_create = {}\n    for node_id, node_data in data.items():\n        if node_id in nodes:\n            continue\n        if node_id in new_nodes:\n            raise WorkflowYAMLLoaderException(f\"Node '{node_id}' already exists\")\n        nodes_to_create[node_id] = node_data\n\n    if len(nodes_to_create) &lt;= 1:\n        for node_id, node_data in nodes_to_create.items():\n            try:\n                node_id, node = cls.get_node_without_depends(\n                    node_id=node_id,\n                    node_data=node_data,\n                    **get_node_kwargs,\n                )\n                new_nodes[node_id] = node\n            except WorkflowYAMLLoaderException:\n                raise\n\n        return new_nodes\n\n    with ContextAwareThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_node_id = {\n            executor.submit(\n                cls.get_node_without_depends,\n                node_id=node_id,\n                node_data=node_data,\n                **get_node_kwargs,\n            ): node_id\n            for node_id, node_data in nodes_to_create.items()\n        }\n\n        for future in as_completed(future_to_node_id):\n            node_id = future_to_node_id[future]\n            try:\n                result_node_id, node = future.result()\n                new_nodes[result_node_id] = node\n            except WorkflowYAMLLoaderException:\n                raise\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(f\"Node '{node_id}' processing failed. Error: {str(e) or repr(e)}\")\n\n    return new_nodes\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_prompts","title":"<code>get_prompts(data)</code>  <code>classmethod</code>","text":"<p>Get prompts from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, dict]</code> <p>The data containing prompt information.</p> required <p>Returns:</p> Type Description <code>dict[str, Prompt]</code> <p>dict[str, Prompt]: A dictionary of prompts.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in prompt data or initialization.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_prompts(cls, data: dict[str, dict]) -&gt; dict[str, Prompt]:\n    \"\"\"\n    Get prompts from the provided data.\n\n    Args:\n        data (dict[str, dict]): The data containing prompt information.\n\n    Returns:\n        dict[str, Prompt]: A dictionary of prompts.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in prompt data or initialization.\n    \"\"\"\n    prompts = {}\n    for prompt_id, prompt_data in data.get(\"prompts\", {}).items():\n        if prompt_id in prompts:\n            raise WorkflowYAMLLoaderException(f\"Prompt '{prompt_id}' already exists\")\n        prompts[prompt_id] = cls.init_prompt(prompt_data | {\"id\": prompt_id})\n\n    return prompts\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_requirements","title":"<code>get_requirements(data)</code>  <code>classmethod</code>","text":"<p>Recursively scan YAML data and extract all requirement dicts.</p> <p>A requirement dict is identified by having both $type and $id fields. This method does NOT initialize any components.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing workflow YAML data.</p> required <p>Returns:</p> Type Description <code>list[RequirementData]</code> <p>List of RequirementData for all requirement dicts.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_requirements(cls, data: dict) -&gt; list[RequirementData]:\n    \"\"\"\n    Recursively scan YAML data and extract all requirement dicts.\n\n    A requirement dict is identified by having both $type and $id fields.\n    This method does NOT initialize any components.\n\n    Args:\n        data: Dictionary containing workflow YAML data.\n\n    Returns:\n        List of RequirementData for all requirement dicts.\n    \"\"\"\n    requirements: list[RequirementData] = []\n    cls._get_requirements_from_dict(data, requirements)\n    return requirements\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_updated_node_init_data_with_initialized_nodes","title":"<code>get_updated_node_init_data_with_initialized_nodes(node_init_data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Get node init data with initialized nodes components recursively (llms, agents, etc)</p> <p>Parameters:</p> Name Type Description Default <code>node_init_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of newly created nodes with dependencies.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_updated_node_init_data_with_initialized_nodes(\n    cls,\n    node_init_data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n):\n    \"\"\"\n    Get node init data with initialized nodes components recursively (llms, agents, etc)\n\n    Args:\n        node_init_data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        A dictionary of newly created nodes with dependencies.\n    \"\"\"\n    updated_node_init_data = {}\n    kwargs = dict(\n        nodes=nodes,\n        flows=flows,\n        connections=connections,\n        prompts=prompts,\n        registry=registry,\n        connection_manager=connection_manager,\n        init_components=init_components,\n        max_workers=max_workers,\n    )\n    for param_name, param_data in node_init_data.items():\n        # TODO: dummy fix, revisit this!\n        # We had to add this condition because some nodes have a `schema`/`response_format` params,\n        # that have a `type` field that contains types supported by JSON schema (e.g., string, object).\n        if param_name in (\"schema\", \"response_format\"):\n            updated_node_init_data[param_name] = param_data\n\n        elif isinstance(param_data, dict):\n            updated_param_data = {}\n            for param_name_inner, param_data_inner in param_data.items():\n                if param_name_inner in (\"prompt\", \"schema\", \"response_format\"):\n                    updated_param_data[param_name_inner] = param_data_inner\n                elif isinstance(param_data_inner, (dict, list)):\n                    param_id = None\n                    updated_param_data[param_name_inner] = cls.get_updated_node_init_data_with_initialized_nodes(\n                        {param_id: param_data_inner}, **kwargs\n                    )[param_id]\n                else:\n                    updated_param_data[param_name_inner] = param_data_inner\n\n            if cls.is_node_type(updated_param_data.get(\"type\")):\n                param_id = updated_param_data.get(\"id\")\n                updated_param_data = cls.get_nodes_without_depends({param_id: updated_param_data}, **kwargs)[\n                    param_id\n                ]\n\n            updated_node_init_data[param_name] = updated_param_data\n\n        elif isinstance(param_data, list):\n            updated_items = []\n            for item in param_data:\n                if isinstance(item, (dict, list)):\n                    param_id = None\n                    updated_items.append(\n                        cls.get_updated_node_init_data_with_initialized_nodes(\n                            node_init_data={param_id: item}, **kwargs\n                        )[param_id]\n                    )\n                else:\n                    updated_items.append(item)\n            updated_node_init_data[param_name] = updated_items\n\n        else:\n            updated_node_init_data[param_name] = param_data\n\n    return updated_node_init_data\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.get_workflows","title":"<code>get_workflows(data, flows)</code>  <code>classmethod</code>","text":"<p>Create workflows from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing workflow data.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <p>Returns:</p> Type Description <code>dict[str, Workflow]</code> <p>A dictionary of newly created workflows.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If workflow data is invalid.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_workflows(cls, data: dict, flows: dict[str, Flow]) -&gt; dict[str, Workflow]:\n    \"\"\"\n    Create workflows from the given data.\n\n    Args:\n        data: Dictionary containing workflow data.\n        flows: Existing flows dictionary.\n\n    Returns:\n        A dictionary of newly created workflows.\n\n    Raises:\n        WorkflowYAMLLoaderException: If workflow data is invalid.\n    \"\"\"\n    workflows = {}\n    for wf_id, wf_data in data.get(\"workflows\", {}).items():\n        if not (flow_id := wf_data.get(\"flow\")):\n            raise WorkflowYAMLLoaderException(f\"Value 'flow' for dynamiq '{wf_id}' not found \")\n        if not (flow := flows.get(flow_id)):\n            raise WorkflowYAMLLoaderException(f\"Flow '{flow_id}' for dynamiq '{wf_id}' not found\")\n        if version := wf_data.get(\"version\"):\n            version = str(version)\n\n        try:\n            wf = Workflow(id=wf_id, flow=flow, version=version)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(f\"Workflow '{wf_id}' data is invalid. Error: {str(e) or repr(e)}\")\n\n        workflows[wf_id] = wf\n    return workflows\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.init_prompt","title":"<code>init_prompt(prompt_init_data)</code>  <code>classmethod</code>","text":"<p>Initialize a prompt from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_init_data</code> <code>dict</code> <p>The data for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The initialized prompt.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified prompt is not found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef init_prompt(cls, prompt_init_data: dict) -&gt; Prompt:\n    \"\"\"\n    Initialize a prompt from the provided data.\n\n    Args:\n        prompt_init_data (dict): The data for the prompt.\n\n    Returns:\n        Prompt: The initialized prompt.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified prompt is not found.\n    \"\"\"\n    try:\n        return Prompt(**prompt_init_data)\n    except Exception as e:\n        raise WorkflowYAMLLoaderException(\n            f\"Prompt '{prompt_init_data.get('id')}' data is invalid. Error: {str(e) or repr(e)}\"\n        )\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.is_node_type","title":"<code>is_node_type(type_value)</code>  <code>staticmethod</code>","text":"<p>Check if the type value represents a node type (dotted path like 'module.ClassName').</p> <p>Node types use dotted path format (e.g., 'dynamiq.nodes.agents.Agent'), while other types (e.g., JSON schema types like 'string', 'object') don't contain dots.</p> <p>Parameters:</p> Name Type Description Default <code>type_value</code> <code>str | None</code> <p>The type string to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the type value is a node type (contains a dot), False otherwise.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@staticmethod\ndef is_node_type(type_value: str | None) -&gt; bool:\n    \"\"\"\n    Check if the type value represents a node type (dotted path like 'module.ClassName').\n\n    Node types use dotted path format (e.g., 'dynamiq.nodes.agents.Agent'),\n    while other types (e.g., JSON schema types like 'string', 'object') don't contain dots.\n\n    Args:\n        type_value: The type string to check.\n\n    Returns:\n        True if the type value is a node type (contains a dot), False otherwise.\n    \"\"\"\n    return bool(type_value and \".\" in type_value)\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.load","title":"<code>load(file_path, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Load data from a YAML file and parse it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike</code> <p>Path to the YAML file.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <code>WorkflowYamlData</code> <p>Parsed WorkflowYamlData object.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    file_path: str | PathLike,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n) -&gt; WorkflowYamlData:\n    \"\"\"\n    Load data from a YAML file and parse it.\n\n    Args:\n        file_path: Path to the YAML file.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        Parsed WorkflowYamlData object.\n    \"\"\"\n    data = cls.loads(file_path)\n    return cls.parse(\n        data=data,\n        connection_manager=connection_manager,\n        init_components=init_components,\n        max_workers=max_workers,\n    )\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.loads","title":"<code>loads(file_path)</code>  <code>classmethod</code>","text":"<p>Load data from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike</code> <p>Path to the YAML file.</p> required <p>Returns:</p> Type Description <p>Parsed data from the YAML file.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the file is not found.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef loads(cls, file_path: str | PathLike):\n    \"\"\"\n    Load data from a YAML file.\n\n    Args:\n        file_path: Path to the YAML file.\n\n    Returns:\n        Parsed data from the YAML file.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the file is not found.\n    \"\"\"\n    from omegaconf import OmegaConf\n\n    try:\n        conf = OmegaConf.load(file_path)\n        logger.debug(f\"Loaded config from '{file_path}'\")\n\n        data = OmegaConf.to_container(conf, resolve=True)\n    except FileNotFoundError:\n        raise WorkflowYAMLLoaderException(f\"File '{file_path}' not found\")\n\n    return data\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.parse","title":"<code>parse(data, connection_manager=None, init_components=False, max_workers=None)</code>  <code>classmethod</code>","text":"<p>Parse dynamiq workflow data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing workflow data.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of worker threads for node parallel processing.</p> <code>None</code> <p>Returns:</p> Type Description <code>WorkflowYamlData</code> <p>Parsed WorkflowYamlData object.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If parsing fails.</p> Usage Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>@classmethod\ndef parse(\n    cls,\n    data: dict,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n    max_workers: int | None = None,\n) -&gt; WorkflowYamlData:\n    \"\"\"\n    Parse dynamiq workflow data.\n\n    Args:\n        data: Dictionary containing workflow data.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n        max_workers: Maximum number of worker threads for node parallel processing.\n\n    Returns:\n        Parsed WorkflowYamlData object.\n\n    Raises:\n        WorkflowYAMLLoaderException: If parsing fails.\n\n    Usage:\n        # Step 1: Extract all requirements without initialization\n        requirements = WorkflowYAMLLoader.get_requirements(data)\n\n        # Step 2: Resolve requirements via external API\n        resolved = {req.id: api_fetch(req.id) for req in requirements}\n\n        # Step 3: Apply resolved requirements to YAML data (mutates in-place)\n        WorkflowYAMLLoader.apply_resolved_requirements(data, resolved)\n\n        # Step 4: Parse the data\n        result = WorkflowYAMLLoader.parse(data)\n    \"\"\"\n    nodes, flows = {}, {}\n    # Mutable shared registry that updates with each new entity.\n    node_registry, connection_registry = {}, {}\n\n    if init_components and connection_manager is None:\n        connection_manager = ConnectionManager()\n\n    try:\n        connections = cls.get_connections(\n            data=data,\n            registry=connection_registry,\n        )\n        prompts = cls.get_prompts(data)\n\n        nodes_data = data.get(\"nodes\", {})\n        flows_data = data.get(\"flows\", {})\n\n        dependant_nodes = cls.get_dependant_nodes(\n            nodes_data=nodes_data,\n            flows_data=flows_data,\n            connections=connections,\n            prompts=prompts,\n            registry=node_registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n        nodes.update(dependant_nodes)\n\n        dependant_flows = cls.get_dependant_flows(\n            nodes_data=nodes_data,\n            flows_data=flows_data,\n            dependant_nodes=dependant_nodes,\n            connection_manager=connection_manager,\n        )\n        flows.update(dependant_flows)\n\n        non_dependant_nodes = cls.get_nodes(\n            nodes_data=nodes_data,\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=node_registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n            max_workers=max_workers,\n        )\n        nodes.update(non_dependant_nodes)\n\n        non_dependant_flows = cls.get_flows(\n            data=flows_data,\n            flows=flows,\n            nodes=nodes,\n            connection_manager=connection_manager,\n        )\n        flows.update(non_dependant_flows)\n\n        workflows = cls.get_workflows(data, flows)\n    except WorkflowYAMLLoaderException:\n        raise\n    except Exception:\n        logger.exception(\"Failed to parse Yaml data with unexpected error\")\n        raise\n\n    return WorkflowYamlData(\n        connections=connections,\n        nodes=nodes,\n        flows=flows,\n        workflows=workflows,\n    )\n</code></pre>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.parse--step-1-extract-all-requirements-without-initialization","title":"Step 1: Extract all requirements without initialization","text":"<p>requirements = WorkflowYAMLLoader.get_requirements(data)</p>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.parse--step-2-resolve-requirements-via-external-api","title":"Step 2: Resolve requirements via external API","text":"<p>resolved = {req.id: api_fetch(req.id) for req in requirements}</p>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.parse--step-3-apply-resolved-requirements-to-yaml-data-mutates-in-place","title":"Step 3: Apply resolved requirements to YAML data (mutates in-place)","text":"<p>WorkflowYAMLLoader.apply_resolved_requirements(data, resolved)</p>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoader.parse--step-4-parse-the-data","title":"Step 4: Parse the data","text":"<p>result = WorkflowYAMLLoader.parse(data)</p>"},{"location":"dynamiq/serializers/loaders/yaml/#dynamiq.serializers.loaders.yaml.WorkflowYAMLLoaderException","title":"<code>WorkflowYAMLLoaderException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in the WorkflowYAMLDumper.</p> Source code in <code>dynamiq/serializers/loaders/yaml.py</code> <pre><code>class WorkflowYAMLLoaderException(Exception):\n    \"\"\"Exception raised for errors in the WorkflowYAMLDumper.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/skills/config/","title":"Config","text":"<p>Skills configuration: enabled + source registry (Dynamiq or FileSystem).</p>"},{"location":"dynamiq/skills/config/#dynamiq.skills.config.SkillsConfig","title":"<code>SkillsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for agent skills.</p> Source code in <code>dynamiq/skills/config.py</code> <pre><code>class SkillsConfig(BaseModel):\n    \"\"\"Configuration for agent skills.\"\"\"\n\n    enabled: bool = Field(default=False, description=\"Enable skill support for the agent.\")\n    source: BaseSkillRegistry | None = Field(\n        default=None,\n        description=\"Registry providing skills (Dynamiq or FileSystem).\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"source\", mode=\"before\")\n    @classmethod\n    def resolve_source(cls, v: Any) -&gt; Any:\n        if v is None or isinstance(v, BaseSkillRegistry):\n            return v\n        if isinstance(v, dict):\n            registry_type = v.get(\"type\")\n            if not registry_type:\n                return v\n            if \".\" not in registry_type:\n                raise ValueError(\n                    \"Registry type must be a fully qualified class name\"\n                    \" (e.g. dynamiq.skills.registries.filesystem.FileSystem).\"\n                )\n            module_name, class_name = registry_type.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            registry_cls = getattr(module, class_name)\n            init_data = {k: val for k, val in v.items() if k != \"type\"}\n            return registry_cls(**init_data)\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_enabled_source(self) -&gt; SkillsConfig:\n        if self.enabled and self.source is None:\n            raise ValueError(\"Skills are enabled but no source registry is configured.\")\n        return self\n\n    def get_skills_metadata(self) -&gt; list[SkillMetadata]:\n        if not self.enabled or self.source is None:\n            return []\n        return self.source.get_skills_metadata()\n\n    def get_skill_instructions(self, name: str) -&gt; SkillInstructions:\n        if not self.enabled or self.source is None:\n            raise SkillRegistryError(\"Skills are disabled for this agent.\")\n        return self.source.get_skill_instructions(name)\n\n    def to_dict(self, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Serialize config to dict for YAML/JSON. Source connection is serialized via connection.to_dict.\"\"\"\n        data = self.model_dump()\n        if self.source is not None and getattr(self.source, \"connection\", None) is not None:\n            if \"source\" in data:\n                for_tracing = kwargs.get(\"for_tracing\", False)\n                data[\"source\"][\"connection\"] = self.source.connection.to_dict(for_tracing=for_tracing)\n        return data\n</code></pre>"},{"location":"dynamiq/skills/config/#dynamiq.skills.config.SkillsConfig.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Serialize config to dict for YAML/JSON. Source connection is serialized via connection.to_dict.</p> Source code in <code>dynamiq/skills/config.py</code> <pre><code>def to_dict(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Serialize config to dict for YAML/JSON. Source connection is serialized via connection.to_dict.\"\"\"\n    data = self.model_dump()\n    if self.source is not None and getattr(self.source, \"connection\", None) is not None:\n        if \"source\" in data:\n            for_tracing = kwargs.get(\"for_tracing\", False)\n            data[\"source\"][\"connection\"] = self.source.connection.to_dict(for_tracing=for_tracing)\n    return data\n</code></pre>"},{"location":"dynamiq/skills/types/","title":"Types","text":""},{"location":"dynamiq/skills/types/#dynamiq.skills.types.SkillInstructions","title":"<code>SkillInstructions</code>","text":"<p>               Bases: <code>SkillMetadata</code></p> <p>Unified instructions shape for skills (registry API).</p> <p>Supports both plain-text instructions and API responses that include an instructions field plus optional metadata (e.g. version, format).</p> Source code in <code>dynamiq/skills/types.py</code> <pre><code>class SkillInstructions(SkillMetadata):\n    \"\"\"Unified instructions shape for skills (registry API).\n\n    Supports both plain-text instructions and API responses that include\n    an instructions field plus optional metadata (e.g. version, format).\n    \"\"\"\n\n    instructions: str = Field(default=\"\", description=\"Full skill instructions (e.g. SKILL.md content).\")\n    metadata: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Optional metadata from the registry API (e.g. version, content_type).\",\n    )\n</code></pre>"},{"location":"dynamiq/skills/types/#dynamiq.skills.types.SkillMetadata","title":"<code>SkillMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Unified metadata shape for skills (registry API).</p> Source code in <code>dynamiq/skills/types.py</code> <pre><code>class SkillMetadata(BaseModel):\n    \"\"\"Unified metadata shape for skills (registry API).\"\"\"\n\n    name: str = Field(..., description=\"Skill name.\")\n    description: str | None = Field(default=None, description=\"Optional skill description.\")\n</code></pre>"},{"location":"dynamiq/skills/types/#dynamiq.skills.types.SkillRegistryError","title":"<code>SkillRegistryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for skill registry operations.</p> Source code in <code>dynamiq/skills/types.py</code> <pre><code>class SkillRegistryError(Exception):\n    \"\"\"Base exception for skill registry operations.\"\"\"\n\n    def __init__(self, message: str, *, details: dict[str, Any] | None = None):\n        super().__init__(message)\n        self.details = details or {}\n</code></pre>"},{"location":"dynamiq/skills/utils/","title":"Utils","text":""},{"location":"dynamiq/skills/utils/#dynamiq.skills.utils.extract_skill_content_slice","title":"<code>extract_skill_content_slice(instructions, section=None, line_start=None, line_end=None)</code>","text":"<p>Extract a slice of skill instructions by section header or line range.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>Full skill instructions (e.g. from SkillInstructions.instructions).</p> required <code>section</code> <code>str | None</code> <p>Markdown header to extract (e.g. \"Welcome messages\"); first # or ## match.</p> <code>None</code> <code>line_start</code> <code>int | None</code> <p>1-based start line (inclusive).</p> <code>None</code> <code>line_end</code> <code>int | None</code> <p>1-based end line (inclusive).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Tuple of (sliced_instructions, section_used). section_used is the section name if</p> <code>str | None</code> <p>section was requested and found, else None.</p> Source code in <code>dynamiq/skills/utils.py</code> <pre><code>def extract_skill_content_slice(\n    instructions: str,\n    section: str | None = None,\n    line_start: int | None = None,\n    line_end: int | None = None,\n) -&gt; tuple[str, str | None]:\n    \"\"\"Extract a slice of skill instructions by section header or line range.\n\n    Args:\n        instructions: Full skill instructions (e.g. from SkillInstructions.instructions).\n        section: Markdown header to extract (e.g. \"Welcome messages\"); first # or ## match.\n        line_start: 1-based start line (inclusive).\n        line_end: 1-based end line (inclusive).\n\n    Returns:\n        Tuple of (sliced_instructions, section_used). section_used is the section name if\n        section was requested and found, else None.\n    \"\"\"\n    lines = instructions.splitlines()\n    section_used: str | None = None\n\n    if section:\n        section_lower = section.strip().lower()\n        start_i: int | None = None\n        end_i = len(lines)\n        for i, line in enumerate(lines):\n            s = line.strip()\n            if s.startswith(\"#\"):\n                header_level = len(s) - len(s.lstrip(\"#\"))\n                header_text = s.lstrip(\"#\").strip().lower()\n                if header_text == section_lower:\n                    start_i = i\n                    for j in range(i + 1, len(lines)):\n                        next_line = lines[j].strip()\n                        if next_line.startswith(\"#\"):\n                            next_level = len(next_line) - len(next_line.lstrip(\"#\"))\n                            if next_level &lt;= header_level:\n                                end_i = j\n                                break\n                    section_used = section\n                    break\n        if start_i is not None:\n            instructions = \"\\n\".join(lines[start_i:end_i])\n        else:\n            instructions = \"\"\n            section_used = None\n    elif line_start is not None or line_end is not None:\n        start = max(0, (line_start or 1) - 1)\n        end = line_end if line_end is not None else len(lines)\n        end = min(end, len(lines))\n        instructions = \"\\n\".join(lines[start:end])\n\n    return instructions, section_used\n</code></pre>"},{"location":"dynamiq/skills/utils/#dynamiq.skills.utils.ingest_skills_into_sandbox","title":"<code>ingest_skills_into_sandbox(sandbox, registry, skill_names=None, sandbox_skills_base_path=None)</code>","text":"<p>Download skill archives from the Dynamiq registry and ingest into the sandbox.</p> <p>Each worker thread downloads one skill and uploads its zip to the sandbox (no need to wait for all downloads before starting uploads). Then a single batch unzip runs in the sandbox. Default base is sandbox.base_path + \"/skills\" (e.g. /home/user/skills). Requires <code>unzip</code> in the sandbox.</p> <p>The base path is created with <code>mkdir -p</code> before any uploads so that existing directories (e.g. from a previous run or another skill) do not cause \"mkdir: file exists\" errors from backends that use non-idempotent mkdir. The sandbox's upload_file is called with ensure_parent_dirs=True so each skill's parent dir is also ensured idempotently.</p> <p>Parameters:</p> Name Type Description Default <code>sandbox</code> <code>Sandbox</code> <p>Sandbox with upload_file(file_name, content, destination_path) and run_command_shell.</p> required <code>registry</code> <code>Dynamiq</code> <p>Dynamiq registry (must have download_skill_archive).</p> required <code>skill_names</code> <code>list[str] | None</code> <p>If set, only ingest these skill names; otherwise all registry skills.</p> <code>None</code> <code>sandbox_skills_base_path</code> <code>str | None</code> <p>Base path in sandbox for skills (default: {sandbox.base_path}/skills).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of skill names that were ingested.</p> <p>Raises:</p> Type Description <code>SkillRegistryError</code> <p>If download, upload, or unzip fails.</p> Source code in <code>dynamiq/skills/utils.py</code> <pre><code>def ingest_skills_into_sandbox(\n    sandbox: \"Sandbox\",\n    registry: \"Dynamiq\",\n    skill_names: list[str] | None = None,\n    sandbox_skills_base_path: str | None = None,\n) -&gt; list[str]:\n    \"\"\"Download skill archives from the Dynamiq registry and ingest into the sandbox.\n\n    Each worker thread downloads one skill and uploads its zip to the sandbox (no need to wait\n    for all downloads before starting uploads). Then a single batch unzip runs in the sandbox.\n    Default base is sandbox.base_path + \"/skills\" (e.g. /home/user/skills).\n    Requires `unzip` in the sandbox.\n\n    The base path is created with ``mkdir -p`` before any uploads so that existing directories\n    (e.g. from a previous run or another skill) do not cause \"mkdir: file exists\" errors from\n    backends that use non-idempotent mkdir. The sandbox's upload_file is called with\n    ensure_parent_dirs=True so each skill's parent dir is also ensured idempotently.\n\n    Args:\n        sandbox: Sandbox with upload_file(file_name, content, destination_path) and run_command_shell.\n        registry: Dynamiq registry (must have download_skill_archive).\n        skill_names: If set, only ingest these skill names; otherwise all registry skills.\n        sandbox_skills_base_path: Base path in sandbox for skills (default: {sandbox.base_path}/skills).\n\n    Returns:\n        List of skill names that were ingested.\n\n    Raises:\n        SkillRegistryError: If download, upload, or unzip fails.\n    \"\"\"\n    base = normalize_sandbox_skills_base_path(sandbox_skills_base_path or f\"{sandbox.base_path.rstrip('/')}/skills\")\n    skills_to_ingest = skill_names if skill_names is not None else [e.name for e in registry.skills]\n    entries_to_ingest = [e for e in registry.skills if e.name in skills_to_ingest]\n    if not entries_to_ingest:\n        return []\n\n    if base:\n        try:\n            sandbox.run_command_shell(f\"mkdir -p {shlex.quote(base)}\")\n            logger.debug(f\"Ingestion: ensured base path exists: {base}\")\n        except Exception as e:\n            logger.warning(f\"Ingestion: mkdir -p {base!r} failed (continuing): {e}\")\n\n    workers = min(MAX_INGEST_WORKERS, len(entries_to_ingest))\n    logger.debug(f\"Ingestion: download+upload {len(entries_to_ingest)} skills with {workers} workers\")\n    uploaded: list[tuple[str, str, str, int]] = []\n    ingested: list[str] = []\n\n    with ContextAwareThreadPoolExecutor(max_workers=workers) as executor:\n        future_to_entry = {\n            executor.submit(_download_and_upload_one, registry, sandbox, base, e): e for e in entries_to_ingest\n        }\n        for future in as_completed(future_to_entry):\n            entry = future_to_entry[future]\n            try:\n                uploaded.append(future.result())\n                ingested.append(entry.name)\n            except Exception as e:\n                logger.warning(f\"Failed to ingest skill {entry.name}: {e}\")\n                raise SkillRegistryError(\n                    f\"Failed to ingest skill '{entry.name}': {e}\",\n                    details={\"skill_id\": entry.id, \"version_id\": entry.version_id, \"skill_name\": entry.name},\n                ) from e\n\n    if uploaded:\n        try:\n            _batch_unzip_in_sandbox(sandbox, uploaded)\n        except Exception as e:\n            logger.warning(f\"Batch unzip failed: {e}\")\n            raise SkillRegistryError(\n                \"Failed to unzip skills in sandbox: \" + str(e),\n                details={\"uploaded_skills\": [u[0] for u in uploaded]},\n            ) from e\n\n    order = {name: i for i, e in enumerate(entries_to_ingest) for name in [e.name]}\n    return sorted(ingested, key=order.__getitem__)\n</code></pre>"},{"location":"dynamiq/skills/utils/#dynamiq.skills.utils.normalize_sandbox_skills_base_path","title":"<code>normalize_sandbox_skills_base_path(path)</code>","text":"<p>Normalize sandbox skills base path so root '/' is preserved.</p> <p>.rstrip('/') turns '/' into '', which drops root paths from prompts and paths. Returns: '' when path is empty/None, '/' when path is exactly '/', else path with trailing slash removed.</p> Source code in <code>dynamiq/skills/utils.py</code> <pre><code>def normalize_sandbox_skills_base_path(path: str | None) -&gt; str:\n    \"\"\"Normalize sandbox skills base path so root '/' is preserved.\n\n    .rstrip('/') turns '/' into '', which drops root paths from prompts and paths.\n    Returns: '' when path is empty/None, '/' when path is exactly '/', else path with trailing slash removed.\n    \"\"\"\n    raw = (path or \"\").strip()\n    if not raw:\n        return \"\"\n    if raw == \"/\":\n        return \"/\"\n    return raw.rstrip(\"/\")\n</code></pre>"},{"location":"dynamiq/skills/registries/base/","title":"Base","text":"<p>Base skill registry interface.</p>"},{"location":"dynamiq/skills/registries/base/#dynamiq.skills.registries.base.BaseSkillRegistry","title":"<code>BaseSkillRegistry</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for skill registries.</p> Source code in <code>dynamiq/skills/registries/base.py</code> <pre><code>class BaseSkillRegistry(ABC, BaseModel):\n    \"\"\"Abstract base class for skill registries.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    skills: list[Any] = Field(\n        default_factory=list,\n        description=\"Skills available in this registry.\",\n    )\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return f\"{self.__module__}.{self.__class__.__name__}\"\n\n    @abstractmethod\n    def get_skills_metadata(self) -&gt; list[SkillMetadata]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_skill_instructions(self, name: str) -&gt; SkillInstructions:\n        raise NotImplementedError\n\n    def get_skill_scripts_path(self, name: str) -&gt; str | None:\n        \"\"\"Return absolute scripts path for a skill when available.\n\n        Registries that can expose runnable scripts should override this.\n        Default behavior returns None.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"dynamiq/skills/registries/base/#dynamiq.skills.registries.base.BaseSkillRegistry.get_skill_scripts_path","title":"<code>get_skill_scripts_path(name)</code>","text":"<p>Return absolute scripts path for a skill when available.</p> <p>Registries that can expose runnable scripts should override this. Default behavior returns None.</p> Source code in <code>dynamiq/skills/registries/base.py</code> <pre><code>def get_skill_scripts_path(self, name: str) -&gt; str | None:\n    \"\"\"Return absolute scripts path for a skill when available.\n\n    Registries that can expose runnable scripts should override this.\n    Default behavior returns None.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"dynamiq/skills/registries/dynamiq/","title":"Dynamiq","text":"<p>Dynamiq API skill registry.</p>"},{"location":"dynamiq/skills/registries/dynamiq/#dynamiq.skills.registries.dynamiq.Dynamiq","title":"<code>Dynamiq</code>","text":"<p>               Bases: <code>BaseSkillRegistry</code></p> <p>Dynamiq skill registry implementation.</p> Source code in <code>dynamiq/skills/registries/dynamiq.py</code> <pre><code>class Dynamiq(BaseSkillRegistry):\n    \"\"\"Dynamiq skill registry implementation.\"\"\"\n\n    connection: DynamiqConnection = Field(default_factory=DynamiqConnection)\n    timeout: float = Field(default=10, description=\"Timeout in seconds for API requests.\")\n    skills: list[DynamiqSkillEntry] = Field(default_factory=list)\n    sandbox_skills_base_path: str | None = Field(\n        default=None,\n        description=\"When set, get_skill_scripts_path returns \"\n        \"sandbox path for ingested skills (e.g. /home/user/skills).\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"skills\", mode=\"before\")\n    @classmethod\n    def normalize_skills(cls, v: Any) -&gt; Any:\n        if v is None:\n            return []\n        return v\n\n    def get_skills_metadata(self) -&gt; list[SkillMetadata]:\n        metadata: list[SkillMetadata] = []\n        for entry in self.skills:\n            metadata.append(SkillMetadata(name=entry.name, description=entry.description))\n        return metadata\n\n    def get_skill_instructions(self, name: str) -&gt; SkillInstructions:\n        entry = self._get_entry_by_name(name)\n        instructions_text, instructions_metadata = self._fetch_skill_instructions(entry.id, entry.version_id)\n        return SkillInstructions(\n            name=entry.name,\n            description=entry.description,\n            instructions=instructions_text,\n            metadata=instructions_metadata,\n        )\n\n    def _fetch_skill_instructions(self, skill_id: str, version_id: str) -&gt; tuple[str, dict[str, Any] | None]:\n        \"\"\"Fetch skill instructions from the API.\n\n        Supports both plain-text (legacy) and JSON responses. JSON shape should follow\n        common API style: {\"instructions\": \"...\", \"metadata\": {...} optional}.\n        \"\"\"\n        response = self._request(\n            HTTPMethod.GET,\n            f\"/v1/skills/{skill_id}/versions/{version_id}/instructions\",\n        )\n        if isinstance(response, str):\n            return response, None\n        if isinstance(response, dict):\n            instructions = response.get(\"instructions\") or response.get(\"content\") or \"\"\n            if not isinstance(instructions, str):\n                raise SkillRegistryError(\n                    \"Dynamiq skills API returned object but 'instructions'/'content' is not a string.\",\n                    details={\n                        \"skill_id\": skill_id,\n                        \"version_id\": version_id,\n                    },\n                )\n            metadata = response.get(\"metadata\")\n            if metadata is not None and not isinstance(metadata, dict):\n                metadata = None\n            return instructions, metadata\n        raise SkillRegistryError(\n            \"Unexpected response from Dynamiq skills API. Expected plain text or JSON object with 'instructions'.\",\n            details={\n                \"skill_id\": skill_id,\n                \"version_id\": version_id,\n                \"response_type\": type(response).__name__,\n            },\n        )\n\n    def _get_entry_by_name(self, name: str) -&gt; DynamiqSkillEntry:\n        for entry in self.skills:\n            if name == entry.name:\n                return entry\n        raise SkillRegistryError(\"Skill not in skills.\", details={\"name\": name})\n\n    def get_skill_scripts_path(self, name: str) -&gt; str | None:\n        \"\"\"Return sandbox path to the skill's scripts directory when skills are ingested into sandbox.\n\n        Used by SkillsTool so the agent knows where to run scripts inside the sandbox\n        (e.g. /home/user/skills/mermaid-tools/scripts).\n        \"\"\"\n        if not self.sandbox_skills_base_path:\n            return None\n        try:\n            self._get_entry_by_name(name)\n        except SkillRegistryError:\n            return None\n        if \"..\" in name or \"/\" in name:\n            return None\n        base = normalize_sandbox_skills_base_path(self.sandbox_skills_base_path)\n        if not base:\n            return None\n        return f\"{base}/{name}/scripts\"\n\n    def download_skill_archive(self, skill_id: str, version_id: str) -&gt; bytes:\n        \"\"\"Download skill version as a zip archive from the API.\n\n        API is assumed to expose GET /v1/skills/{skill_id}/versions/{version_id}/download\n        returning the zip (SKILL.md + scripts/ etc.). Full response body is loaded into\n        memory (no streaming). Caller should unzip and upload to sandbox.\n        \"\"\"\n        path = f\"/v1/skills/{skill_id}/versions/{version_id}/download\"\n        response = self._request_bytes(\"GET\", path)\n        logger.debug(f\"Skill archive API: GET {path} -&gt; {len(response)} bytes\")\n        return response\n\n    def _execute_request(\n        self,\n        method: str,\n        path: str,\n        *,\n        headers_extra: dict[str, str] | None = None,\n        params: dict[str, Any] | None = None,\n        json: dict[str, Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Build URL and client, execute HTTP request, raise on error. Returns the response object.\"\"\"\n        conn_params = self.connection.conn_params\n        base_url = (conn_params.get(\"api_base\") or \"\").rstrip(\"/\")\n        if not base_url:\n            raise SkillRegistryError(\"Dynamiq API base URL is not configured.\")\n\n        url = f\"{base_url}/{path.lstrip('/')}\"\n        headers: dict[str, str] = {}\n        if headers_extra:\n            headers.update(headers_extra)\n        conn_headers = conn_params.get(\"headers\")\n        if isinstance(conn_headers, dict):\n            headers.update(conn_headers)\n\n        client = self.connection.connect()\n        try:\n            response = client.request(\n                method,\n                url,\n                headers=headers,\n                params=params,\n                json=json,\n                timeout=self.timeout,\n            )\n        except Exception as exc:\n            raise SkillRegistryError(f\"Failed to call Dynamiq skills API: {exc}\") from exc\n\n        if response.status_code &gt;= 400:\n            raise SkillRegistryError(\n                f\"Request to Dynamiq skills API failed: {response.status_code} {response.text}\",\n                details={\"path\": path},\n            )\n        return response\n\n    def _request_bytes(self, method: str, path: str) -&gt; bytes:\n        \"\"\"Perform a request that returns raw bytes (e.g. zip download).\"\"\"\n        response = self._execute_request(method, path)\n        return response.content\n\n    def _request(\n        self,\n        method: HTTPMethod,\n        path: str,\n        params: dict[str, Any] | None = None,\n        json: dict[str, Any] | None = None,\n    ) -&gt; Any:\n        verb = method.value if isinstance(method, HTTPMethod) else method\n        response = self._execute_request(\n            verb,\n            path,\n            headers_extra={\"Content-Type\": \"application/json\"},\n            params=params,\n            json=json,\n        )\n\n        if response.status_code == 204 or not response.content:\n            logger.debug(\"Dynamiq skills API returned empty response for %s\", path)\n            return \"\"\n\n        content_type = (response.headers.get(\"content-type\") or \"\").split(\";\")[0].strip().lower()\n        if content_type == \"application/json\":\n            try:\n                return response.json()\n            except Exception as e:\n                raise SkillRegistryError(\n                    \"Dynamiq skills API returned application/json but body is not valid JSON.\",\n                    details={\"path\": path, \"error\": str(e)},\n                ) from e\n        return response.text\n</code></pre>"},{"location":"dynamiq/skills/registries/dynamiq/#dynamiq.skills.registries.dynamiq.Dynamiq.download_skill_archive","title":"<code>download_skill_archive(skill_id, version_id)</code>","text":"<p>Download skill version as a zip archive from the API.</p> <p>API is assumed to expose GET /v1/skills/{skill_id}/versions/{version_id}/download returning the zip (SKILL.md + scripts/ etc.). Full response body is loaded into memory (no streaming). Caller should unzip and upload to sandbox.</p> Source code in <code>dynamiq/skills/registries/dynamiq.py</code> <pre><code>def download_skill_archive(self, skill_id: str, version_id: str) -&gt; bytes:\n    \"\"\"Download skill version as a zip archive from the API.\n\n    API is assumed to expose GET /v1/skills/{skill_id}/versions/{version_id}/download\n    returning the zip (SKILL.md + scripts/ etc.). Full response body is loaded into\n    memory (no streaming). Caller should unzip and upload to sandbox.\n    \"\"\"\n    path = f\"/v1/skills/{skill_id}/versions/{version_id}/download\"\n    response = self._request_bytes(\"GET\", path)\n    logger.debug(f\"Skill archive API: GET {path} -&gt; {len(response)} bytes\")\n    return response\n</code></pre>"},{"location":"dynamiq/skills/registries/dynamiq/#dynamiq.skills.registries.dynamiq.Dynamiq.get_skill_scripts_path","title":"<code>get_skill_scripts_path(name)</code>","text":"<p>Return sandbox path to the skill's scripts directory when skills are ingested into sandbox.</p> <p>Used by SkillsTool so the agent knows where to run scripts inside the sandbox (e.g. /home/user/skills/mermaid-tools/scripts).</p> Source code in <code>dynamiq/skills/registries/dynamiq.py</code> <pre><code>def get_skill_scripts_path(self, name: str) -&gt; str | None:\n    \"\"\"Return sandbox path to the skill's scripts directory when skills are ingested into sandbox.\n\n    Used by SkillsTool so the agent knows where to run scripts inside the sandbox\n    (e.g. /home/user/skills/mermaid-tools/scripts).\n    \"\"\"\n    if not self.sandbox_skills_base_path:\n        return None\n    try:\n        self._get_entry_by_name(name)\n    except SkillRegistryError:\n        return None\n    if \"..\" in name or \"/\" in name:\n        return None\n    base = normalize_sandbox_skills_base_path(self.sandbox_skills_base_path)\n    if not base:\n        return None\n    return f\"{base}/{name}/scripts\"\n</code></pre>"},{"location":"dynamiq/skills/registries/dynamiq/#dynamiq.skills.registries.dynamiq.DynamiqSkillEntry","title":"<code>DynamiqSkillEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Allowed skill entry describing which skills are available to the agent.</p> Source code in <code>dynamiq/skills/registries/dynamiq.py</code> <pre><code>class DynamiqSkillEntry(BaseModel):\n    \"\"\"Allowed skill entry describing which skills are available to the agent.\"\"\"\n\n    id: str = Field(..., min_length=1, description=\"Skill identifier.\")\n    version_id: str = Field(..., min_length=1, description=\"Skill version identifier.\")\n    name: str = Field(..., min_length=1, description=\"Skill name.\")\n    description: str | None = Field(default=None, description=\"Optional cached skill description.\")\n</code></pre>"},{"location":"dynamiq/skills/registries/filesystem/","title":"Filesystem","text":"<p>Filesystem-backed skill registry.</p>"},{"location":"dynamiq/skills/registries/filesystem/#dynamiq.skills.registries.filesystem.FileSystem","title":"<code>FileSystem</code>","text":"<p>               Bases: <code>BaseSkillRegistry</code></p> <p>Filesystem-backed skill registry.</p> Source code in <code>dynamiq/skills/registries/filesystem.py</code> <pre><code>class FileSystem(BaseSkillRegistry):\n    \"\"\"Filesystem-backed skill registry.\"\"\"\n\n    base_path: str = Field(\n        default=\"~/.dynamiq/skills\",\n        description=\"Base path for skills on the filesystem.\",\n    )\n    skill_filename: str = Field(\n        default=\"SKILL.md\",\n        description=\"Filename for skill instructions within each skill directory.\",\n    )\n    skills: list[FileSystemSkillEntry] = Field(default_factory=list)\n\n    def get_skills_metadata(self) -&gt; list[SkillMetadata]:\n        metadata: list[SkillMetadata] = []\n        for entry in self.skills:\n            metadata.append(SkillMetadata(name=entry.name, description=entry.description))\n        return metadata\n\n    def get_skill_instructions(self, name: str) -&gt; SkillInstructions:\n        entry = self._get_entry_by_name(name)\n        skill_path = self._resolve_skill_path(name)\n        if not skill_path.is_file():\n            raise SkillRegistryError(\n                \"Skill instructions file not found or path is not a file.\",\n                details={\"name\": name, \"path\": str(skill_path)},\n            )\n        instructions = skill_path.read_text(encoding=\"utf-8\")\n        return SkillInstructions(\n            name=entry.name,\n            description=entry.description,\n            instructions=instructions,\n        )\n\n    def _get_entry_by_name(self, name: str) -&gt; FileSystemSkillEntry:\n        for entry in self.skills:\n            if entry.name == name:\n                return entry\n        raise SkillRegistryError(\"Skill not in skills.\", details={\"name\": name})\n\n    def _resolve_skill_path(self, skill_name: str) -&gt; Path:\n        if \"/\" in skill_name or \"\\\\\" in skill_name or \"..\" in skill_name:\n            raise SkillRegistryError(\n                \"Invalid skill name: path components are not allowed.\",\n                details={\"name\": skill_name},\n            )\n        base = Path(self.base_path).expanduser().resolve()\n        full_path = (base / skill_name / self.skill_filename).resolve()\n        try:\n            full_path.relative_to(base)\n        except ValueError:\n            raise SkillRegistryError(\n                \"Invalid skill path: resolved path is outside base path.\",\n                details={\"name\": skill_name, \"path\": str(full_path)},\n            )\n        return full_path\n\n    def get_skill_scripts_path(self, name: str) -&gt; str | None:\n        \"\"\"Return the absolute path to the skill's scripts directory if it exists.\n\n        Used by SkillsTool to expose scripts_path in get responses so the agent\n        can run bundled scripts (e.g. mermaid-tools, markdown-tools, xlsx).\n        \"\"\"\n        try:\n            self._get_entry_by_name(name)\n        except SkillRegistryError:\n            return None\n        base = Path(self.base_path).expanduser().resolve()\n        scripts_dir = (base / name / \"scripts\").resolve()\n        try:\n            scripts_dir.relative_to(base)\n        except ValueError:\n            return None\n        if scripts_dir.is_dir():\n            return str(scripts_dir)\n        return None\n</code></pre>"},{"location":"dynamiq/skills/registries/filesystem/#dynamiq.skills.registries.filesystem.FileSystem.get_skill_scripts_path","title":"<code>get_skill_scripts_path(name)</code>","text":"<p>Return the absolute path to the skill's scripts directory if it exists.</p> <p>Used by SkillsTool to expose scripts_path in get responses so the agent can run bundled scripts (e.g. mermaid-tools, markdown-tools, xlsx).</p> Source code in <code>dynamiq/skills/registries/filesystem.py</code> <pre><code>def get_skill_scripts_path(self, name: str) -&gt; str | None:\n    \"\"\"Return the absolute path to the skill's scripts directory if it exists.\n\n    Used by SkillsTool to expose scripts_path in get responses so the agent\n    can run bundled scripts (e.g. mermaid-tools, markdown-tools, xlsx).\n    \"\"\"\n    try:\n        self._get_entry_by_name(name)\n    except SkillRegistryError:\n        return None\n    base = Path(self.base_path).expanduser().resolve()\n    scripts_dir = (base / name / \"scripts\").resolve()\n    try:\n        scripts_dir.relative_to(base)\n    except ValueError:\n        return None\n    if scripts_dir.is_dir():\n        return str(scripts_dir)\n    return None\n</code></pre>"},{"location":"dynamiq/skills/registries/filesystem/#dynamiq.skills.registries.filesystem.FileSystemSkillEntry","title":"<code>FileSystemSkillEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Allowed skill entry for filesystem skill registry.</p> Source code in <code>dynamiq/skills/registries/filesystem.py</code> <pre><code>class FileSystemSkillEntry(BaseModel):\n    \"\"\"Allowed skill entry for filesystem skill registry.\"\"\"\n\n    name: str = Field(..., min_length=1, description=\"Skill name.\")\n    description: str | None = Field(default=None, description=\"Optional cached skill description.\")\n</code></pre>"},{"location":"dynamiq/storages/file/base/","title":"Base","text":"<p>Base file storage interface and common data structures.</p>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileExistsError","title":"<code>FileExistsError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Raised when trying to create a file that already exists.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class FileExistsError(StorageError):\n    \"\"\"Raised when trying to create a file that already exists.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileInfo","title":"<code>FileInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a stored file.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class FileInfo(BaseModel):\n    \"\"\"Information about a stored file.\"\"\"\n\n    name: str\n    path: str\n    size: int\n    content_type: str = \"text/plain\"\n    created_at: datetime = Field(default_factory=datetime.now)\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    content: bytes | None = Field(default=None)\n\n    @field_serializer(\"content\", when_used=\"json-unless-none\")\n    def bytes_as_base64(self, b: bytes) -&gt; str:\n        return base64.b64encode(b).decode(\"ascii\")\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileNotFoundError","title":"<code>FileNotFoundError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Raised when a file is not found in storage.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class FileNotFoundError(StorageError):\n    \"\"\"Raised when a file is not found in storage.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore","title":"<code>FileStore</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for file storage implementations.</p> <p>This interface provides a unified way to interact with different file storage backends (in-memory, file system, cloud storage, etc.).</p> <p>The <code>_clone_shared</code> flag tells the Node.clone() machinery to share this instance by reference instead of deep-copying it. This preserves stored files across cloned tool invocations.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class FileStore(abc.ABC, BaseModel):\n    \"\"\"Abstract base class for file storage implementations.\n\n    This interface provides a unified way to interact with different\n    file storage backends (in-memory, file system, cloud storage, etc.).\n\n    The ``_clone_shared`` flag tells the Node.clone() machinery to\n    share this instance by reference instead of deep-copying it.\n    This preserves stored files across cloned tool invocations.\n    \"\"\"\n\n    _clone_shared: ClassVar[bool] = True\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        \"\"\"Returns the backend type as a string.\"\"\"\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Convert the FileStore instance to a dictionary.\n\n        Returns:\n            dict: Dictionary representation of the FileStore instance.\n        \"\"\"\n        for param in (\"include_secure_params\", \"for_tracing\"):\n            kwargs.pop(param, None)\n        data = self.model_dump(**kwargs)\n        data[\"type\"] = self.type\n        return data\n\n    @abc.abstractmethod\n    def list_files_bytes(self, file_paths: list[str] | None = None) -&gt; list[BytesIO]:\n        \"\"\"Return stored files as BytesIO objects.\n\n        Args:\n            file_paths: If provided, return only these files. Otherwise return all files.\n\n        Returns:\n            List of BytesIO objects with name, description, and content_type attributes.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def store(\n        self,\n        file_path: str | Path,\n        content: str | bytes | BinaryIO,\n        content_type: str = None,\n        metadata: dict[str, Any] = None,\n        overwrite: bool = False,\n    ) -&gt; FileInfo:\n        \"\"\"Store a file in the storage backend.\n\n        Args:\n            file_path: Path where the file should be stored\n            content: File content as string, bytes, or file-like object\n            content_type: MIME type of the file content\n            metadata: Additional metadata to store with the file\n            overwrite: Whether to overwrite existing files\n\n        Returns:\n            FileInfo object with details about the stored file\n\n        Raises:\n            FileExistsError: If file exists and overwrite=False\n            PermissionError: If storage operation is not permitted\n            StorageError: For other storage-related errors\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def retrieve(self, file_path: str | Path) -&gt; bytes:\n        \"\"\"Retrieve file content from storage.\n\n        Args:\n            file_path: Path of the file to retrieve\n\n        Returns:\n            File content as bytes\n\n        Raises:\n            FileNotFoundError: If file doesn't exist\n            PermissionError: If retrieval is not permitted\n            StorageError: For other storage-related errors\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def exists(self, file_path: str | Path) -&gt; bool:\n        \"\"\"Check if a file exists in storage.\n\n        Args:\n            file_path: Path of the file to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def delete(self, file_path: str | Path) -&gt; bool:\n        \"\"\"Delete a file from storage.\n\n        Args:\n            file_path: Path of the file to delete\n\n        Returns:\n            True if file was deleted, False if it didn't exist\n\n        Raises:\n            PermissionError: If deletion is not permitted\n            StorageError: For other storage-related errors\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def list_files(self, directory: str | Path = \"\", recursive: bool = False, pattern: str = None) -&gt; list[FileInfo]:\n        \"\"\"List files in storage.\n\n        Args:\n            directory: Directory to list (empty string for root)\n            recursive: Whether to list files recursively\n            pattern: Glob pattern to filter files\n\n        Returns:\n            List of FileInfo objects\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.type","title":"<code>type: str</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the backend type as a string.</p>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.delete","title":"<code>delete(file_path)</code>  <code>abstractmethod</code>","text":"<p>Delete a file from storage.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path of the file to delete</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file was deleted, False if it didn't exist</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If deletion is not permitted</p> <code>StorageError</code> <p>For other storage-related errors</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef delete(self, file_path: str | Path) -&gt; bool:\n    \"\"\"Delete a file from storage.\n\n    Args:\n        file_path: Path of the file to delete\n\n    Returns:\n        True if file was deleted, False if it didn't exist\n\n    Raises:\n        PermissionError: If deletion is not permitted\n        StorageError: For other storage-related errors\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.exists","title":"<code>exists(file_path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in storage.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path of the file to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef exists(self, file_path: str | Path) -&gt; bool:\n    \"\"\"Check if a file exists in storage.\n\n    Args:\n        file_path: Path of the file to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.list_files","title":"<code>list_files(directory='', recursive=False, pattern=None)</code>  <code>abstractmethod</code>","text":"<p>List files in storage.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | Path</code> <p>Directory to list (empty string for root)</p> <code>''</code> <code>recursive</code> <code>bool</code> <p>Whether to list files recursively</p> <code>False</code> <code>pattern</code> <code>str</code> <p>Glob pattern to filter files</p> <code>None</code> <p>Returns:</p> Type Description <code>list[FileInfo]</code> <p>List of FileInfo objects</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef list_files(self, directory: str | Path = \"\", recursive: bool = False, pattern: str = None) -&gt; list[FileInfo]:\n    \"\"\"List files in storage.\n\n    Args:\n        directory: Directory to list (empty string for root)\n        recursive: Whether to list files recursively\n        pattern: Glob pattern to filter files\n\n    Returns:\n        List of FileInfo objects\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.list_files_bytes","title":"<code>list_files_bytes(file_paths=None)</code>  <code>abstractmethod</code>","text":"<p>Return stored files as BytesIO objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str] | None</code> <p>If provided, return only these files. Otherwise return all files.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[BytesIO]</code> <p>List of BytesIO objects with name, description, and content_type attributes.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef list_files_bytes(self, file_paths: list[str] | None = None) -&gt; list[BytesIO]:\n    \"\"\"Return stored files as BytesIO objects.\n\n    Args:\n        file_paths: If provided, return only these files. Otherwise return all files.\n\n    Returns:\n        List of BytesIO objects with name, description, and content_type attributes.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.retrieve","title":"<code>retrieve(file_path)</code>  <code>abstractmethod</code>","text":"<p>Retrieve file content from storage.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path of the file to retrieve</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>File content as bytes</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>PermissionError</code> <p>If retrieval is not permitted</p> <code>StorageError</code> <p>For other storage-related errors</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef retrieve(self, file_path: str | Path) -&gt; bytes:\n    \"\"\"Retrieve file content from storage.\n\n    Args:\n        file_path: Path of the file to retrieve\n\n    Returns:\n        File content as bytes\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        PermissionError: If retrieval is not permitted\n        StorageError: For other storage-related errors\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.store","title":"<code>store(file_path, content, content_type=None, metadata=None, overwrite=False)</code>  <code>abstractmethod</code>","text":"<p>Store a file in the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path where the file should be stored</p> required <code>content</code> <code>str | bytes | BinaryIO</code> <p>File content as string, bytes, or file-like object</p> required <code>content_type</code> <code>str</code> <p>MIME type of the file content</p> <code>None</code> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata to store with the file</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files</p> <code>False</code> <p>Returns:</p> Type Description <code>FileInfo</code> <p>FileInfo object with details about the stored file</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If file exists and overwrite=False</p> <code>PermissionError</code> <p>If storage operation is not permitted</p> <code>StorageError</code> <p>For other storage-related errors</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>@abc.abstractmethod\ndef store(\n    self,\n    file_path: str | Path,\n    content: str | bytes | BinaryIO,\n    content_type: str = None,\n    metadata: dict[str, Any] = None,\n    overwrite: bool = False,\n) -&gt; FileInfo:\n    \"\"\"Store a file in the storage backend.\n\n    Args:\n        file_path: Path where the file should be stored\n        content: File content as string, bytes, or file-like object\n        content_type: MIME type of the file content\n        metadata: Additional metadata to store with the file\n        overwrite: Whether to overwrite existing files\n\n    Returns:\n        FileInfo object with details about the stored file\n\n    Raises:\n        FileExistsError: If file exists and overwrite=False\n        PermissionError: If storage operation is not permitted\n        StorageError: For other storage-related errors\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStore.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the FileStore instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary representation of the FileStore instance.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Convert the FileStore instance to a dictionary.\n\n    Returns:\n        dict: Dictionary representation of the FileStore instance.\n    \"\"\"\n    for param in (\"include_secure_params\", \"for_tracing\"):\n        kwargs.pop(param, None)\n    data = self.model_dump(**kwargs)\n    data[\"type\"] = self.type\n    return data\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStoreConfig","title":"<code>FileStoreConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for file storage and related features.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether file storage is enabled.</p> <code>backend</code> <code>FileStore</code> <p>The file storage backend to use.</p> <code>agent_file_write_enabled</code> <code>bool</code> <p>Whether the agent can write files.</p> <code>todo_enabled</code> <code>bool</code> <p>Whether to enable todo management tools (stored in ._agent/todos.json).</p> <code>config</code> <code>dict[str, Any]</code> <p>Additional configuration options.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class FileStoreConfig(BaseModel):\n    \"\"\"Configuration for file storage and related features.\n\n    Attributes:\n        enabled: Whether file storage is enabled.\n        backend: The file storage backend to use.\n        agent_file_write_enabled: Whether the agent can write files.\n        todo_enabled: Whether to enable todo management tools (stored in ._agent/todos.json).\n        config: Additional configuration options.\n    \"\"\"\n\n    enabled: bool = False\n    backend: FileStore = Field(..., description=\"File storage to use.\")\n    agent_file_write_enabled: bool = Field(\n        default=False, description=\"Whether the agent is permitted to write files to the file store.\"\n    )\n    todo_enabled: bool = Field(\n        default=False, description=\"Whether to enable todo management tools (todos stored in ._agent/todos.json).\"\n    )\n    config: dict[str, Any] = Field(default_factory=dict)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict[str, bool]:\n        \"\"\"Define parameters to exclude during serialization.\"\"\"\n        return {\"backend\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Convert the FileStoreConfig instance to a dictionary.\"\"\"\n        for_tracing = kwargs.pop(\"for_tracing\", False)\n        if for_tracing and not self.enabled:\n            return {\"enabled\": False}\n        kwargs.pop(\"include_secure_params\", None)\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n        config_data = self.model_dump(exclude=exclude, **kwargs)\n        config_data[\"backend\"] = self.backend.to_dict()\n        return config_data\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStoreConfig.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict[str, bool]</code>  <code>property</code>","text":"<p>Define parameters to exclude during serialization.</p>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.FileStoreConfig.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the FileStoreConfig instance to a dictionary.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Convert the FileStoreConfig instance to a dictionary.\"\"\"\n    for_tracing = kwargs.pop(\"for_tracing\", False)\n    if for_tracing and not self.enabled:\n        return {\"enabled\": False}\n    kwargs.pop(\"include_secure_params\", None)\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n    config_data = self.model_dump(exclude=exclude, **kwargs)\n    config_data[\"backend\"] = self.backend.to_dict()\n    return config_data\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.PermissionError","title":"<code>PermissionError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Raised when permission is denied for a storage operation.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class PermissionError(StorageError):\n    \"\"\"Raised when permission is denied for a storage operation.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/file/base/#dynamiq.storages.file.base.StorageError","title":"<code>StorageError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for storage operations.</p> Source code in <code>dynamiq/storages/file/base.py</code> <pre><code>class StorageError(Exception):\n    \"\"\"Base exception for storage operations.\"\"\"\n\n    def __init__(self, message: str, operation: str = None, path: str = None):\n        self.message = message\n        self.operation = operation\n        self.path = path\n        super().__init__(self.message)\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/","title":"In memory","text":"<p>In-memory file storage implementation.</p>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore","title":"<code>InMemoryFileStore</code>","text":"<p>               Bases: <code>FileStore</code></p> <p>In-memory file storage implementation.</p> <p>This implementation stores files in memory using Python dictionaries. Files are lost when the process terminates.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>class InMemoryFileStore(FileStore):\n    \"\"\"In-memory file storage implementation.\n\n    This implementation stores files in memory using Python dictionaries.\n    Files are lost when the process terminates.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the in-memory storage.\n\n        Args:\n            **kwargs: Additional keyword arguments (ignored)\n        \"\"\"\n        super().__init__(**kwargs)\n        self._files: dict[str, dict[str, Any]] = {}\n\n    def list_files_bytes(self, file_paths: list[str] | None = None) -&gt; list[BytesIO]:\n        \"\"\"Return stored files as BytesIO objects.\n\n        Args:\n            file_paths: If provided, return only these files. Otherwise return all files.\n\n        Returns:\n            List of BytesIO objects with name, description, and content_type attributes.\n        \"\"\"\n        keys = file_paths if file_paths else list(self._files.keys())\n        files = []\n        for file_path in keys:\n            if file_path not in self._files:\n                continue\n            data = self._files[file_path]\n            file = BytesIO(data[\"content\"])\n            file.name = file_path\n            file.description = data[\"metadata\"].get(\"description\", \"\")\n            file.content_type = data[\"content_type\"]\n            files.append(file)\n        return files\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the file store is empty.\"\"\"\n        return len(self._files) == 0\n\n    def store(\n        self,\n        file_path: str | Path,\n        content: str | bytes | BinaryIO,\n        content_type: str = None,\n        metadata: dict[str, Any] = None,\n        overwrite: bool = False,\n    ) -&gt; FileInfo:\n        \"\"\"Store a file in memory.\"\"\"\n        file_path = str(file_path)\n\n        if file_path in self._files and not overwrite:\n            logger.info(f\"File '{file_path}' already exists. Skipping...\")\n            return self._create_file_info(file_path, self._files[file_path])\n\n        # Convert content to bytes\n        if isinstance(content, str):\n            content_bytes = content.encode(\"utf-8\")\n        elif isinstance(content, bytes):\n            content_bytes = content\n        elif hasattr(content, \"read\"):  # BinaryIO-like object\n            content_bytes = content.read()\n            if hasattr(content, \"seek\"):\n                content.seek(0)  # Reset position for future reads\n        else:\n            raise StorageError(f\"Unsupported content type: {type(content)}\", operation=\"store\", path=file_path)\n\n        if content_type is None:\n            content_type, _ = mimetypes.guess_type(file_path)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n\n        now = datetime.now()\n        file_info = {\n            \"content\": content_bytes,\n            \"size\": len(content_bytes),\n            \"content_type\": content_type,\n            \"created_at\": now,\n            \"metadata\": metadata or {},\n        }\n\n        self._files[file_path] = file_info\n\n        return self._create_file_info(file_path, file_info)\n\n    def retrieve(self, file_path: str | Path) -&gt; bytes:\n        \"\"\"Retrieve file content as bytes.\"\"\"\n        file_path = str(file_path)\n\n        if file_path not in self._files:\n            raise FileNotFoundError(f\"File '{file_path}' not found\", operation=\"retrieve\", path=file_path)\n\n        return self._files[file_path][\"content\"]\n\n    def exists(self, file_path: str | Path) -&gt; bool:\n        \"\"\"Check if file exists.\"\"\"\n        return str(file_path) in self._files\n\n    def delete(self, file_path: str | Path) -&gt; bool:\n        \"\"\"Delete a file.\"\"\"\n        file_path = str(file_path)\n\n        if file_path in self._files:\n            del self._files[file_path]\n            return True\n\n        return False\n\n    def list_files(\n        self,\n        directory: str | Path = \"\",\n        recursive: bool = False,\n    ) -&gt; list[FileInfo]:\n        \"\"\"List files in storage.\"\"\"\n        directory = str(directory)\n        files_list = []\n\n        for file_path in self._files.keys():\n            if directory and not file_path.startswith(directory):\n                continue\n\n            if not recursive:\n                rel_path = file_path[len(directory) :].lstrip(\"/\")\n                if \"/\" in rel_path:\n                    continue\n\n            files_list.append(self._create_file_info(file_path, self._files[file_path]))\n\n        return files_list\n\n    def _create_file_info(self, file_path: str, file_data: dict[str, Any]) -&gt; FileInfo:\n        \"\"\"Create a FileInfo object from internal file data.\"\"\"\n        return FileInfo(\n            name=os.path.basename(file_path),\n            path=file_path,\n            size=file_data[\"size\"],\n            content_type=file_data[\"content_type\"],\n            created_at=file_data[\"created_at\"],\n            metadata=file_data.get(\"metadata\", {}),\n            content=file_data[\"content\"],\n        )\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the in-memory storage.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments (ignored)</p> <code>{}</code> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the in-memory storage.\n\n    Args:\n        **kwargs: Additional keyword arguments (ignored)\n    \"\"\"\n    super().__init__(**kwargs)\n    self._files: dict[str, dict[str, Any]] = {}\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.delete","title":"<code>delete(file_path)</code>","text":"<p>Delete a file.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def delete(self, file_path: str | Path) -&gt; bool:\n    \"\"\"Delete a file.\"\"\"\n    file_path = str(file_path)\n\n    if file_path in self._files:\n        del self._files[file_path]\n        return True\n\n    return False\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.exists","title":"<code>exists(file_path)</code>","text":"<p>Check if file exists.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def exists(self, file_path: str | Path) -&gt; bool:\n    \"\"\"Check if file exists.\"\"\"\n    return str(file_path) in self._files\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if the file store is empty.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the file store is empty.\"\"\"\n    return len(self._files) == 0\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.list_files","title":"<code>list_files(directory='', recursive=False)</code>","text":"<p>List files in storage.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def list_files(\n    self,\n    directory: str | Path = \"\",\n    recursive: bool = False,\n) -&gt; list[FileInfo]:\n    \"\"\"List files in storage.\"\"\"\n    directory = str(directory)\n    files_list = []\n\n    for file_path in self._files.keys():\n        if directory and not file_path.startswith(directory):\n            continue\n\n        if not recursive:\n            rel_path = file_path[len(directory) :].lstrip(\"/\")\n            if \"/\" in rel_path:\n                continue\n\n        files_list.append(self._create_file_info(file_path, self._files[file_path]))\n\n    return files_list\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.list_files_bytes","title":"<code>list_files_bytes(file_paths=None)</code>","text":"<p>Return stored files as BytesIO objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str] | None</code> <p>If provided, return only these files. Otherwise return all files.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[BytesIO]</code> <p>List of BytesIO objects with name, description, and content_type attributes.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def list_files_bytes(self, file_paths: list[str] | None = None) -&gt; list[BytesIO]:\n    \"\"\"Return stored files as BytesIO objects.\n\n    Args:\n        file_paths: If provided, return only these files. Otherwise return all files.\n\n    Returns:\n        List of BytesIO objects with name, description, and content_type attributes.\n    \"\"\"\n    keys = file_paths if file_paths else list(self._files.keys())\n    files = []\n    for file_path in keys:\n        if file_path not in self._files:\n            continue\n        data = self._files[file_path]\n        file = BytesIO(data[\"content\"])\n        file.name = file_path\n        file.description = data[\"metadata\"].get(\"description\", \"\")\n        file.content_type = data[\"content_type\"]\n        files.append(file)\n    return files\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.retrieve","title":"<code>retrieve(file_path)</code>","text":"<p>Retrieve file content as bytes.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def retrieve(self, file_path: str | Path) -&gt; bytes:\n    \"\"\"Retrieve file content as bytes.\"\"\"\n    file_path = str(file_path)\n\n    if file_path not in self._files:\n        raise FileNotFoundError(f\"File '{file_path}' not found\", operation=\"retrieve\", path=file_path)\n\n    return self._files[file_path][\"content\"]\n</code></pre>"},{"location":"dynamiq/storages/file/in_memory/#dynamiq.storages.file.in_memory.InMemoryFileStore.store","title":"<code>store(file_path, content, content_type=None, metadata=None, overwrite=False)</code>","text":"<p>Store a file in memory.</p> Source code in <code>dynamiq/storages/file/in_memory.py</code> <pre><code>def store(\n    self,\n    file_path: str | Path,\n    content: str | bytes | BinaryIO,\n    content_type: str = None,\n    metadata: dict[str, Any] = None,\n    overwrite: bool = False,\n) -&gt; FileInfo:\n    \"\"\"Store a file in memory.\"\"\"\n    file_path = str(file_path)\n\n    if file_path in self._files and not overwrite:\n        logger.info(f\"File '{file_path}' already exists. Skipping...\")\n        return self._create_file_info(file_path, self._files[file_path])\n\n    # Convert content to bytes\n    if isinstance(content, str):\n        content_bytes = content.encode(\"utf-8\")\n    elif isinstance(content, bytes):\n        content_bytes = content\n    elif hasattr(content, \"read\"):  # BinaryIO-like object\n        content_bytes = content.read()\n        if hasattr(content, \"seek\"):\n            content.seek(0)  # Reset position for future reads\n    else:\n        raise StorageError(f\"Unsupported content type: {type(content)}\", operation=\"store\", path=file_path)\n\n    if content_type is None:\n        content_type, _ = mimetypes.guess_type(file_path)\n        if content_type is None:\n            content_type = \"application/octet-stream\"\n\n    now = datetime.now()\n    file_info = {\n        \"content\": content_bytes,\n        \"size\": len(content_bytes),\n        \"content_type\": content_type,\n        \"created_at\": now,\n        \"metadata\": metadata or {},\n    }\n\n    self._files[file_path] = file_info\n\n    return self._create_file_info(file_path, file_info)\n</code></pre>"},{"location":"dynamiq/storages/graph/base/","title":"Base","text":""},{"location":"dynamiq/storages/graph/base/#dynamiq.storages.graph.base.BaseGraphStore","title":"<code>BaseGraphStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interface for graph backends that support Cypher-like queries.</p> Source code in <code>dynamiq/storages/graph/base.py</code> <pre><code>class BaseGraphStore(ABC):\n    \"\"\"Base interface for graph backends that support Cypher-like queries.\"\"\"\n\n    @abstractmethod\n    def run_cypher(\n        self,\n        query: str,\n        parameters: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[Any, Any, list[str]]:\n        \"\"\"Execute a Cypher query and return (records, summary, keys).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def introspect_schema(self, *, include_properties: bool, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Return schema details: labels, relationship_types, node_properties, relationship_properties.\"\"\"\n        raise NotImplementedError\n\n    def supports_graph_result(self) -&gt; bool:\n        \"\"\"Whether the backend can return native graph objects.\"\"\"\n        return False\n\n    def update_client(self, client: Any) -&gt; None:\n        \"\"\"Update the underlying client reference if the connection is reinitialized.\"\"\"\n        if hasattr(self, \"client\"):\n            self.client = client\n\n    @staticmethod\n    def format_records(records: Iterable[Any]) -&gt; list[dict[str, Any]]:\n        formatted: list[dict[str, Any]] = []\n        for record in records:\n            if isinstance(record, dict):\n                formatted.append(record)\n            elif hasattr(record, \"data\") and callable(record.data):\n                formatted.append(record.data())\n            else:\n                formatted.append({\"value\": record})\n        return formatted\n</code></pre>"},{"location":"dynamiq/storages/graph/base/#dynamiq.storages.graph.base.BaseGraphStore.introspect_schema","title":"<code>introspect_schema(*, include_properties, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return schema details: labels, relationship_types, node_properties, relationship_properties.</p> Source code in <code>dynamiq/storages/graph/base.py</code> <pre><code>@abstractmethod\ndef introspect_schema(self, *, include_properties: bool, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Return schema details: labels, relationship_types, node_properties, relationship_properties.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/storages/graph/base/#dynamiq.storages.graph.base.BaseGraphStore.run_cypher","title":"<code>run_cypher(query, parameters=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute a Cypher query and return (records, summary, keys).</p> Source code in <code>dynamiq/storages/graph/base.py</code> <pre><code>@abstractmethod\ndef run_cypher(\n    self,\n    query: str,\n    parameters: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; tuple[Any, Any, list[str]]:\n    \"\"\"Execute a Cypher query and return (records, summary, keys).\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/storages/graph/base/#dynamiq.storages.graph.base.BaseGraphStore.supports_graph_result","title":"<code>supports_graph_result()</code>","text":"<p>Whether the backend can return native graph objects.</p> Source code in <code>dynamiq/storages/graph/base.py</code> <pre><code>def supports_graph_result(self) -&gt; bool:\n    \"\"\"Whether the backend can return native graph objects.\"\"\"\n    return False\n</code></pre>"},{"location":"dynamiq/storages/graph/base/#dynamiq.storages.graph.base.BaseGraphStore.update_client","title":"<code>update_client(client)</code>","text":"<p>Update the underlying client reference if the connection is reinitialized.</p> Source code in <code>dynamiq/storages/graph/base.py</code> <pre><code>def update_client(self, client: Any) -&gt; None:\n    \"\"\"Update the underlying client reference if the connection is reinitialized.\"\"\"\n    if hasattr(self, \"client\"):\n        self.client = client\n</code></pre>"},{"location":"dynamiq/storages/graph/age/age/","title":"Age","text":""},{"location":"dynamiq/storages/graph/age/age/#dynamiq.storages.graph.age.age.ApacheAgeGraphStore","title":"<code>ApacheAgeGraphStore</code>","text":"<p>               Bases: <code>BaseGraphStore</code></p> <p>Wrapper for Apache AGE openCypher execution via PostgreSQL.</p> Source code in <code>dynamiq/storages/graph/age/age.py</code> <pre><code>class ApacheAgeGraphStore(BaseGraphStore):\n    \"\"\"Wrapper for Apache AGE openCypher execution via PostgreSQL.\"\"\"\n\n    def __init__(\n        self,\n        connection: ApacheAGE | None = None,\n        client: Any | None = None,\n        graph_name: str | None = None,\n        create_graph_if_not_exists: bool = False,\n    ):\n        if client is None and connection is None:\n            raise ValueError(\"Either 'connection' or 'client' must be provided.\")\n\n        self.connection = connection\n        self.client = client or connection.connect()\n        self.graph_name = graph_name\n        self.create_graph_if_not_exists = create_graph_if_not_exists\n\n        if not self.graph_name:\n            raise ValueError(\"graph_name must be provided for Apache AGE.\")\n\n        self._age_loaded = False\n        self._ensure_age_loaded()\n        if self.create_graph_if_not_exists:\n            self._ensure_graph_exists()\n\n    def run_cypher(\n        self,\n        query: str,\n        parameters: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[list[Any], dict[str, Any], list[str]]:\n        self._ensure_age_loaded()\n        params = parameters or {}\n        safe_graph = self._validate_label(self.graph_name)\n        sql = \"SELECT ag_catalog.agtype_to_json(result) AS result \" \"FROM cypher(%s, %s, %s::agtype) AS (result agtype)\"\n        with self.client.cursor() as cursor:\n            cursor.execute(sql, (safe_graph, query, json.dumps(params)))\n            rows = cursor.fetchall()\n\n        records: list[Any] = []\n        for row in rows:\n            value = row[\"result\"] if isinstance(row, dict) else row[0]\n            records.append(value)\n\n        summary = {\"query\": query, \"counters\": {}, \"result_available_after\": None}\n        return records, summary, []\n\n    def update_client(self, client: Any) -&gt; None:\n        self.client = client\n        self._age_loaded = False\n        self._ensure_age_loaded()\n        if self.create_graph_if_not_exists:\n            self._ensure_graph_exists()\n\n    def introspect_schema(self, *, include_properties: bool, **kwargs: Any) -&gt; dict[str, Any]:\n        labels = self._filter_internal(self._get_labels(kind=\"v\"))\n        rel_types = self._filter_internal(self._get_labels(kind=\"e\"))\n\n        node_props: list[dict[str, Any]] = []\n        rel_props: list[dict[str, Any]] = []\n        if include_properties:\n            node_props = self._sample_node_properties(labels)\n            rel_props = self._sample_edge_properties(rel_types)\n\n        return {\n            \"labels\": labels,\n            \"relationship_types\": rel_types,\n            \"node_properties\": node_props,\n            \"relationship_properties\": rel_props,\n            \"relationships\": self._sample_triples(rel_types),\n        }\n\n    def _ensure_age_loaded(self) -&gt; None:\n        if self._age_loaded:\n            return\n        try:\n            with self.client.cursor() as cursor:\n                cursor.execute(\"LOAD 'age';\")\n                cursor.execute('SET search_path = ag_catalog, \"$user\", public;')\n            self._age_loaded = True\n        except Exception as exc:  # noqa: BLE001\n            logger.error(f\"Failed to load Apache AGE extension: {exc}\")\n            raise\n\n    def _ensure_graph_exists(self) -&gt; None:\n        with self.client.cursor() as cursor:\n            cursor.execute(\n                \"SELECT 1 FROM ag_catalog.ag_graph WHERE name = %s\",\n                (self.graph_name,),\n            )\n            exists = cursor.fetchone()\n            if exists:\n                return\n            cursor.execute(\"SELECT * FROM ag_catalog.create_graph(%s)\", (self.graph_name,))\n\n    def _get_labels(self, *, kind: str) -&gt; list[str]:\n        graph_id_column = self._get_graph_id_column()\n        with self.client.cursor() as cursor:\n            query = sql.SQL(\n                \"\"\"\n                SELECT l.name AS label\n                FROM ag_catalog.ag_label l\n                JOIN ag_catalog.ag_graph g ON g.{graph_id_column} = l.graph\n                WHERE g.name = %s AND l.kind = %s\n                ORDER BY l.name\n                \"\"\"\n            ).format(graph_id_column=sql.Identifier(graph_id_column))\n            cursor.execute(query, (self.graph_name, kind))\n            rows = cursor.fetchall()\n        labels = [row[\"label\"] if isinstance(row, dict) else row[0] for row in rows]\n        return labels\n\n    def _sample_node_properties(self, labels: list[str]) -&gt; list[dict[str, Any]]:\n        node_properties: list[dict[str, Any]] = []\n        type_mapping = {\n            \"str\": \"STRING\",\n            \"float\": \"DOUBLE\",\n            \"int\": \"INTEGER\",\n            \"list\": \"LIST\",\n            \"dict\": \"MAP\",\n            \"bool\": \"BOOLEAN\",\n        }\n        for label in labels:\n            safe_label = self._validate_label(label)\n            query = f\"MATCH (n:`{safe_label}`) RETURN properties(n) AS result LIMIT 100\"\n            records, _, _ = self.run_cypher(query)\n            seen: set[tuple[str, str]] = set()\n            for record in records:\n                props = record or {}\n                for key, value in props.items():\n                    seen.add((key, type_mapping.get(type(value).__name__, \"STRING\")))\n            node_properties.append({\"labels\": label, \"properties\": [{\"property\": k, \"type\": v} for k, v in seen]})\n        return node_properties\n\n    def _sample_edge_properties(self, labels: list[str]) -&gt; list[dict[str, Any]]:\n        edge_properties: list[dict[str, Any]] = []\n        type_mapping = {\n            \"str\": \"STRING\",\n            \"float\": \"DOUBLE\",\n            \"int\": \"INTEGER\",\n            \"list\": \"LIST\",\n            \"dict\": \"MAP\",\n            \"bool\": \"BOOLEAN\",\n        }\n        for label in labels:\n            safe_label = self._validate_label(label)\n            query = f\"MATCH ()-[e:`{safe_label}`]-&gt;() RETURN properties(e) AS result LIMIT 100\"\n            records, _, _ = self.run_cypher(query)\n            seen: set[tuple[str, str]] = set()\n            for record in records:\n                props = record or {}\n                for key, value in props.items():\n                    seen.add((key, type_mapping.get(type(value).__name__, \"STRING\")))\n            edge_properties.append({\"type\": label, \"properties\": [{\"property\": k, \"type\": v} for k, v in seen]})\n        return edge_properties\n\n    def _sample_triples(self, edge_labels: list[str]) -&gt; list[str]:\n        triple_template = \"(:`{a}`)-[:`{e}`]-&gt;(:`{b}`)\"\n        triples: list[str] = []\n        for label in edge_labels:\n            safe_label = self._validate_label(label)\n            query = (\n                f\"MATCH (a)-[e:`{safe_label}`]-&gt;(b) \"\n                \"RETURN {from: labels(a), edge: type(e), to: labels(b)} AS result LIMIT 10\"\n            )\n            records, _, _ = self.run_cypher(query)\n            for record in records:\n                if not isinstance(record, dict):\n                    continue\n                from_labels = record.get(\"from\", [])\n                to_labels = record.get(\"to\", [])\n                if not from_labels or not to_labels:\n                    continue\n                triples.append(triple_template.format(a=from_labels[0], e=record.get(\"edge\"), b=to_labels[0]))\n        return triples\n\n    @staticmethod\n    def _filter_internal(items: list[str]) -&gt; list[str]:\n        return [item for item in items if not item.startswith(\"_ag_\")]\n\n    @staticmethod\n    def _validate_label(label: str) -&gt; str:\n        if not LABEL_PATTERN.match(label):\n            raise ValueError(f\"Invalid Apache AGE label: '{label}'\")\n        return label\n\n    @staticmethod\n    def _to_dollar_quoted(query: str) -&gt; str:\n        tag_base = \"age\"\n        tag = tag_base\n        counter = 0\n        while f\"${tag}$\" in query:\n            counter += 1\n            tag = f\"{tag_base}{counter}\"\n        return f\"${tag}$\\n{query}\\n${tag}$\"\n\n    def _get_graph_id_column(self) -&gt; str:\n        with self.client.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT column_name\n                FROM information_schema.columns\n                WHERE table_schema = 'ag_catalog' AND table_name = 'ag_graph'\n                \"\"\"\n            )\n            rows = cursor.fetchall()\n        columns = {row[\"column_name\"] if isinstance(row, dict) else row[0] for row in rows}\n        for candidate in (\"graphid\", \"id\", \"oid\"):\n            if candidate in columns:\n                return candidate\n        if columns:\n            fallback = sorted(columns)[0]\n            logger.warning(\n                \"Falling back to graph id column '%s' from ag_catalog.ag_graph; available columns: %s\",\n                fallback,\n                sorted(columns),\n            )\n            return fallback\n        raise RuntimeError(\"Unable to introspect ag_catalog.ag_graph columns for Apache AGE.\")\n</code></pre>"},{"location":"dynamiq/storages/graph/neo4j/neo4j/","title":"Neo4j","text":""},{"location":"dynamiq/storages/graph/neo4j/neo4j/#dynamiq.storages.graph.neo4j.neo4j.Neo4jGraphStore","title":"<code>Neo4jGraphStore</code>","text":"<p>               Bases: <code>BaseGraphStore</code></p> <p>Lightweight wrapper around the Neo4j Python driver.</p> <p>Provides helpers to run Cypher and to upsert simple node/relationship payloads.</p> Source code in <code>dynamiq/storages/graph/neo4j/neo4j.py</code> <pre><code>class Neo4jGraphStore(BaseGraphStore):\n    \"\"\"\n    Lightweight wrapper around the Neo4j Python driver.\n\n    Provides helpers to run Cypher and to upsert simple node/relationship payloads.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection: Neo4jConnection | None = None,\n        client: Any | None = None,\n        database: str | None = None,\n    ):\n        if client is None and connection is None:\n            raise ValueError(\"Either 'connection' or 'client' must be provided.\")\n\n        self.connection = connection\n        self.client = client or connection.connect()\n        self.database = database or (connection.database if connection else None)\n\n    def run_cypher(\n        self,\n        query: str,\n        parameters: dict[str, Any] | None = None,\n        database: str | None = None,\n        routing: str | RoutingControl | None = None,\n        result_transformer: Any | None = None,\n    ) -&gt; tuple[Any, Any, list[str]]:\n        \"\"\"\n        Execute a Cypher query with optional parameters and transformers.\n\n        Args:\n            query: Cypher string.\n            parameters: Query parameters passed as a dict.\n            database: Target database; defaults to connection/database on store.\n            routing: Optional routing flag ('r' or 'w').\n            result_transformer: Optional transformer passed to `result_transformer_`.\n        \"\"\"\n        params = parameters or {}\n        execute_kwargs = {}\n        target_db = database or self.database\n        if target_db:\n            execute_kwargs[\"database_\"] = target_db\n        if routing:\n            execute_kwargs[\"routing_\"] = self._normalize_routing(routing)\n        if result_transformer:\n\n            def _transform_with_metadata(result: Any) -&gt; tuple[Any, Any, list[str]]:\n                if callable(result_transformer):\n                    transformed = result_transformer(result)\n                elif hasattr(result_transformer, \"__get__\"):\n                    transformed = result_transformer.__get__(result, type(result))\n                else:\n                    transformed = result_transformer\n                summary = result.consume()\n                keys = result.keys()\n                return transformed, summary, keys\n\n            execute_kwargs[\"result_transformer_\"] = _transform_with_metadata\n\n        try:\n            return self.client.execute_query(query, parameters_=params, **execute_kwargs)\n        except Neo4jError as exc:\n            logger.error(f\"Neo4j query failed: {exc.code} - {exc.message}\")\n            raise\n\n    def update_client(self, client: Any) -&gt; None:\n        self.client = client\n\n    @staticmethod\n    def format_records(records: Iterable[Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert Neo4j Record objects to plain dicts.\"\"\"\n        return [record.data() for record in records]\n\n    def introspect_schema(self, *, include_properties: bool, **kwargs: Any) -&gt; dict[str, Any]:\n        database = kwargs.get(\"database\")\n        labels_records, _, _ = self.run_cypher(\n            \"CALL db.labels() YIELD label RETURN label ORDER BY label\",\n            database=database,\n        )\n        reltype_records, _, _ = self.run_cypher(\n            \"CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType ORDER BY relationshipType\",\n            database=database,\n        )\n        labels = [r[\"label\"] for r in labels_records]\n        rel_types = [r[\"relationshipType\"] for r in reltype_records]\n\n        node_props: list[dict[str, Any]] = []\n        rel_props: list[dict[str, Any]] = []\n\n        if include_properties:\n            try:\n                node_props_records, _, _ = self.run_cypher(\n                    \"CALL db.schema.nodeTypeProperties()\",\n                    database=database,\n                )\n                node_props = [r.data() for r in node_props_records]\n            except Exception as exc:  # noqa: BLE001\n                logger.warning(f\"Node property introspection failed: {exc}\")\n\n            try:\n                rel_props_records, _, _ = self.run_cypher(\n                    \"CALL db.schema.relTypeProperties()\",\n                    database=database,\n                )\n                rel_props = [r.data() for r in rel_props_records]\n            except Exception as exc:  # noqa: BLE001\n                logger.warning(f\"Relationship property introspection failed: {exc}\")\n\n        return {\n            \"labels\": labels,\n            \"relationship_types\": rel_types,\n            \"node_properties\": node_props,\n            \"relationship_properties\": rel_props,\n        }\n\n    def supports_graph_result(self) -&gt; bool:\n        return True\n\n    def write_graph(\n        self,\n        *,\n        nodes: list[dict[str, Any]],\n        relationships: list[dict[str, Any]],\n        database: str | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Upsert nodes and relationships using MERGE + SET.\n\n        Nodes must include:\n            - labels: list[str]\n            - properties: dict (must contain the identity_key)\n            - identity_key: str (defaults to 'id' if missing)\n\n        Relationships must include:\n            - start_label, end_label: str\n            - start_identity_key, end_identity_key: str\n            - start_identity, end_identity: Any\n            - type: str\n            - properties: dict (optional)\n        \"\"\"\n        if not nodes and not relationships:\n            raise ValueError(\"At least one node or relationship must be provided.\")\n\n        total_nodes_created = 0\n        total_properties_set = 0\n        total_relationships_created = 0\n        all_records: list[dict[str, Any]] = []\n        last_keys: list[str] = []\n\n        if nodes:\n            node_lines: list[str] = []\n            node_params: dict[str, Any] = {}\n            return_nodes: list[str] = []\n            for idx, node in enumerate(nodes):\n                labels = node.get(\"labels\") or []\n                identity_key = self._format_property_key(node.get(\"identity_key\") or \"id\")\n                properties = node.get(\"properties\") or {}\n                if identity_key not in properties:\n                    raise ValueError(f\"Node {idx} is missing identity key '{identity_key}' in properties.\")\n\n                label_string = self._format_labels(labels)\n                param_props = f\"node_{idx}_props\"\n                param_id = f\"node_{idx}_id\"\n                node_params[param_props] = properties\n                node_params[param_id] = properties[identity_key]\n\n                node_lines.append(\n                    f\"MERGE (n{idx}{label_string} {{{identity_key}: ${param_id}}})\\n\" f\"SET n{idx} += ${param_props}\"\n                )\n                return_nodes.append(f\"n{idx}\")\n\n            node_query = \"\\n\".join(node_lines) + \"\\nRETURN \" + \", \".join(return_nodes)\n            node_records, node_summary, node_keys = self.run_cypher(node_query, node_params, database=database)\n            total_nodes_created += node_summary.counters.nodes_created\n            total_properties_set += node_summary.counters.properties_set\n            all_records.extend(self.format_records(node_records))\n            if node_keys:\n                last_keys = node_keys\n\n        if relationships:\n            for idx, rel in enumerate(relationships):\n                rel_type = self._format_relationship_type(rel.get(\"type\") or \"\")\n                start_label = self._format_single_label(rel.get(\"start_label\") or \"\")\n                end_label = self._format_single_label(rel.get(\"end_label\") or \"\")\n                start_identity_key = self._format_property_key(rel.get(\"start_identity_key\") or \"id\")\n                end_identity_key = self._format_property_key(rel.get(\"end_identity_key\") or \"id\")\n                start_identity = rel.get(\"start_identity\")\n                end_identity = rel.get(\"end_identity\")\n                properties = rel.get(\"properties\") or {}\n\n                if start_identity is None or end_identity is None:\n                    raise ValueError(f\"Relationship {idx} missing start or end identity value.\")\n\n                rel_query = (\n                    f\"MATCH (s:{start_label} {{{start_identity_key}: $start_id}})\\n\"\n                    f\"MATCH (e:{end_label} {{{end_identity_key}: $end_id}})\\n\"\n                    f\"MERGE (s)-[r:{rel_type}]-&gt;(e)\\n\"\n                    f\"SET r += $props\\n\"\n                    \"RETURN r\"\n                )\n                rel_params = {\n                    \"start_id\": start_identity,\n                    \"end_id\": end_identity,\n                    \"props\": properties,\n                }\n                rel_records, rel_summary, rel_keys = self.run_cypher(rel_query, rel_params, database=database)\n                total_relationships_created += rel_summary.counters.relationships_created\n                total_properties_set += rel_summary.counters.properties_set\n                all_records.extend(self.format_records(rel_records))\n                if rel_keys:\n                    last_keys = rel_keys\n\n        return {\n            \"nodes_created\": total_nodes_created,\n            \"properties_set\": total_properties_set,\n            \"relationships_created\": total_relationships_created,\n            \"records\": all_records,\n            \"keys\": last_keys,\n        }\n\n    @staticmethod\n    def _format_labels(labels: list[str]) -&gt; str:\n        if not labels:\n            raise ValueError(\"At least one label is required for a node.\")\n        cleaned = [Neo4jGraphStore._format_single_label(label) for label in labels]\n        return \":\" + \":\".join(cleaned)\n\n    @staticmethod\n    def _format_single_label(label: str) -&gt; str:\n        if not LABEL_PATTERN.match(label):\n            raise ValueError(f\"Invalid Neo4j label: '{label}'\")\n        return label\n\n    @staticmethod\n    def _format_relationship_type(rel_type: str) -&gt; str:\n        if not LABEL_PATTERN.match(rel_type):\n            raise ValueError(f\"Invalid Neo4j relationship type: '{rel_type}'\")\n        return rel_type\n\n    @staticmethod\n    def _format_property_key(key: str) -&gt; str:\n        if not PROPERTY_KEY_PATTERN.match(key):\n            raise ValueError(f\"Invalid Neo4j property key: '{key}'\")\n        return key\n\n    def close(self: \"Neo4jGraphStore\") -&gt; None:\n        if self.client:\n            self.client.close()\n\n    @staticmethod\n    def _normalize_routing(routing: str | RoutingControl) -&gt; RoutingControl:\n        if isinstance(routing, RoutingControl):\n            return routing\n        routing_value = routing.strip().lower()\n        if routing_value in {\"r\", \"read\"}:\n            return RoutingControl.READ\n        if routing_value in {\"w\", \"write\"}:\n            return RoutingControl.WRITE\n        raise ValueError(\"routing must be 'r'/'read' or 'w'/'write' when provided.\")\n</code></pre>"},{"location":"dynamiq/storages/graph/neo4j/neo4j/#dynamiq.storages.graph.neo4j.neo4j.Neo4jGraphStore.format_records","title":"<code>format_records(records)</code>  <code>staticmethod</code>","text":"<p>Convert Neo4j Record objects to plain dicts.</p> Source code in <code>dynamiq/storages/graph/neo4j/neo4j.py</code> <pre><code>@staticmethod\ndef format_records(records: Iterable[Any]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Neo4j Record objects to plain dicts.\"\"\"\n    return [record.data() for record in records]\n</code></pre>"},{"location":"dynamiq/storages/graph/neo4j/neo4j/#dynamiq.storages.graph.neo4j.neo4j.Neo4jGraphStore.run_cypher","title":"<code>run_cypher(query, parameters=None, database=None, routing=None, result_transformer=None)</code>","text":"<p>Execute a Cypher query with optional parameters and transformers.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Cypher string.</p> required <code>parameters</code> <code>dict[str, Any] | None</code> <p>Query parameters passed as a dict.</p> <code>None</code> <code>database</code> <code>str | None</code> <p>Target database; defaults to connection/database on store.</p> <code>None</code> <code>routing</code> <code>str | RoutingControl | None</code> <p>Optional routing flag ('r' or 'w').</p> <code>None</code> <code>result_transformer</code> <code>Any | None</code> <p>Optional transformer passed to <code>result_transformer_</code>.</p> <code>None</code> Source code in <code>dynamiq/storages/graph/neo4j/neo4j.py</code> <pre><code>def run_cypher(\n    self,\n    query: str,\n    parameters: dict[str, Any] | None = None,\n    database: str | None = None,\n    routing: str | RoutingControl | None = None,\n    result_transformer: Any | None = None,\n) -&gt; tuple[Any, Any, list[str]]:\n    \"\"\"\n    Execute a Cypher query with optional parameters and transformers.\n\n    Args:\n        query: Cypher string.\n        parameters: Query parameters passed as a dict.\n        database: Target database; defaults to connection/database on store.\n        routing: Optional routing flag ('r' or 'w').\n        result_transformer: Optional transformer passed to `result_transformer_`.\n    \"\"\"\n    params = parameters or {}\n    execute_kwargs = {}\n    target_db = database or self.database\n    if target_db:\n        execute_kwargs[\"database_\"] = target_db\n    if routing:\n        execute_kwargs[\"routing_\"] = self._normalize_routing(routing)\n    if result_transformer:\n\n        def _transform_with_metadata(result: Any) -&gt; tuple[Any, Any, list[str]]:\n            if callable(result_transformer):\n                transformed = result_transformer(result)\n            elif hasattr(result_transformer, \"__get__\"):\n                transformed = result_transformer.__get__(result, type(result))\n            else:\n                transformed = result_transformer\n            summary = result.consume()\n            keys = result.keys()\n            return transformed, summary, keys\n\n        execute_kwargs[\"result_transformer_\"] = _transform_with_metadata\n\n    try:\n        return self.client.execute_query(query, parameters_=params, **execute_kwargs)\n    except Neo4jError as exc:\n        logger.error(f\"Neo4j query failed: {exc.code} - {exc.message}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/graph/neo4j/neo4j/#dynamiq.storages.graph.neo4j.neo4j.Neo4jGraphStore.write_graph","title":"<code>write_graph(*, nodes, relationships, database=None)</code>","text":"<p>Upsert nodes and relationships using MERGE + SET.</p> Nodes must include <ul> <li>labels: list[str]</li> <li>properties: dict (must contain the identity_key)</li> <li>identity_key: str (defaults to 'id' if missing)</li> </ul> Relationships must include <ul> <li>start_label, end_label: str</li> <li>start_identity_key, end_identity_key: str</li> <li>start_identity, end_identity: Any</li> <li>type: str</li> <li>properties: dict (optional)</li> </ul> Source code in <code>dynamiq/storages/graph/neo4j/neo4j.py</code> <pre><code>def write_graph(\n    self,\n    *,\n    nodes: list[dict[str, Any]],\n    relationships: list[dict[str, Any]],\n    database: str | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Upsert nodes and relationships using MERGE + SET.\n\n    Nodes must include:\n        - labels: list[str]\n        - properties: dict (must contain the identity_key)\n        - identity_key: str (defaults to 'id' if missing)\n\n    Relationships must include:\n        - start_label, end_label: str\n        - start_identity_key, end_identity_key: str\n        - start_identity, end_identity: Any\n        - type: str\n        - properties: dict (optional)\n    \"\"\"\n    if not nodes and not relationships:\n        raise ValueError(\"At least one node or relationship must be provided.\")\n\n    total_nodes_created = 0\n    total_properties_set = 0\n    total_relationships_created = 0\n    all_records: list[dict[str, Any]] = []\n    last_keys: list[str] = []\n\n    if nodes:\n        node_lines: list[str] = []\n        node_params: dict[str, Any] = {}\n        return_nodes: list[str] = []\n        for idx, node in enumerate(nodes):\n            labels = node.get(\"labels\") or []\n            identity_key = self._format_property_key(node.get(\"identity_key\") or \"id\")\n            properties = node.get(\"properties\") or {}\n            if identity_key not in properties:\n                raise ValueError(f\"Node {idx} is missing identity key '{identity_key}' in properties.\")\n\n            label_string = self._format_labels(labels)\n            param_props = f\"node_{idx}_props\"\n            param_id = f\"node_{idx}_id\"\n            node_params[param_props] = properties\n            node_params[param_id] = properties[identity_key]\n\n            node_lines.append(\n                f\"MERGE (n{idx}{label_string} {{{identity_key}: ${param_id}}})\\n\" f\"SET n{idx} += ${param_props}\"\n            )\n            return_nodes.append(f\"n{idx}\")\n\n        node_query = \"\\n\".join(node_lines) + \"\\nRETURN \" + \", \".join(return_nodes)\n        node_records, node_summary, node_keys = self.run_cypher(node_query, node_params, database=database)\n        total_nodes_created += node_summary.counters.nodes_created\n        total_properties_set += node_summary.counters.properties_set\n        all_records.extend(self.format_records(node_records))\n        if node_keys:\n            last_keys = node_keys\n\n    if relationships:\n        for idx, rel in enumerate(relationships):\n            rel_type = self._format_relationship_type(rel.get(\"type\") or \"\")\n            start_label = self._format_single_label(rel.get(\"start_label\") or \"\")\n            end_label = self._format_single_label(rel.get(\"end_label\") or \"\")\n            start_identity_key = self._format_property_key(rel.get(\"start_identity_key\") or \"id\")\n            end_identity_key = self._format_property_key(rel.get(\"end_identity_key\") or \"id\")\n            start_identity = rel.get(\"start_identity\")\n            end_identity = rel.get(\"end_identity\")\n            properties = rel.get(\"properties\") or {}\n\n            if start_identity is None or end_identity is None:\n                raise ValueError(f\"Relationship {idx} missing start or end identity value.\")\n\n            rel_query = (\n                f\"MATCH (s:{start_label} {{{start_identity_key}: $start_id}})\\n\"\n                f\"MATCH (e:{end_label} {{{end_identity_key}: $end_id}})\\n\"\n                f\"MERGE (s)-[r:{rel_type}]-&gt;(e)\\n\"\n                f\"SET r += $props\\n\"\n                \"RETURN r\"\n            )\n            rel_params = {\n                \"start_id\": start_identity,\n                \"end_id\": end_identity,\n                \"props\": properties,\n            }\n            rel_records, rel_summary, rel_keys = self.run_cypher(rel_query, rel_params, database=database)\n            total_relationships_created += rel_summary.counters.relationships_created\n            total_properties_set += rel_summary.counters.properties_set\n            all_records.extend(self.format_records(rel_records))\n            if rel_keys:\n                last_keys = rel_keys\n\n    return {\n        \"nodes_created\": total_nodes_created,\n        \"properties_set\": total_properties_set,\n        \"relationships_created\": total_relationships_created,\n        \"records\": all_records,\n        \"keys\": last_keys,\n    }\n</code></pre>"},{"location":"dynamiq/storages/graph/neptune/neptune/","title":"Neptune","text":""},{"location":"dynamiq/storages/graph/neptune/neptune/#dynamiq.storages.graph.neptune.neptune.NeptuneGraphStore","title":"<code>NeptuneGraphStore</code>","text":"<p>               Bases: <code>BaseGraphStore</code></p> <p>Wrapper for Amazon Neptune openCypher execution over HTTP.</p> Source code in <code>dynamiq/storages/graph/neptune/neptune.py</code> <pre><code>class NeptuneGraphStore(BaseGraphStore):\n    \"\"\"Wrapper for Amazon Neptune openCypher execution over HTTP.\"\"\"\n\n    def __init__(\n        self,\n        connection: AWSNeptuneConnection | None = None,\n        client: Any | None = None,\n        endpoint: str | None = None,\n        verify_ssl: bool | None = None,\n        timeout: int | None = None,\n    ) -&gt; None:\n        if client is None and connection is None:\n            raise ValueError(\"Either 'connection' or 'client' must be provided.\")\n\n        self.connection = connection\n        self.client = client or connection.connect()\n        self.endpoint = endpoint or (connection.endpoint if connection else None)\n        self.verify_ssl = verify_ssl if verify_ssl is not None else (connection.verify_ssl if connection else True)\n        self.timeout = timeout if timeout is not None else (connection.timeout if connection else 30)\n\n        if not self.endpoint:\n            raise ValueError(\"endpoint must be provided for Neptune.\")\n\n    def run_cypher(\n        self,\n        query: str,\n        parameters: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[list[Any], dict[str, Any], list[str]]:\n        params = parameters or {}\n        payload = {\"query\": query}\n        if params:\n            payload[\"parameters\"] = params\n        try:\n            response = self.client.post(\n                self.endpoint,\n                data=json.dumps(payload),\n                headers={\"Content-Type\": \"application/json\"},\n                verify=self.verify_ssl,\n                timeout=self.timeout,\n            )\n            response.raise_for_status()\n            records = response.json().get(\"results\", [])\n        except Exception as exc:  # noqa: BLE001\n            logger.error(f\"Neptune query failed: {exc}\")\n            raise\n\n        summary = {\"query\": query, \"counters\": {}, \"result_available_after\": None}\n        return records, summary, []\n\n    def introspect_schema(self, *, include_properties: bool, **kwargs: Any) -&gt; dict[str, Any]:\n        node_labels = self._sample_labels()\n        edge_labels = self._sample_relationship_types()\n\n        node_properties: list[dict[str, Any]] = []\n        edge_properties: list[dict[str, Any]] = []\n        relationships: list[str] = []\n\n        if include_properties:\n            node_properties = self._sample_node_properties(node_labels)\n            edge_properties = self._sample_edge_properties(edge_labels)\n\n        relationships = self._sample_triples(edge_labels)\n\n        return {\n            \"labels\": node_labels,\n            \"relationship_types\": edge_labels,\n            \"node_properties\": node_properties,\n            \"relationship_properties\": edge_properties,\n            \"relationships\": relationships,\n        }\n\n    def _sample_labels(self) -&gt; list[str]:\n        query = \"MATCH (n) RETURN DISTINCT labels(n) AS labels LIMIT 200\"\n        records, _, _ = self.run_cypher(query)\n        labels: set[str] = set()\n        for record in records:\n            row = record.get(\"labels\", []) if isinstance(record, dict) else []\n            for label in row:\n                if isinstance(label, str):\n                    labels.add(label)\n        return sorted(labels)\n\n    def _sample_relationship_types(self) -&gt; list[str]:\n        query = \"MATCH ()-[r]-&gt;() RETURN DISTINCT type(r) AS type LIMIT 200\"\n        records, _, _ = self.run_cypher(query)\n        rel_types = sorted(\n            {record.get(\"type\") for record in records if isinstance(record, dict) and record.get(\"type\")}\n        )\n        return rel_types\n\n    def _sample_node_properties(self, labels: list[str]) -&gt; list[dict[str, Any]]:\n        node_properties: list[dict[str, Any]] = []\n        type_mapping = {\n            \"str\": \"STRING\",\n            \"float\": \"DOUBLE\",\n            \"int\": \"INTEGER\",\n            \"list\": \"LIST\",\n            \"dict\": \"MAP\",\n            \"bool\": \"BOOLEAN\",\n        }\n        for label in labels:\n            safe_label = self._validate_label(label)\n            query = f\"MATCH (a:`{safe_label}`) RETURN properties(a) AS props LIMIT 100\"\n            records, _, _ = self.run_cypher(query)\n            seen: set[tuple[str, str]] = set()\n            for record in records:\n                props = record.get(\"props\", {}) if isinstance(record, dict) else {}\n                for key, value in props.items():\n                    seen.add((key, type_mapping.get(type(value).__name__, \"STRING\")))\n            node_properties.append(\n                {\n                    \"labels\": label,\n                    \"properties\": [{\"property\": key, \"type\": value} for key, value in seen],\n                }\n            )\n        return node_properties\n\n    def _sample_edge_properties(self, labels: list[str]) -&gt; list[dict[str, Any]]:\n        edge_properties: list[dict[str, Any]] = []\n        type_mapping = {\n            \"str\": \"STRING\",\n            \"float\": \"DOUBLE\",\n            \"int\": \"INTEGER\",\n            \"list\": \"LIST\",\n            \"dict\": \"MAP\",\n            \"bool\": \"BOOLEAN\",\n        }\n        for label in labels:\n            safe_label = self._validate_label(label)\n            query = f\"MATCH ()-[e:`{safe_label}`]-&gt;() RETURN properties(e) AS props LIMIT 100\"\n            records, _, _ = self.run_cypher(query)\n            seen: set[tuple[str, str]] = set()\n            for record in records:\n                props = record.get(\"props\", {}) if isinstance(record, dict) else {}\n                for key, value in props.items():\n                    seen.add((key, type_mapping.get(type(value).__name__, \"STRING\")))\n            edge_properties.append(\n                {\n                    \"type\": label,\n                    \"properties\": [{\"property\": key, \"type\": value} for key, value in seen],\n                }\n            )\n        return edge_properties\n\n    def _sample_triples(self, edge_labels: list[str]) -&gt; list[str]:\n        triple_template = \"(:`{a}`)-[:`{e}`]-&gt;(:`{b}`)\"\n        triples: list[str] = []\n        for label in edge_labels:\n            safe_label = self._validate_label(label)\n            query = (\n                f\"MATCH (a)-[e:`{safe_label}`]-&gt;(b) \"\n                \"RETURN {from: labels(a), edge: type(e), to: labels(b)} AS result LIMIT 10\"\n            )\n            records, _, _ = self.run_cypher(query)\n            for record in records:\n                if not isinstance(record, dict):\n                    continue\n                payload = record.get(\"result\", record)\n                from_labels = payload.get(\"from\", [])\n                to_labels = payload.get(\"to\", [])\n                if not from_labels or not to_labels:\n                    continue\n                triples.append(\n                    triple_template.format(\n                        a=from_labels[0],\n                        e=payload.get(\"edge\"),\n                        b=to_labels[0],\n                    )\n                )\n        return triples\n\n    @staticmethod\n    def _validate_label(label: str) -&gt; str:\n        \"\"\"Validate label to prevent Cypher injection.\n\n        Args:\n            label: Label name to validate.\n\n        Returns:\n            The validated label.\n\n        Raises:\n            ValueError: If label contains invalid characters.\n        \"\"\"\n        if not LABEL_PATTERN.match(label):\n            raise ValueError(f\"Invalid Neptune label: '{label}'\")\n        return label\n</code></pre>"},{"location":"dynamiq/storages/vector/base/","title":"Base","text":""},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStore","title":"<code>BaseVectorStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all vector stores.</p> <p>This abstract class provides a consistent interface for all vector store implementations, including common methods for document deletion by file ID(s).</p> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>class BaseVectorStore(ABC):\n    \"\"\"Base class for all vector stores.\n\n    This abstract class provides a consistent interface for all vector store implementations,\n    including common methods for document deletion by file ID(s).\n    \"\"\"\n\n    @abstractmethod\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided filters.\n\n        Args:\n            filters (dict[str, Any]): Filters to select documents to delete.\n        \"\"\"\n        pass\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided file ID.\n        File ID should be located in the metadata of the document.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def delete_documents_by_file_ids(self, file_ids: list[str], batch_size: int = 500) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided list of file IDs.\n        File IDs should be located in the metadata of the documents.\n\n        Args:\n            file_ids (list[str]): The list of file IDs to filter by.\n            batch_size (int): Maximum number of file IDs to process in a single batch. Defaults to 500.\n        \"\"\"\n        if not file_ids:\n            logger.warning(\"No file IDs provided. No documents will be deleted.\")\n            return\n\n        if len(file_ids) &gt; batch_size:\n            for i in range(0, len(file_ids), batch_size):\n                batch = file_ids[i : i + batch_size]\n                filters = create_file_ids_filter(batch)\n                self.delete_documents_by_filters(filters)\n                logger.debug(f\"Deleted documents batch {i//batch_size + 1} with {len(batch)} file IDs\")\n        else:\n            filters = create_file_ids_filter(file_ids)\n            self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the vector store based on the provided file ID. File ID should be located in the metadata of the document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided file ID.\n    File ID should be located in the metadata of the document.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStore.delete_documents_by_file_ids","title":"<code>delete_documents_by_file_ids(file_ids, batch_size=500)</code>","text":"<p>Delete documents from the vector store based on the provided list of file IDs. File IDs should be located in the metadata of the documents.</p> <p>Parameters:</p> Name Type Description Default <code>file_ids</code> <code>list[str]</code> <p>The list of file IDs to filter by.</p> required <code>batch_size</code> <code>int</code> <p>Maximum number of file IDs to process in a single batch. Defaults to 500.</p> <code>500</code> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>def delete_documents_by_file_ids(self, file_ids: list[str], batch_size: int = 500) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided list of file IDs.\n    File IDs should be located in the metadata of the documents.\n\n    Args:\n        file_ids (list[str]): The list of file IDs to filter by.\n        batch_size (int): Maximum number of file IDs to process in a single batch. Defaults to 500.\n    \"\"\"\n    if not file_ids:\n        logger.warning(\"No file IDs provided. No documents will be deleted.\")\n        return\n\n    if len(file_ids) &gt; batch_size:\n        for i in range(0, len(file_ids), batch_size):\n            batch = file_ids[i : i + batch_size]\n            filters = create_file_ids_filter(batch)\n            self.delete_documents_by_filters(filters)\n            logger.debug(f\"Deleted documents batch {i//batch_size + 1} with {len(batch)} file IDs\")\n    else:\n        filters = create_file_ids_filter(file_ids)\n        self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>  <code>abstractmethod</code>","text":"<p>Delete documents from the vector store based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters to select documents to delete.</p> required Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>@abstractmethod\ndef delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided filters.\n\n    Args:\n        filters (dict[str, Any]): Filters to select documents to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStoreParams","title":"<code>BaseVectorStoreParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base parameters for vector store.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> <code>content_key</code> <code>str</code> <p>Key for content field. Defaults to \"content\".</p> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>class BaseVectorStoreParams(BaseModel):\n    \"\"\"Base parameters for vector store.\n\n    Attributes:\n        index_name (str): Name of the index. Defaults to \"default\".\n        content_key (str): Key for content field. Defaults to \"content\".\n    \"\"\"\n    index_name: str = \"default\"\n    content_key: str = \"content\"\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseWriterVectorStoreParams","title":"<code>BaseWriterVectorStoreParams</code>","text":"<p>               Bases: <code>BaseVectorStoreParams</code></p> <p>Parameters for writer vector store.</p> <p>Attributes:</p> Name Type Description <code>create_if_not_exist</code> <code>bool</code> <p>Flag to create index if it does not exist. Defaults to True.</p> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>class BaseWriterVectorStoreParams(BaseVectorStoreParams):\n    \"\"\"Parameters for writer vector store.\n\n    Attributes:\n        create_if_not_exist (bool): Flag to create index if it does not exist. Defaults to True.\n    \"\"\"\n\n    create_if_not_exist: bool = False\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreDuplicateDocumentException","title":"<code>VectorStoreDuplicateDocumentException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when attempting to add a duplicate document to the vector store.</p> <p>This exception is thrown when a document with the same identifier or content is already present in the vector store and an attempt is made to add it again.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreDuplicateDocumentException(Exception):\n    \"\"\"\n    Exception raised when attempting to add a duplicate document to the vector store.\n\n    This exception is thrown when a document with the same identifier or content is already present\n    in the vector store and an attempt is made to add it again.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreException","title":"<code>VectorStoreException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for vector store related errors.</p> <p>This exception is raised when a general error occurs in the vector store operations.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreException(Exception):\n    \"\"\"\n    Base exception class for vector store related errors.\n\n    This exception is raised when a general error occurs in the vector store operations.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreFilterException","title":"<code>VectorStoreFilterException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an error in filtering operations on the vector store.</p> <p>This exception is thrown when an invalid filter is applied or when there's an issue with the filtering process in the vector store.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreFilterException(Exception):\n    \"\"\"\n    Exception raised when there's an error in filtering operations on the vector store.\n\n    This exception is thrown when an invalid filter is applied or when there's an issue with the\n    filtering process in the vector store.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/policies/","title":"Policies","text":""},{"location":"dynamiq/storages/vector/policies/#dynamiq.storages.vector.policies.DuplicatePolicy","title":"<code>DuplicatePolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of policies for handling duplicate items.</p> <p>Attributes:</p> Name Type Description <code>NONE</code> <code>str</code> <p>No specific policy for handling duplicates.</p> <code>SKIP</code> <code>str</code> <p>Skip duplicate items without modifying existing ones.</p> <code>OVERWRITE</code> <code>str</code> <p>Overwrite existing items with duplicate entries.</p> <code>FAIL</code> <code>str</code> <p>Raise an error when encountering duplicate items.</p> Source code in <code>dynamiq/storages/vector/policies.py</code> <pre><code>class DuplicatePolicy(str, Enum):\n    \"\"\"\n    Enumeration of policies for handling duplicate items.\n\n    Attributes:\n        NONE (str): No specific policy for handling duplicates.\n        SKIP (str): Skip duplicate items without modifying existing ones.\n        OVERWRITE (str): Overwrite existing items with duplicate entries.\n        FAIL (str): Raise an error when encountering duplicate items.\n    \"\"\"\n\n    NONE = \"none\"\n    SKIP = \"skip\"\n    OVERWRITE = \"overwrite\"\n    FAIL = \"fail\"\n</code></pre>"},{"location":"dynamiq/storages/vector/utils/","title":"Utils","text":""},{"location":"dynamiq/storages/vector/utils/#dynamiq.storages.vector.utils.create_file_id_filter","title":"<code>create_file_id_filter(file_id)</code>","text":"<p>Create filters for vector store query based on file_id.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The filter conditions.</p> Source code in <code>dynamiq/storages/vector/utils.py</code> <pre><code>def create_file_id_filter(file_id: str) -&gt; dict:\n    \"\"\"\n    Create filters for vector store query based on file_id.\n\n    Args:\n        file_id (str): The file ID to filter by.\n\n    Returns:\n        dict: The filter conditions.\n    \"\"\"\n    return {\n        \"operator\": \"AND\",\n        \"conditions\": [\n            {\"field\": \"file_id\", \"operator\": \"==\", \"value\": file_id},\n        ],\n    }\n</code></pre>"},{"location":"dynamiq/storages/vector/utils/#dynamiq.storages.vector.utils.create_file_ids_filter","title":"<code>create_file_ids_filter(file_ids)</code>","text":"<p>Create filters for vector store query based on multiple file_ids.</p> <p>Parameters:</p> Name Type Description Default <code>file_ids</code> <code>list[str]</code> <p>The list of file IDs to filter by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The filter conditions.</p> Source code in <code>dynamiq/storages/vector/utils.py</code> <pre><code>def create_file_ids_filter(file_ids: list[str]) -&gt; dict:\n    \"\"\"\n    Create filters for vector store query based on multiple file_ids.\n\n    Args:\n        file_ids (list[str]): The list of file IDs to filter by.\n\n    Returns:\n        dict: The filter conditions.\n    \"\"\"\n    return {\n        \"operator\": \"AND\",\n        \"conditions\": [\n            {\"field\": \"file_id\", \"operator\": \"in\", \"value\": file_ids},\n        ],\n    }\n</code></pre>"},{"location":"dynamiq/storages/vector/utils/#dynamiq.storages.vector.utils.create_pgvector_file_id_filter","title":"<code>create_pgvector_file_id_filter(file_id)</code>","text":"<p>Create filters for pgvector query based on file_id.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The filter conditions for pgvector.</p> Source code in <code>dynamiq/storages/vector/utils.py</code> <pre><code>def create_pgvector_file_id_filter(file_id: str) -&gt; dict:\n    \"\"\"\n    Create filters for pgvector query based on file_id.\n\n    Args:\n        file_id (str): The file ID to filter by.\n\n    Returns:\n        dict: The filter conditions for pgvector.\n    \"\"\"\n    return {\"field\": \"metadata.file_id\", \"operator\": \"==\", \"value\": file_id}\n</code></pre>"},{"location":"dynamiq/storages/vector/utils/#dynamiq.storages.vector.utils.create_pgvector_file_ids_filter","title":"<code>create_pgvector_file_ids_filter(file_ids)</code>","text":"<p>Create filters for pgvector query based on multiple file_ids.</p> <p>Parameters:</p> Name Type Description Default <code>file_ids</code> <code>list[str]</code> <p>The list of file IDs to filter by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The filter conditions for pgvector.</p> Source code in <code>dynamiq/storages/vector/utils.py</code> <pre><code>def create_pgvector_file_ids_filter(file_ids: list[str]) -&gt; dict:\n    \"\"\"\n    Create filters for pgvector query based on multiple file_ids.\n\n    Args:\n        file_ids (list[str]): The list of file IDs to filter by.\n\n    Returns:\n        dict: The filter conditions for pgvector.\n    \"\"\"\n    return {\"field\": \"metadata.file_id\", \"operator\": \"in\", \"value\": file_ids}\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/","title":"Chroma","text":""},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore","title":"<code>ChromaVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using Chroma.</p> <p>This class provides an interface to interact with a Chroma vector store for document storage and retrieval.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>ClientAPI</code> <p>The Chroma client API instance.</p> <code>index_name</code> <code>str</code> <p>The name of the index or collection in the vector store.</p> <code>_collection</code> <p>The Chroma collection object.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>class ChromaVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"\n    Vector store using Chroma.\n\n    This class provides an interface to interact with a Chroma vector store for document storage and\n    retrieval.\n\n    Attributes:\n        client (ClientAPI): The Chroma client API instance.\n        index_name (str): The name of the index or collection in the vector store.\n        _collection: The Chroma collection object.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection: Chroma | None = None,\n        client: Optional[\"ClientAPI\"] = None,\n        index_name: str = \"default\",\n        create_if_not_exist: bool = False,\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize the ChromaVectorStore.\n\n        Args:\n            connection (Chroma | None): A Chroma connection object. Defaults to None.\n            client (Optional[ClientAPI]): A Chroma client API instance. Defaults to None.\n            index_name (str): The name of the index or collection. Defaults to \"default\".\n            create_if_not_exist (bool): Whether to create the collection if it doesn't exist. Defaults to False.\n            dry_run_config (DryRunConfig | None): Configuration for dry run mode. Defaults to None.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        self.client = client\n        if self.client is None:\n            connection = connection or Chroma()\n            self.client = connection.connect()\n        self.index_name = index_name\n        if create_if_not_exist:\n            collection_exists = self.client.get_collection(name=index_name)\n            if not collection_exists:\n                self._collection = self.client.create_collection(name=index_name)\n                self._track_collection(index_name)\n            else:\n                self._collection = self.client.get_collection(name=index_name)\n        else:\n            self._collection = self.client.get_collection(name=index_name)\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Get the number of documents in the collection.\n\n        Returns:\n            int: The number of documents in the collection.\n        \"\"\"\n        return self._collection.count()\n\n    def write_documents(self, documents: list[Document]) -&gt; int:\n        \"\"\"\n        Write (or overwrite) documents into the store.\n\n        This method processes a list of documents and writes them into the vector store.\n\n        Args:\n            documents (list[Document]): A list of Document objects to be written into the document\n                store.\n\n        Raises:\n            ValueError: If an item in the documents list is not an instance of the Document class.\n\n        Returns:\n            int: The number of documents successfully written to the document store.\n        \"\"\"\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = (\n                    \"param 'documents' must contain a list of objects of type Document\"\n                )\n                raise ValueError(msg)\n\n            data = {\"ids\": [doc.id], \"documents\": [doc.content]}\n\n            if doc.metadata:\n                data[\"metadatas\"] = [doc.metadata]\n\n            if doc.embedding:\n                data[\"embeddings\"] = [doc.embedding]\n\n            self._collection.add(**data)\n            self._track_documents([doc.id])\n\n        return len(documents)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on their IDs.\n\n        Args:\n            document_ids (list[str]): A list containing the IDs of documents to be deleted from the store.\n            delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n        \"\"\"\n\n        if delete_all and self._collection is not None:\n            self.client.delete_collection(name=self.index_name)\n            self._collection = self.client.get_or_create_collection(\n                name=self.index_name\n            )\n        else:\n            if not document_ids:\n                logger.warning(\n                    \"No document IDs provided. No documents will be deleted.\"\n                )\n            else:\n                self._collection.delete(ids=document_ids)\n\n    def delete_documents_by_filters(self, filters: dict[str, Any] | None = None) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided filters.\n\n        Args:\n            filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n                None.\n        \"\"\"\n        if filters is None:\n            raise ValueError(\"No filters provided. No documents will be deleted.\")\n        else:\n            ids, where, where_document = self._normalize_filters(filters)\n            self._collection.delete(ids=ids, where=where, where_document=where_document)\n\n    def delete_collection(self, collection_name: str | None = None):\n        \"\"\"\n        Delete a Chroma collection.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.delete_collection(name=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def search_embeddings(\n        self,\n        query_embeddings: list[list[float]],\n        top_k: int,\n        filters: dict[str, Any] | None = None,\n    ) -&gt; list[list[Document]]:\n        \"\"\"\n        Perform vector search on the stored documents using query embeddings.\n\n        Args:\n            query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n            top_k (int): The maximum number of documents to retrieve.\n            filters (dict[str, Any] | None): A dictionary of filters to apply to the search.\n                Defaults to None.\n\n        Returns:\n            list[list[Document]]: A list of lists containing documents that match the given filters,\n                for each query embedding provided.\n        \"\"\"\n        if filters is None:\n            results = self._collection.query(\n                query_embeddings=query_embeddings,\n                n_results=top_k,\n                include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n            )\n        else:\n            chroma_filters = self._normalize_filters(filters=filters)\n            results = self._collection.query(\n                query_embeddings=query_embeddings,\n                n_results=top_k,\n                where=chroma_filters[1],\n                where_document=chroma_filters[2],\n                include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n            )\n\n        return self._query_result_to_documents(results)\n\n    def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents that match the provided filters.\n\n        Filters can be defined in two formats:\n        1. Old format: Nested dictionaries with logical operators and comparison operators.\n        2. New format: Nested dictionaries of Comparison and Logic types.\n\n        For the new format:\n        Comparison dictionaries must contain the following keys:\n        - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years').\n        - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'.\n        - 'value': A single value or (for 'in' and 'not in') a list of values.\n\n        Logic dictionaries must contain the following keys:\n        - 'operator': One of 'NOT', 'OR', 'AND'.\n        - 'conditions': A list of Comparison or Logic dictionaries.\n\n        Example of new format:\n        {\n            \"operator\": \"AND\",\n            \"conditions\": [\n              {\n                \"field\": \"metadata.years\",\n                \"operator\": \"==\",\n                \"value\": \"2019\"\n              },\n              {\n                \"field\": \"metadata.companies\",\n                \"operator\": \"in\",\n                \"value\": [\"BMW\", \"Mercedes\"]\n              }\n            ]\n        }\n\n        Args:\n            filters (Dict[str, Any] | None): The filters to apply to the document list.\n            filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n                None.\n\n        Returns:\n            list[Document]: A list of Document instances that match the given filters.\n        \"\"\"\n        if filters:\n            ids, where, where_document = self._normalize_filters(filters)\n            kwargs: dict[str, Any] = {\"where\": where}\n\n            if ids:\n                kwargs[\"ids\"] = ids\n            if where_document:\n                kwargs[\"where_document\"] = where_document\n\n            result = self._collection.get(**kwargs)\n        else:\n            raise ValueError(\n                \"No filters provided. No documents will be retrieved with filters.\"\n            )\n\n        return self._get_result_to_documents(result)\n\n    def list_documents(self) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the collection.\n\n        Returns:\n            list[Document]: A list of Document instances representing all documents in the collection.\n        \"\"\"\n        result = self._collection.get()\n        return self._get_result_to_documents(result)\n\n    @staticmethod\n    def _normalize_filters(\n        filters: dict[str, Any]\n    ) -&gt; tuple[list[str], dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Translate filters to Chroma filters.\n\n        Args:\n            filters (Dict[str, Any]): The filters to normalize.\n\n        Returns:\n            Tuple[List[str], Dict[str, Any], Dict[str, Any]]: A tuple containing:\n                - A list of document IDs\n                - A dictionary of 'where' conditions\n                - A dictionary of 'where_document' conditions\n\n        Raises:\n            TypeError: If the 'filters' parameter is not a dictionary.\n            ValueError: If the filter structure is invalid or contains unsupported operators.\n        \"\"\"\n        if not isinstance(filters, dict):\n            raise TypeError(\"'filters' parameter must be a dictionary\")\n\n        # Check if it's the new format\n        if \"operator\" in filters or \"conditions\" in filters:\n            processed_filters = ChromaVectorStore._process_filter_node(filters)\n        else:\n            # It's the old format, use the old processing method\n            return ChromaVectorStore._process_old_filters(filters)\n\n        ids = []\n        where_document = {}\n\n        # Extract 'id' and 'content' filters if present\n        if \"metadata.id\" in processed_filters:\n            ids = processed_filters[\"metadata.id\"].get(\"$eq\", [])\n            del processed_filters[\"metadata.id\"]\n\n        if \"content\" in processed_filters:\n            where_document[\"$contains\"] = processed_filters[\"content\"].get(\"$eq\", \"\")\n            del processed_filters[\"content\"]\n\n        where = processed_filters\n\n        if \"$and\" in where and \"$or\" not in where:\n            and_conditions = where[\"$and\"]\n            if len(and_conditions) == 1:\n                where = and_conditions[0]\n        if \"$or\" in where and \"$and\" not in where:\n            or_conditions = where[\"$or\"]\n            if len(or_conditions) == 1:\n                where = or_conditions[0]\n\n        try:\n            if where:\n                from chromadb.api.types import validate_where\n\n                validate_where(where)\n            if where_document:\n                from chromadb.api.types import validate_where_document\n\n                validate_where_document(where_document)\n        except ValueError as e:\n            raise ValueError(e) from e\n\n        return ids, where, where_document\n\n    @staticmethod\n    def _process_old_filters(\n        filters: dict[str, Any]\n    ) -&gt; tuple[list[str], dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Process filters in the old format.\n        \"\"\"\n        ids = []\n        where = defaultdict(list)\n        where_document = defaultdict(list)\n        keys_to_remove = []\n\n        for field, value in filters.items():\n            if field == \"content\":\n                keys_to_remove.append(field)\n                where_document[\"$contains\"] = value\n            elif field == \"id\":\n                keys_to_remove.append(field)\n                ids.append(value)\n            elif isinstance(value, (list, tuple)):\n                keys_to_remove.append(field)\n                if len(value) == 0:\n                    continue\n                if len(value) == 1:\n                    where[field] = value[0]\n                    continue\n                for v in value:\n                    where[\"$or\"].append({field: v})\n\n        for k in keys_to_remove:\n            del filters[k]\n\n        final_where = dict(filters)\n        final_where.update(dict(where))\n\n        return ids, final_where, dict(where_document)\n\n    @staticmethod\n    def _process_filter_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a single node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a filter node.\n\n        Returns:\n            Dict[str, Any]: The processed filter node.\n\n        Raises:\n            ValueError: If the node structure is invalid.\n        \"\"\"\n        if \"operator\" in node and \"conditions\" in node:  # Logic node\n            return ChromaVectorStore._process_logic_node(node)\n        elif (\n            \"field\" in node and \"operator\" in node and \"value\" in node\n        ):  # Comparison node\n            return ChromaVectorStore._process_comparison_node(node)\n        else:\n            raise ValueError(\"Invalid filter node structure\")\n\n    @staticmethod\n    def _process_logic_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a logic node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a logic node.\n\n        Returns:\n            Dict[str, Any]: The processed logic node.\n        \"\"\"\n        operator = node[\"operator\"].lower()\n        conditions = [\n            ChromaVectorStore._process_filter_node(condition)\n            for condition in node[\"conditions\"]\n        ]\n        return {f\"${operator}\": conditions}\n\n    @staticmethod\n    def _process_comparison_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a comparison node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a comparison node.\n\n        Returns:\n            Dict[str, Any]: The processed comparison node.\n\n        Raises:\n            ValueError: If the operator is unsupported.\n        \"\"\"\n        field = node[\"field\"]\n        operator = node[\"operator\"]\n        value = node[\"value\"]\n\n        chroma_operator = CHROMA_OPERATOR_MAPPING.get(operator)\n\n        if chroma_operator is None:\n            raise ValueError(f\"Unsupported operator: {operator}\")\n\n        return {field: {chroma_operator: value}}\n\n    @staticmethod\n    def _query_result_to_documents(result: \"QueryResult\") -&gt; list[list[Document]]:\n        \"\"\"\n        Convert Chroma query results into Dynamiq Documents.\n\n        Args:\n            result (QueryResult): The result from a Chroma query operation.\n\n        Returns:\n            list[list[Document]]: A list of lists containing Document objects created from the\n                Chroma query result.\n        \"\"\"\n        return_value: list[list[Document]] = []\n        documents = result.get(\"documents\")\n        if documents is None:\n            return return_value\n\n        for i, answers in enumerate(documents):\n            converted_answers = []\n            for j in range(len(answers)):\n                document_dict: dict[str, Any] = {\n                    \"id\": result[\"ids\"][i][j],\n                    \"content\": documents[i][j],\n                }\n\n                if metadatas := result.get(\"metadatas\"):\n                    document_dict[\"metadata\"] = dict(metadatas[i][j])\n\n                if embeddings := result.get(\"embeddings\"):\n                    document_dict[\"embedding\"] = embeddings[i][j]\n\n                if distances := result.get(\"distances\"):\n                    document_dict[\"score\"] = distances[i][j]\n\n                converted_answers.append(Document(**document_dict))\n            return_value.append(converted_answers)\n\n        return return_value\n\n    @staticmethod\n    def _get_result_to_documents(result: \"QueryResult\") -&gt; list[Document]:\n        \"\"\"\n        Convert Chroma get result into Dynamiq Documents.\n\n        Args:\n            result (GetResult): The result from a Chroma get operation.\n\n        Returns:\n            list[Document]: A list containing Document objects created from the\n                Chroma get result.\n        \"\"\"\n        return_value: list[Document] = []\n        documents = result.get(\"documents\")\n        if documents is None:\n            return return_value\n\n        for i, content in enumerate(documents):\n            document_dict: dict[str, Any] = {\n                \"id\": result[\"ids\"][i],\n                \"content\": content,\n            }\n\n            if metadatas := result.get(\"metadatas\"):\n                document_dict[\"metadata\"] = dict(metadatas[i])\n\n            if embeddings := result.get(\"embeddings\"):\n                document_dict[\"embedding\"] = embeddings[i]\n\n            if distances := result.get(\"distances\"):\n                document_dict[\"score\"] = distances[i]\n\n            return_value.append(Document(**document_dict))\n        return return_value\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', create_if_not_exist=False, dry_run_config=None)</code>","text":"<p>Initialize the ChromaVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Chroma | None</code> <p>A Chroma connection object. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[ClientAPI]</code> <p>A Chroma client API instance. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>The name of the index or collection. Defaults to \"default\".</p> <code>'default'</code> <code>create_if_not_exist</code> <code>bool</code> <p>Whether to create the collection if it doesn't exist. Defaults to False.</p> <code>False</code> <code>dry_run_config</code> <code>DryRunConfig | None</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def __init__(\n    self,\n    connection: Chroma | None = None,\n    client: Optional[\"ClientAPI\"] = None,\n    index_name: str = \"default\",\n    create_if_not_exist: bool = False,\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize the ChromaVectorStore.\n\n    Args:\n        connection (Chroma | None): A Chroma connection object. Defaults to None.\n        client (Optional[ClientAPI]): A Chroma client API instance. Defaults to None.\n        index_name (str): The name of the index or collection. Defaults to \"default\".\n        create_if_not_exist (bool): Whether to create the collection if it doesn't exist. Defaults to False.\n        dry_run_config (DryRunConfig | None): Configuration for dry run mode. Defaults to None.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    self.client = client\n    if self.client is None:\n        connection = connection or Chroma()\n        self.client = connection.connect()\n    self.index_name = index_name\n    if create_if_not_exist:\n        collection_exists = self.client.get_collection(name=index_name)\n        if not collection_exists:\n            self._collection = self.client.create_collection(name=index_name)\n            self._track_collection(index_name)\n        else:\n            self._collection = self.client.get_collection(name=index_name)\n    else:\n        self._collection = self.client.get_collection(name=index_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Get the number of documents in the collection.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the collection.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Get the number of documents in the collection.\n\n    Returns:\n        int: The number of documents in the collection.\n    \"\"\"\n    return self._collection.count()\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete a Chroma collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None):\n    \"\"\"\n    Delete a Chroma collection.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.delete_collection(name=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the vector store based on their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>A list containing the IDs of documents to be deleted from the store.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>A flag to delete all documents from the store. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on their IDs.\n\n    Args:\n        document_ids (list[str]): A list containing the IDs of documents to be deleted from the store.\n        delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n    \"\"\"\n\n    if delete_all and self._collection is not None:\n        self.client.delete_collection(name=self.index_name)\n        self._collection = self.client.get_or_create_collection(\n            name=self.index_name\n        )\n    else:\n        if not document_ids:\n            logger.warning(\n                \"No document IDs provided. No documents will be deleted.\"\n            )\n        else:\n            self._collection.delete(ids=document_ids)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters=None)</code>","text":"<p>Delete documents from the vector store based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided filters.\n\n    Args:\n        filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n            None.\n    \"\"\"\n    if filters is None:\n        raise ValueError(\"No filters provided. No documents will be deleted.\")\n    else:\n        ids, where, where_document = self._normalize_filters(filters)\n        self._collection.delete(ids=ids, where=where, where_document=where_document)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Retrieve documents that match the provided filters.</p> <p>Filters can be defined in two formats: 1. Old format: Nested dictionaries with logical operators and comparison operators. 2. New format: Nested dictionaries of Comparison and Logic types.</p> <p>For the new format: Comparison dictionaries must contain the following keys: - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years'). - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'. - 'value': A single value or (for 'in' and 'not in') a list of values.</p> <p>Logic dictionaries must contain the following keys: - 'operator': One of 'NOT', 'OR', 'AND'. - 'conditions': A list of Comparison or Logic dictionaries.</p> <p>Example of new format: {     \"operator\": \"AND\",     \"conditions\": [       {         \"field\": \"metadata.years\",         \"operator\": \"==\",         \"value\": \"2019\"       },       {         \"field\": \"metadata.companies\",         \"operator\": \"in\",         \"value\": [\"BMW\", \"Mercedes\"]       }     ] }</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances that match the given filters.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n    \"\"\"\n    Retrieve documents that match the provided filters.\n\n    Filters can be defined in two formats:\n    1. Old format: Nested dictionaries with logical operators and comparison operators.\n    2. New format: Nested dictionaries of Comparison and Logic types.\n\n    For the new format:\n    Comparison dictionaries must contain the following keys:\n    - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years').\n    - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'.\n    - 'value': A single value or (for 'in' and 'not in') a list of values.\n\n    Logic dictionaries must contain the following keys:\n    - 'operator': One of 'NOT', 'OR', 'AND'.\n    - 'conditions': A list of Comparison or Logic dictionaries.\n\n    Example of new format:\n    {\n        \"operator\": \"AND\",\n        \"conditions\": [\n          {\n            \"field\": \"metadata.years\",\n            \"operator\": \"==\",\n            \"value\": \"2019\"\n          },\n          {\n            \"field\": \"metadata.companies\",\n            \"operator\": \"in\",\n            \"value\": [\"BMW\", \"Mercedes\"]\n          }\n        ]\n    }\n\n    Args:\n        filters (Dict[str, Any] | None): The filters to apply to the document list.\n        filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n            None.\n\n    Returns:\n        list[Document]: A list of Document instances that match the given filters.\n    \"\"\"\n    if filters:\n        ids, where, where_document = self._normalize_filters(filters)\n        kwargs: dict[str, Any] = {\"where\": where}\n\n        if ids:\n            kwargs[\"ids\"] = ids\n        if where_document:\n            kwargs[\"where_document\"] = where_document\n\n        result = self._collection.get(**kwargs)\n    else:\n        raise ValueError(\n            \"No filters provided. No documents will be retrieved with filters.\"\n        )\n\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.list_documents","title":"<code>list_documents()</code>","text":"<p>List all documents in the collection.</p> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances representing all documents in the collection.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def list_documents(self) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the collection.\n\n    Returns:\n        list[Document]: A list of Document instances representing all documents in the collection.\n    \"\"\"\n    result = self._collection.get()\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.search_embeddings","title":"<code>search_embeddings(query_embeddings, top_k, filters=None)</code>","text":"<p>Perform vector search on the stored documents using query embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query_embeddings</code> <code>list[list[float]]</code> <p>A list of embeddings to use as queries.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> required <code>filters</code> <code>dict[str, Any] | None</code> <p>A dictionary of filters to apply to the search. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Document]]</code> <p>list[list[Document]]: A list of lists containing documents that match the given filters, for each query embedding provided.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def search_embeddings(\n    self,\n    query_embeddings: list[list[float]],\n    top_k: int,\n    filters: dict[str, Any] | None = None,\n) -&gt; list[list[Document]]:\n    \"\"\"\n    Perform vector search on the stored documents using query embeddings.\n\n    Args:\n        query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n        top_k (int): The maximum number of documents to retrieve.\n        filters (dict[str, Any] | None): A dictionary of filters to apply to the search.\n            Defaults to None.\n\n    Returns:\n        list[list[Document]]: A list of lists containing documents that match the given filters,\n            for each query embedding provided.\n    \"\"\"\n    if filters is None:\n        results = self._collection.query(\n            query_embeddings=query_embeddings,\n            n_results=top_k,\n            include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n        )\n    else:\n        chroma_filters = self._normalize_filters(filters=filters)\n        results = self._collection.query(\n            query_embeddings=query_embeddings,\n            n_results=top_k,\n            where=chroma_filters[1],\n            where_document=chroma_filters[2],\n            include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n        )\n\n    return self._query_result_to_documents(results)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.write_documents","title":"<code>write_documents(documents)</code>","text":"<p>Write (or overwrite) documents into the store.</p> <p>This method processes a list of documents and writes them into the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>A list of Document objects to be written into the document store.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an item in the documents list is not an instance of the Document class.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents successfully written to the document store.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def write_documents(self, documents: list[Document]) -&gt; int:\n    \"\"\"\n    Write (or overwrite) documents into the store.\n\n    This method processes a list of documents and writes them into the vector store.\n\n    Args:\n        documents (list[Document]): A list of Document objects to be written into the document\n            store.\n\n    Raises:\n        ValueError: If an item in the documents list is not an instance of the Document class.\n\n    Returns:\n        int: The number of documents successfully written to the document store.\n    \"\"\"\n    for doc in documents:\n        if not isinstance(doc, Document):\n            msg = (\n                \"param 'documents' must contain a list of objects of type Document\"\n            )\n            raise ValueError(msg)\n\n        data = {\"ids\": [doc.id], \"documents\": [doc.content]}\n\n        if doc.metadata:\n            data[\"metadatas\"] = [doc.metadata]\n\n        if doc.embedding:\n            data[\"embeddings\"] = [doc.embedding]\n\n        self._collection.add(**data)\n        self._track_documents([doc.id])\n\n    return len(documents)\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/","title":"Elasticsearch","text":""},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchSimilarityMetric","title":"<code>ElasticsearchSimilarityMetric</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported similarity metrics for Elasticsearch.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>class ElasticsearchSimilarityMetric(str, Enum):\n    \"\"\"Supported similarity metrics for Elasticsearch.\"\"\"\n\n    COSINE = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    L2 = \"l2_norm\"\n    MAX_INNER_PRODUCT = \"max_inner_product\"\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore","title":"<code>ElasticsearchVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using Elasticsearch for dense vector search.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>class ElasticsearchVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"Vector store using Elasticsearch for dense vector search.\"\"\"\n\n    def __init__(\n        self,\n        connection: Elasticsearch | None = None,\n        client: Optional[\"ElasticsearchClient\"] | None = None,\n        index_name: str = \"default\",\n        dimension: int = 1536,\n        similarity: ElasticsearchSimilarityMetric = ElasticsearchSimilarityMetric.COSINE,\n        create_if_not_exist: bool = False,\n        content_key: str = \"content\",\n        embedding_key: str = \"embedding\",\n        batch_size: int = 100,\n        index_settings: dict | None = None,\n        mapping_settings: dict | None = None,\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize ElasticsearchVectorStore.\n\n        Args:\n            connection (Optional[Elasticsearch]): Elasticsearch connection. Defaults to None.\n            client (Optional[ElasticsearchClient]): Elasticsearch client. Defaults to None.\n            index_name (str): Name of the index. Defaults to \"default\".\n            dimension (int): Dimension of vectors. Defaults to 1536.\n            similarity (ElasticsearchSimilarityMetric): Similarity metric.\n                        Defaults to ElasticsearchSimilarityMetric.COSINE.\n            create_if_not_exist (bool): Whether to create the index if it does not exist. Defaults to False.\n            content_key (str): Key for content field. Defaults to \"content\".\n            embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n            batch_size (int): Batch size for write operations. Defaults to 100.\n            index_settings (Optional[dict]): Custom index settings. Defaults to None.\n            mapping_settings (Optional[dict]): Custom mapping settings. Defaults to None.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        if client is None:\n            if connection is None:\n                connection = Elasticsearch()\n            self.client = connection.connect()\n        else:\n            self.client = client\n\n        self.index_name = index_name\n        self.dimension = dimension\n        self.similarity = similarity\n        self.content_key = content_key\n        self.embedding_key = embedding_key\n        self.batch_size = batch_size\n        self.index_settings = index_settings or {}\n        self.mapping_settings = mapping_settings or {}\n\n        if not self.client.indices.exists(index=self.index_name):\n            if create_if_not_exist:\n                logger.info(f\"Index {self.index_name} does not exist. Creating a new index.\")\n                self._create_index_if_not_exists()\n                self._track_collection(self.index_name)\n            else:\n                raise ValueError(\n                    f\"Index {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n                )\n        else:\n            logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n        logger.debug(f\"ElasticsearchVectorStore initialized with index: {self.index_name}\")\n\n    def _create_index_if_not_exists(self) -&gt; None:\n        \"\"\"Create the index if it doesn't exist.\"\"\"\n        # Base mapping\n        mapping = {\n            \"mappings\": {\n                \"properties\": {\n                    self.content_key: {\"type\": \"text\"},\n                    \"metadata\": {\"type\": \"object\"},\n                    self.embedding_key: {\n                        \"type\": \"dense_vector\",\n                        \"dims\": self.dimension,\n                        \"index\": True,\n                        \"similarity\": self.similarity,\n                    },\n                }\n            }\n        }\n\n        # Add custom mapping settings if provided\n        if self.mapping_settings:\n            mapping[\"mappings\"].update(self.mapping_settings)\n\n        # Add index settings if provided\n        if self.index_settings:\n            mapping[\"settings\"] = self.index_settings\n\n        self.client.indices.create(index=self.index_name, body=mapping)\n\n    def delete_collection(self, collection_name: str | None = None) -&gt; None:\n        \"\"\"\n        Delete the collection in the database.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.indices.delete(index=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def _handle_duplicate_documents(\n        self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.FAIL\n    ) -&gt; list[Document]:\n        \"\"\"\n        Handle duplicate documents based on policy.\n\n        Args:\n            documents (list[Document]): List of documents to check for duplicates.\n            policy (DuplicatePolicy): Policy for handling duplicates. Defaults to DuplicatePolicy.FAIL.\n\n        Returns:\n            list[Document]: List of documents after applying the specified policy.\n\n        Raises:\n            VectorStoreException: If duplicates are found and the policy is set to FAIL.\n        \"\"\"\n        if policy == DuplicatePolicy.OVERWRITE:\n            return documents\n\n        # Get unique documents\n        unique_docs = {}\n        for doc in documents:\n            if doc.id in unique_docs:\n                logger.warning(f\"Duplicate document ID found: {doc.id}\")\n            unique_docs[doc.id] = doc\n\n        if policy in {DuplicatePolicy.SKIP, DuplicatePolicy.FAIL}:\n            # Check which documents already exist\n            existing_ids = set()\n            for doc_id in unique_docs.keys():\n                try:\n                    self.retrieve_document_by_file_id(file_id=doc_id)\n                    existing_ids.add(doc_id)\n                except NotFoundError:\n                    pass\n\n            if policy == DuplicatePolicy.FAIL and existing_ids:\n                raise VectorStoreException(f\"Documents with IDs {existing_ids} already exist\")\n\n            # Remove existing documents\n            return [doc for doc in documents if doc.id not in existing_ids]\n\n        return list(unique_docs.values())\n\n    def write_documents(\n        self,\n        documents: list[Document],\n        policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n        batch_size: int | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; int:\n        \"\"\"\n        Write documents to Elasticsearch.\n\n        Args:\n            documents (list[Document]): List of documents to write.\n            policy (DuplicatePolicy): Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.\n            batch_size (Optional[int]): Size of batches for bulk operations. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage. Defaults to None.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            int: Number of documents successfully written.\n\n        Raises:\n            ValueError: If the provided documents are invalid.\n            VectorStoreException: If duplicates are found when using the FAIL policy.\n        \"\"\"\n        if not documents:\n            return 0\n\n        if not isinstance(documents[0], Document):\n            raise ValueError(\"Documents must be of type Document\")\n\n        # Handle duplicates\n        documents = self._handle_duplicate_documents(documents, policy)\n        if not documents:\n            return 0\n\n        # Process in batches\n        batch_size = batch_size or self.batch_size\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n        total_written = 0\n\n        for i in range(0, len(documents), batch_size):\n            batch = documents[i : i + batch_size]\n            operations = []\n            for doc in batch:\n                operations.extend(\n                    [\n                        {\"index\": {\"_index\": self.index_name, \"_id\": doc.id}},\n                        {\n                            content_key: doc.content,\n                            \"metadata\": doc.metadata,\n                            embedding_key: doc.embedding,\n                        },\n                    ]\n                )\n                self._track_documents([doc.id for doc in batch])\n\n            if operations:\n                result = self.client.bulk(operations=operations, refresh=True)\n                total_written += sum(\n                    item.get(\"index\", {}).get(\"_shards\", {}).get(\"successful\", 0)\n                    for item in result.raw.get(\"items\", [])\n                )\n\n        return total_written\n\n    def _scale_score(self, score: float, similarity: ElasticsearchSimilarityMetric) -&gt; float:\n        \"\"\"\n        Scale the score based on the similarity metric.\n\n        Args:\n            score (float): Raw score from Elasticsearch.\n            similarity (ElasticsearchSimilarityMetric): Similarity metric used.\n\n        Returns:\n            float: Scaled score between 0 and 1, depending on the similarity metric used.\n        \"\"\"\n        if similarity == ElasticsearchSimilarityMetric.COSINE:\n            # Elasticsearch cosine scores are between -1 and 1\n            return (score + 1) / 2\n        elif similarity == ElasticsearchSimilarityMetric.DOT_PRODUCT:\n            # Normalize dot product using sigmoid\n            return float(1 / (1 + np.exp(-score / 100)))\n        else:  # L2\n            # L2 distance is inverse - smaller is better\n            # Convert to similarity score\n            return 1 / (1 + score)\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        top_k: int = 10,\n        num_candidates: int = 500,\n        exclude_document_embeddings: bool = True,\n        filters: dict[str, Any] | None = None,\n        scale_scores: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents by vector similarity.\n\n        Args:\n            query_embedding (List[float]): Query vector.\n            top_k (int): Number of results. Defaults to 10.\n            num_candidates (int): Number of candidates to consider in the retriever. Defaults to 500.\n            exclude_document_embeddings (bool): Exclude embeddings in response. Defaults to True.\n            filters (dict[str, Any] | None): Metadata filters. Defaults to None.\n            scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n        Returns:\n            List[Document]: Retrieved documents.\n\n        Raises:\n            ValueError: If query_embedding is invalid.\n        \"\"\"\n        if not query_embedding:\n            raise ValueError(\"query_embedding must not be empty\")\n\n        embedding_key = embedding_key or self.embedding_key\n        # Build the query\n        query = {\n            \"knn\": {\n                \"field\": embedding_key,\n                \"query_vector\": query_embedding,\n                \"k\": top_k,\n                \"num_candidates\": num_candidates,\n            }\n        }\n\n        if filters:\n            # Normalise filters to Elasticsearch format\n            filters = _normalize_filters(filters)\n            query[\"knn\"][\"filter\"] = {\"bool\": filters}\n\n        # Execute search\n        response = self.client.search(\n            index=self.index_name,\n            query=query,\n            size=top_k,\n            _source_excludes=([embedding_key] if exclude_document_embeddings else None),\n        )\n        content_key = content_key or self.content_key\n\n        # Convert results to Documents with optional score scaling\n        documents = []\n        for hit in response[\"hits\"][\"hits\"]:\n            score = hit[\"_score\"]\n            if scale_scores:\n                score = self._scale_score(score, self.similarity)\n\n            if content_key not in hit[\"_source\"]:\n                continue\n\n            doc = Document(\n                id=hit[\"_id\"],\n                content=hit[\"_source\"][content_key],\n                metadata=hit[\"_source\"][\"metadata\"],\n                score=score,\n            )\n            if not exclude_document_embeddings:\n                doc.embedding = hit[\"_source\"][embedding_key]\n            documents.append(doc)\n\n        return documents\n\n    def retrieve_document_by_file_id(\n        self,\n        file_id: str,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ):\n        embedding_key = embedding_key or self.embedding_key\n        content_key = content_key or self.content_key\n\n        response = self.client.get(\n            index=self.index_name,\n            id=file_id,\n            _source_excludes=([embedding_key] if not include_embeddings else None),\n        )\n\n        # Convert result to Document\n        doc = Document(\n            id=response[\"_id\"],\n            content=response[\"_source\"][content_key],\n            metadata=response[\"_source\"][\"metadata\"],\n        )\n        if include_embeddings:\n            doc.embedding = response[\"_source\"][embedding_key]\n\n        return doc\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the store.\n\n        Args:\n            document_ids (Optional[List[str]]): IDs to delete. Defaults to None.\n            delete_all (bool): Delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all:\n            self.client.delete_by_query(index=self.index_name, query={\"match_all\": {}}, refresh=True)\n        elif document_ids:\n            operations = []\n            for doc_id in document_ids:\n                operations.append({\"delete\": {\"_index\": self.index_name, \"_id\": doc_id}})\n            if operations:\n                self.client.bulk(operations=operations, refresh=True)\n        else:\n            logger.warning(\"No document IDs provided. No documents will be deleted.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"Delete documents matching filters.\n\n        Args:\n            filters (dict[str, Any]): Metadata filters.\n        \"\"\"\n        if not filters:\n            logger.warning(\"No filters provided. No documents will be deleted.\")\n            return\n\n        filters = _normalize_filters(filters)\n        bool_query = {\"bool\": filters}\n\n        self.client.delete_by_query(index=self.index_name, query=bool_query, refresh=True)\n\n    def list_documents(\n        self,\n        top_k: int | None = 100,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        scale_scores: bool = False,\n    ) -&gt; list[Document]:\n        \"\"\"\n        List documents in the Pinecone vector store.\n\n        Args:\n            top_k (Optional[int]): Maximal number of documents to retrieve. Defaults to 100.\n            include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n            scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n\n        Returns:\n            list[Document]: List of Document objects retrieved.\n        \"\"\"\n        embedding_key = embedding_key or self.embedding_key\n        content_key = content_key or self.content_key\n\n        response = self.client.search(\n            index=self.index_name,\n            query={\"match_all\": {}},\n            size=top_k,\n            _source_excludes=([embedding_key] if not include_embeddings else None),\n        )\n\n        # Convert results to Documents with optional score scaling\n        all_documents = []\n        for hit in response[\"hits\"][\"hits\"]:\n            score = hit[\"_score\"]\n            if scale_scores:\n                score = self._scale_score(score, self.similarity)\n\n            if content_key not in hit[\"_source\"]:\n                continue\n\n            doc = Document(\n                id=hit[\"_id\"],\n                content=hit[\"_source\"][content_key],\n                metadata=hit[\"_source\"][\"metadata\"],\n                score=score,\n            )\n            if include_embeddings:\n                doc.embedding = hit[\"_source\"][embedding_key]\n            all_documents.append(doc)\n\n        return all_documents\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the store.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        response = self.client.count(\n            index=self.index_name,\n            query={\"match_all\": {}},\n        )\n        return response.get(\"count\", 0)\n\n    def get_field_statistics(self, field: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Get statistics for a numeric field.\n\n        Args:\n            field (str): Full field name (must be numeric)\n\n        Returns:\n            Dictionary with min, max, avg, sum\n        \"\"\"\n        response = self.client.search(\n            index=self.index_name,\n            body={\"size\": 0, \"aggs\": {\"stats\": {\"stats\": {\"field\": field}}}},\n        )\n        return response[\"aggregations\"][\"stats\"]\n\n    def update_document_by_file_id(\n        self,\n        file_id: str,\n        content: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        embedding: list[float] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"Update an existing document.\n\n        Args:\n            file_id (str): Document ID\n            content (Optional[str]): Update content\n            metadata (Optional[dict[str, Any]]): Update field metadata or add new fields\n            embedding (Optional[list[float]]): New embedding vector\n            content_key (Optional[str]): Key for content field.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n        \"\"\"\n        update_fields = {}\n        if content is not None:\n            update_fields[content_key or self.content_key] = content\n        if metadata is not None:\n            update_fields[\"metadata\"] = metadata\n        if embedding is not None:\n            update_fields[embedding_key or self.embedding_key] = embedding\n\n        if update_fields:\n            self.client.update(index=self.index_name, id=file_id, body={\"doc\": update_fields}, refresh=True)\n\n    def update_documents_batch(\n        self,\n        documents: list[Document],\n        batch_size: int | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; int:\n        \"\"\"\n        Update multiple documents in batches.\n\n        Args:\n            documents (list[Document]): List of documents to update.\n            batch_size (Optional[int]): Size of batches for bulk operations.\n            content_key (Optional[str]): Key for content field.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n        Returns:\n            int: Number of documents successfully updated.\n\n        \"\"\"\n        batch_size = batch_size or self.batch_size\n        total_updated = 0\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        def generate_actions(docs, content_key, embedding_key):\n            for i, doc in enumerate(docs):\n                docs[i] = {\n                    \"_op_type\": \"update\",\n                    \"_index\": \"documents\",\n                    \"_id\": doc.id,\n                    \"doc\": {\n                        content_key: doc.content,\n                        \"metadata\": doc.metadata,\n                        embedding_key: doc.embedding,\n                    },\n                }\n            return docs\n\n        for i in range(0, len(documents), batch_size):\n            sub_set_docs = documents[i : i + batch_size]\n            batch = generate_actions(sub_set_docs, content_key, embedding_key)\n            success, failed = bulk(self.client, batch, refresh=True, raise_on_error=True)\n            total_updated += success\n        return total_updated\n\n    def create_alias(\n        self,\n        alias_name: str,\n        index_names: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Create an alias for one or more indices.\n\n        Args:\n            alias_name (str): Name of the alias.\n            index_names (Optional[list[str]]): List of indices to include in the alias. Defaults to None.\n        \"\"\"\n        index_names = index_names or [self.index_name]\n        actions = []\n        for index in index_names:\n            actions.append({\"add\": {\"index\": index, \"alias\": alias_name}})\n        self.client.indices.update_aliases({\"actions\": actions})\n\n    def close(self) -&gt; None:\n        \"\"\"Close the client connection.\"\"\"\n        if hasattr(self, \"client\"):\n            self.client.close()\n\n    def __del__(self):\n        \"\"\"Cleanup on deletion.\"\"\"\n        self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup on deletion.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup on deletion.\"\"\"\n    self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', dimension=1536, similarity=ElasticsearchSimilarityMetric.COSINE, create_if_not_exist=False, content_key='content', embedding_key='embedding', batch_size=100, index_settings=None, mapping_settings=None, dry_run_config=None)</code>","text":"<p>Initialize ElasticsearchVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Elasticsearch]</code> <p>Elasticsearch connection. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[Elasticsearch]</code> <p>Elasticsearch client. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> <code>'default'</code> <code>dimension</code> <code>int</code> <p>Dimension of vectors. Defaults to 1536.</p> <code>1536</code> <code>similarity</code> <code>ElasticsearchSimilarityMetric</code> <p>Similarity metric.         Defaults to ElasticsearchSimilarityMetric.COSINE.</p> <code>COSINE</code> <code>create_if_not_exist</code> <code>bool</code> <p>Whether to create the index if it does not exist. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>str</code> <p>Key for content field. Defaults to \"content\".</p> <code>'content'</code> <code>embedding_key</code> <code>str</code> <p>Key for embedding field. Defaults to \"embedding\".</p> <code>'embedding'</code> <code>batch_size</code> <code>int</code> <p>Batch size for write operations. Defaults to 100.</p> <code>100</code> <code>index_settings</code> <code>Optional[dict]</code> <p>Custom index settings. Defaults to None.</p> <code>None</code> <code>mapping_settings</code> <code>Optional[dict]</code> <p>Custom mapping settings. Defaults to None.</p> <code>None</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def __init__(\n    self,\n    connection: Elasticsearch | None = None,\n    client: Optional[\"ElasticsearchClient\"] | None = None,\n    index_name: str = \"default\",\n    dimension: int = 1536,\n    similarity: ElasticsearchSimilarityMetric = ElasticsearchSimilarityMetric.COSINE,\n    create_if_not_exist: bool = False,\n    content_key: str = \"content\",\n    embedding_key: str = \"embedding\",\n    batch_size: int = 100,\n    index_settings: dict | None = None,\n    mapping_settings: dict | None = None,\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize ElasticsearchVectorStore.\n\n    Args:\n        connection (Optional[Elasticsearch]): Elasticsearch connection. Defaults to None.\n        client (Optional[ElasticsearchClient]): Elasticsearch client. Defaults to None.\n        index_name (str): Name of the index. Defaults to \"default\".\n        dimension (int): Dimension of vectors. Defaults to 1536.\n        similarity (ElasticsearchSimilarityMetric): Similarity metric.\n                    Defaults to ElasticsearchSimilarityMetric.COSINE.\n        create_if_not_exist (bool): Whether to create the index if it does not exist. Defaults to False.\n        content_key (str): Key for content field. Defaults to \"content\".\n        embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n        batch_size (int): Batch size for write operations. Defaults to 100.\n        index_settings (Optional[dict]): Custom index settings. Defaults to None.\n        mapping_settings (Optional[dict]): Custom mapping settings. Defaults to None.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    if client is None:\n        if connection is None:\n            connection = Elasticsearch()\n        self.client = connection.connect()\n    else:\n        self.client = client\n\n    self.index_name = index_name\n    self.dimension = dimension\n    self.similarity = similarity\n    self.content_key = content_key\n    self.embedding_key = embedding_key\n    self.batch_size = batch_size\n    self.index_settings = index_settings or {}\n    self.mapping_settings = mapping_settings or {}\n\n    if not self.client.indices.exists(index=self.index_name):\n        if create_if_not_exist:\n            logger.info(f\"Index {self.index_name} does not exist. Creating a new index.\")\n            self._create_index_if_not_exists()\n            self._track_collection(self.index_name)\n        else:\n            raise ValueError(\n                f\"Index {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n            )\n    else:\n        logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n    logger.debug(f\"ElasticsearchVectorStore initialized with index: {self.index_name}\")\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.close","title":"<code>close()</code>","text":"<p>Close the client connection.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the client connection.\"\"\"\n    if hasattr(self, \"client\"):\n        self.client.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the store.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the store.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    response = self.client.count(\n        index=self.index_name,\n        query={\"match_all\": {}},\n    )\n    return response.get(\"count\", 0)\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.create_alias","title":"<code>create_alias(alias_name, index_names=None)</code>","text":"<p>Create an alias for one or more indices.</p> <p>Parameters:</p> Name Type Description Default <code>alias_name</code> <code>str</code> <p>Name of the alias.</p> required <code>index_names</code> <code>Optional[list[str]]</code> <p>List of indices to include in the alias. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def create_alias(\n    self,\n    alias_name: str,\n    index_names: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Create an alias for one or more indices.\n\n    Args:\n        alias_name (str): Name of the alias.\n        index_names (Optional[list[str]]): List of indices to include in the alias. Defaults to None.\n    \"\"\"\n    index_names = index_names or [self.index_name]\n    actions = []\n    for index in index_names:\n        actions.append({\"add\": {\"index\": index, \"alias\": alias_name}})\n    self.client.indices.update_aliases({\"actions\": actions})\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete the collection in the database.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None) -&gt; None:\n    \"\"\"\n    Delete the collection in the database.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.indices.delete(index=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>Optional[List[str]]</code> <p>IDs to delete. Defaults to None.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>Delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the store.\n\n    Args:\n        document_ids (Optional[List[str]]): IDs to delete. Defaults to None.\n        delete_all (bool): Delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all:\n        self.client.delete_by_query(index=self.index_name, query={\"match_all\": {}}, refresh=True)\n    elif document_ids:\n        operations = []\n        for doc_id in document_ids:\n            operations.append({\"delete\": {\"_index\": self.index_name, \"_id\": doc_id}})\n        if operations:\n            self.client.bulk(operations=operations, refresh=True)\n    else:\n        logger.warning(\"No document IDs provided. No documents will be deleted.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents matching filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Metadata filters.</p> required Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"Delete documents matching filters.\n\n    Args:\n        filters (dict[str, Any]): Metadata filters.\n    \"\"\"\n    if not filters:\n        logger.warning(\"No filters provided. No documents will be deleted.\")\n        return\n\n    filters = _normalize_filters(filters)\n    bool_query = {\"bool\": filters}\n\n    self.client.delete_by_query(index=self.index_name, query=bool_query, refresh=True)\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.get_field_statistics","title":"<code>get_field_statistics(field)</code>","text":"<p>Get statistics for a numeric field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Full field name (must be numeric)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with min, max, avg, sum</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def get_field_statistics(self, field: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Get statistics for a numeric field.\n\n    Args:\n        field (str): Full field name (must be numeric)\n\n    Returns:\n        Dictionary with min, max, avg, sum\n    \"\"\"\n    response = self.client.search(\n        index=self.index_name,\n        body={\"size\": 0, \"aggs\": {\"stats\": {\"stats\": {\"field\": field}}}},\n    )\n    return response[\"aggregations\"][\"stats\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.list_documents","title":"<code>list_documents(top_k=100, include_embeddings=False, content_key=None, embedding_key=None, scale_scores=False)</code>","text":"<p>List documents in the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>Optional[int]</code> <p>Maximal number of documents to retrieve. Defaults to 100.</p> <code>100</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the results. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to 0-1 range. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of Document objects retrieved.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def list_documents(\n    self,\n    top_k: int | None = 100,\n    include_embeddings: bool = False,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    scale_scores: bool = False,\n) -&gt; list[Document]:\n    \"\"\"\n    List documents in the Pinecone vector store.\n\n    Args:\n        top_k (Optional[int]): Maximal number of documents to retrieve. Defaults to 100.\n        include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n        scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n\n    Returns:\n        list[Document]: List of Document objects retrieved.\n    \"\"\"\n    embedding_key = embedding_key or self.embedding_key\n    content_key = content_key or self.content_key\n\n    response = self.client.search(\n        index=self.index_name,\n        query={\"match_all\": {}},\n        size=top_k,\n        _source_excludes=([embedding_key] if not include_embeddings else None),\n    )\n\n    # Convert results to Documents with optional score scaling\n    all_documents = []\n    for hit in response[\"hits\"][\"hits\"]:\n        score = hit[\"_score\"]\n        if scale_scores:\n            score = self._scale_score(score, self.similarity)\n\n        if content_key not in hit[\"_source\"]:\n            continue\n\n        doc = Document(\n            id=hit[\"_id\"],\n            content=hit[\"_source\"][content_key],\n            metadata=hit[\"_source\"][\"metadata\"],\n            score=score,\n        )\n        if include_embeddings:\n            doc.embedding = hit[\"_source\"][embedding_key]\n        all_documents.append(doc)\n\n    return all_documents\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.update_document_by_file_id","title":"<code>update_document_by_file_id(file_id, content=None, metadata=None, embedding=None, content_key=None, embedding_key=None)</code>","text":"<p>Update an existing document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>Document ID</p> required <code>content</code> <code>Optional[str]</code> <p>Update content</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Update field metadata or add new fields</p> <code>None</code> <code>embedding</code> <code>Optional[list[float]]</code> <p>New embedding vector</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Key for content field.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def update_document_by_file_id(\n    self,\n    file_id: str,\n    content: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    embedding: list[float] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; None:\n    \"\"\"Update an existing document.\n\n    Args:\n        file_id (str): Document ID\n        content (Optional[str]): Update content\n        metadata (Optional[dict[str, Any]]): Update field metadata or add new fields\n        embedding (Optional[list[float]]): New embedding vector\n        content_key (Optional[str]): Key for content field.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n    \"\"\"\n    update_fields = {}\n    if content is not None:\n        update_fields[content_key or self.content_key] = content\n    if metadata is not None:\n        update_fields[\"metadata\"] = metadata\n    if embedding is not None:\n        update_fields[embedding_key or self.embedding_key] = embedding\n\n    if update_fields:\n        self.client.update(index=self.index_name, id=file_id, body={\"doc\": update_fields}, refresh=True)\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.update_documents_batch","title":"<code>update_documents_batch(documents, batch_size=None, content_key=None, embedding_key=None)</code>","text":"<p>Update multiple documents in batches.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of documents to update.</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Size of batches for bulk operations.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Key for content field.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully updated.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def update_documents_batch(\n    self,\n    documents: list[Document],\n    batch_size: int | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Update multiple documents in batches.\n\n    Args:\n        documents (list[Document]): List of documents to update.\n        batch_size (Optional[int]): Size of batches for bulk operations.\n        content_key (Optional[str]): Key for content field.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n    Returns:\n        int: Number of documents successfully updated.\n\n    \"\"\"\n    batch_size = batch_size or self.batch_size\n    total_updated = 0\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    def generate_actions(docs, content_key, embedding_key):\n        for i, doc in enumerate(docs):\n            docs[i] = {\n                \"_op_type\": \"update\",\n                \"_index\": \"documents\",\n                \"_id\": doc.id,\n                \"doc\": {\n                    content_key: doc.content,\n                    \"metadata\": doc.metadata,\n                    embedding_key: doc.embedding,\n                },\n            }\n        return docs\n\n    for i in range(0, len(documents), batch_size):\n        sub_set_docs = documents[i : i + batch_size]\n        batch = generate_actions(sub_set_docs, content_key, embedding_key)\n        success, failed = bulk(self.client, batch, refresh=True, raise_on_error=True)\n        total_updated += success\n    return total_updated\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.FAIL, batch_size=None, content_key=None, embedding_key=None)</code>","text":"<p>Write documents to Elasticsearch.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of documents to write.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.</p> <code>FAIL</code> <code>batch_size</code> <code>Optional[int]</code> <p>Size of batches for bulk operations. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided documents are invalid.</p> <code>VectorStoreException</code> <p>If duplicates are found when using the FAIL policy.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>def write_documents(\n    self,\n    documents: list[Document],\n    policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n    batch_size: int | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Write documents to Elasticsearch.\n\n    Args:\n        documents (list[Document]): List of documents to write.\n        policy (DuplicatePolicy): Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.\n        batch_size (Optional[int]): Size of batches for bulk operations. Defaults to None.\n        content_key (Optional[str]): The field used to store content in the storage. Defaults to None.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage. Defaults to None.\n\n    Returns:\n        int: Number of documents successfully written.\n\n    Raises:\n        ValueError: If the provided documents are invalid.\n        VectorStoreException: If duplicates are found when using the FAIL policy.\n    \"\"\"\n    if not documents:\n        return 0\n\n    if not isinstance(documents[0], Document):\n        raise ValueError(\"Documents must be of type Document\")\n\n    # Handle duplicates\n    documents = self._handle_duplicate_documents(documents, policy)\n    if not documents:\n        return 0\n\n    # Process in batches\n    batch_size = batch_size or self.batch_size\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n    total_written = 0\n\n    for i in range(0, len(documents), batch_size):\n        batch = documents[i : i + batch_size]\n        operations = []\n        for doc in batch:\n            operations.extend(\n                [\n                    {\"index\": {\"_index\": self.index_name, \"_id\": doc.id}},\n                    {\n                        content_key: doc.content,\n                        \"metadata\": doc.metadata,\n                        embedding_key: doc.embedding,\n                    },\n                ]\n            )\n            self._track_documents([doc.id for doc in batch])\n\n        if operations:\n            result = self.client.bulk(operations=operations, refresh=True)\n            total_written += sum(\n                item.get(\"index\", {}).get(\"_shards\", {}).get(\"successful\", 0)\n                for item in result.raw.get(\"items\", [])\n            )\n\n    return total_written\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStoreParams","title":"<code>ElasticsearchVectorStoreParams</code>","text":"<p>               Bases: <code>BaseVectorStoreParams</code></p> <p>Parameters for Elasticsearch vector store.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> <code>content_key</code> <code>str</code> <p>Key for content field. Defaults to \"content\".</p> <code>dimension</code> <code>int</code> <p>Dimension of the vectors. Defaults to 1536.</p> <code>similarity</code> <code>str</code> <p>Similarity metric to use. Defaults to \"cosine\".</p> <code>embedding_key</code> <code>str</code> <p>Key for embedding field. Defaults to \"embedding\".</p> <code>batch_size</code> <code>int</code> <p>Batch size for writing operations. Defaults to 100.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>class ElasticsearchVectorStoreParams(BaseVectorStoreParams):\n    \"\"\"Parameters for Elasticsearch vector store.\n\n    Attributes:\n        index_name (str): Name of the index. Defaults to \"default\".\n        content_key (str): Key for content field. Defaults to \"content\".\n        dimension (int): Dimension of the vectors. Defaults to 1536.\n        similarity (str): Similarity metric to use. Defaults to \"cosine\".\n        embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n        batch_size (int): Batch size for writing operations. Defaults to 100.\n    \"\"\"\n    similarity: ElasticsearchSimilarityMetric = ElasticsearchSimilarityMetric.COSINE\n    embedding_key: str = \"embedding\"\n    batch_size: int = 100\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/elasticsearch/#dynamiq.storages.vector.elasticsearch.elasticsearch.ElasticsearchVectorStoreWriterParams","title":"<code>ElasticsearchVectorStoreWriterParams</code>","text":"<p>               Bases: <code>ElasticsearchVectorStoreParams</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Parameters for Elasticsearch vector store writer.</p> Source code in <code>dynamiq/storages/vector/elasticsearch/elasticsearch.py</code> <pre><code>class ElasticsearchVectorStoreWriterParams(ElasticsearchVectorStoreParams, BaseWriterVectorStoreParams):\n    \"\"\"Parameters for Elasticsearch vector store writer.\"\"\"\n    dimension: int = 1536\n</code></pre>"},{"location":"dynamiq/storages/vector/elasticsearch/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/milvus/filter/","title":"Filter","text":""},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter","title":"<code>Filter</code>","text":"Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>class Filter:\n    LOGICAL_OPERATORS = {\"AND\": \" and \", \"OR\": \" or \", \"NOT\": \"not \"}\n    COMPARISON_OPERATORS = {\n        \"==\": \"==\",\n        \"!=\": \"!=\",\n        \"&gt;\": \"&gt;\",\n        \"&gt;=\": \"&gt;=\",\n        \"&lt;\": \"&lt;\",\n        \"&lt;=\": \"&lt;=\",\n        \"in\": \"in\",\n        \"not in\": \"not in\",\n    }\n\n    def __init__(self, filter_criteria: dict[str, Any]):\n        \"\"\"\n        Initializes the Filter object with filter criteria.\n\n        Args:\n            filter_criteria (Dict[str, Any]): The filters to apply.\n        \"\"\"\n        self.filter_criteria = filter_criteria\n\n    def build_filter_expression(self) -&gt; str:\n        \"\"\"\n        Builds the filter expression string from the filter criteria.\n\n        Returns:\n            str: The constructed filter expression string compatible with the database.\n        \"\"\"\n        return self._parse_filter(self.filter_criteria)\n\n    def _parse_filter(self, filter_term: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Recursively parse the filter criteria to build a Milvus-compatible filter expression.\n\n        Args:\n            filter_term (Dict[str, Any]): The filter dictionary to parse.\n\n        Returns:\n            str: A Milvus-compatible filter expression.\n        \"\"\"\n        # Handle logical operators with nested conditions\n        if \"operator\" in filter_term and \"conditions\" in filter_term:\n            operator = filter_term[\"operator\"]\n            if operator not in self.LOGICAL_OPERATORS:\n                raise ValueError(f\"Unsupported logical operator: {operator}\")\n\n            # Process each condition recursively\n            sub_expressions = [self._parse_filter(cond) for cond in filter_term[\"conditions\"]]\n            return f\"({self.LOGICAL_OPERATORS[operator].join(sub_expressions)})\"\n\n        # Handle comparison conditions\n        elif \"field\" in filter_term and \"operator\" in filter_term and \"value\" in filter_term:\n            field = filter_term[\"field\"]\n            operator = filter_term[\"operator\"]\n            value = filter_term[\"value\"]\n\n            # Build comparison expression\n            return self._build_comparison_expression(field, operator, value)\n\n        else:\n            raise ValueError(\"Invalid filter structure\")\n\n    def _build_comparison_expression(self, field: str, operator: str, value: Any) -&gt; str:\n        \"\"\"\n        Constructs a comparison expression based on field, operator, and value.\n\n        Args:\n            field (str): The field to filter on.\n            operator (str): The comparison operator.\n            value (Any): The value to compare against.\n\n        Returns:\n            str: A Milvus-compatible comparison expression.\n        \"\"\"\n        if operator not in self.COMPARISON_OPERATORS:\n            raise ValueError(f\"Unsupported comparison operator: {operator}\")\n\n        if operator == \"in\" and isinstance(value, list):\n            return f\"{field} in {value}\"\n        elif operator == \"not in\" and isinstance(value, list):\n            return f\"{field} not in {value}\"\n        elif isinstance(value, str):\n            return f'{field} {self.COMPARISON_OPERATORS[operator]} \"{value}\"'\n        else:\n            return f\"{field} {self.COMPARISON_OPERATORS[operator]} {value}\"\n\n    @staticmethod\n    def from_dict(filter_dict: dict[str, Any]) -&gt; \"Filter\":\n        \"\"\"\n        Creates a Filter instance from a dictionary.\n\n        Args:\n            filter_dict (Dict[str, Any]): Dictionary defining filter criteria.\n\n        Returns:\n            Filter: The Filter instance.\n        \"\"\"\n        return Filter(filter_dict)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.__init__","title":"<code>__init__(filter_criteria)</code>","text":"<p>Initializes the Filter object with filter criteria.</p> <p>Parameters:</p> Name Type Description Default <code>filter_criteria</code> <code>Dict[str, Any]</code> <p>The filters to apply.</p> required Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>def __init__(self, filter_criteria: dict[str, Any]):\n    \"\"\"\n    Initializes the Filter object with filter criteria.\n\n    Args:\n        filter_criteria (Dict[str, Any]): The filters to apply.\n    \"\"\"\n    self.filter_criteria = filter_criteria\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.build_filter_expression","title":"<code>build_filter_expression()</code>","text":"<p>Builds the filter expression string from the filter criteria.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The constructed filter expression string compatible with the database.</p> Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>def build_filter_expression(self) -&gt; str:\n    \"\"\"\n    Builds the filter expression string from the filter criteria.\n\n    Returns:\n        str: The constructed filter expression string compatible with the database.\n    \"\"\"\n    return self._parse_filter(self.filter_criteria)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.from_dict","title":"<code>from_dict(filter_dict)</code>  <code>staticmethod</code>","text":"<p>Creates a Filter instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>filter_dict</code> <code>Dict[str, Any]</code> <p>Dictionary defining filter criteria.</p> required <p>Returns:</p> Name Type Description <code>Filter</code> <code>Filter</code> <p>The Filter instance.</p> Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>@staticmethod\ndef from_dict(filter_dict: dict[str, Any]) -&gt; \"Filter\":\n    \"\"\"\n    Creates a Filter instance from a dictionary.\n\n    Args:\n        filter_dict (Dict[str, Any]): Dictionary defining filter criteria.\n\n    Returns:\n        Filter: The Filter instance.\n    \"\"\"\n    return Filter(filter_dict)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/","title":"Milvus","text":""},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore","title":"<code>MilvusVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using Milvus.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>class MilvusVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"Vector store using Milvus.\"\"\"\n\n    def __init__(\n        self,\n        connection: Milvus | None = None,\n        client: Optional[\"MilvusClient\"] = None,\n        index_name: str = \"default\",\n        metric_type: str = \"COSINE\",\n        index_type: str = \"AUTOINDEX\",\n        dimension: int = 1536,\n        create_if_not_exist: bool = False,\n        content_key: str = \"content\",\n        embedding_key: str = \"embedding\",\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize a MilvusVectorStore instance.\n\n        Args:\n            connection (Milvus | None): A Milvus connection object. Defaults to None.\n            client (Optional[MilvusClient]): A Milvus client. Defaults to None.\n            index_name (str): The name of the index to use. Defaults to \"default\".\n            metric_type (str): The metric type to use for the index. Defaults to \"COSINE\".\n            index_type (str): The index type to use for the index. Defaults to \"AUTOINDEX\".\n            dimension (int): The dimension of the vectors. Defaults to 1536.\n            create_if_not_exist (bool): Whether to create the collection if it does not exist. Defaults to False.\n            content_key (str): The field used to store content in the storage. Defaults to \"content\".\n            embedding_key (str): The field used to store vector in the storage. Defaults to \"embedding\".\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        self.client = client\n        if self.client is None:\n            connection = connection or Milvus()\n            self.client = connection.connect()\n        self.index_name = index_name\n        self.metric_type = metric_type\n        self.index_type = index_type\n        self.content_key = content_key\n        self.embedding_key = embedding_key\n        self.dimension = dimension\n        self.create_if_not_exist = create_if_not_exist\n        self.schema = self.client.create_schema(auto_id=False, enable_dynamic_field=True)\n        self.schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=True, max_length=65_535)\n        self.schema.add_field(\n            field_name=self.content_key, datatype=DataType.VARCHAR, max_length=65_535, enable_analyzer=True\n        )\n        self.schema.add_field(field_name=self.embedding_key, datatype=DataType.FLOAT_VECTOR, dim=self.dimension)\n        self.schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n\n        bm25_function = Function(\n            name=\"text_bm25_emb\",\n            input_field_names=[self.content_key],\n            output_field_names=[\"sparse\"],\n            function_type=FunctionType.BM25,\n        )\n        self.schema.add_function(bm25_function)\n\n        self.index_params = self.client.prepare_index_params()\n        self.index_params.add_index(field_name=\"id\")\n        self.index_params.add_index(\n            field_name=self.embedding_key, index_type=self.index_type, metric_type=self.metric_type\n        )\n        self.index_params.add_index(field_name=\"sparse\", index_type=\"SPARSE_INVERTED_INDEX\", metric_type=\"BM25\")\n\n        if not self.client.has_collection(self.index_name):\n            if self.create_if_not_exist:\n                logger.info(f\"Collection {self.index_name} does not exist. Creating a new collection.\")\n                self.client.create_collection(\n                    collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n                )\n                self._track_collection(self.index_name)\n            else:\n                raise ValueError(\n                    f\"Collection {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n                )\n        else:\n            logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n        self.client.load_collection(self.index_name)\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Get the number of documents in the collection.\n\n        Returns:\n            int: The number of documents in the collection.\n        \"\"\"\n        return self.client.get_collection_stats(self.index_name)[\"row_count\"]\n\n    def write_documents(\n        self, documents: list[Document], content_key: str | None = None, embedding_key: str | None = None\n    ) -&gt; int:\n        \"\"\"\n        Write (or overwrite) documents into the Milvus store.\n\n        This method processes a list of Document objects and writes them into the vector store.\n\n        Args:\n            documents (List[Document]): A list of Document objects to be written into the document store.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n\n        Raises:\n            ValueError: If an item in the documents list is not an instance of the Document class.\n\n        Returns:\n            int: The number of documents successfully written to the document store.\n        \"\"\"\n        data_to_upsert = []\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n        for doc in documents:\n            if not isinstance(doc, Document):\n                raise ValueError(\"All items in 'documents' must be of type Document.\")\n\n            document_data = {\n                \"id\": doc.id,\n                embedding_key: doc.embedding,\n                content_key: doc.content,\n            }\n\n            if doc.metadata:\n                document_data.update(doc.metadata)\n\n            data_to_upsert.append(document_data)\n\n        self._track_documents([doc.id for doc in documents])\n\n        response = self.client.upsert(\n            collection_name=self.index_name,\n            data=data_to_upsert,\n        )\n        return response[\"upsert_count\"]\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the Milvus vector store based on their IDs.\n\n        Args:\n            document_ids (List[str]): A list containing the IDs of documents to be deleted from the store.\n            delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n\n        Raises:\n            ValueError: If neither document_ids nor delete_all is provided.\n        \"\"\"\n        if delete_all:\n            self.client.drop_collection(collection_name=self.index_name)\n            self.client.create_collection(\n                collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n            )\n            self.client.load_collection(self.index_name)\n            logger.info(f\"All documents in the collection {self.index_name} have been deleted.\")\n        elif document_ids:\n            response = self.client.delete(collection_name=self.index_name, ids=document_ids)\n            logger.info(f\"Deleted {len(response)} documents from collection {self.index_name}.\")\n        else:\n            raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents based on filters.\n\n        Args:\n            filters (Dict[str, Any]): Filter criteria for deleting documents.\n        \"\"\"\n        if not filters:\n            raise ValueError(\"Filters must be provided to delete documents.\")\n\n        filter_expression = Filter(filters).build_filter_expression()\n\n        delete_result = self.client.delete(collection_name=self.index_name, filter=filter_expression)\n\n        logger.info(f\"Deleted {len(delete_result)} entities from collection {self.index_name} based on filters.\")\n\n    def delete_collection(self, collection_name: str | None = None):\n        \"\"\"\n        Delete a Milvus collection.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.drop_collection(collection_name=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def list_documents(\n        self, limit: int = 1000, content_key: str | None = None, embedding_key: str | None = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the collection up to a specified limit.\n\n        Args:\n            limit (int): Maximum number of documents to retrieve. Defaults to 1000.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n\n        Returns:\n            List[Document]: A list of Document instances representing all documents in the collection.\n        \"\"\"\n        if not self.client.has_collection(self.index_name):\n            raise ValueError(f\"Collection '{self.index_name}' does not exist.\")\n\n        result = self.client.query(collection_name=self.index_name, filter=\"\", output_fields=[\"*\"], limit=limit)\n\n        return self._get_result_to_documents(result, content_key=content_key, embedding_key=embedding_key)\n\n    def _embedding_retrieval(\n        self,\n        query_embeddings: list[list[float]],\n        top_k: int,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        return_embeddings: bool = False,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform vector search on the stored documents using query embeddings.\n\n        Args:\n            query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n            top_k (int): The maximum number of documents to retrieve.\n            filters (dict[str, Any] | None): A dictionary of filters to apply to the search. Defaults to None.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n            return_embeddings (bool): Whether to return the embeddings of the retrieved documents.\n\n        Returns:\n            List[Document]: A list of Document objects containing the retrieved documents.\n        \"\"\"\n        search_params = {\"metric_type\": self.metric_type, \"params\": {}}\n\n        filter_expression = Filter(filters).build_filter_expression() if filters else \"\"\n\n        results = self.client.search(\n            collection_name=self.index_name,\n            data=query_embeddings,\n            limit=top_k,\n            filter=filter_expression,\n            output_fields=[\"*\"],\n            anns_field=embedding_key or self.embedding_key,\n            search_params=search_params,\n        )\n\n        return self._convert_query_result_to_documents(\n            results[0], content_key=content_key, embedding_key=embedding_key, return_embeddings=return_embeddings\n        )\n\n    def _convert_query_result_to_documents(\n        self,\n        result: list[dict[str, Any]],\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        return_embeddings: bool = False,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Convert Milvus search results to Document objects.\n\n        Args:\n            result (List[Dict[str, Any]]): The result from a Milvus search operation.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n            return_embeddings (bool): Whether to return the embeddings of the retrieved documents.\n\n        Returns:\n            List[Document]: A list of Document instances created from the Milvus search result.\n        \"\"\"\n        documents = []\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n        for hit in result:\n            entity = hit.get(\"entity\", {})\n            content = entity.get(content_key, \"\")\n            embedding = entity.get(embedding_key, [])\n            metadata = {k: v for k, v in entity.items() if k not in (content_key, embedding_key)}\n\n            doc = Document(\n                id=str(hit.get(\"id\", \"\")),\n                content=content,\n                metadata=metadata,\n                score=hit.get(\"distance\", None),\n            )\n            if return_embeddings:\n                doc.embedding = embedding\n\n            documents.append(doc)\n\n        return documents\n\n    def filter_documents(\n        self, filters: dict[str, Any] | None = None, content_key: str | None = None, embedding_key: str | None = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents that match the provided filters.\n\n        Args:\n            filters (Dict[str, Any] | None): The filters to apply to the document list.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n\n        Returns:\n            list[Document]: A list of Document instances that match the given filters.\n\n        Raises:\n            ValueError: If no filters are provided.\n        \"\"\"\n        if not filters:\n            raise ValueError(\"No filters provided. No documents will be retrieved with filters.\")\n\n        filter_expression = Filter(filters).build_filter_expression()\n\n        result = self.client.query(\n            collection_name=self.index_name,\n            filter=filter_expression,\n            output_fields=[\"*\"],\n        )\n        return self._get_result_to_documents(result, content_key=content_key, embedding_key=embedding_key)\n\n    def _get_result_to_documents(\n        self, result: list[dict[str, Any]], content_key: str | None = None, embedding_key: str | None = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        Convert Milvus query result into Documents.\n\n        Args:\n            result (List[Dict[str, Any]]): The result from a Milvus query operation.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n\n        Returns:\n            List[Document]: A list containing Document objects created from the Milvus query result.\n        \"\"\"\n        documents = []\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n        for entry in result:\n            document_dict: dict[str, Any] = {\n                \"id\": str(entry.get(\"id\", \"\")),\n                \"content\": entry.get(content_key, \"\"),\n                \"embedding\": entry.get(embedding_key, []),\n            }\n            metadata = {k: v for k, v in entry.items() if k not in (\"id\", content_key, embedding_key)}\n\n            if metadata:\n                document_dict[\"metadata\"] = metadata\n\n            try:\n                documents.append(Document(**document_dict))\n            except Exception as e:\n                logger.error(f\"Error creating Document: {e}, data: {document_dict}\")\n\n        return documents\n\n    def _hybrid_retrieval(\n        self,\n        query: str,\n        query_embeddings: list[list[float]],\n        top_k: int,\n        top_k_dense: int | None = None,\n        top_k_sparse: int | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        return_embeddings: bool = False,\n        drop_ratio_build: float = 0.0,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform a hybrid search using both dense (vector-based) and sparse (text-based) retrieval techniques.\n\n        Args:\n            query (str): The textual query used for sparse search (BM25).\n            query_embeddings (list[list[float]]): A list of embeddings representing the query for dense search.\n            top_k (int): The maximum number of documents to retrieve with hybrid search.\n            top_k_dense (int | None, optional): The number of top results to retrieve from the dense search.\n                If None, defaults to `top_k`.\n            top_k_sparse (int | None, optional): The number of top results to retrieve from the sparse search.\n                If None, defaults to `top_k`.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store vector in the storage.\n            return_embeddings (bool): Whether to return the embeddings of the retrieved documents.\n            drop_ratio_build (float): The ratio of small vector values to be dropped during indexing during text search.\n\n        Returns:\n            List[Document]: A list of Document objects containing the retrieved documents.\n        \"\"\"\n\n        search_param_1 = {\n            \"data\": query_embeddings,\n            \"anns_field\": embedding_key or self.embedding_key,\n            \"param\": {\n                \"metric_type\": self.metric_type,\n            },\n            \"limit\": top_k_dense or top_k,\n        }\n        request_1 = AnnSearchRequest(**search_param_1)\n\n        search_param_2 = {\n            \"data\": [query],\n            \"anns_field\": \"sparse\",\n            \"param\": {\"metric_type\": \"BM25\", \"params\": {\"drop_ratio_build\": drop_ratio_build}},\n            \"limit\": top_k_sparse or top_k,\n        }\n        request_2 = AnnSearchRequest(**search_param_2)\n\n        ranker = RRFRanker()\n        results = self.client.hybrid_search(\n            collection_name=self.index_name,\n            output_fields=[\"*\"],\n            reqs=[request_1, request_2],\n            ranker=ranker,\n            limit=top_k,\n        )\n\n        return self._convert_query_result_to_documents(\n            results[0], content_key=content_key, embedding_key=embedding_key, return_embeddings=return_embeddings\n        )\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', metric_type='COSINE', index_type='AUTOINDEX', dimension=1536, create_if_not_exist=False, content_key='content', embedding_key='embedding', dry_run_config=None)</code>","text":"<p>Initialize a MilvusVectorStore instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Milvus | None</code> <p>A Milvus connection object. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[MilvusClient]</code> <p>A Milvus client. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>The name of the index to use. Defaults to \"default\".</p> <code>'default'</code> <code>metric_type</code> <code>str</code> <p>The metric type to use for the index. Defaults to \"COSINE\".</p> <code>'COSINE'</code> <code>index_type</code> <code>str</code> <p>The index type to use for the index. Defaults to \"AUTOINDEX\".</p> <code>'AUTOINDEX'</code> <code>dimension</code> <code>int</code> <p>The dimension of the vectors. Defaults to 1536.</p> <code>1536</code> <code>create_if_not_exist</code> <code>bool</code> <p>Whether to create the collection if it does not exist. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>str</code> <p>The field used to store content in the storage. Defaults to \"content\".</p> <code>'content'</code> <code>embedding_key</code> <code>str</code> <p>The field used to store vector in the storage. Defaults to \"embedding\".</p> <code>'embedding'</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def __init__(\n    self,\n    connection: Milvus | None = None,\n    client: Optional[\"MilvusClient\"] = None,\n    index_name: str = \"default\",\n    metric_type: str = \"COSINE\",\n    index_type: str = \"AUTOINDEX\",\n    dimension: int = 1536,\n    create_if_not_exist: bool = False,\n    content_key: str = \"content\",\n    embedding_key: str = \"embedding\",\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize a MilvusVectorStore instance.\n\n    Args:\n        connection (Milvus | None): A Milvus connection object. Defaults to None.\n        client (Optional[MilvusClient]): A Milvus client. Defaults to None.\n        index_name (str): The name of the index to use. Defaults to \"default\".\n        metric_type (str): The metric type to use for the index. Defaults to \"COSINE\".\n        index_type (str): The index type to use for the index. Defaults to \"AUTOINDEX\".\n        dimension (int): The dimension of the vectors. Defaults to 1536.\n        create_if_not_exist (bool): Whether to create the collection if it does not exist. Defaults to False.\n        content_key (str): The field used to store content in the storage. Defaults to \"content\".\n        embedding_key (str): The field used to store vector in the storage. Defaults to \"embedding\".\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    self.client = client\n    if self.client is None:\n        connection = connection or Milvus()\n        self.client = connection.connect()\n    self.index_name = index_name\n    self.metric_type = metric_type\n    self.index_type = index_type\n    self.content_key = content_key\n    self.embedding_key = embedding_key\n    self.dimension = dimension\n    self.create_if_not_exist = create_if_not_exist\n    self.schema = self.client.create_schema(auto_id=False, enable_dynamic_field=True)\n    self.schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=True, max_length=65_535)\n    self.schema.add_field(\n        field_name=self.content_key, datatype=DataType.VARCHAR, max_length=65_535, enable_analyzer=True\n    )\n    self.schema.add_field(field_name=self.embedding_key, datatype=DataType.FLOAT_VECTOR, dim=self.dimension)\n    self.schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n\n    bm25_function = Function(\n        name=\"text_bm25_emb\",\n        input_field_names=[self.content_key],\n        output_field_names=[\"sparse\"],\n        function_type=FunctionType.BM25,\n    )\n    self.schema.add_function(bm25_function)\n\n    self.index_params = self.client.prepare_index_params()\n    self.index_params.add_index(field_name=\"id\")\n    self.index_params.add_index(\n        field_name=self.embedding_key, index_type=self.index_type, metric_type=self.metric_type\n    )\n    self.index_params.add_index(field_name=\"sparse\", index_type=\"SPARSE_INVERTED_INDEX\", metric_type=\"BM25\")\n\n    if not self.client.has_collection(self.index_name):\n        if self.create_if_not_exist:\n            logger.info(f\"Collection {self.index_name} does not exist. Creating a new collection.\")\n            self.client.create_collection(\n                collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n            )\n            self._track_collection(self.index_name)\n        else:\n            raise ValueError(\n                f\"Collection {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n            )\n    else:\n        logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n    self.client.load_collection(self.index_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Get the number of documents in the collection.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the collection.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Get the number of documents in the collection.\n\n    Returns:\n        int: The number of documents in the collection.\n    \"\"\"\n    return self.client.get_collection_stats(self.index_name)[\"row_count\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete a Milvus collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None):\n    \"\"\"\n    Delete a Milvus collection.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.drop_collection(collection_name=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the Milvus vector store based on their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>List[str]</code> <p>A list containing the IDs of documents to be deleted from the store.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>A flag to delete all documents from the store. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither document_ids nor delete_all is provided.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the Milvus vector store based on their IDs.\n\n    Args:\n        document_ids (List[str]): A list containing the IDs of documents to be deleted from the store.\n        delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n\n    Raises:\n        ValueError: If neither document_ids nor delete_all is provided.\n    \"\"\"\n    if delete_all:\n        self.client.drop_collection(collection_name=self.index_name)\n        self.client.create_collection(\n            collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n        )\n        self.client.load_collection(self.index_name)\n        logger.info(f\"All documents in the collection {self.index_name} have been deleted.\")\n    elif document_ids:\n        response = self.client.delete(collection_name=self.index_name, ids=document_ids)\n        logger.info(f\"Deleted {len(response)} documents from collection {self.index_name}.\")\n    else:\n        raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents based on filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any]</code> <p>Filter criteria for deleting documents.</p> required Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents based on filters.\n\n    Args:\n        filters (Dict[str, Any]): Filter criteria for deleting documents.\n    \"\"\"\n    if not filters:\n        raise ValueError(\"Filters must be provided to delete documents.\")\n\n    filter_expression = Filter(filters).build_filter_expression()\n\n    delete_result = self.client.delete(collection_name=self.index_name, filter=filter_expression)\n\n    logger.info(f\"Deleted {len(delete_result)} entities from collection {self.index_name} based on filters.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.filter_documents","title":"<code>filter_documents(filters=None, content_key=None, embedding_key=None)</code>","text":"<p>Retrieve documents that match the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store vector in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances that match the given filters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no filters are provided.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def filter_documents(\n    self, filters: dict[str, Any] | None = None, content_key: str | None = None, embedding_key: str | None = None\n) -&gt; list[Document]:\n    \"\"\"\n    Retrieve documents that match the provided filters.\n\n    Args:\n        filters (Dict[str, Any] | None): The filters to apply to the document list.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store vector in the storage.\n\n    Returns:\n        list[Document]: A list of Document instances that match the given filters.\n\n    Raises:\n        ValueError: If no filters are provided.\n    \"\"\"\n    if not filters:\n        raise ValueError(\"No filters provided. No documents will be retrieved with filters.\")\n\n    filter_expression = Filter(filters).build_filter_expression()\n\n    result = self.client.query(\n        collection_name=self.index_name,\n        filter=filter_expression,\n        output_fields=[\"*\"],\n    )\n    return self._get_result_to_documents(result, content_key=content_key, embedding_key=embedding_key)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.list_documents","title":"<code>list_documents(limit=1000, content_key=None, embedding_key=None)</code>","text":"<p>List all documents in the collection up to a specified limit.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of documents to retrieve. Defaults to 1000.</p> <code>1000</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store vector in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: A list of Document instances representing all documents in the collection.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def list_documents(\n    self, limit: int = 1000, content_key: str | None = None, embedding_key: str | None = None\n) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the collection up to a specified limit.\n\n    Args:\n        limit (int): Maximum number of documents to retrieve. Defaults to 1000.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store vector in the storage.\n\n    Returns:\n        List[Document]: A list of Document instances representing all documents in the collection.\n    \"\"\"\n    if not self.client.has_collection(self.index_name):\n        raise ValueError(f\"Collection '{self.index_name}' does not exist.\")\n\n    result = self.client.query(collection_name=self.index_name, filter=\"\", output_fields=[\"*\"], limit=limit)\n\n    return self._get_result_to_documents(result, content_key=content_key, embedding_key=embedding_key)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.write_documents","title":"<code>write_documents(documents, content_key=None, embedding_key=None)</code>","text":"<p>Write (or overwrite) documents into the Milvus store.</p> <p>This method processes a list of Document objects and writes them into the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>A list of Document objects to be written into the document store.</p> required <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store vector in the storage.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an item in the documents list is not an instance of the Document class.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents successfully written to the document store.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def write_documents(\n    self, documents: list[Document], content_key: str | None = None, embedding_key: str | None = None\n) -&gt; int:\n    \"\"\"\n    Write (or overwrite) documents into the Milvus store.\n\n    This method processes a list of Document objects and writes them into the vector store.\n\n    Args:\n        documents (List[Document]): A list of Document objects to be written into the document store.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store vector in the storage.\n\n    Raises:\n        ValueError: If an item in the documents list is not an instance of the Document class.\n\n    Returns:\n        int: The number of documents successfully written to the document store.\n    \"\"\"\n    data_to_upsert = []\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n    for doc in documents:\n        if not isinstance(doc, Document):\n            raise ValueError(\"All items in 'documents' must be of type Document.\")\n\n        document_data = {\n            \"id\": doc.id,\n            embedding_key: doc.embedding,\n            content_key: doc.content,\n        }\n\n        if doc.metadata:\n            document_data.update(doc.metadata)\n\n        data_to_upsert.append(document_data)\n\n    self._track_documents([doc.id for doc in documents])\n\n    response = self.client.upsert(\n        collection_name=self.index_name,\n        data=data_to_upsert,\n    )\n    return response[\"upsert_count\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/opensearch/opensearch/","title":"Opensearch","text":""},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchEngine","title":"<code>OpenSearchEngine</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported engine names for OpenSearch.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchEngine(str, Enum):\n    \"\"\"Supported engine names for OpenSearch.\"\"\"\n\n    FAISS = \"faiss\"\n    LUCENE = \"lucene\"\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchMethod","title":"<code>OpenSearchMethod</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported method names for OpenSearch.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchMethod(str, Enum):\n    \"\"\"Supported method names for OpenSearch.\"\"\"\n\n    HNSW = \"hnsw\"\n    IVF = \"ivf\"\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchSimilarityMetric","title":"<code>OpenSearchSimilarityMetric</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported similarity metrics for OpenSearch.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchSimilarityMetric(str, Enum):\n    \"\"\"Supported similarity metrics for OpenSearch.\"\"\"\n\n    COSINE = \"cosinesimil\"\n    L1 = \"l1\"\n    L2 = \"l2\"\n    LINF = \"linf\"\n    INNER_PRODUCT = \"innerproduct\"\n    HAMMING = \"hamming\"\n    HAMMING_BIT = \"hammingbit\"\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore","title":"<code>OpenSearchVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using OpenSearch for dense vector search.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"Vector store using OpenSearch for dense vector search.\"\"\"\n\n    def __init__(\n        self,\n        connection: AWSOpenSearch | None = None,\n        client: Optional[\"OpenSearchClient\"] | None = None,\n        index_name: str = \"default\",\n        dimension: int = 1536,\n        similarity: OpenSearchSimilarityMetric = OpenSearchSimilarityMetric.COSINE,\n        create_if_not_exist: bool = False,\n        content_key: str = \"content\",\n        embedding_key: str = \"embedding\",\n        batch_size: int = 100,\n        index_settings: dict | None = None,\n        mapping_settings: dict | None = None,\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize OpenSearchVectorStore.\n\n        Args:\n            connection (Optional[AWSOpenSearch]): AWS OpenSearch connection.\n            client (Optional[OpenSearchClient]): OpenSearch client. Defaults to None.\n            index_name (str): Name of the index. Defaults to \"default\".\n            dimension (int): Dimension of vectors. Defaults to 1536.\n            similarity (OpenSearchSimilarityMetric): Similarity metric.\n                        Defaults to OpenSearchSimilarityMetric.COSINE.\n            create_if_not_exist (bool): Whether to create the index if it does not exist. Defaults to False.\n            content_key (str): Key for content field. Defaults to \"content\".\n            embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n            batch_size (int): Batch size for write operations. Defaults to 100.\n            index_settings (Optional[dict]): Custom index settings. Defaults to None.\n            mapping_settings (Optional[dict]): Custom mapping settings. Defaults to None.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        if client is None:\n            if connection is None:\n                connection = AWSOpenSearch()\n            self.client = connection.connect()\n        else:\n            self.client = client\n\n        self.index_name = index_name\n        self.dimension = dimension\n        self.similarity = similarity\n        self.content_key = content_key\n        self.embedding_key = embedding_key\n        self.batch_size = batch_size\n        self.index_settings = index_settings or {}\n        self.mapping_settings = mapping_settings or {}\n\n        if not self.client.indices.exists(index=self.index_name):\n            if create_if_not_exist:\n                logger.info(f\"Index {self.index_name} does not exist. Creating a new index.\")\n                self._create_index_if_not_exists()\n                self._track_collection(self.index_name)\n            else:\n                raise ValueError(\n                    f\"Index {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n                )\n        else:\n            logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n        logger.debug(f\"OpenSearchVectorStore initialized with index: {self.index_name}\")\n\n    def _create_index_if_not_exists(self) -&gt; None:\n        \"\"\"Create the index if it doesn't exist.\"\"\"\n\n        base_settings = {\"index\": {\"knn\": True}}\n        settings = base_settings.copy()\n\n        # Add custom index settings if provided\n        if self.index_settings:\n            for key, value in self.index_settings.items():\n                if key in settings and isinstance(settings[key], dict) and isinstance(value, dict):\n                    settings[key].update(value)\n                else:\n                    settings[key] = value\n\n        mapping = {\n            \"settings\": settings,\n            \"mappings\": {\n                \"properties\": {\n                    self.content_key: {\n                        \"type\": \"text\",\n                        \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n                    },\n                    \"metadata\": {\"type\": \"object\"},\n                    self.embedding_key: {\n                        \"type\": \"knn_vector\",\n                        \"dimension\": self.dimension,\n                        \"method\": {\n                            \"engine\": OpenSearchEngine.FAISS,\n                            \"name\": OpenSearchMethod.HNSW,\n                            \"parameters\": {\"ef_construction\": 128, \"m\": 24},\n                            \"space_type\": self.similarity,\n                        },\n                    },\n                }\n            },\n        }\n\n        # Add custom mapping settings if provided\n        if self.mapping_settings:\n            merged_mappings = mapping[\"mappings\"].copy()\n            for key, value in self.mapping_settings.items():\n                if (\n                    key == \"properties\"\n                    and key in merged_mappings\n                    and isinstance(merged_mappings[key], dict)\n                    and isinstance(value, dict)\n                ):\n                    merged_mappings[key].update(value)\n                else:\n                    merged_mappings[key] = value\n            mapping[\"mappings\"] = merged_mappings\n\n        self.client.indices.create(index=self.index_name, body=mapping)\n\n    def delete_collection(self, collection_name: str | None = None) -&gt; None:\n        \"\"\"\n        Delete the collection in the database.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.indices.delete(index=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def _handle_duplicate_documents(\n        self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.FAIL\n    ) -&gt; list[Document]:\n        \"\"\"\n        Handle duplicate documents based on policy.\n\n        Args:\n            documents (list[Document]): List of documents to check for duplicates.\n            policy (DuplicatePolicy): Policy for handling duplicates. Defaults to DuplicatePolicy.FAIL.\n\n        Returns:\n            list[Document]: List of documents after applying the specified policy.\n\n        Raises:\n            VectorStoreException: If duplicates are found and the policy is set to FAIL.\n        \"\"\"\n        if policy == DuplicatePolicy.OVERWRITE:\n            return documents\n\n        # Get unique documents\n        unique_docs = {}\n        for doc in documents:\n            if doc.id in unique_docs:\n                logger.warning(f\"Duplicate document ID found: {doc.id}\")\n            unique_docs[doc.id] = doc\n\n        if policy == DuplicatePolicy.NONE:\n            return list(unique_docs.values())\n\n        existing_ids = set()\n        for doc_id in unique_docs.keys():\n            try:\n                self.retrieve_document_by_file_id(file_id=doc_id)\n                existing_ids.add(doc_id)\n            except NotFoundError:\n                pass\n\n        if policy == DuplicatePolicy.FAIL and existing_ids:\n            raise VectorStoreException(f\"Documents with IDs {existing_ids} already exist\")\n\n        filtered_docs = [doc for doc_id, doc in unique_docs.items() if doc_id not in existing_ids]\n        return filtered_docs\n\n    def write_documents(\n        self,\n        documents: list[Document],\n        policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n        batch_size: int | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; int:\n        \"\"\"\n        Write documents to OpenSearch.\n\n        Args:\n            documents (list[Document]): List of documents to write.\n            policy (DuplicatePolicy): Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.\n            batch_size (Optional[int]): Size of batches for bulk operations. Defaults to None.\n            content_key (Optional[str]): The field used to store content. Defaults to None.\n            embedding_key (Optional[str]): The field used to store embeddings. Defaults to None.\n\n        Returns:\n            int: Number of documents successfully written.\n\n        Raises:\n            ValueError: If the provided documents are invalid.\n            VectorStoreException: If duplicates are found when using the FAIL policy.\n        \"\"\"\n        if not documents:\n            return 0\n\n        if not isinstance(documents[0], Document):\n            raise ValueError(\"Documents must be of type Document\")\n\n        # Handle duplicates\n        documents = self._handle_duplicate_documents(documents, policy)\n        if not documents:\n            return 0\n\n        batch_size = batch_size or self.batch_size\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        total_written = 0\n        actions = []\n        for doc in documents:\n            action = {\n                \"_op_type\": \"index\",\n                \"_index\": self.index_name,\n                \"_id\": doc.id,\n                \"_source\": {content_key: doc.content, \"metadata\": doc.metadata},\n            }\n            # Only include embedding field if present and not None\n            if getattr(doc, \"embedding\", None) is not None:\n                action[\"_source\"][embedding_key] = doc.embedding\n\n            actions.append(action)\n\n        for i in range(0, len(actions), batch_size):\n            chunk = actions[i : i + batch_size]\n\n            success_count, _ = bulk(self.client, chunk, refresh=True)\n            total_written += success_count\n\n            self._track_documents([action[\"_id\"] for action in chunk])\n\n        return total_written\n\n    def _scale_score(self, score: float, similarity: OpenSearchSimilarityMetric) -&gt; float:\n        \"\"\"\n        Scale the score based on the similarity metric.\n\n        Args:\n            score (float): Raw score from OpenSearch.\n            similarity (OpenSearchSimilarityMetric): Similarity metric used.\n\n        Returns:\n            float: Scaled score between 0 and 1, depending on the similarity metric used.\n        \"\"\"\n        if similarity == OpenSearchSimilarityMetric.COSINE:\n            # Normalize range [0, 2] to [0, 1]\n            return score / 2\n        elif similarity == OpenSearchSimilarityMetric.INNER_PRODUCT:\n            # Normalize using sigmoid function as inner product scores can be any range\n            return float(1 / (1 + np.exp(-score / 100)))\n        else:  # L1, L2, LINF, HAMMING, HAMMING_BIT\n            # L1, L2, LINF, HAMMING, HAMMING_BIT distance is inverse - smaller is better\n            # Convert to similarity score\n            return 1 / (1 + score)\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n        filters: dict[str, Any] | None = None,\n        scale_scores: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents by vector similarity.\n\n        Args:\n            query_embedding (List[float]): Query vector.\n            top_k (int): Number of results. Defaults to 10.\n            exclude_document_embeddings (bool): Exclude embeddings in response. Defaults to True.\n            filters (dict[str, Any] | None): Metadata filters. Defaults to None.\n            scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n        Returns:\n            List[Document]: Retrieved documents.\n\n        Raises:\n            ValueError: If query_embedding is invalid.\n        \"\"\"\n        if not query_embedding:\n            raise ValueError(\"query_embedding must not be empty\")\n\n        embedding_key = embedding_key or self.embedding_key\n        content_key = content_key or self.content_key\n\n        knn_query = {embedding_key: {\"vector\": query_embedding, \"k\": top_k}}\n\n        if filters:\n            normalized_filters = _normalize_filters(filters)\n            knn_query[embedding_key][\"filter\"] = {\"bool\": normalized_filters}\n\n        body = {\n            \"size\": top_k,\n            \"query\": {\"knn\": knn_query},\n            \"_source\": {\"excludes\": [embedding_key] if exclude_document_embeddings else []},\n        }\n\n        response = self.client.search(index=self.index_name, body=body)\n\n        documents = []\n        for hit in response[\"hits\"][\"hits\"]:\n            score = hit.get(\"_score\", None)\n            if score is None:\n                continue\n\n            if scale_scores:\n                score = self._scale_score(score, self.similarity)\n\n            source = hit.get(\"_source\", {})\n            if content_key not in source:\n                continue\n\n            doc = Document(id=hit[\"_id\"], content=source[content_key], metadata=source.get(\"metadata\", {}), score=score)\n            if not exclude_document_embeddings and embedding_key in source:\n                doc.embedding = source[embedding_key]\n            documents.append(doc)\n\n        return documents\n\n    def retrieve_document_by_file_id(\n        self,\n        file_id: str,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ):\n        embedding_key = embedding_key or self.embedding_key\n        content_key = content_key or self.content_key\n\n        response = self.client.get(\n            index=self.index_name,\n            id=file_id,\n            _source_excludes=([embedding_key] if not include_embeddings else None),\n        )\n\n        # Convert result to Document\n        doc = Document(\n            id=response[\"_id\"],\n            content=response[\"_source\"][content_key],\n            metadata=response[\"_source\"][\"metadata\"],\n        )\n        if include_embeddings:\n            doc.embedding = response[\"_source\"][embedding_key]\n\n        return doc\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the store.\n\n        Args:\n            document_ids (Optional[List[str]]): IDs to delete. Defaults to None.\n            delete_all (bool): Delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all:\n            self.client.delete_by_query(index=self.index_name, body={\"query\": {\"match_all\": {}}}, refresh=True)\n\n        elif document_ids:\n            operations = [{\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": doc_id} for doc_id in document_ids]\n            bulk(self.client, operations, refresh=True)\n\n        else:\n            logger.warning(\"No document IDs provided. No documents will be deleted.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"Delete documents matching filters.\n\n        Args:\n            filters (dict[str, Any]): Metadata filters.\n        \"\"\"\n        if not filters:\n            logger.warning(\"No filters provided. No documents will be deleted.\")\n            return\n\n        filters = _normalize_filters(filters)\n        bool_query = {\"bool\": filters}\n\n        body = {\"query\": bool_query}\n\n        response = self.client.delete_by_query(index=self.index_name, body=body, refresh=True)\n\n        deleted_count = response.get(\"deleted\", 0)\n        logger.info(f\"Deleted {deleted_count} documents matching filters.\")\n\n    def list_documents(\n        self,\n        top_k: int | None = 100,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        scale_scores: bool = False,\n    ) -&gt; list[Document]:\n        \"\"\"\n        List documents in the OpenSearch vector store.\n\n        Args:\n            top_k (Optional[int]): Maximal number of documents to retrieve. Defaults to 100.\n            include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n            content_key (Optional[str]): The field used to store content in the storage.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n            scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n\n        Returns:\n            list[Document]: List of Document objects retrieved.\n        \"\"\"\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        # Build search query\n        search_body = {\n            \"size\": top_k,\n            \"query\": {\"match_all\": {}},\n        }\n\n        if not include_embeddings:\n            search_body[\"_source\"] = {\"excludes\": [embedding_key]}\n\n        response = self.client.search(index=self.index_name, body=search_body)\n\n        # Convert hits to Document objects\n        all_documents = []\n        for hit in response[\"hits\"][\"hits\"]:\n            score = hit.get(\"_score\", None)\n            if scale_scores and score is not None:\n                score = self._scale_score(score, self.similarity)\n\n            if content_key not in hit[\"_source\"]:\n                continue\n\n            doc = Document(\n                id=hit[\"_id\"],\n                content=hit[\"_source\"][content_key],\n                metadata=hit[\"_source\"].get(\"metadata\", {}),\n                score=score,\n            )\n            if include_embeddings and embedding_key in hit[\"_source\"]:\n                doc.embedding = hit[\"_source\"][embedding_key]\n\n            all_documents.append(doc)\n\n        return all_documents\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the OpenSearch index.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        response = self.client.count(index=self.index_name, body={\"query\": {\"match_all\": {}}})\n        return response.get(\"count\", 0)\n\n    def get_field_statistics(self, field: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Get statistics for a numeric field.\n\n        Args:\n            field (str): Full field name (must be numeric)\n\n        Returns:\n            Dictionary with min, max, avg, sum\n        \"\"\"\n        response = self.client.search(\n            index=self.index_name,\n            body={\"size\": 0, \"aggs\": {\"stats\": {\"stats\": {\"field\": field}}}},\n        )\n        return response[\"aggregations\"][\"stats\"]\n\n    def update_document_by_file_id(\n        self,\n        file_id: str,\n        content: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        embedding: list[float] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"Update an existing document.\n\n        Args:\n            file_id (str): Document ID\n            content (Optional[str]): Update content\n            metadata (Optional[dict[str, Any]]): Update field metadata or add new fields\n            embedding (Optional[list[float]]): New embedding vector\n            content_key (Optional[str]): Key for content field.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n        \"\"\"\n        update_fields = {}\n        if content is not None:\n            update_fields[content_key or self.content_key] = content\n        if metadata is not None:\n            update_fields[\"metadata\"] = metadata\n        if embedding is not None:\n            update_fields[embedding_key or self.embedding_key] = embedding\n\n        if update_fields:\n            self.client.update(index=self.index_name, id=file_id, body={\"doc\": update_fields}, refresh=True)\n\n    def update_documents_batch(\n        self,\n        documents: list[Document],\n        batch_size: int | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; int:\n        \"\"\"\n        Update multiple documents in batches.\n\n        Args:\n            documents (list[Document]): List of documents to update.\n            batch_size (Optional[int]): Size of batches for bulk operations.\n            content_key (Optional[str]): Key for content field.\n            embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n        Returns:\n            int: Number of documents successfully updated.\n\n        \"\"\"\n        batch_size = batch_size or self.batch_size\n        total_updated = 0\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        def generate_actions(docs):\n            actions = []\n            for doc in docs:\n                update_doc = {\n                    content_key: doc.content,\n                    \"metadata\": doc.metadata,\n                }\n                if getattr(doc, \"embedding\", None) is not None:\n                    update_doc[embedding_key] = doc.embedding\n\n                action = {\n                    \"_op_type\": \"update\",\n                    \"_index\": self.index_name,\n                    \"_id\": doc.id,\n                    \"doc\": update_doc,\n                }\n                actions.append(action)\n            return actions\n\n        for i in range(0, len(documents), batch_size):\n            sub_set_docs = documents[i : i + batch_size]\n            batch_actions = generate_actions(sub_set_docs)\n            success_count, _ = bulk(self.client, batch_actions, refresh=True)\n            total_updated += success_count\n        return total_updated\n\n    def create_alias(\n        self,\n        alias_name: str,\n        index_names: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Create an alias for one or more indices.\n\n        Args:\n            alias_name (str): Name of the alias.\n            index_names (Optional[list[str]]): List of indices to include in the alias. Defaults to None.\n        \"\"\"\n        index_names = index_names or [self.index_name]\n        actions = []\n        for index in index_names:\n            actions.append({\"add\": {\"index\": index, \"alias\": alias_name}})\n        self.client.indices.update_aliases({\"actions\": actions})\n\n    def close(self) -&gt; None:\n        \"\"\"Close the client connection.\"\"\"\n        if hasattr(self, \"client\"):\n            self.client.close()\n\n    def __del__(self):\n        \"\"\"Cleanup on deletion.\"\"\"\n        self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup on deletion.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup on deletion.\"\"\"\n    self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', dimension=1536, similarity=OpenSearchSimilarityMetric.COSINE, create_if_not_exist=False, content_key='content', embedding_key='embedding', batch_size=100, index_settings=None, mapping_settings=None, dry_run_config=None)</code>","text":"<p>Initialize OpenSearchVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[AWSOpenSearch]</code> <p>AWS OpenSearch connection.</p> <code>None</code> <code>client</code> <code>Optional[OpenSearch]</code> <p>OpenSearch client. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> <code>'default'</code> <code>dimension</code> <code>int</code> <p>Dimension of vectors. Defaults to 1536.</p> <code>1536</code> <code>similarity</code> <code>OpenSearchSimilarityMetric</code> <p>Similarity metric.         Defaults to OpenSearchSimilarityMetric.COSINE.</p> <code>COSINE</code> <code>create_if_not_exist</code> <code>bool</code> <p>Whether to create the index if it does not exist. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>str</code> <p>Key for content field. Defaults to \"content\".</p> <code>'content'</code> <code>embedding_key</code> <code>str</code> <p>Key for embedding field. Defaults to \"embedding\".</p> <code>'embedding'</code> <code>batch_size</code> <code>int</code> <p>Batch size for write operations. Defaults to 100.</p> <code>100</code> <code>index_settings</code> <code>Optional[dict]</code> <p>Custom index settings. Defaults to None.</p> <code>None</code> <code>mapping_settings</code> <code>Optional[dict]</code> <p>Custom mapping settings. Defaults to None.</p> <code>None</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def __init__(\n    self,\n    connection: AWSOpenSearch | None = None,\n    client: Optional[\"OpenSearchClient\"] | None = None,\n    index_name: str = \"default\",\n    dimension: int = 1536,\n    similarity: OpenSearchSimilarityMetric = OpenSearchSimilarityMetric.COSINE,\n    create_if_not_exist: bool = False,\n    content_key: str = \"content\",\n    embedding_key: str = \"embedding\",\n    batch_size: int = 100,\n    index_settings: dict | None = None,\n    mapping_settings: dict | None = None,\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize OpenSearchVectorStore.\n\n    Args:\n        connection (Optional[AWSOpenSearch]): AWS OpenSearch connection.\n        client (Optional[OpenSearchClient]): OpenSearch client. Defaults to None.\n        index_name (str): Name of the index. Defaults to \"default\".\n        dimension (int): Dimension of vectors. Defaults to 1536.\n        similarity (OpenSearchSimilarityMetric): Similarity metric.\n                    Defaults to OpenSearchSimilarityMetric.COSINE.\n        create_if_not_exist (bool): Whether to create the index if it does not exist. Defaults to False.\n        content_key (str): Key for content field. Defaults to \"content\".\n        embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n        batch_size (int): Batch size for write operations. Defaults to 100.\n        index_settings (Optional[dict]): Custom index settings. Defaults to None.\n        mapping_settings (Optional[dict]): Custom mapping settings. Defaults to None.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    if client is None:\n        if connection is None:\n            connection = AWSOpenSearch()\n        self.client = connection.connect()\n    else:\n        self.client = client\n\n    self.index_name = index_name\n    self.dimension = dimension\n    self.similarity = similarity\n    self.content_key = content_key\n    self.embedding_key = embedding_key\n    self.batch_size = batch_size\n    self.index_settings = index_settings or {}\n    self.mapping_settings = mapping_settings or {}\n\n    if not self.client.indices.exists(index=self.index_name):\n        if create_if_not_exist:\n            logger.info(f\"Index {self.index_name} does not exist. Creating a new index.\")\n            self._create_index_if_not_exists()\n            self._track_collection(self.index_name)\n        else:\n            raise ValueError(\n                f\"Index {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n            )\n    else:\n        logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n    logger.debug(f\"OpenSearchVectorStore initialized with index: {self.index_name}\")\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.close","title":"<code>close()</code>","text":"<p>Close the client connection.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the client connection.\"\"\"\n    if hasattr(self, \"client\"):\n        self.client.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the OpenSearch index.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the OpenSearch index.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    response = self.client.count(index=self.index_name, body={\"query\": {\"match_all\": {}}})\n    return response.get(\"count\", 0)\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.create_alias","title":"<code>create_alias(alias_name, index_names=None)</code>","text":"<p>Create an alias for one or more indices.</p> <p>Parameters:</p> Name Type Description Default <code>alias_name</code> <code>str</code> <p>Name of the alias.</p> required <code>index_names</code> <code>Optional[list[str]]</code> <p>List of indices to include in the alias. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def create_alias(\n    self,\n    alias_name: str,\n    index_names: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Create an alias for one or more indices.\n\n    Args:\n        alias_name (str): Name of the alias.\n        index_names (Optional[list[str]]): List of indices to include in the alias. Defaults to None.\n    \"\"\"\n    index_names = index_names or [self.index_name]\n    actions = []\n    for index in index_names:\n        actions.append({\"add\": {\"index\": index, \"alias\": alias_name}})\n    self.client.indices.update_aliases({\"actions\": actions})\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete the collection in the database.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None) -&gt; None:\n    \"\"\"\n    Delete the collection in the database.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.indices.delete(index=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>Optional[List[str]]</code> <p>IDs to delete. Defaults to None.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>Delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the store.\n\n    Args:\n        document_ids (Optional[List[str]]): IDs to delete. Defaults to None.\n        delete_all (bool): Delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all:\n        self.client.delete_by_query(index=self.index_name, body={\"query\": {\"match_all\": {}}}, refresh=True)\n\n    elif document_ids:\n        operations = [{\"_op_type\": \"delete\", \"_index\": self.index_name, \"_id\": doc_id} for doc_id in document_ids]\n        bulk(self.client, operations, refresh=True)\n\n    else:\n        logger.warning(\"No document IDs provided. No documents will be deleted.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents matching filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Metadata filters.</p> required Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"Delete documents matching filters.\n\n    Args:\n        filters (dict[str, Any]): Metadata filters.\n    \"\"\"\n    if not filters:\n        logger.warning(\"No filters provided. No documents will be deleted.\")\n        return\n\n    filters = _normalize_filters(filters)\n    bool_query = {\"bool\": filters}\n\n    body = {\"query\": bool_query}\n\n    response = self.client.delete_by_query(index=self.index_name, body=body, refresh=True)\n\n    deleted_count = response.get(\"deleted\", 0)\n    logger.info(f\"Deleted {deleted_count} documents matching filters.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.get_field_statistics","title":"<code>get_field_statistics(field)</code>","text":"<p>Get statistics for a numeric field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Full field name (must be numeric)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with min, max, avg, sum</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def get_field_statistics(self, field: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Get statistics for a numeric field.\n\n    Args:\n        field (str): Full field name (must be numeric)\n\n    Returns:\n        Dictionary with min, max, avg, sum\n    \"\"\"\n    response = self.client.search(\n        index=self.index_name,\n        body={\"size\": 0, \"aggs\": {\"stats\": {\"stats\": {\"field\": field}}}},\n    )\n    return response[\"aggregations\"][\"stats\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.list_documents","title":"<code>list_documents(top_k=100, include_embeddings=False, content_key=None, embedding_key=None, scale_scores=False)</code>","text":"<p>List documents in the OpenSearch vector store.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>Optional[int]</code> <p>Maximal number of documents to retrieve. Defaults to 100.</p> <code>100</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the results. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> <code>scale_scores</code> <code>bool</code> <p>Whether to scale scores to 0-1 range. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of Document objects retrieved.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def list_documents(\n    self,\n    top_k: int | None = 100,\n    include_embeddings: bool = False,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    scale_scores: bool = False,\n) -&gt; list[Document]:\n    \"\"\"\n    List documents in the OpenSearch vector store.\n\n    Args:\n        top_k (Optional[int]): Maximal number of documents to retrieve. Defaults to 100.\n        include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n        content_key (Optional[str]): The field used to store content in the storage.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n        scale_scores (bool): Whether to scale scores to 0-1 range. Defaults to False.\n\n    Returns:\n        list[Document]: List of Document objects retrieved.\n    \"\"\"\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    # Build search query\n    search_body = {\n        \"size\": top_k,\n        \"query\": {\"match_all\": {}},\n    }\n\n    if not include_embeddings:\n        search_body[\"_source\"] = {\"excludes\": [embedding_key]}\n\n    response = self.client.search(index=self.index_name, body=search_body)\n\n    # Convert hits to Document objects\n    all_documents = []\n    for hit in response[\"hits\"][\"hits\"]:\n        score = hit.get(\"_score\", None)\n        if scale_scores and score is not None:\n            score = self._scale_score(score, self.similarity)\n\n        if content_key not in hit[\"_source\"]:\n            continue\n\n        doc = Document(\n            id=hit[\"_id\"],\n            content=hit[\"_source\"][content_key],\n            metadata=hit[\"_source\"].get(\"metadata\", {}),\n            score=score,\n        )\n        if include_embeddings and embedding_key in hit[\"_source\"]:\n            doc.embedding = hit[\"_source\"][embedding_key]\n\n        all_documents.append(doc)\n\n    return all_documents\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.update_document_by_file_id","title":"<code>update_document_by_file_id(file_id, content=None, metadata=None, embedding=None, content_key=None, embedding_key=None)</code>","text":"<p>Update an existing document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>Document ID</p> required <code>content</code> <code>Optional[str]</code> <p>Update content</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Update field metadata or add new fields</p> <code>None</code> <code>embedding</code> <code>Optional[list[float]]</code> <p>New embedding vector</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Key for content field.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def update_document_by_file_id(\n    self,\n    file_id: str,\n    content: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    embedding: list[float] | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; None:\n    \"\"\"Update an existing document.\n\n    Args:\n        file_id (str): Document ID\n        content (Optional[str]): Update content\n        metadata (Optional[dict[str, Any]]): Update field metadata or add new fields\n        embedding (Optional[list[float]]): New embedding vector\n        content_key (Optional[str]): Key for content field.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n    \"\"\"\n    update_fields = {}\n    if content is not None:\n        update_fields[content_key or self.content_key] = content\n    if metadata is not None:\n        update_fields[\"metadata\"] = metadata\n    if embedding is not None:\n        update_fields[embedding_key or self.embedding_key] = embedding\n\n    if update_fields:\n        self.client.update(index=self.index_name, id=file_id, body={\"doc\": update_fields}, refresh=True)\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.update_documents_batch","title":"<code>update_documents_batch(documents, batch_size=None, content_key=None, embedding_key=None)</code>","text":"<p>Update multiple documents in batches.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of documents to update.</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Size of batches for bulk operations.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>Key for content field.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings in the storage.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully updated.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def update_documents_batch(\n    self,\n    documents: list[Document],\n    batch_size: int | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Update multiple documents in batches.\n\n    Args:\n        documents (list[Document]): List of documents to update.\n        batch_size (Optional[int]): Size of batches for bulk operations.\n        content_key (Optional[str]): Key for content field.\n        embedding_key (Optional[str]): The field used to store embeddings in the storage.\n\n    Returns:\n        int: Number of documents successfully updated.\n\n    \"\"\"\n    batch_size = batch_size or self.batch_size\n    total_updated = 0\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    def generate_actions(docs):\n        actions = []\n        for doc in docs:\n            update_doc = {\n                content_key: doc.content,\n                \"metadata\": doc.metadata,\n            }\n            if getattr(doc, \"embedding\", None) is not None:\n                update_doc[embedding_key] = doc.embedding\n\n            action = {\n                \"_op_type\": \"update\",\n                \"_index\": self.index_name,\n                \"_id\": doc.id,\n                \"doc\": update_doc,\n            }\n            actions.append(action)\n        return actions\n\n    for i in range(0, len(documents), batch_size):\n        sub_set_docs = documents[i : i + batch_size]\n        batch_actions = generate_actions(sub_set_docs)\n        success_count, _ = bulk(self.client, batch_actions, refresh=True)\n        total_updated += success_count\n    return total_updated\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.FAIL, batch_size=None, content_key=None, embedding_key=None)</code>","text":"<p>Write documents to OpenSearch.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of documents to write.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.</p> <code>FAIL</code> <code>batch_size</code> <code>Optional[int]</code> <p>Size of batches for bulk operations. Defaults to None.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>Optional[str]</code> <p>The field used to store embeddings. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided documents are invalid.</p> <code>VectorStoreException</code> <p>If duplicates are found when using the FAIL policy.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>def write_documents(\n    self,\n    documents: list[Document],\n    policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n    batch_size: int | None = None,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n) -&gt; int:\n    \"\"\"\n    Write documents to OpenSearch.\n\n    Args:\n        documents (list[Document]): List of documents to write.\n        policy (DuplicatePolicy): Policy for handling duplicate documents. Defaults to DuplicatePolicy.FAIL.\n        batch_size (Optional[int]): Size of batches for bulk operations. Defaults to None.\n        content_key (Optional[str]): The field used to store content. Defaults to None.\n        embedding_key (Optional[str]): The field used to store embeddings. Defaults to None.\n\n    Returns:\n        int: Number of documents successfully written.\n\n    Raises:\n        ValueError: If the provided documents are invalid.\n        VectorStoreException: If duplicates are found when using the FAIL policy.\n    \"\"\"\n    if not documents:\n        return 0\n\n    if not isinstance(documents[0], Document):\n        raise ValueError(\"Documents must be of type Document\")\n\n    # Handle duplicates\n    documents = self._handle_duplicate_documents(documents, policy)\n    if not documents:\n        return 0\n\n    batch_size = batch_size or self.batch_size\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    total_written = 0\n    actions = []\n    for doc in documents:\n        action = {\n            \"_op_type\": \"index\",\n            \"_index\": self.index_name,\n            \"_id\": doc.id,\n            \"_source\": {content_key: doc.content, \"metadata\": doc.metadata},\n        }\n        # Only include embedding field if present and not None\n        if getattr(doc, \"embedding\", None) is not None:\n            action[\"_source\"][embedding_key] = doc.embedding\n\n        actions.append(action)\n\n    for i in range(0, len(actions), batch_size):\n        chunk = actions[i : i + batch_size]\n\n        success_count, _ = bulk(self.client, chunk, refresh=True)\n        total_written += success_count\n\n        self._track_documents([action[\"_id\"] for action in chunk])\n\n    return total_written\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStoreParams","title":"<code>OpenSearchVectorStoreParams</code>","text":"<p>               Bases: <code>BaseVectorStoreParams</code></p> <p>Parameters for OpenSearch vector store.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> <code>content_key</code> <code>str</code> <p>Key for content field. Defaults to \"content\".</p> <code>dimension</code> <code>int</code> <p>Dimension of the vectors. Defaults to 1536.</p> <code>similarity</code> <code>str</code> <p>Similarity metric to use. Defaults to \"cosine\".</p> <code>embedding_key</code> <code>str</code> <p>Key for embedding field. Defaults to \"embedding\".</p> <code>batch_size</code> <code>int</code> <p>Batch size for writing operations. Defaults to 100.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchVectorStoreParams(BaseVectorStoreParams):\n    \"\"\"Parameters for OpenSearch vector store.\n\n    Attributes:\n        index_name (str): Name of the index. Defaults to \"default\".\n        content_key (str): Key for content field. Defaults to \"content\".\n        dimension (int): Dimension of the vectors. Defaults to 1536.\n        similarity (str): Similarity metric to use. Defaults to \"cosine\".\n        embedding_key (str): Key for embedding field. Defaults to \"embedding\".\n        batch_size (int): Batch size for writing operations. Defaults to 100.\n    \"\"\"\n\n    similarity: OpenSearchSimilarityMetric = OpenSearchSimilarityMetric.COSINE\n    embedding_key: str = \"embedding\"\n    batch_size: int = 100\n</code></pre>"},{"location":"dynamiq/storages/vector/opensearch/opensearch/#dynamiq.storages.vector.opensearch.opensearch.OpenSearchVectorStoreWriterParams","title":"<code>OpenSearchVectorStoreWriterParams</code>","text":"<p>               Bases: <code>OpenSearchVectorStoreParams</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Parameters for OpenSearch vector store writer.</p> Source code in <code>dynamiq/storages/vector/opensearch/opensearch.py</code> <pre><code>class OpenSearchVectorStoreWriterParams(OpenSearchVectorStoreParams, BaseWriterVectorStoreParams):\n    \"\"\"Parameters for OpenSearch vector store writer.\"\"\"\n\n    dimension: int = 1536\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/pgvector/pgvector/","title":"Pgvector","text":""},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore","title":"<code>PGVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using pgvector.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>class PGVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"Vector store using pgvector.\"\"\"\n\n    def __init__(\n        self,\n        connection: PostgreSQL | str | None = None,\n        client: Optional[\"PsycopgConnection\"] = None,\n        create_extension: bool = True,\n        table_name: str = DEFAULT_TABLE_NAME,\n        schema_name: str = DEFAULT_SCHEMA_NAME,\n        dimension: int = 1536,\n        vector_function: PGVectorVectorFunction = PGVectorVectorFunction.COSINE_SIMILARITY,\n        index_method: PGVectorIndexMethod = PGVectorIndexMethod.EXACT,\n        index_name: str | None = None,\n        create_if_not_exist: bool = False,\n        content_key: str = \"content\",\n        embedding_key: str = \"embedding\",\n        keyword_index_name: str | None = None,\n        language: str = DEFAULT_LANGUAGE,\n        ivfflat_lists: int | None = DEFAULT_IVFFLAT_LISTS,\n        hnsw_m: int = DEFAULT_HNSW_M,\n        hnsw_ef_construction: int = DEFAULT_HNSW_EF_CONSTRUCTION,\n        set_runtime_params: bool = True,\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize a PGVectorStore instance.\n\n        Args:\n            connection (PostgreSQL | str): PostgreSQL connection instance. Defaults to None.\n            client (Optional[PsycopgConnection]): Psycopg connection instance. Defaults to None.\n            create_extension (bool): Whether to create the vector extension (if it does not exist). Defaults to True.\n            table_name (str): Name of the table in the database. Defaults to \"default\".\n            schema_name (str): Name of the schema in the database.\n            dimension (int): Dimension of the embeddings. Defaults to 1536.\n            vector_function (PGVectorVectorFunction): The vector function to use for similarity calculations.\n            index_method (PGVectorIndexMethod): The index method to use for the vector store.\n            index_name (str | None): Name of the index to create.\n            create_if_not_exist (bool): Whether to create the table and index if they do not exist.\n            content_key (str): The field used to store content in the storage.\n            embedding_key (str): The field used to store embeddings in the storage.\n            keyword_index_name (str | None): Name of the keyword index.\n            language (str): Language for full-text search.\n            ivfflat_lists (int | None): Number of lists for IVFFLAT index. Auto-calculated if None.\n            hnsw_m (int): Number of connections per layer for HNSW index.\n            hnsw_ef_construction (int): Size of the dynamic candidate list for HNSW index construction.\n            set_runtime_params (bool): Whether to automatically set pgvector runtime parameters for optimal performance.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        if vector_function not in PGVectorVectorFunction:\n            raise ValueError(f\"vector_function must be one of {list(PGVectorVectorFunction)}\")\n        if index_method is not None and index_method not in PGVectorIndexMethod:\n            raise ValueError(f\"index_method must be one of {list(PGVectorIndexMethod)}\")\n\n        if client is None or (hasattr(client, \"closed\") and client.closed):\n            if isinstance(connection, str):\n                self.connection_string = connection\n                self._conn = psycopg.connect(self.connection_string)\n            elif isinstance(connection, PostgreSQL):\n                self._conn = connection.connect()\n                self.connection_string = connection.conn_params\n            else:\n                raise ValueError(\"Either 'connection' (str or PostgreSQL) or 'client' must be provided\")\n            self.client = self._conn\n        else:\n            self.client = client\n            self._conn = client\n            self.connection_string = None\n\n        self.create_extension = create_extension\n        if self.create_extension:\n            self._conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            self._conn.commit()\n\n        register_vector(self._conn)\n\n        self.table_name = table_name\n        self.schema_name = schema_name\n        self.dimension = dimension\n        self.index_method = index_method\n        self.vector_function = vector_function\n        self.index_name = index_name or f\"{self.table_name}_{self.index_method}_index\"\n        self.keyword_index_name = keyword_index_name or f\"{self.table_name}_keyword_index\"\n        self.language = language\n        self.ivfflat_lists = ivfflat_lists\n        self.hnsw_m = hnsw_m\n        self.hnsw_ef_construction = hnsw_ef_construction\n        self.set_runtime_params = set_runtime_params\n\n        self.content_key = content_key\n        self.embedding_key = embedding_key\n\n        self.create_if_not_exist = create_if_not_exist\n\n        if (\n            self.index_method == PGVectorIndexMethod.IVFFLAT\n            and self.vector_function == PGVectorVectorFunction.L1_DISTANCE\n        ):\n            msg = \"IVFFLAT index does not support L1 distance metric\"\n            raise VectorStoreException(msg)\n\n        if self.create_if_not_exist:\n            with self._get_connection() as conn:\n                # Check if table exists before creating it\n                table_exists = self._check_if_table_exists(conn)\n\n                self._create_schema(conn)\n                self._create_tables(conn)\n                if self.index_method in [PGVectorIndexMethod.IVFFLAT, PGVectorIndexMethod.HNSW]:\n                    self._create_index(conn)\n                self._create_keyword_index(conn)\n\n                if not table_exists:\n                    self._track_collection(f\"{self.schema_name}.{self.table_name}\")\n        else:\n            try:\n                if not self._check_if_schema_exists(self._conn):\n                    msg = f\"Schema '{self.schema_name}' does not exist\"\n                    raise VectorStoreException(msg)\n                if not self._check_if_table_exists(self._conn):\n                    msg = f\"Table '{self.table_name}' does not exist\"\n                    raise VectorStoreException(msg)\n            except Exception:\n                self._safe_rollback()\n                raise\n\n        logger.debug(f\"PGVectorStore initialized with table_name: {self.table_name}\")\n\n    def _sanitize_error_message(self, error_msg: str) -&gt; str:\n        \"\"\"\n        Sanitize error messages to prevent exposure of credentials.\n\n        Args:\n            error_msg: Raw error message from database operations.\n\n        Returns:\n            Sanitized error message safe for logging.\n        \"\"\"\n        sanitized = error_msg\n\n        # Remove connection string patterns\n        sanitized = re.sub(r'postgresql://[^\\'\"\\s]+', f\"postgresql://{SANITIZED_VALUE_PLACEHOLDER}\", sanitized)\n        sanitized = re.sub(r'password=[^\\'\"\\s&amp;]+', f\"password={SANITIZED_VALUE_PLACEHOLDER}\", sanitized)\n        sanitized = re.sub(r'user=[^\\'\"\\s&amp;]+', f\"user={SANITIZED_VALUE_PLACEHOLDER}\", sanitized)\n\n        if len(sanitized) &gt; 500:\n            sanitized = sanitized[:497] + \"...\"\n\n        return sanitized\n\n    def _prepare_filters(self, filters: dict[str, Any] | None) -&gt; tuple[SQL, tuple]:\n        \"\"\"\n        Prepare filters for SQL queries.\n\n        Args:\n            filters: Dictionary of filters to convert to SQL WHERE clause.\n\n        Returns:\n            Tuple of (SQL where clause, parameter tuple).\n        \"\"\"\n        if not filters:\n            return SQL(\"\"), ()\n        return _convert_filters_to_query(filters)\n\n    def _set_pgvector_runtime_params(self, conn: \"PsycopgConnection\", top_k: int) -&gt; None:\n        \"\"\"\n        Set pgvector runtime parameters for optimal query performance.\n\n        Args:\n            conn: Database connection to set parameters on.\n            top_k: Number of results requested, used to tune parameters.\n        \"\"\"\n        if not self.set_runtime_params:\n            return\n\n        if self.index_method == PGVectorIndexMethod.IVFFLAT:\n            # For IVFFLAT, probes should be proportional to top_k but capped\n            probes = max(top_k // 10, 1)\n            probes = min(probes, 100)\n            conn.execute(f\"SET ivfflat.probes = {probes}\")\n        elif self.index_method == PGVectorIndexMethod.HNSW:\n            # For HNSW, ef_search should be at least top_k\n            ef_search = max(top_k * 2, 40)\n            ef_search = min(ef_search, 1000)\n            conn.execute(f\"SET hnsw.ef_search = {ef_search}\")\n\n    @contextmanager\n    def _get_connection(self):\n        \"\"\"Context manager for handling a single connection\"\"\"\n        if self._conn is None or self._conn.closed:\n            if not self.connection_string:\n                raise VectorStoreException(\"Connection is closed and no connection string available for reconnection\")\n            self._conn = psycopg.connect(self.connection_string)\n            register_vector(self._conn)\n            self.client = self._conn\n        try:\n            yield self._conn\n        except Exception as e:\n            self._safe_rollback()\n            raise e\n\n    def _check_if_schema_exists(self, conn: psycopg.Connection) -&gt; bool:\n        \"\"\"\n        Check if the schema exists in the database.\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n\n        Returns:\n            bool: True if the schema exists, False otherwise.\n        \"\"\"\n\n        query = SQL(\n            \"\"\"\n            SELECT EXISTS (\n                SELECT 1\n                FROM information_schema.schemata\n                WHERE schema_name = %s\n            );\n            \"\"\"\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            result = self._execute_sql_query(query, (self.schema_name,), cursor=cur).fetchone()\n            return bool(result[\"exists\"]) if result else False\n\n    def _check_if_table_exists(self, conn: psycopg.Connection) -&gt; bool:\n        \"\"\"\n        Check if the table exists in the database.\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n\n        query = SQL(\n            \"\"\"\n            SELECT EXISTS (\n                SELECT 1\n                FROM information_schema.tables\n                WHERE table_schema = %s\n                AND table_name = %s\n            );\n            \"\"\"\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            result = self._execute_sql_query(query, (self.schema_name, self.table_name), cursor=cur).fetchone()\n            return bool(result[\"exists\"]) if result else False\n\n    def _safe_rollback(self) -&gt; None:\n        \"\"\"\n        Safely attempt to rollback the current transaction.\n        \"\"\"\n        if self._conn and not self._conn.closed:\n            try:\n                self._conn.rollback()\n            except Exception as rollback_error:\n                logger.warning(f\"Failed to rollback transaction: {rollback_error}\")\n\n    def _handle_db_exception(self, e: BaseException, log_context: str = \"\") -&gt; None:\n        \"\"\"\n        Handle database exceptions: rollback, sanitize, log, and re-raise.\n\n        Args:\n            e: The exception that was raised.\n            log_context: Optional suffix for the generic error log message (e.g. \" during batch insert\").\n\n        Raises:\n            VectorStoreException: For connection, privilege, syntax, and unexpected errors.\n            ValueError: For invalid document data (e.g. wrong dimension).\n        \"\"\"\n        self._safe_rollback()\n        sanitized = self._sanitize_error_message(str(e))\n        if isinstance(e, psycopg_errors.OperationalError):\n            logger.warning(f\"Database connection error: {sanitized}\")\n            raise VectorStoreException(\"Database connection error\") from e\n        if isinstance(e, psycopg_errors.UniqueViolation):\n            logger.debug(f\"Duplicate key violation: {sanitized}\")\n            raise VectorStoreException(\"Document already exists\") from e\n        if isinstance(e, psycopg_errors.DataError):\n            logger.error(f\"Data validation error: {sanitized}\")\n            raise ValueError(\"Invalid document data provided\") from e\n        if isinstance(e, psycopg_errors.SyntaxError):\n            logger.error(f\"SQL syntax error: {sanitized}\")\n            raise VectorStoreException(\"Database query syntax error\") from e\n        if isinstance(e, psycopg_errors.InsufficientPrivilege):\n            logger.error(f\"Insufficient privileges: {sanitized}\")\n            raise VectorStoreException(\"Insufficient database privileges\") from e\n        logger.error(f\"Unexpected database error{log_context}: {sanitized}\", exc_info=True)\n        raise VectorStoreException(\"Unexpected database error\") from e\n\n    def _validate_query_embedding(self, query_embedding: list[float]) -&gt; None:\n        \"\"\"Validate query_embedding is non-empty and matches store dimension. Raises ValueError if not.\"\"\"\n        if not query_embedding:\n            raise ValueError(\"query_embedding must be a non-empty list\")\n        if len(query_embedding) != self.dimension:\n            raise ValueError(f\"query_embedding must be of dimension {self.dimension}, got {len(query_embedding)}\")\n\n    def _validate_top_k(self, top_k: int) -&gt; None:\n        \"\"\"Validate top_k is positive. Raises ValueError if not.\"\"\"\n        if top_k &lt;= 0:\n            raise ValueError(f\"top_k must be positive, got {top_k}\")\n\n    def _execute_sql_query(self, sql_query: Any, params: tuple | None = None, cursor: Cursor | None = None) -&gt; Cursor:\n        \"\"\"\n        Internal method to execute a SQL query.\n\n        Args:\n            sql_query (Any): The SQL query to execute.\n            params (tuple | None): The parameters to pass to the query. Defaults to None.\n            cursor (Cursor | None): The cursor to use for the query. Defaults to None.\n\n        Raises:\n            VectorStoreException: If an error occurs while executing the query.\n            ValueError: If invalid data is provided.\n\n        Returns:\n            Cursor: The cursor with query results.\n        \"\"\"\n\n        params = params or ()\n\n        try:\n            result = cursor.execute(sql_query, params)\n            return result\n        except (\n            psycopg_errors.OperationalError,\n            psycopg_errors.UniqueViolation,\n            psycopg_errors.DataError,\n            psycopg_errors.SyntaxError,\n            psycopg_errors.InsufficientPrivilege,\n            Exception,\n        ) as e:\n            self._handle_db_exception(e)\n\n    def _create_tables(\n        self,\n        conn: psycopg.Connection,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Internal method to create the tables in the database (if they do not exist).\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n            content_key (str | None): The field used to store content in the storage. Defaults to None.\n            embedding_key (str | None): The field used to store embeddings in the storage. Defaults to None.\n        \"\"\"\n\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        query = SQL(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR(128) PRIMARY KEY,\n                {content_key} TEXT,\n                metadata JSONB,\n                {embedding_key} vector({dimension})\n            );\n            \"\"\"\n        ).format(\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            content_key=Identifier(content_key),\n            embedding_key=Identifier(embedding_key),\n            dimension=self.dimension,\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def _drop_tables(self, conn: psycopg.Connection) -&gt; None:\n        \"\"\"\n        Internal method to drop the tables in the database (if they exist).\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n        \"\"\"\n\n        query = SQL(\n            \"\"\"\n            DROP TABLE IF EXISTS {schema_name}.{table_name};\n            \"\"\"\n        ).format(\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def _create_index(\n        self,\n        conn: psycopg.Connection,\n        embedding_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Internal method to create the index in the database (if it does not exist).\n        Should only be called if the index method is either `ivfflat` or `hnsw`.\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n            embedding_key (str | None): The field used to store embeddings in the storage. Defaults to None.\n\n        Raises:\n            ValueError: If the index method is not valid.\n        \"\"\"\n\n        embedding_key = embedding_key or self.embedding_key\n\n        if self.index_method not in PGVectorIndexMethod:\n            msg = f\"Invalid index method: {self.index_method}\"\n            raise ValueError(msg)\n\n        vector_ops = VECTOR_FUNCTION_TO_POSTGRESQL_OPS[self.vector_function]\n\n        if self.index_method == PGVectorIndexMethod.IVFFLAT:\n            lists = self.ivfflat_lists\n            if lists is None:\n                try:\n                    # Estimate based on the amount of rows\n                    row_count = self.count_documents()\n                    lists = max(min(row_count // 1000, 1000), 10)\n                except Exception:\n                    lists = 100\n\n            query = SQL(\n                \"\"\"\n                CREATE INDEX IF NOT EXISTS {index_name}\n                ON {schema_name}.{table_name} USING ivfflat ({embedding} {vector_ops})\n                WITH (lists = {lists})\n                \"\"\"\n            ).format(\n                index_name=Identifier(self.index_name),\n                schema_name=Identifier(self.schema_name),\n                table_name=Identifier(self.table_name),\n                embedding=Identifier(embedding_key),\n                vector_ops=Identifier(vector_ops),\n                lists=SQLLiteral(lists),\n            )\n        elif self.index_method == PGVectorIndexMethod.HNSW:\n            query = SQL(\n                \"\"\"\n                CREATE INDEX IF NOT EXISTS {index_name}\n                ON {schema_name}.{table_name} USING hnsw ({embedding} {vector_ops})\n                WITH (m = {m}, ef_construction = {ef_construction})\n                \"\"\"\n            ).format(\n                index_name=Identifier(self.index_name),\n                schema_name=Identifier(self.schema_name),\n                table_name=Identifier(self.table_name),\n                embedding=Identifier(embedding_key),\n                vector_ops=Identifier(vector_ops),\n                m=SQLLiteral(self.hnsw_m),\n                ef_construction=SQLLiteral(self.hnsw_ef_construction),\n            )\n        else:\n            # EXACT search\n            return\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def _create_keyword_index(\n        self,\n        conn: psycopg.Connection,\n        content_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Internal method to create the keyword index in the database (if it does not exist).\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n            content_key (str | None): The field used to store content in the storage. Defaults to None.\n        \"\"\"\n\n        content_key = content_key or self.content_key\n\n        create_keyword_index_query = SQL(\n            \"\"\"\n            CREATE INDEX IF NOT EXISTS {index_name}\n            ON {schema_name}.{table_name}\n            USING gin(to_tsvector({language}, {content_key}));\n            \"\"\"\n        ).format(\n            index_name=Identifier(self.keyword_index_name),\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            content_key=Identifier(content_key),\n            language=SQLLiteral(self.language),\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(create_keyword_index_query, cursor=cur)\n            conn.commit()\n\n    def _drop_index(self, conn: psycopg.Connection) -&gt; None:\n        \"\"\"\n        Internal method to drop the index in the database (if it exists).\n        Should only be called if the index method is either `ivfflat` or `hnsw`.\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n\n        Raises:\n            ValueError: If the index method is not valid.\n        \"\"\"\n        if self.index_method not in PGVectorIndexMethod:\n            msg = f\"Invalid index method: {self.index_method}\"\n            raise ValueError(msg)\n\n        query = SQL(\n            \"\"\"\n            DROP INDEX IF EXISTS {index_name};\n            \"\"\"\n        ).format(\n            index_name=Identifier(self.index_name),\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def _create_schema(self, conn: psycopg.Connection) -&gt; None:\n        \"\"\"\n        Internal method to create the schema in the database (if it does not exist).\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n        \"\"\"\n\n        query = SQL(\n            \"\"\"\n            CREATE SCHEMA IF NOT EXISTS {schema_name};\n            \"\"\"\n        ).format(\n            schema_name=Identifier(self.schema_name),\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def _drop_schema(self, conn: psycopg.Connection) -&gt; None:\n        \"\"\"\n        Internal method to drop the schema in the database (if it exists).\n\n        Args:\n            conn (psycopg.Connection): The connection to the database.\n        \"\"\"\n\n        query = SQL(\n            \"\"\"\n            DROP SCHEMA IF EXISTS {schema_name} CASCADE;\n            \"\"\"\n        ).format(\n            schema_name=Identifier(self.schema_name),\n        )\n\n        with conn.cursor(row_factory=dict_row) as cur:\n            self._execute_sql_query(query, cursor=cur)\n            conn.commit()\n\n    def delete_collection(self, collection_name: str | None = None) -&gt; None:\n        \"\"\"\n        Delete the collection in the database.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            with self._get_connection() as conn:\n                self._drop_tables(conn)\n                if self.schema_name and self.schema_name != DEFAULT_SCHEMA_NAME:\n                    self._drop_schema(conn)\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{self.schema_name}.{self.table_name}': {e}\")\n            raise\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the store.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                query = SQL(\"SELECT COUNT(*) FROM {schema_name}.{table_name}\").format(\n                    schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n                )\n                result = self._execute_sql_query(query, cursor=cur)\n                row = result.fetchone()\n                if row is None:\n                    return 0\n                return int(row.get(\"count\", 0))\n\n    def write_documents(\n        self, documents: list[Document], content_key: str | None = None, embedding_key: str | None = None\n    ) -&gt; int:\n        \"\"\"\n        Write documents to the pgvector vector store.\n\n        Args:\n            documents (list[Document]): List of Document objects to write.\n            content_key (str | None): The field used to store content in the storage. Defaults to None.\n            embedding_key (str | None): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            int: Number of documents successfully written.\n\n        Raises:\n            ValueError: If documents are not of type Document or have invalid embeddings.\n        \"\"\"\n        if not documents:\n            return 0\n\n        if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n            msg = \"param 'documents' must contain a list of objects of type Document\"\n            raise ValueError(msg)\n\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        document_ids = []\n        for doc in documents:\n            if doc.embedding is not None and len(doc.embedding) != self.dimension:\n                raise ValueError(\n                    f\"Document {doc.id} embedding dimension {len(doc.embedding)} \"\n                    f\"does not match configured dimension {self.dimension}\"\n                )\n            document_ids.append(doc.id)\n\n        # Prepare batch data\n        batch_data = [(doc.id, doc.content, Jsonb(doc.metadata), doc.embedding) for doc in documents]\n\n        query = SQL(\n            \"\"\"\n            INSERT INTO {schema_name}.{table_name} (id, {content_key}, metadata, {embedding_key})\n            VALUES (%s, %s, %s, %s)\n            ON CONFLICT (id) DO UPDATE SET\n                {content_key} = EXCLUDED.{content_key},\n                metadata = EXCLUDED.metadata,\n                {embedding_key} = EXCLUDED.{embedding_key}\n            \"\"\"\n        ).format(\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            content_key=Identifier(content_key),\n            embedding_key=Identifier(embedding_key),\n        )\n\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                try:\n                    cur.executemany(query, batch_data)\n                    conn.commit()\n                except (\n                    psycopg_errors.OperationalError,\n                    psycopg_errors.UniqueViolation,\n                    psycopg_errors.DataError,\n                    psycopg_errors.SyntaxError,\n                    psycopg_errors.InsufficientPrivilege,\n                    Exception,\n                ) as e:\n                    self._handle_db_exception(e, log_context=\" during batch insert\")\n\n        self._track_documents(document_ids)\n        return len(documents)\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the pgvector vector store using filters.\n\n        Args:\n            filters (dict[str, Any]): Filters to select documents to delete.\n        \"\"\"\n        if filters:\n            with self._get_connection() as conn:\n                with conn.cursor(row_factory=dict_row) as cur:\n                    sql_where_clause, params = self._prepare_filters(filters)\n                    query = SQL(\"DELETE FROM {schema_name}.{table_name}\").format(\n                        schema_name=Identifier(self.schema_name),\n                        table_name=Identifier(self.table_name),\n                    )\n                    query += sql_where_clause\n                    self._execute_sql_query(query, params, cursor=cur)\n                    conn.commit()\n        else:\n            logger.warning(\"No filters provided. No documents will be deleted.\")\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided file ID.\n        File ID should be located in the metadata of the document.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_pgvector_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def delete_documents_by_file_ids(self, file_ids: list[str], batch_size: int = 500) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided list of file IDs.\n        File IDs should be located in the metadata of the documents.\n\n        Args:\n            file_ids (list[str]): The list of file IDs to filter by.\n            batch_size (int): Maximum number of file IDs to process in a single batch. Defaults to 500.\n        \"\"\"\n        if not file_ids:\n            logger.warning(\"No file IDs provided. No documents will be deleted.\")\n            return\n\n        if len(file_ids) &gt; batch_size:\n            for i in range(0, len(file_ids), batch_size):\n                batch = file_ids[i : i + batch_size]\n                filters = create_pgvector_file_ids_filter(batch)\n                self.delete_documents_by_filters(filters)\n                logger.debug(f\"Deleted documents batch {i//batch_size + 1} with {len(batch)} file IDs\")\n        else:\n            filters = create_pgvector_file_ids_filter(file_ids)\n            self.delete_documents_by_filters(filters)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the pgvector vector store.\n\n        Args:\n            document_ids (list[str]): List of document IDs to delete. Defaults to None.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all:\n            with self._get_connection() as conn:\n                with conn.cursor(row_factory=dict_row) as cur:\n                    query = SQL(\"DELETE FROM {schema_name}.{table_name}\").format(\n                        schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n                    )\n                    self._execute_sql_query(query, cursor=cur)\n                    conn.commit()\n        else:\n            if not document_ids:\n                logger.warning(\"No document IDs provided. No documents will be deleted.\")\n            else:\n                with self._get_connection() as conn:\n                    with conn.cursor(row_factory=dict_row) as cur:\n                        query = SQL(\"DELETE FROM {schema_name}.{table_name} WHERE id = ANY(%s::text[])\").format(\n                            schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n                        )\n                        self._execute_sql_query(query, (document_ids,), cursor=cur)\n                        conn.commit()\n\n    def list_documents(\n        self,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n        offset: int = 0,\n        limit: int | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        List documents in the pgvector vector store with optional pagination.\n\n        Args:\n            include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n            content_key (str): The field used to store content in the storage. Defaults to None.\n            embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n            offset (int): Number of documents to skip. Defaults to 0.\n            limit (int | None): Maximum number of documents to return. If None, returns all documents.\n\n        Returns:\n            list[Document]: List of Document objects retrieved.\n        \"\"\"\n        if offset &lt; 0:\n            msg = f\"offset must be non-negative, got {offset}\"\n            raise ValueError(msg)\n        if limit is not None and limit &lt;= 0:\n            msg = f\"limit must be positive, got {limit}\"\n            raise ValueError(msg)\n\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        select_columns = [SQL(\"id\"), Identifier(content_key), SQL(\"metadata\")]\n        if include_embeddings:\n            select_columns.append(Identifier(embedding_key))\n\n        query = SQL(\"SELECT {} FROM {}.{} ORDER BY id\").format(\n            SQL(\", \").join(select_columns),\n            Identifier(self.schema_name),\n            Identifier(self.table_name),\n        )\n\n        if limit is not None:\n            query = query + SQL(\" LIMIT {} OFFSET {}\").format(SQLLiteral(limit), SQLLiteral(offset))\n        elif offset &gt; 0:\n            query = query + SQL(\" OFFSET {}\").format(SQLLiteral(offset))\n\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                result = self._execute_sql_query(query, cursor=cur)\n                records = result.fetchall()\n\n                documents = self._convert_query_result_to_documents(records)\n                return documents\n\n    def _convert_query_result_to_documents(\n        self,\n        query_result: dict[str, Any],\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Convert pgvector query results to Document objects.\n\n        Args:\n            query_result (dict[str, Any]): The query result from pgvector.\n            content_key (str): The field used to store content in the storage. Defaults to None.\n            embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            list[Document]: List of Document objects created from the query result.\n        \"\"\"\n        documents = []\n\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        for doc in query_result:\n            document = Document(\n                id=doc[\"id\"],\n                content=doc[content_key],\n                metadata=doc[\"metadata\"],\n            )\n\n            if doc.get(embedding_key) is not None:\n                document.embedding = self._convert_pg_embedding_to_list(doc[embedding_key])\n            else:\n                document.embedding = None\n\n            if doc.get(\"score\") is not None:\n                document.score = float(doc[\"score\"])\n            else:\n                document.score = None\n\n            documents.append(document)\n        return documents\n\n    def _convert_pg_embedding_to_list(self, pg_embedding: Any) -&gt; list[float]:\n        \"\"\"\n        Helper method to convert a pgvector embedding type to a list of floats.\n        e.g. '[0.1,0.2,0.3]' -&gt; [0.1, 0.2, 0.3]\n\n        Args:\n            pg_embedding (Any): The pgvector embedding.\n\n        Returns:\n            list[float]: The embedding as a list of floats.\n        \"\"\"\n\n        if isinstance(pg_embedding, str):\n            return [float(x) for x in pg_embedding.strip(\"[]\").split(\",\") if x]\n        return pg_embedding\n\n    def _convert_query_embedding_to_pgvector_format(self, query_embedding: list[float]) -&gt; str:\n        \"\"\"\n        Helper method to convert query embedding to pgvector format.\n        e.g. [0.1, 0.2, 0.3] -&gt; '[0.1,0.2,0.3]'\n\n        Args:\n            query_embedding (list[float]): The query embedding vector.\n\n        Returns:\n            str: The query embedding in pgvector format (e.g. '[0.1,0.2,0.3]').\n        \"\"\"\n        return f\"'[{','.join(str(el) for el in query_embedding)}]'\"\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents similar to the given query embedding.\n\n        Args:\n            query_embedding (list[float]): The query embedding vector.\n            top_k (int): Maximum number of documents to retrieve. Defaults to 10.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            filters (dict[str, Any] | None): Filters for the query. Defaults to None.\n            content_key (str): The field used to store content in the storage. Defaults to None.\n            embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            list[Document]: List of retrieved Document objects.\n\n        Raises:\n            ValueError: If query_embedding is empty or has incorrect dimension.\n        \"\"\"\n        self._validate_query_embedding(query_embedding)\n        self._validate_top_k(top_k)\n\n        vector_function = self.vector_function\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        if vector_function not in PGVectorVectorFunction:\n            msg = f\"Invalid vector function: {vector_function}\"\n            raise ValueError(msg)\n\n        query_embedding = self._convert_query_embedding_to_pgvector_format(query_embedding)\n\n        # Generate the score calculation based on the vector function\n        score_definition = VECTOR_FUNCTION_TO_SCORE_DEFINITION[vector_function].format(\n            embedding_key=embedding_key, query_embedding=query_embedding\n        )\n        score_definition = f\"{score_definition} AS score\"\n\n        # Do not select the embeddings if exclude_document_embeddings is True\n        select_fields = f\"id, {content_key}, metadata\" if exclude_document_embeddings else \"*\"\n\n        # Build the base SELECT query with score\n        base_select = SQL(\"SELECT {fields}, {score} FROM {schema_name}.{table_name}\").format(\n            fields=SQL(select_fields),\n            score=SQL(score_definition),\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n        )\n\n        # Handle filters if they exist\n        where_clause, params = self._prepare_filters(filters)\n\n        # Determine sort order based on vector function type\n        is_distance_metric = vector_function in [\"l2_distance\", \"l1_distance\"]\n\n        # Sort by score in ascending order if using a distance metric\n        # as the smaller the distance, the more similar the vectors are\n        sort_order = \"ASC\" if is_distance_metric else \"DESC\"\n\n        # Build the ORDER BY and LIMIT clause\n        order_by = SQL(\" ORDER BY score {sort_order} LIMIT {limit}\").format(\n            sort_order=SQL(sort_order), limit=SQLLiteral(top_k)\n        )\n\n        # Combine all parts into final query\n        sql_query = base_select + where_clause + order_by\n\n        with self._get_connection() as conn:\n            # Set runtime params for the current index method\n            self._set_pgvector_runtime_params(conn, top_k)\n\n            with conn.cursor(row_factory=dict_row) as cur:\n                result = self._execute_sql_query(sql_query, params, cursor=cur)\n                records = result.fetchall()\n\n                documents = self._convert_query_result_to_documents(records)\n                return documents\n\n    def _keyword_retrieval(\n        self,\n        query: str,\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n        filters: dict[str, Any] | None = None,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents similar to the given query using keyword search.\n\n        Args:\n            query (str): The query string.\n            top_k (int): Maximum number of documents to retrieve. Defaults to 10.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            filters (dict[str, Any] | None): Filters for the query. Defaults to None.\n            content_key (str): The field used to store content in the storage. Defaults to None.\n            embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            list[Document]: List of retrieved Document objects.\n\n        Raises:\n            ValueError: If query is empty.\n        \"\"\"\n        if not query:\n            raise ValueError(\"query must be provided for keyword retrieval\")\n\n        self._validate_top_k(top_k)\n\n        content_key = content_key or self.content_key\n\n        # Do not select the embeddings if exclude_document_embeddings is True\n        select_fields = f\"id, {content_key}, metadata\" if exclude_document_embeddings else \"*\"\n\n        # Build the base SELECT query with score\n        base_select = SQL(\n            \"\"\"\n            SELECT {fields}, ts_rank_cd(to_tsvector({language}, {content_key}), query) AS score\n            FROM {schema_name}.{table_name}, plainto_tsquery({language}, %s) query\n            WHERE to_tsvector({language}, {content_key}) @@ query\n            \"\"\"\n        ).format(\n            fields=SQL(select_fields),\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            language=SQLLiteral(self.language),\n            content_key=Identifier(content_key),\n        )\n\n        # Handle filters if they exist\n        if filters:\n            where_clause, params = _convert_filters_to_query(filters, operator=\"AND\")\n        else:\n            where_clause = SQL(\"\")\n            params = ()\n\n        # Build the ORDER BY and LIMIT clause\n        order_by = SQL(\" ORDER BY score DESC LIMIT {limit}\").format(limit=SQLLiteral(top_k))\n\n        # Combine all parts into final query\n        sql_query = base_select + where_clause + order_by\n\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                result = self._execute_sql_query(sql_query, (query, *params), cursor=cur)\n                records = result.fetchall()\n\n                documents = self._convert_query_result_to_documents(records)\n                return documents\n\n    def _hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: list[float],\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n        keyword_rank_constant: int = DEFAULT_KEYWORD_RANK_CONSTANT,\n        top_k_subquery_multiplier: int = DEFAULT_TOP_K_SUBQUERY_MULTIPLIER,\n        filters: dict[str, Any] | None = None,\n        alpha: float = 0.5,\n        content_key: str | None = None,\n        embedding_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents similar to the given query using a hybrid approach.\n\n        Args:\n            query (str): The query string.\n            query_embedding (list[float]): The query embedding vector.\n            top_k (int): Maximum number of documents to retrieve. Defaults to 10.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            keyword_rank_constant (int): Constant used in RRF (Reciprocal Rank Fusion) score calculation.\n            top_k_subquery_multiplier (int): Multiplier for subquery limits to ensure enough candidates.\n            filters (dict[str, Any] | None): Filters for the query. Defaults to None.\n            alpha (float): Weight for semantic search (0-1). 0 = pure keyword, 1 = pure semantic, 0.5 = balanced.\n            content_key (str): The field used to store content in the storage. Defaults to None.\n            embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n\n        Returns:\n            list[Document]: List of retrieved Document objects.\n\n        Raises:\n            ValueError: If query is empty, query_embedding is None/empty, or parameters are invalid.\n        \"\"\"\n\n        if not query:\n            raise ValueError(\"query must be provided for hybrid retrieval\")\n\n        self._validate_query_embedding(query_embedding)\n        self._validate_top_k(top_k)\n\n        if not 0 &lt;= alpha &lt;= 1:\n            raise ValueError(f\"alpha must be between 0 and 1, got {alpha}\")\n\n        vector_function = self.vector_function\n        content_key = content_key or self.content_key\n        embedding_key = embedding_key or self.embedding_key\n\n        # If alpha is 0, perform purely keyword search\n        if alpha == 0:\n            return self._keyword_retrieval(\n                query,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                filters=filters,\n                content_key=content_key,\n                embedding_key=embedding_key,\n            )\n        # If alpha is 1, perform purely embedding search\n        elif alpha == 1:\n            return self._embedding_retrieval(\n                query_embedding,\n                top_k=top_k,\n                exclude_document_embeddings=exclude_document_embeddings,\n                filters=filters,\n                content_key=content_key,\n                embedding_key=embedding_key,\n            )\n\n        query_embedding = self._convert_query_embedding_to_pgvector_format(query_embedding)\n\n        # Generate the score calculation based on the vector function\n        score_definition = VECTOR_FUNCTION_TO_SCORE_DEFINITION[vector_function].format(\n            embedding_key=embedding_key, query_embedding=query_embedding\n        )\n\n        # Determine sort order based on vector function type\n        is_distance_metric = vector_function in [\"l2_distance\", \"l1_distance\"]\n        sort_order = \"ASC\" if is_distance_metric else \"DESC\"\n\n        # Optimize subquery limit based on alpha\n        if alpha &lt; 0.1 or alpha &gt; 0.9:\n            subquery_limit = max(top_k * 2, 20)\n        else:\n            subquery_limit = max(top_k * top_k_subquery_multiplier, 50)\n\n        # Apply filters to avoid duplication\n        base_where_clause, params = self._prepare_filters(filters)\n        semantic_where_clause = base_where_clause\n        keyword_where_clause = _convert_filters_to_query(filters, operator=\"AND\")[0] if filters else SQL(\"\")\n\n        embedding_select = SQL(\"\") if exclude_document_embeddings else SQL(\", \") + Identifier(embedding_key)\n        semantic_search_query = SQL(\n            \"\"\"\n            WITH semantic_search AS (\n                SELECT id, {content_key}, metadata{embedding_select},\n                       RANK() OVER (ORDER BY {score_definition} {sort_order}) AS rank\n                FROM {schema_name}.{table_name}\n                {where_clause}\n                LIMIT {subquery_limit}\n            ),\n            \"\"\"\n        ).format(\n            content_key=Identifier(content_key),\n            embedding_select=embedding_select,\n            score_definition=SQL(score_definition),\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            where_clause=semantic_where_clause,\n            subquery_limit=SQLLiteral(subquery_limit),\n            sort_order=SQL(sort_order),\n        )\n\n        keyword_search_query = SQL(\n            \"\"\"\n            keyword_search AS (\n                SELECT id, {content_key}, metadata{embedding_select},\n                       RANK() OVER (ORDER BY ts_rank_cd(to_tsvector({language}, {content_key}), query) DESC) AS rank\n                FROM {schema_name}.{table_name}, plainto_tsquery({language}, {query}) query\n                WHERE to_tsvector({language}, {content_key}) @@ query\n                {where_clause}\n                LIMIT {subquery_limit}\n            )\n            \"\"\"\n        ).format(\n            content_key=Identifier(content_key),\n            embedding_select=embedding_select,\n            schema_name=Identifier(self.schema_name),\n            table_name=Identifier(self.table_name),\n            language=SQLLiteral(self.language),\n            query=SQLLiteral(query),\n            where_clause=keyword_where_clause,\n            subquery_limit=SQLLiteral(subquery_limit),\n        )\n\n        # Build the final query to merge the results and sort them by score\n\n        embedding_line = (\n            SQL(\"\")\n            if exclude_document_embeddings\n            else SQL(\n                \", COALESCE(semantic_search.{embedding_key}, keyword_search.{embedding_key}) AS {embedding_key}\"\n            ).format(embedding_key=Identifier(embedding_key))\n        )\n\n        merge_query = SQL(\n            \"\"\"\n            SELECT\n                COALESCE(semantic_search.id, keyword_search.id) AS id,\n                COALESCE(semantic_search.{content_key}, keyword_search.{content_key}) AS {content_key},\n                COALESCE(semantic_search.metadata, keyword_search.metadata) AS metadata\n                {embedding_line},\n                COALESCE({alpha} / ({k} + semantic_search.rank), 0.0)\n            + COALESCE((1 - {alpha}) / ({k} + keyword_search.rank), 0.0)\n                AS score\n            FROM semantic_search\n            FULL OUTER JOIN keyword_search\n                ON semantic_search.id = keyword_search.id\n            ORDER BY score DESC\n            LIMIT {top_k}\n            \"\"\"\n        ).format(\n            content_key=Identifier(content_key),\n            embedding_line=embedding_line,\n            top_k=SQLLiteral(top_k),\n            alpha=SQLLiteral(alpha),\n            k=SQLLiteral(keyword_rank_constant),\n        )\n\n        sql_query = semantic_search_query + keyword_search_query + merge_query\n\n        params = params + params\n\n        with self._get_connection() as conn:\n            # Set runtime params for the current index method\n            self._set_pgvector_runtime_params(conn, top_k)\n\n            with conn.cursor(row_factory=dict_row) as cur:\n                result = self._execute_sql_query(sql_query, params, cursor=cur)\n                records = result.fetchall()\n\n                documents = self._convert_query_result_to_documents(records)\n                return documents\n\n    def close(self):\n        \"\"\"Close the connection to the PostgreSQL database.\"\"\"\n        if hasattr(self, \"_conn\") and self._conn is not None and not self._conn.closed:\n            self._conn.close()\n\n    def __del__(self):\n        \"\"\"Close the connection when the object is deleted.\"\"\"\n        self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.__del__","title":"<code>__del__()</code>","text":"<p>Close the connection when the object is deleted.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def __del__(self):\n    \"\"\"Close the connection when the object is deleted.\"\"\"\n    self.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.__init__","title":"<code>__init__(connection=None, client=None, create_extension=True, table_name=DEFAULT_TABLE_NAME, schema_name=DEFAULT_SCHEMA_NAME, dimension=1536, vector_function=PGVectorVectorFunction.COSINE_SIMILARITY, index_method=PGVectorIndexMethod.EXACT, index_name=None, create_if_not_exist=False, content_key='content', embedding_key='embedding', keyword_index_name=None, language=DEFAULT_LANGUAGE, ivfflat_lists=DEFAULT_IVFFLAT_LISTS, hnsw_m=DEFAULT_HNSW_M, hnsw_ef_construction=DEFAULT_HNSW_EF_CONSTRUCTION, set_runtime_params=True, dry_run_config=None)</code>","text":"<p>Initialize a PGVectorStore instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>PostgreSQL | str</code> <p>PostgreSQL connection instance. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[Connection]</code> <p>Psycopg connection instance. Defaults to None.</p> <code>None</code> <code>create_extension</code> <code>bool</code> <p>Whether to create the vector extension (if it does not exist). Defaults to True.</p> <code>True</code> <code>table_name</code> <code>str</code> <p>Name of the table in the database. Defaults to \"default\".</p> <code>DEFAULT_TABLE_NAME</code> <code>schema_name</code> <code>str</code> <p>Name of the schema in the database.</p> <code>DEFAULT_SCHEMA_NAME</code> <code>dimension</code> <code>int</code> <p>Dimension of the embeddings. Defaults to 1536.</p> <code>1536</code> <code>vector_function</code> <code>PGVectorVectorFunction</code> <p>The vector function to use for similarity calculations.</p> <code>COSINE_SIMILARITY</code> <code>index_method</code> <code>PGVectorIndexMethod</code> <p>The index method to use for the vector store.</p> <code>EXACT</code> <code>index_name</code> <code>str | None</code> <p>Name of the index to create.</p> <code>None</code> <code>create_if_not_exist</code> <code>bool</code> <p>Whether to create the table and index if they do not exist.</p> <code>False</code> <code>content_key</code> <code>str</code> <p>The field used to store content in the storage.</p> <code>'content'</code> <code>embedding_key</code> <code>str</code> <p>The field used to store embeddings in the storage.</p> <code>'embedding'</code> <code>keyword_index_name</code> <code>str | None</code> <p>Name of the keyword index.</p> <code>None</code> <code>language</code> <code>str</code> <p>Language for full-text search.</p> <code>DEFAULT_LANGUAGE</code> <code>ivfflat_lists</code> <code>int | None</code> <p>Number of lists for IVFFLAT index. Auto-calculated if None.</p> <code>DEFAULT_IVFFLAT_LISTS</code> <code>hnsw_m</code> <code>int</code> <p>Number of connections per layer for HNSW index.</p> <code>DEFAULT_HNSW_M</code> <code>hnsw_ef_construction</code> <code>int</code> <p>Size of the dynamic candidate list for HNSW index construction.</p> <code>DEFAULT_HNSW_EF_CONSTRUCTION</code> <code>set_runtime_params</code> <code>bool</code> <p>Whether to automatically set pgvector runtime parameters for optimal performance.</p> <code>True</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def __init__(\n    self,\n    connection: PostgreSQL | str | None = None,\n    client: Optional[\"PsycopgConnection\"] = None,\n    create_extension: bool = True,\n    table_name: str = DEFAULT_TABLE_NAME,\n    schema_name: str = DEFAULT_SCHEMA_NAME,\n    dimension: int = 1536,\n    vector_function: PGVectorVectorFunction = PGVectorVectorFunction.COSINE_SIMILARITY,\n    index_method: PGVectorIndexMethod = PGVectorIndexMethod.EXACT,\n    index_name: str | None = None,\n    create_if_not_exist: bool = False,\n    content_key: str = \"content\",\n    embedding_key: str = \"embedding\",\n    keyword_index_name: str | None = None,\n    language: str = DEFAULT_LANGUAGE,\n    ivfflat_lists: int | None = DEFAULT_IVFFLAT_LISTS,\n    hnsw_m: int = DEFAULT_HNSW_M,\n    hnsw_ef_construction: int = DEFAULT_HNSW_EF_CONSTRUCTION,\n    set_runtime_params: bool = True,\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize a PGVectorStore instance.\n\n    Args:\n        connection (PostgreSQL | str): PostgreSQL connection instance. Defaults to None.\n        client (Optional[PsycopgConnection]): Psycopg connection instance. Defaults to None.\n        create_extension (bool): Whether to create the vector extension (if it does not exist). Defaults to True.\n        table_name (str): Name of the table in the database. Defaults to \"default\".\n        schema_name (str): Name of the schema in the database.\n        dimension (int): Dimension of the embeddings. Defaults to 1536.\n        vector_function (PGVectorVectorFunction): The vector function to use for similarity calculations.\n        index_method (PGVectorIndexMethod): The index method to use for the vector store.\n        index_name (str | None): Name of the index to create.\n        create_if_not_exist (bool): Whether to create the table and index if they do not exist.\n        content_key (str): The field used to store content in the storage.\n        embedding_key (str): The field used to store embeddings in the storage.\n        keyword_index_name (str | None): Name of the keyword index.\n        language (str): Language for full-text search.\n        ivfflat_lists (int | None): Number of lists for IVFFLAT index. Auto-calculated if None.\n        hnsw_m (int): Number of connections per layer for HNSW index.\n        hnsw_ef_construction (int): Size of the dynamic candidate list for HNSW index construction.\n        set_runtime_params (bool): Whether to automatically set pgvector runtime parameters for optimal performance.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    if vector_function not in PGVectorVectorFunction:\n        raise ValueError(f\"vector_function must be one of {list(PGVectorVectorFunction)}\")\n    if index_method is not None and index_method not in PGVectorIndexMethod:\n        raise ValueError(f\"index_method must be one of {list(PGVectorIndexMethod)}\")\n\n    if client is None or (hasattr(client, \"closed\") and client.closed):\n        if isinstance(connection, str):\n            self.connection_string = connection\n            self._conn = psycopg.connect(self.connection_string)\n        elif isinstance(connection, PostgreSQL):\n            self._conn = connection.connect()\n            self.connection_string = connection.conn_params\n        else:\n            raise ValueError(\"Either 'connection' (str or PostgreSQL) or 'client' must be provided\")\n        self.client = self._conn\n    else:\n        self.client = client\n        self._conn = client\n        self.connection_string = None\n\n    self.create_extension = create_extension\n    if self.create_extension:\n        self._conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n        self._conn.commit()\n\n    register_vector(self._conn)\n\n    self.table_name = table_name\n    self.schema_name = schema_name\n    self.dimension = dimension\n    self.index_method = index_method\n    self.vector_function = vector_function\n    self.index_name = index_name or f\"{self.table_name}_{self.index_method}_index\"\n    self.keyword_index_name = keyword_index_name or f\"{self.table_name}_keyword_index\"\n    self.language = language\n    self.ivfflat_lists = ivfflat_lists\n    self.hnsw_m = hnsw_m\n    self.hnsw_ef_construction = hnsw_ef_construction\n    self.set_runtime_params = set_runtime_params\n\n    self.content_key = content_key\n    self.embedding_key = embedding_key\n\n    self.create_if_not_exist = create_if_not_exist\n\n    if (\n        self.index_method == PGVectorIndexMethod.IVFFLAT\n        and self.vector_function == PGVectorVectorFunction.L1_DISTANCE\n    ):\n        msg = \"IVFFLAT index does not support L1 distance metric\"\n        raise VectorStoreException(msg)\n\n    if self.create_if_not_exist:\n        with self._get_connection() as conn:\n            # Check if table exists before creating it\n            table_exists = self._check_if_table_exists(conn)\n\n            self._create_schema(conn)\n            self._create_tables(conn)\n            if self.index_method in [PGVectorIndexMethod.IVFFLAT, PGVectorIndexMethod.HNSW]:\n                self._create_index(conn)\n            self._create_keyword_index(conn)\n\n            if not table_exists:\n                self._track_collection(f\"{self.schema_name}.{self.table_name}\")\n    else:\n        try:\n            if not self._check_if_schema_exists(self._conn):\n                msg = f\"Schema '{self.schema_name}' does not exist\"\n                raise VectorStoreException(msg)\n            if not self._check_if_table_exists(self._conn):\n                msg = f\"Table '{self.table_name}' does not exist\"\n                raise VectorStoreException(msg)\n        except Exception:\n            self._safe_rollback()\n            raise\n\n    logger.debug(f\"PGVectorStore initialized with table_name: {self.table_name}\")\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.close","title":"<code>close()</code>","text":"<p>Close the connection to the PostgreSQL database.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection to the PostgreSQL database.\"\"\"\n    if hasattr(self, \"_conn\") and self._conn is not None and not self._conn.closed:\n        self._conn.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the store.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the store.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n\n    with self._get_connection() as conn:\n        with conn.cursor(row_factory=dict_row) as cur:\n            query = SQL(\"SELECT COUNT(*) FROM {schema_name}.{table_name}\").format(\n                schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n            )\n            result = self._execute_sql_query(query, cursor=cur)\n            row = result.fetchone()\n            if row is None:\n                return 0\n            return int(row.get(\"count\", 0))\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete the collection in the database.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None) -&gt; None:\n    \"\"\"\n    Delete the collection in the database.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        with self._get_connection() as conn:\n            self._drop_tables(conn)\n            if self.schema_name and self.schema_name != DEFAULT_SCHEMA_NAME:\n                self._drop_schema(conn)\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{self.schema_name}.{self.table_name}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the pgvector vector store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>List of document IDs to delete. Defaults to None.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the pgvector vector store.\n\n    Args:\n        document_ids (list[str]): List of document IDs to delete. Defaults to None.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all:\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                query = SQL(\"DELETE FROM {schema_name}.{table_name}\").format(\n                    schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n                )\n                self._execute_sql_query(query, cursor=cur)\n                conn.commit()\n    else:\n        if not document_ids:\n            logger.warning(\"No document IDs provided. No documents will be deleted.\")\n        else:\n            with self._get_connection() as conn:\n                with conn.cursor(row_factory=dict_row) as cur:\n                    query = SQL(\"DELETE FROM {schema_name}.{table_name} WHERE id = ANY(%s::text[])\").format(\n                        schema_name=Identifier(self.schema_name), table_name=Identifier(self.table_name)\n                    )\n                    self._execute_sql_query(query, (document_ids,), cursor=cur)\n                    conn.commit()\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the vector store based on the provided file ID. File ID should be located in the metadata of the document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided file ID.\n    File ID should be located in the metadata of the document.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_pgvector_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.delete_documents_by_file_ids","title":"<code>delete_documents_by_file_ids(file_ids, batch_size=500)</code>","text":"<p>Delete documents from the vector store based on the provided list of file IDs. File IDs should be located in the metadata of the documents.</p> <p>Parameters:</p> Name Type Description Default <code>file_ids</code> <code>list[str]</code> <p>The list of file IDs to filter by.</p> required <code>batch_size</code> <code>int</code> <p>Maximum number of file IDs to process in a single batch. Defaults to 500.</p> <code>500</code> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def delete_documents_by_file_ids(self, file_ids: list[str], batch_size: int = 500) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided list of file IDs.\n    File IDs should be located in the metadata of the documents.\n\n    Args:\n        file_ids (list[str]): The list of file IDs to filter by.\n        batch_size (int): Maximum number of file IDs to process in a single batch. Defaults to 500.\n    \"\"\"\n    if not file_ids:\n        logger.warning(\"No file IDs provided. No documents will be deleted.\")\n        return\n\n    if len(file_ids) &gt; batch_size:\n        for i in range(0, len(file_ids), batch_size):\n            batch = file_ids[i : i + batch_size]\n            filters = create_pgvector_file_ids_filter(batch)\n            self.delete_documents_by_filters(filters)\n            logger.debug(f\"Deleted documents batch {i//batch_size + 1} with {len(batch)} file IDs\")\n    else:\n        filters = create_pgvector_file_ids_filter(file_ids)\n        self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the pgvector vector store using filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters to select documents to delete.</p> required Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the pgvector vector store using filters.\n\n    Args:\n        filters (dict[str, Any]): Filters to select documents to delete.\n    \"\"\"\n    if filters:\n        with self._get_connection() as conn:\n            with conn.cursor(row_factory=dict_row) as cur:\n                sql_where_clause, params = self._prepare_filters(filters)\n                query = SQL(\"DELETE FROM {schema_name}.{table_name}\").format(\n                    schema_name=Identifier(self.schema_name),\n                    table_name=Identifier(self.table_name),\n                )\n                query += sql_where_clause\n                self._execute_sql_query(query, params, cursor=cur)\n                conn.commit()\n    else:\n        logger.warning(\"No filters provided. No documents will be deleted.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False, content_key=None, embedding_key=None, offset=0, limit=None)</code>","text":"<p>List documents in the pgvector vector store with optional pagination.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the results. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>str</code> <p>The field used to store content in the storage. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>str</code> <p>The field used to store embeddings in the storage. Defaults to None.</p> <code>None</code> <code>offset</code> <code>int</code> <p>Number of documents to skip. Defaults to 0.</p> <code>0</code> <code>limit</code> <code>int | None</code> <p>Maximum number of documents to return. If None, returns all documents.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of Document objects retrieved.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def list_documents(\n    self,\n    include_embeddings: bool = False,\n    content_key: str | None = None,\n    embedding_key: str | None = None,\n    offset: int = 0,\n    limit: int | None = None,\n) -&gt; list[Document]:\n    \"\"\"\n    List documents in the pgvector vector store with optional pagination.\n\n    Args:\n        include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n        content_key (str): The field used to store content in the storage. Defaults to None.\n        embedding_key (str): The field used to store embeddings in the storage. Defaults to None.\n        offset (int): Number of documents to skip. Defaults to 0.\n        limit (int | None): Maximum number of documents to return. If None, returns all documents.\n\n    Returns:\n        list[Document]: List of Document objects retrieved.\n    \"\"\"\n    if offset &lt; 0:\n        msg = f\"offset must be non-negative, got {offset}\"\n        raise ValueError(msg)\n    if limit is not None and limit &lt;= 0:\n        msg = f\"limit must be positive, got {limit}\"\n        raise ValueError(msg)\n\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    select_columns = [SQL(\"id\"), Identifier(content_key), SQL(\"metadata\")]\n    if include_embeddings:\n        select_columns.append(Identifier(embedding_key))\n\n    query = SQL(\"SELECT {} FROM {}.{} ORDER BY id\").format(\n        SQL(\", \").join(select_columns),\n        Identifier(self.schema_name),\n        Identifier(self.table_name),\n    )\n\n    if limit is not None:\n        query = query + SQL(\" LIMIT {} OFFSET {}\").format(SQLLiteral(limit), SQLLiteral(offset))\n    elif offset &gt; 0:\n        query = query + SQL(\" OFFSET {}\").format(SQLLiteral(offset))\n\n    with self._get_connection() as conn:\n        with conn.cursor(row_factory=dict_row) as cur:\n            result = self._execute_sql_query(query, cursor=cur)\n            records = result.fetchall()\n\n            documents = self._convert_query_result_to_documents(records)\n            return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/pgvector/pgvector/#dynamiq.storages.vector.pgvector.pgvector.PGVectorStore.write_documents","title":"<code>write_documents(documents, content_key=None, embedding_key=None)</code>","text":"<p>Write documents to the pgvector vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of Document objects to write.</p> required <code>content_key</code> <code>str | None</code> <p>The field used to store content in the storage. Defaults to None.</p> <code>None</code> <code>embedding_key</code> <code>str | None</code> <p>The field used to store embeddings in the storage. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If documents are not of type Document or have invalid embeddings.</p> Source code in <code>dynamiq/storages/vector/pgvector/pgvector.py</code> <pre><code>def write_documents(\n    self, documents: list[Document], content_key: str | None = None, embedding_key: str | None = None\n) -&gt; int:\n    \"\"\"\n    Write documents to the pgvector vector store.\n\n    Args:\n        documents (list[Document]): List of Document objects to write.\n        content_key (str | None): The field used to store content in the storage. Defaults to None.\n        embedding_key (str | None): The field used to store embeddings in the storage. Defaults to None.\n\n    Returns:\n        int: Number of documents successfully written.\n\n    Raises:\n        ValueError: If documents are not of type Document or have invalid embeddings.\n    \"\"\"\n    if not documents:\n        return 0\n\n    if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n        msg = \"param 'documents' must contain a list of objects of type Document\"\n        raise ValueError(msg)\n\n    content_key = content_key or self.content_key\n    embedding_key = embedding_key or self.embedding_key\n\n    document_ids = []\n    for doc in documents:\n        if doc.embedding is not None and len(doc.embedding) != self.dimension:\n            raise ValueError(\n                f\"Document {doc.id} embedding dimension {len(doc.embedding)} \"\n                f\"does not match configured dimension {self.dimension}\"\n            )\n        document_ids.append(doc.id)\n\n    # Prepare batch data\n    batch_data = [(doc.id, doc.content, Jsonb(doc.metadata), doc.embedding) for doc in documents]\n\n    query = SQL(\n        \"\"\"\n        INSERT INTO {schema_name}.{table_name} (id, {content_key}, metadata, {embedding_key})\n        VALUES (%s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            {content_key} = EXCLUDED.{content_key},\n            metadata = EXCLUDED.metadata,\n            {embedding_key} = EXCLUDED.{embedding_key}\n        \"\"\"\n    ).format(\n        schema_name=Identifier(self.schema_name),\n        table_name=Identifier(self.table_name),\n        content_key=Identifier(content_key),\n        embedding_key=Identifier(embedding_key),\n    )\n\n    with self._get_connection() as conn:\n        with conn.cursor(row_factory=dict_row) as cur:\n            try:\n                cur.executemany(query, batch_data)\n                conn.commit()\n            except (\n                psycopg_errors.OperationalError,\n                psycopg_errors.UniqueViolation,\n                psycopg_errors.DataError,\n                psycopg_errors.SyntaxError,\n                psycopg_errors.InsufficientPrivilege,\n                Exception,\n            ) as e:\n                self._handle_db_exception(e, log_context=\" during batch insert\")\n\n    self._track_documents(document_ids)\n    return len(documents)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/pinecone/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeIndexType","title":"<code>PineconeIndexType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Pinecone index deployment types.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>class PineconeIndexType(str, enum.Enum):\n    \"\"\"Pinecone index deployment types.\"\"\"\n\n    SERVERLESS = \"serverless\"\n    POD = \"pod\"\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeSimilarityMetric","title":"<code>PineconeSimilarityMetric</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported Pinecone similarity metrics.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>class PineconeSimilarityMetric(str, enum.Enum):\n    \"\"\"Supported Pinecone similarity metrics.\"\"\"\n\n    COSINE = \"cosine\"\n    EUCLIDEAN = \"euclidean\"\n    DOT_PRODUCT = \"dotproduct\"\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore","title":"<code>PineconeVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>Vector store using Pinecone.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>class PineconeVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"Vector store using Pinecone.\"\"\"\n\n    def __init__(\n        self,\n        connection: Pinecone | None = None,\n        client: Optional[\"PineconeClient\"] = None,\n        index_name: str = \"default\",\n        namespace: str = \"default\",\n        batch_size: int = 100,\n        dimension: int = 1536,\n        metric: PineconeSimilarityMetric = PineconeSimilarityMetric.COSINE,\n        create_if_not_exist: bool = False,\n        index_type: PineconeIndexType | None = None,\n        cloud: str | None = None,\n        region: str | None = None,\n        environment: str | None = None,\n        pod_type: str | None = None,\n        pods: int = 1,\n        content_key: str = \"content\",\n        dry_run_config: DryRunConfig | None = None,\n        **index_creation_kwargs,\n    ):\n        \"\"\"\n        Initialize a PineconeVectorStore instance.\n\n        Args:\n            connection (Optional[Pinecone]): Pinecone connection instance. Defaults to None.\n            client (Optional[PineconeClient]): Pinecone client instance. Defaults to None.\n            index_name (str): Name of the Pinecone index. Defaults to None.\n            namespace (str): Namespace for the index. Defaults to 'default'.\n            batch_size (int): Size of batches for operations. Defaults to 100.\n            dimension (int): Number of dimensions for vectors. Defaults to 1536.\n            metric (PineconeSimilarityMetric): Metric for calculating vector similarity. Defaults to 'cosine'.\n            content_key (Optional[str]): The field used to store content in the storage. Defaults to 'content'.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n            **index_creation_kwargs: Additional arguments for index creation.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        self.client = client\n        if self.client is None:\n            if connection is None:\n                connection = Pinecone()\n            self.client = connection.connect()\n\n        self.index_name = validate_pinecone_index_name(index_name)\n        self.namespace = namespace\n        self.index_type = index_type\n        self.content_key = content_key\n\n        self.create_if_not_exist = create_if_not_exist\n\n        self.batch_size = batch_size\n\n        self.metric = metric\n        self.dimension = dimension\n        self.cloud = cloud\n        self.region = region\n        self.environment = environment\n        self.pod_type = pod_type\n        self.pods = pods\n\n        self.index_creation_kwargs = index_creation_kwargs\n\n        self._spec = self._get_spec()\n        self._dummy_vector = [-10.0] * dimension\n        self._index = self.connect_to_index()\n        logger.debug(f\"PineconeVectorStore initialized with index {self.index_name} and namespace {self.namespace}.\")\n\n    def _get_spec(self):\n        \"\"\"\n        Returns the serverless or pod specification for the Pinecone service.\n\n        Returns:\n            ServerlessSpec | PodSpec | None: The serverless or pod specification.\n        \"\"\"\n        if self.index_type == PineconeIndexType.SERVERLESS:\n            return self.serverless_spec\n        elif self.index_type == PineconeIndexType.POD:\n            return self.pod_spec\n\n    @property\n    def serverless_spec(self):\n        \"\"\"\n        Returns the serverless specification for the Pinecone service.\n\n        Returns:\n            ServerlessSpec: The serverless specification.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import ServerlessSpec\n\n        if self.cloud is None or self.region is None:\n            raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n        return ServerlessSpec(cloud=self.cloud, region=self.region)\n\n    @property\n    def pod_spec(self):\n        \"\"\"\n        Returns the pod specification for the Pinecone service.\n\n        Returns:\n            PodSpec: The pod specification.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import PodSpec\n\n        if self.environment is None or self.pod_type is None:\n            raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n        return PodSpec(environment=self.environment, pod_type=self.pod_type, pods=self.pods)\n\n    def connect_to_index(self):\n        \"\"\"\n        Create or connect to an existing Pinecone index.\n\n        Returns:\n            The initialized Pinecone index object.\n        \"\"\"\n        try:\n            index = self.client.Index(name=self.index_name)\n            return index\n        except NotFoundException:\n            if self.create_if_not_exist and self.index_type is not None:\n                logger.debug(f\"Index {self.index_name} does not exist. Creating a new index.\")\n                self.client.create_index(\n                    name=self.index_name,\n                    spec=self._spec,\n                    dimension=self.dimension,\n                    metric=self.metric.value,\n                    **self.index_creation_kwargs,\n                )\n                self._track_collection(self.index_name)\n                return self.client.Index(name=self.index_name)\n\n            raise ValueError(\n                f\"Index {self.index_name} does not exist. \"\n                f\"'create_if_not_exist' must be set to True and 'index_type' must be specified.\"\n            )\n\n    def _set_dimension(self, dimension: int):\n        \"\"\"\n        Set the dimension for the index, with a warning if it differs from the actual dimension.\n\n        Args:\n            dimension (int): The desired dimension.\n\n        Returns:\n            int: The actual dimension of the index.\n        \"\"\"\n        actual_dimension = self._index.describe_index_stats().get(\"dimension\")\n        if actual_dimension and actual_dimension != dimension:\n            logger.warning(\n                f\"Dimension of index {self.index_name} is {actual_dimension}, but {dimension} was specified. \"\n                \"The specified dimension will be ignored. \"\n                \"If you need an index with a different dimension, please create a new one.\"\n            )\n        return actual_dimension or dimension\n\n    def delete_index(self, index_name: str | None = None):\n        \"\"\"Delete the entire index.\n\n        Args:\n            index_name (str): Name of the index to delete.\n        \"\"\"\n        try:\n            index_to_delete = index_name or self.index_name\n\n            if index_name and index_name != self.index_name:\n                target_index = self.client.Index(name=index_name)\n            else:\n                target_index = self._index\n\n            if self._namespace_exists(target_index):\n                target_index.delete(delete_all=True, namespace=self.namespace)\n\n            self.client.delete_index(name=index_to_delete)\n            logger.info(f\"Deleted index '{index_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete index '{index_to_delete}': {e}\")\n            raise\n\n    def delete_collection(self, collection_name: str | None = None):\n        \"\"\"\n        Delete a Pinecone collection (index).\n\n        Args:\n            collection_name (str | None): Name of the collection to delete. Defaults to None.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.delete_index(index_name=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete index '{collection_name}': {e}\")\n            raise\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the Pinecone vector store.\n\n        Args:\n            document_ids (list[str]): List of document IDs to delete. Defaults to None.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n        \"\"\"\n        if not self._namespace_exists():\n            logger.debug(\n                f\"Namespace '{self.namespace}' does not exist in index '{self.index_name}'. \"\n                \"Skipping deletion as there are no documents to delete.\"\n            )\n            return\n\n        if delete_all and self._index is not None:\n            self._index.delete(delete_all=True, namespace=self.namespace)\n            self._index = self.connect_to_index()\n        else:\n            if not document_ids:\n                logger.warning(\n                    \"No document IDs provided. No documents will be deleted.\"\n                )\n            else:\n                self._index.delete(ids=document_ids, namespace=self.namespace)\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the Pinecone vector store using filters.\n\n        Args:\n            filters (dict[str, Any]): Filters to select documents to delete.\n        \"\"\"\n        if not self._namespace_exists():\n            logger.debug(\n                f\"Namespace '{self.namespace}' does not exist in index '{self.index_name}'. \"\n                \"Skipping deletion as there are no documents to delete.\"\n            )\n            return\n\n        filters = _normalize_filters(filters)\n        self._index.delete(filter=filters, namespace=self.namespace)\n\n    def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n        \"\"\"\n        List documents in the Pinecone vector store.\n\n        Args:\n            include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: List of Document objects retrieved.\n        \"\"\"\n\n        all_documents = []\n        for batch_doc_ids in self._index.list(namespace=self.namespace):\n            response = self._index.fetch(ids=batch_doc_ids, namespace=self.namespace)\n\n            documents = []\n            for pinecone_doc in response[\"vectors\"].values():\n                content = pinecone_doc[\"metadata\"].pop(content_key or self.content_key, \"\")\n\n                embedding = None\n                if include_embeddings and pinecone_doc[\"values\"] != self._dummy_vector:\n                    embedding = pinecone_doc[\"values\"]\n\n                doc = Document(\n                    id=pinecone_doc[\"id\"],\n                    content=content,\n                    metadata=pinecone_doc[\"metadata\"],\n                    embedding=embedding,\n                    score=None,\n                )\n                documents.append(doc)\n\n            all_documents.extend(documents)\n        return all_documents\n\n    def _namespace_exists(self, index=None) -&gt; bool:\n        \"\"\"\n        Check if the namespace exists in an index.\n\n        Args:\n            index: The index to check. If None, uses self._index.\n\n        Returns:\n            bool: True if the namespace exists, False otherwise.\n        \"\"\"\n        index = index or self._index\n        try:\n            index_namespaces = index.describe_index_stats()[\"namespaces\"]\n            return self.namespace in index_namespaces\n        except KeyError:\n            return False\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the store.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        try:\n            count = self._index.describe_index_stats()[\"namespaces\"][self.namespace][\n                \"vector_count\"\n            ]\n        except KeyError:\n            count = 0\n        return count\n\n    def write_documents(self, documents: list[Document], content_key: str | None = None) -&gt; int:\n        \"\"\"\n        Write documents to the Pinecone vector store.\n\n        Args:\n            documents (list[Document]): List of Document objects to write.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            int: Number of documents successfully written.\n\n        Raises:\n            ValueError: If documents are not of type Document.\n        \"\"\"\n        if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n            msg = \"param 'documents' must contain a list of objects of type Document\"\n            raise ValueError(msg)\n\n        self._track_documents([doc.id for doc in documents])\n\n        documents_for_pinecone = self._convert_documents_to_pinecone_format(\n            documents, content_key=content_key or self.content_key\n        )\n\n        result = self._index.upsert(\n            vectors=documents_for_pinecone,\n            namespace=self.namespace,\n            batch_size=self.batch_size,\n        )\n\n        written_docs = result[\"upserted_count\"]\n        return written_docs\n\n    def _convert_documents_to_pinecone_format(\n        self,\n        documents: list[Document],\n        content_key: str | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Convert Document objects to Pinecone-compatible format.\n\n        Args:\n            documents (list[Document]): List of Document objects to convert.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[dict[str, Any]]: List of documents in Pinecone-compatible format.\n        \"\"\"\n        documents_for_pinecone = []\n        for document in documents:\n            embedding = copy(document.embedding)\n            if embedding is None:\n                logger.warning(\n                    f\"Document {document.id} has no embedding. A dummy embedding will be used.\"\n                )\n                embedding = self._dummy_vector\n            doc_for_pinecone = {\n                \"id\": document.id,\n                \"values\": embedding,\n                \"metadata\": dict(document.metadata or {}),\n            }\n\n            if document.content is not None:\n                doc_for_pinecone[\"metadata\"][content_key] = document.content\n\n            documents_for_pinecone.append(doc_for_pinecone)\n        return documents_for_pinecone\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        *,\n        namespace: str | None = None,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents similar to the given query embedding.\n\n        Args:\n            query_embedding (list[float]): The query embedding vector.\n            namespace (str | None): The namespace to query. Defaults to None.\n            filters (dict[str, Any] | None): Filters for the query. Defaults to None.\n            top_k (int): Maximum number of documents to retrieve. Defaults to 10.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: List of retrieved Document objects.\n\n        Raises:\n            ValueError: If query_embedding is empty or filter format is incorrect.\n        \"\"\"\n        if not query_embedding:\n            msg = \"query_embedding must be a non-empty list of floats\"\n            raise ValueError(msg)\n\n        filters = _normalize_filters(filters) if filters else None\n\n        result = self._index.query(\n            vector=query_embedding,\n            top_k=top_k,\n            namespace=namespace or self.namespace,\n            filter=filters,\n            include_values=not exclude_document_embeddings,\n            include_metadata=True,\n        )\n\n        return self._convert_query_result_to_documents(result, content_key=content_key or self.content_key)\n\n    def _convert_query_result_to_documents(\n        self, query_result: dict[str, Any], content_key: str | None = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        Convert Pinecone query results to Document objects.\n\n        Args:\n            query_result (dict[str, Any]): The query result from Pinecone.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: List of Document objects created from the query result.\n        \"\"\"\n        pinecone_docs = query_result[\"matches\"]\n        documents = []\n        for pinecone_doc in pinecone_docs:\n            content = pinecone_doc[\"metadata\"].pop(content_key, \"\")\n\n            embedding = None\n            if pinecone_doc[\"values\"] != self._dummy_vector:\n                embedding = pinecone_doc[\"values\"]\n\n            doc = Document(\n                id=pinecone_doc[\"id\"],\n                content=content,\n                metadata=pinecone_doc[\"metadata\"],\n                embedding=embedding,\n                score=pinecone_doc[\"score\"],\n            )\n            documents.append(doc)\n\n        return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.pod_spec","title":"<code>pod_spec</code>  <code>property</code>","text":"<p>Returns the pod specification for the Pinecone service.</p> <p>Returns:</p> Name Type Description <code>PodSpec</code> <p>The pod specification.</p>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.serverless_spec","title":"<code>serverless_spec</code>  <code>property</code>","text":"<p>Returns the serverless specification for the Pinecone service.</p> <p>Returns:</p> Name Type Description <code>ServerlessSpec</code> <p>The serverless specification.</p>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', namespace='default', batch_size=100, dimension=1536, metric=PineconeSimilarityMetric.COSINE, create_if_not_exist=False, index_type=None, cloud=None, region=None, environment=None, pod_type=None, pods=1, content_key='content', dry_run_config=None, **index_creation_kwargs)</code>","text":"<p>Initialize a PineconeVectorStore instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Pinecone]</code> <p>Pinecone connection instance. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[Pinecone]</code> <p>Pinecone client instance. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>Name of the Pinecone index. Defaults to None.</p> <code>'default'</code> <code>namespace</code> <code>str</code> <p>Namespace for the index. Defaults to 'default'.</p> <code>'default'</code> <code>batch_size</code> <code>int</code> <p>Size of batches for operations. Defaults to 100.</p> <code>100</code> <code>dimension</code> <code>int</code> <p>Number of dimensions for vectors. Defaults to 1536.</p> <code>1536</code> <code>metric</code> <code>PineconeSimilarityMetric</code> <p>Metric for calculating vector similarity. Defaults to 'cosine'.</p> <code>COSINE</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage. Defaults to 'content'.</p> <code>'content'</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> <code>**index_creation_kwargs</code> <p>Additional arguments for index creation.</p> <code>{}</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def __init__(\n    self,\n    connection: Pinecone | None = None,\n    client: Optional[\"PineconeClient\"] = None,\n    index_name: str = \"default\",\n    namespace: str = \"default\",\n    batch_size: int = 100,\n    dimension: int = 1536,\n    metric: PineconeSimilarityMetric = PineconeSimilarityMetric.COSINE,\n    create_if_not_exist: bool = False,\n    index_type: PineconeIndexType | None = None,\n    cloud: str | None = None,\n    region: str | None = None,\n    environment: str | None = None,\n    pod_type: str | None = None,\n    pods: int = 1,\n    content_key: str = \"content\",\n    dry_run_config: DryRunConfig | None = None,\n    **index_creation_kwargs,\n):\n    \"\"\"\n    Initialize a PineconeVectorStore instance.\n\n    Args:\n        connection (Optional[Pinecone]): Pinecone connection instance. Defaults to None.\n        client (Optional[PineconeClient]): Pinecone client instance. Defaults to None.\n        index_name (str): Name of the Pinecone index. Defaults to None.\n        namespace (str): Namespace for the index. Defaults to 'default'.\n        batch_size (int): Size of batches for operations. Defaults to 100.\n        dimension (int): Number of dimensions for vectors. Defaults to 1536.\n        metric (PineconeSimilarityMetric): Metric for calculating vector similarity. Defaults to 'cosine'.\n        content_key (Optional[str]): The field used to store content in the storage. Defaults to 'content'.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n        **index_creation_kwargs: Additional arguments for index creation.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    self.client = client\n    if self.client is None:\n        if connection is None:\n            connection = Pinecone()\n        self.client = connection.connect()\n\n    self.index_name = validate_pinecone_index_name(index_name)\n    self.namespace = namespace\n    self.index_type = index_type\n    self.content_key = content_key\n\n    self.create_if_not_exist = create_if_not_exist\n\n    self.batch_size = batch_size\n\n    self.metric = metric\n    self.dimension = dimension\n    self.cloud = cloud\n    self.region = region\n    self.environment = environment\n    self.pod_type = pod_type\n    self.pods = pods\n\n    self.index_creation_kwargs = index_creation_kwargs\n\n    self._spec = self._get_spec()\n    self._dummy_vector = [-10.0] * dimension\n    self._index = self.connect_to_index()\n    logger.debug(f\"PineconeVectorStore initialized with index {self.index_name} and namespace {self.namespace}.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.connect_to_index","title":"<code>connect_to_index()</code>","text":"<p>Create or connect to an existing Pinecone index.</p> <p>Returns:</p> Type Description <p>The initialized Pinecone index object.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def connect_to_index(self):\n    \"\"\"\n    Create or connect to an existing Pinecone index.\n\n    Returns:\n        The initialized Pinecone index object.\n    \"\"\"\n    try:\n        index = self.client.Index(name=self.index_name)\n        return index\n    except NotFoundException:\n        if self.create_if_not_exist and self.index_type is not None:\n            logger.debug(f\"Index {self.index_name} does not exist. Creating a new index.\")\n            self.client.create_index(\n                name=self.index_name,\n                spec=self._spec,\n                dimension=self.dimension,\n                metric=self.metric.value,\n                **self.index_creation_kwargs,\n            )\n            self._track_collection(self.index_name)\n            return self.client.Index(name=self.index_name)\n\n        raise ValueError(\n            f\"Index {self.index_name} does not exist. \"\n            f\"'create_if_not_exist' must be set to True and 'index_type' must be specified.\"\n        )\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the store.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the store.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    try:\n        count = self._index.describe_index_stats()[\"namespaces\"][self.namespace][\n            \"vector_count\"\n        ]\n    except KeyError:\n        count = 0\n    return count\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete a Pinecone collection (index).</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None):\n    \"\"\"\n    Delete a Pinecone collection (index).\n\n    Args:\n        collection_name (str | None): Name of the collection to delete. Defaults to None.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.delete_index(index_name=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete index '{collection_name}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>List of document IDs to delete. Defaults to None.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the Pinecone vector store.\n\n    Args:\n        document_ids (list[str]): List of document IDs to delete. Defaults to None.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n    \"\"\"\n    if not self._namespace_exists():\n        logger.debug(\n            f\"Namespace '{self.namespace}' does not exist in index '{self.index_name}'. \"\n            \"Skipping deletion as there are no documents to delete.\"\n        )\n        return\n\n    if delete_all and self._index is not None:\n        self._index.delete(delete_all=True, namespace=self.namespace)\n        self._index = self.connect_to_index()\n    else:\n        if not document_ids:\n            logger.warning(\n                \"No document IDs provided. No documents will be deleted.\"\n            )\n        else:\n            self._index.delete(ids=document_ids, namespace=self.namespace)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the Pinecone vector store using filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters to select documents to delete.</p> required Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the Pinecone vector store using filters.\n\n    Args:\n        filters (dict[str, Any]): Filters to select documents to delete.\n    \"\"\"\n    if not self._namespace_exists():\n        logger.debug(\n            f\"Namespace '{self.namespace}' does not exist in index '{self.index_name}'. \"\n            \"Skipping deletion as there are no documents to delete.\"\n        )\n        return\n\n    filters = _normalize_filters(filters)\n    self._index.delete(filter=filters, namespace=self.namespace)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_index","title":"<code>delete_index(index_name=None)</code>","text":"<p>Delete the entire index.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_index(self, index_name: str | None = None):\n    \"\"\"Delete the entire index.\n\n    Args:\n        index_name (str): Name of the index to delete.\n    \"\"\"\n    try:\n        index_to_delete = index_name or self.index_name\n\n        if index_name and index_name != self.index_name:\n            target_index = self.client.Index(name=index_name)\n        else:\n            target_index = self._index\n\n        if self._namespace_exists(target_index):\n            target_index.delete(delete_all=True, namespace=self.namespace)\n\n        self.client.delete_index(name=index_to_delete)\n        logger.info(f\"Deleted index '{index_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete index '{index_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False, content_key=None)</code>","text":"<p>List documents in the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the results. Defaults to False.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of Document objects retrieved.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n    \"\"\"\n    List documents in the Pinecone vector store.\n\n    Args:\n        include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        list[Document]: List of Document objects retrieved.\n    \"\"\"\n\n    all_documents = []\n    for batch_doc_ids in self._index.list(namespace=self.namespace):\n        response = self._index.fetch(ids=batch_doc_ids, namespace=self.namespace)\n\n        documents = []\n        for pinecone_doc in response[\"vectors\"].values():\n            content = pinecone_doc[\"metadata\"].pop(content_key or self.content_key, \"\")\n\n            embedding = None\n            if include_embeddings and pinecone_doc[\"values\"] != self._dummy_vector:\n                embedding = pinecone_doc[\"values\"]\n\n            doc = Document(\n                id=pinecone_doc[\"id\"],\n                content=content,\n                metadata=pinecone_doc[\"metadata\"],\n                embedding=embedding,\n                score=None,\n            )\n            documents.append(doc)\n\n        all_documents.extend(documents)\n    return all_documents\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.write_documents","title":"<code>write_documents(documents, content_key=None)</code>","text":"<p>Write documents to the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of Document objects to write.</p> required <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If documents are not of type Document.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def write_documents(self, documents: list[Document], content_key: str | None = None) -&gt; int:\n    \"\"\"\n    Write documents to the Pinecone vector store.\n\n    Args:\n        documents (list[Document]): List of Document objects to write.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        int: Number of documents successfully written.\n\n    Raises:\n        ValueError: If documents are not of type Document.\n    \"\"\"\n    if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n        msg = \"param 'documents' must contain a list of objects of type Document\"\n        raise ValueError(msg)\n\n    self._track_documents([doc.id for doc in documents])\n\n    documents_for_pinecone = self._convert_documents_to_pinecone_format(\n        documents, content_key=content_key or self.content_key\n    )\n\n    result = self._index.upsert(\n        vectors=documents_for_pinecone,\n        namespace=self.namespace,\n        batch_size=self.batch_size,\n    )\n\n    written_docs = result[\"upserted_count\"]\n    return written_docs\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/converters/","title":"Converters","text":""},{"location":"dynamiq/storages/vector/qdrant/converters/#dynamiq.storages.vector.qdrant.converters.convert_id","title":"<code>convert_id(_id)</code>","text":"<p>Converts any string into a UUID-like format in a deterministic way.</p> <p>Qdrant does not accept any string as an id, so an internal id has to be generated for each point. This is a deterministic way of doing so.</p> Source code in <code>dynamiq/storages/vector/qdrant/converters.py</code> <pre><code>def convert_id(_id: str) -&gt; str:\n    \"\"\"\n    Converts any string into a UUID-like format in a deterministic way.\n\n    Qdrant does not accept any string as an id, so an internal id has to be\n    generated for each point. This is a deterministic way of doing so.\n    \"\"\"\n    UUID_NAMESPACE = uuid.UUID(\"00000000-0000-0000-0000-000000000000\")\n    return uuid.uuid5(UUID_NAMESPACE, _id).hex\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/qdrant/filters/#dynamiq.storages.vector.qdrant.filters.build_filters_for_repeated_operators","title":"<code>build_filters_for_repeated_operators(must_clauses, should_clauses, must_not_clauses, qdrant_filter)</code>","text":"<p>Flattens the nested lists of clauses by creating separate Filters for each clause of a logical operator.</p> <p>Parameters:</p> Name Type Description Default <code>must_clauses</code> <code>list[Filter]</code> <p>A nested list of must clauses or an empty list.</p> required <code>should_clauses</code> <code>list[Filter]</code> <p>A nested list of should clauses or an empty list.</p> required <code>must_not_clauses</code> <code>list[Filter]</code> <p>A nested list of must_not clauses or an empty list.</p> required <code>qdrant_filter</code> <code>list[Filter]</code> <p>A list where the generated Filter objects will be appended. This list will be modified in-place.</p> required <p>Returns:</p> Type Description <code>list[Filter]</code> <p>The modified <code>qdrant_filter</code> list with appended generated Filter objects.</p> Source code in <code>dynamiq/storages/vector/qdrant/filters.py</code> <pre><code>def build_filters_for_repeated_operators(\n    must_clauses: list[models.Filter],\n    should_clauses: list[models.Filter],\n    must_not_clauses: list[models.Filter],\n    qdrant_filter: list[models.Filter],\n) -&gt; list[models.Filter]:\n    \"\"\"\n    Flattens the nested lists of clauses by creating separate Filters for each clause of a logical\n    operator.\n\n    Args:\n        must_clauses: A nested list of must clauses or an empty list.\n        should_clauses: A nested list of should clauses or an empty list.\n        must_not_clauses: A nested list of must_not clauses or an empty list.\n        qdrant_filter: A list where the generated Filter objects will be appended. This list will be\n            modified in-place.\n\n    Returns:\n        The modified `qdrant_filter` list with appended generated Filter objects.\n    \"\"\"\n\n    if any(isinstance(i, list) for i in must_clauses):\n        for i in must_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=i or None,\n                    should=should_clauses or None,\n                    must_not=must_not_clauses or None,\n                )\n            )\n    if any(isinstance(i, list) for i in should_clauses):\n        for i in should_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=must_clauses or None,\n                    should=i or None,\n                    must_not=must_not_clauses or None,\n                )\n            )\n    if any(isinstance(i, list) for i in must_not_clauses):\n        for i in must_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=must_clauses or None,\n                    should=should_clauses or None,\n                    must_not=i or None,\n                )\n            )\n\n    return qdrant_filter\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/filters/#dynamiq.storages.vector.qdrant.filters.convert_filters_to_qdrant","title":"<code>convert_filters_to_qdrant(filter_term=None, is_parent_call=True)</code>","text":"<p>Converts Dynamiq filters to the format used by Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>filter_term</code> <code>list[dict] | dict | Filter | None</code> <p>The Dynamiq filter to be converted to Qdrant.</p> <code>None</code> <code>is_parent_call</code> <code>bool</code> <p>Indicates if this is the top-level call to the function. If True, the function returns a single models.Filter object; if False, it may return a list of filters or conditions for further processing.</p> <code>True</code> <p>Returns:</p> Type Description <code>Filter | list[Filter] | list[Condition] | None</code> <p>A single Qdrant Filter in the parent call or a list of such Filters in recursive calls.</p> <p>Raises:</p> Type Description <code>VectorStoreFilterException</code> <p>If the invalid filter criteria is provided or if an unknown operator is encountered.</p> Source code in <code>dynamiq/storages/vector/qdrant/filters.py</code> <pre><code>def convert_filters_to_qdrant(\n    filter_term: list[dict] | dict | models.Filter | None = None, is_parent_call: bool = True\n) -&gt; models.Filter | list[models.Filter] | list[models.Condition] | None:\n    \"\"\"Converts Dynamiq filters to the format used by Qdrant.\n\n    Args:\n        filter_term: The Dynamiq filter to be converted to Qdrant.\n        is_parent_call: Indicates if this is the top-level call to the function. If True, the function\n            returns a single models.Filter object; if False, it may return a list of filters or\n            conditions for further processing.\n\n    Returns:\n        A single Qdrant Filter in the parent call or a list of such Filters in recursive calls.\n\n    Raises:\n        FilterError: If the invalid filter criteria is provided or if an unknown operator is\n            encountered.\n    \"\"\"\n\n    if isinstance(filter_term, models.Filter):\n        return filter_term\n    if not filter_term:\n        return None\n\n    must_clauses: list[models.Filter] = []\n    should_clauses: list[models.Filter] = []\n    must_not_clauses: list[models.Filter] = []\n    # Indicates if there are multiple same LOGICAL OPERATORS on each level\n    # and prevents them from being combined\n    same_operator_flag = False\n    conditions, qdrant_filter, current_level_operators = (\n        [],\n        [],\n        [],\n    )\n\n    if isinstance(filter_term, dict):\n        filter_term = [filter_term]\n\n    # ======== IDENTIFY FILTER ITEMS ON EACH LEVEL ========\n\n    for item in filter_term:\n        operator = item.get(\"operator\")\n\n        # Check for repeated similar operators on each level\n        same_operator_flag = operator in current_level_operators and operator in LOGICAL_OPERATORS\n        if not same_operator_flag:\n            current_level_operators.append(operator)\n\n        if operator is None:\n            msg = \"Operator not found in filters\"\n            raise FilterError(msg)\n\n        if operator in LOGICAL_OPERATORS and \"conditions\" not in item:\n            msg = f\"'conditions' not found for '{operator}'\"\n            raise FilterError(msg)\n\n        if operator in LOGICAL_OPERATORS:\n            # Recursively process nested conditions\n            current_filter = convert_filters_to_qdrant(item.get(\"conditions\", []), is_parent_call=False) or []\n\n            # When same_operator_flag is set to True,\n            # ensure each clause is appended as an independent list to avoid merging distinct clauses.\n            if operator == \"AND\":\n                must_clauses = [must_clauses, current_filter] if same_operator_flag else must_clauses + current_filter\n            elif operator == \"OR\":\n                should_clauses = (\n                    [should_clauses, current_filter] if same_operator_flag else should_clauses + current_filter\n                )\n            elif operator == \"NOT\":\n                must_not_clauses = (\n                    [must_not_clauses, current_filter] if same_operator_flag else must_not_clauses + current_filter\n                )\n\n        elif operator in COMPARISON_OPERATORS:\n            field = item.get(\"field\")\n            if not field.startswith(\"metadata.\"):\n                field = f\"metadata.{field}\"\n            value = item.get(\"value\")\n            if field is None or value is None:\n                msg = f\"'field' or 'value' not found for '{operator}'\"\n                raise FilterError(msg)\n\n            parsed_conditions = _parse_comparison_operation(comparison_operation=operator, key=field, value=value)\n\n            # check if the parsed_conditions are models.Filter or models.Condition\n            for condition in parsed_conditions:\n                if isinstance(condition, models.Filter):\n                    qdrant_filter.append(condition)\n                else:\n                    conditions.append(condition)\n\n        else:\n            msg = f\"Unknown operator {operator} used in filters\"\n            raise FilterError(msg)\n\n    # ======== PROCESS FILTER ITEMS ON EACH LEVEL ========\n\n    # If same logical operators have separate clauses, create separate filters\n    if same_operator_flag:\n        qdrant_filter = build_filters_for_repeated_operators(\n            must_clauses, should_clauses, must_not_clauses, qdrant_filter\n        )\n\n    # else append a single Filter for existing clauses\n    elif must_clauses or should_clauses or must_not_clauses:\n        qdrant_filter.append(\n            models.Filter(\n                must=must_clauses or None,\n                should=should_clauses or None,\n                must_not=must_not_clauses or None,\n            )\n        )\n\n    # In case of parent call, a single Filter is returned\n    if is_parent_call:\n        # If qdrant_filter has just a single Filter in parent call,\n        # then it might be returned instead.\n        if len(qdrant_filter) == 1 and isinstance(qdrant_filter[0], models.Filter):\n            return qdrant_filter[0]\n        else:\n            must_clauses.extend(conditions)\n            return models.Filter(\n                must=must_clauses or None,\n                should=should_clauses or None,\n                must_not=must_not_clauses or None,\n            )\n\n    # Store conditions of each level in output of the loop\n    elif conditions:\n        qdrant_filter.extend(conditions)\n\n    return qdrant_filter\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore","title":"<code>QdrantVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>QdrantVectorStore a Document Store for Qdrant.</p> <p>Usage example:</p> <pre><code>from dynamiq.types import Document\nfrom dynamiq.storages.vector.qdrant import QdrantVectorStore\n\ndocument_store = QdrantVectorStore(\n        url=\"https://xxxxxx-xxxxx-xxxxx-xxxx-xxxxxxxxx.us-east.aws.cloud.qdrant.io:6333\",\n    api_key=\"&lt;your-api-key&gt;\",\n)\n\ndocument_store.count_documents()\n</code></pre> <p>Attributes:</p> Name Type Description <code>DISTANCE_BY_SIMILARITY</code> <code>ClassVar[dict[QdrantSimilarityMetric, str]]</code> <p>Mapping of metrics to distances.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>class QdrantVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"QdrantVectorStore a Document Store for Qdrant.\n\n    Usage example:\n\n    ```python\n    from dynamiq.types import Document\n    from dynamiq.storages.vector.qdrant import QdrantVectorStore\n\n    document_store = QdrantVectorStore(\n            url=\"https://xxxxxx-xxxxx-xxxxx-xxxx-xxxxxxxxx.us-east.aws.cloud.qdrant.io:6333\",\n        api_key=\"&lt;your-api-key&gt;\",\n    )\n\n    document_store.count_documents()\n    ```\n\n    Attributes:\n        DISTANCE_BY_SIMILARITY (ClassVar[dict[QdrantSimilarityMetric, str]]): Mapping of metrics to distances.\n    \"\"\"\n\n    DISTANCE_BY_SIMILARITY: ClassVar[dict[QdrantSimilarityMetric, str]] = {\n        QdrantSimilarityMetric.COSINE: rest.Distance.COSINE,\n        QdrantSimilarityMetric.DOT_PRODUCT: rest.Distance.DOT,\n        QdrantSimilarityMetric.L2: rest.Distance.EUCLID,\n    }\n    _MISSING_INDEX_PATTERN: ClassVar[Pattern[str]] = re.compile(\n        r'Index required but not found for \"(?P&lt;field&gt;[^\"]+)\" of one of the following types: \\[(?P&lt;types&gt;[^\\]]+)\\]',\n        re.IGNORECASE,\n    )\n    _PAYLOAD_SCHEMA_BY_NAME: ClassVar[dict[str, rest.PayloadSchemaType]] = {\n        schema.value.lower(): schema for schema in rest.PayloadSchemaType\n    }\n\n    def __init__(\n        self,\n        connection: QdrantConnection | None = None,\n        client: Optional[\"QdrantClient\"] = None,\n        location: str | None = None,\n        url: str | None = None,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: bool | None = None,\n        api_key: str | None = None,\n        prefix: str | None = None,\n        timeout: int | None = None,\n        host: str | None = None,\n        path: str | None = None,\n        force_disable_check_same_thread: bool = False,\n        index_name: str = \"Document\",\n        dimension: int = 1536,\n        on_disk: bool = False,\n        use_sparse_embeddings: bool = False,\n        sparse_idf: bool = False,\n        metric: QdrantSimilarityMetric = QdrantSimilarityMetric.COSINE,\n        return_embedding: bool = False,\n        create_if_not_exist: bool = False,\n        recreate_index: bool = False,\n        shard_number: int | None = None,\n        replication_factor: int | None = None,\n        write_consistency_factor: int | None = None,\n        on_disk_payload: bool | None = None,\n        hnsw_config: dict | None = None,\n        optimizers_config: dict | None = None,\n        wal_config: dict | None = None,\n        quantization_config: dict | None = None,\n        init_from: dict | None = None,\n        wait_result_from_api: bool = True,\n        metadata: dict | None = None,\n        write_batch_size: int = 100,\n        scroll_size: int = 10_000,\n        payload_fields_to_index: list[dict] | None = None,\n        content_key: str = \"content\",\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"Initializes the QdrantDocumentStore.\n\n        Args:\n            location: If `memory` - use in-memory Qdrant instance. If `str` - use it as a URL parameter. If `None` - use\n                default values for host and port.\n            url: Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.\n            port: Port of the REST API interface.\n            grpc_port: Port of the gRPC interface.\n            prefer_grpc: If `True` - use gRPC interface whenever possible in custom methods.\n            https: If `True` - use HTTPS(SSL) protocol.\n            api_key: API key for authentication in Qdrant Cloud.\n            prefix: If not `None` - add prefix to the REST URL path. Example: service/v1 will result in\n                http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\n            timeout: Timeout for REST and gRPC API requests.\n            host: Host name of Qdrant service. If `url` and `host` are `None`, set to `localhost`.\n            path: Persistence path for QdrantLocal.\n            force_disable_check_same_thread: For QdrantLocal, force disable check_same_thread. Only use this if you can\n                guarantee that you can resolve the thread safety outside QdrantClient.\n            index_name: Name of the index.\n            dimension: Dimension of the embeddings.\n            on_disk: Whether to store the collection on disk.\n            use_sparse_embedding: If set to `True`, enables support for sparse embeddings.\n            sparse_idf: If set to `True`, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It\n                is required to use techniques like BM42. It is ignored if `use_sparse_embeddings` is `False`.\n            metric: The similarity metric to use.\n            return_embedding: Whether to return embeddings in the search results.\n            recreate_index: Whether to recreate the index.\n            shard_number: Number of shards in the collection.\n            replication_factor: Replication factor for the collection. Defines how many copies of each shard will be\n                created. Effective only in distributed mode.\n            write_consistency_factor: Write consistency factor for the collection. Minimum value is 1. Defines how many\n                replicas should apply to the operation for it to be considered successful. Increasing this number makes\n                the collection more resilient to inconsistencies but will cause failures if not enough replicas are\n                available. Effective only in distributed mode.\n            on_disk_payload: If `True`, the point's payload will not be stored in memory and will be read from the disk\n                every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed\n                payload values remain in RAM.\n            hnsw_config: Params for HNSW index.\n            optimizers_config: Params for optimizer.\n            wal_config: Params for Write-Ahead-Log.\n            quantization_config: Params for quantization. If `None`, quantization will be disabled.\n            init_from: Use data stored in another collection to initialize this collection.\n            wait_result_from_api: Whether to wait for the result from the API after each request.\n            metadata: Additional metadata to include with the documents.\n            write_batch_size: The batch size for writing documents.\n            scroll_size: The scroll size for reading documents.\n            payload_fields_to_index: List of payload fields to index.\n            content_key (Optional[str]): The field used to store content in the storage.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        self._client = client\n        if self._client is None:\n            connection = connection or QdrantConnection()\n            self._client = connection.connect()\n\n        # Store the Qdrant client specific attributes\n        self.location = location\n        self.url = url\n        self.port = port\n        self.grpc_port = grpc_port\n        self.prefer_grpc = prefer_grpc\n        self.https = https\n        self.api_key = api_key\n        self.prefix = prefix\n        self.timeout = timeout\n        self.host = host\n        self.path = path\n        self.force_disable_check_same_thread = force_disable_check_same_thread\n        self.metadata = metadata or {}\n        self.api_key = api_key\n\n        # Store the Qdrant collection specific attributes\n        self.shard_number = shard_number\n        self.replication_factor = replication_factor\n        self.write_consistency_factor = write_consistency_factor\n        self.on_disk_payload = on_disk_payload\n        self.hnsw_config = hnsw_config\n        self.optimizers_config = optimizers_config\n        self.wal_config = wal_config\n        self.quantization_config = quantization_config\n        self.init_from = init_from\n        self.wait_result_from_api = wait_result_from_api\n        self.create_if_not_exist = create_if_not_exist\n        self.recreate_index = recreate_index\n        self.payload_fields_to_index = payload_fields_to_index\n        self.use_sparse_embeddings = use_sparse_embeddings\n        self.sparse_idf = use_sparse_embeddings and sparse_idf\n        self.dimension = dimension\n        self.on_disk = on_disk\n        self.metric = metric\n        self.index_name = index_name\n        self.return_embedding = return_embedding\n        self.write_batch_size = write_batch_size\n        self.scroll_size = scroll_size\n        self.content_key = content_key\n        self._indexed_payload_fields: set[str] = set()\n        self._auto_index_attempted: set[str] = set()\n\n    @property\n    def client(self):\n        if not self._client:\n            self._client = qdrant_client.QdrantClient(\n                location=self.location,\n                url=self.url,\n                port=self.port,\n                grpc_port=self.grpc_port,\n                prefer_grpc=self.prefer_grpc,\n                https=self.https,\n                api_key=self.api_key.resolve_value() if self.api_key else None,\n                prefix=self.prefix,\n                timeout=self.timeout,\n                host=self.host,\n                path=self.path,\n                metadata=self.metadata,\n                force_disable_check_same_thread=self.force_disable_check_same_thread,\n            )\n            # Make sure the collection is properly set up\n            self._set_up_collection(\n                collection_name=self.index_name,\n                embedding_dim=self.dimension,\n                create_if_not_exist=self.create_if_not_exist,\n                recreate_collection=self.recreate_index,\n                similarity=self.metric,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                sparse_idf=self.sparse_idf,\n                on_disk=self.on_disk,\n                payload_fields_to_index=self.payload_fields_to_index,\n            )\n        return self._client\n\n    def _collection_exists(self, collection_name: str) -&gt; bool:\n        \"\"\"Safely determine if the collection is present in Qdrant.\"\"\"\n\n        client = self._client or self.client\n\n        try:\n            return client.collection_exists(collection_name)\n        except UnexpectedResponse as exc:\n            if getattr(exc, \"status_code\", None) == 404:\n                try:\n                    client.get_collection(collection_name)\n                    return True\n                except UnexpectedResponse as inner_exc:\n                    if getattr(inner_exc, \"status_code\", None) == 404:\n                        return False\n                    raise\n            raise\n        except ValueError:\n            return False\n\n    def count_documents(self) -&gt; int:\n        \"\"\"Returns the number of documents present in the Document Store.\n\n        Returns:\n            The number of documents in the Document Store.\n        \"\"\"\n        try:\n            response = self.client.count(\n                collection_name=self.index_name,\n            )\n            return response.count\n        except (UnexpectedResponse, ValueError):\n            # Qdrant local raises ValueError if the collection is not found, but\n            # with the remote server UnexpectedResponse is raised. Until that's unified,\n            # we need to catch both.\n            return 0\n\n    def filter_documents(\n        self,\n        filters: dict[str, Any] | rest.Filter | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Returns the documents that match the provided filters.\n\n        For a detailed specification of the filters, refer to the\n        [documentation](https://docs.dynamiq.deepset.ai/docs/metadata-filtering)\n\n        Args:\n            filters: The filters to apply to the document list.\n\n        Returns:\n            A list of documents that match the given filters.\n        \"\"\"\n        if filters and not isinstance(filters, dict) and not isinstance(filters, rest.Filter):\n            msg = \"Filter must be a dictionary or an instance of `qdrant_client.http.models.Filter`\"\n            raise ValueError(msg)\n\n        if filters and not isinstance(filters, rest.Filter) and \"operator\" not in filters:\n            raise ValueError(\"Filter must contain an 'operator' key\")\n\n        return list(\n            self.get_documents_generator(\n                filters,\n            )\n        )\n\n    def write_documents(\n        self,\n        documents: list[Document],\n        policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n        content_key: str | None = None,\n    ) -&gt; int:\n        \"\"\"Writes documents to Qdrant using the specified policy.\n\n        The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are:\n        - `FAIL`: The operation will raise an error if any document already exists.\n        - `OVERWRITE`: Existing documents will be overwritten with the new ones.\n        - `SKIP`: Existing documents will be skipped, and only new documents will be added.\n\n        Args:\n            documents: A list of Document objects to write to Qdrant.\n            policy: The policy for handling duplicate documents.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            The number of documents written to the document store.\n        \"\"\"\n        if not self._collection_exists(self.index_name):\n            if self.create_if_not_exist:\n                logger.info(f\"Collection {self.index_name} doesn't exist. Creating...\")\n                self._set_up_collection(\n                    collection_name=self.index_name,\n                    embedding_dim=self.dimension,\n                    create_if_not_exist=True,\n                    recreate_collection=self.recreate_index,\n                    similarity=self.metric,\n                    use_sparse_embeddings=self.use_sparse_embeddings,\n                    sparse_idf=self.sparse_idf,\n                    on_disk=self.on_disk,\n                )\n            else:\n                raise QdrantStoreError(f\"Collection {self.index_name} doesn't exist\")\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = f\"DocumentStore.write_documents() expects a list of Documents but got an element of {type(doc)}.\"\n                raise ValueError(msg)\n\n        if len(documents) == 0:\n            logger.warning(\"Calling QdrantDocumentStore.write_documents() with empty list\")\n            return 0\n\n        document_objects = self._handle_duplicate_documents(\n            documents=documents,\n            index=self.index_name,\n            policy=policy,\n        )\n\n        batched_documents = get_batches_from_generator(document_objects, self.write_batch_size)\n        for document_batch in batched_documents:\n            batch = convert_dynamiq_documents_to_qdrant_points(\n                document_batch,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                content_key=content_key or self.content_key,\n            )\n\n            self.client.upsert(\n                collection_name=self.index_name,\n                points=batch,\n                wait=self.wait_result_from_api,\n            )\n\n        self._track_documents([doc.id for doc in documents])\n\n        return len(document_objects)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"Deletes documents that match the provided `document_ids` from the document store.\n\n        Args:\n            document_ids: The document ids to delete.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all:\n            self.client.delete_collection(collection_name=self.index_name)\n        elif document_ids:\n            ids = [convert_id(_id) for _id in document_ids]\n            try:\n                self.client.delete(\n                    collection_name=self.index_name,\n                    points_selector=ids,\n                    wait=self.wait_result_from_api,\n                )\n            except KeyError:\n                logger.warning(\n                    \"Called QdrantDocumentStore.delete_documents() on a non-existing ID\",\n                )\n        else:\n            raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the document list.\n        \"\"\"\n        if filters:\n            documents = self.filter_documents(filters=filters)\n            document_ids = [doc.id for doc in documents]\n            self.delete_documents(document_ids=document_ids)\n        else:\n            raise ValueError(\"No filters provided to delete documents.\")\n\n    def delete_collection(self, collection_name: str | None = None):\n        \"\"\"\n        Delete a Qdrant collection.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.delete_collection(collection_name=collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def get_documents_generator(\n        self,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        include_embeddings: bool = False,\n        content_key: str | None = None,\n    ) -&gt; Generator[Document, None, None]:\n        \"\"\"Returns a generator that yields documents from Qdrant based on the provided filters.\n\n        Args:\n            filters: Filters applied to the retrieved documents.\n            include_embeddings: Whether to include the embeddings of the retrieved documents.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            A generator that yields documents retrieved from Qdrant.\n        \"\"\"\n\n        index = self.index_name\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        next_offset = None\n        stop_scrolling = False\n        while not stop_scrolling:\n            records, next_offset = self._execute_with_payload_index_retry(\n                lambda: self.client.scroll(\n                    collection_name=index,\n                    scroll_filter=qdrant_filters,\n                    limit=self.scroll_size,\n                    offset=next_offset,\n                    with_payload=True,\n                    with_vectors=include_embeddings,\n                )\n            )\n            stop_scrolling = next_offset is None or (\n                isinstance(next_offset, grpc.PointId) and next_offset.num == 0 and next_offset.uuid == \"\"\n            )\n\n            for record in records:\n                yield convert_qdrant_point_to_dynamiq_document(\n                    record,\n                    use_sparse_embeddings=self.use_sparse_embeddings,\n                    content_key=content_key or self.content_key,\n                )\n\n    def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n        \"\"\"Returns a list of all documents in the Document Store.\n\n        Args:\n            include_embeddings: Whether to include the embeddings of the retrieved documents.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            A list of all documents in the Document Store.\n        \"\"\"\n        return list(\n            self.get_documents_generator(\n                include_embeddings=include_embeddings, content_key=content_key or self.content_key\n            )\n        )\n\n    def get_documents_by_id(\n        self, ids: list[str], index: str | None = None, content_key: str | None = None\n    ) -&gt; list[Document]:\n        \"\"\"Retrieves documents from Qdrant by their IDs.\n\n        Args:\n            ids: A list of document IDs to retrieve.\n            index: The name of the index to retrieve documents from.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            A list of documents.\n        \"\"\"\n        index = index or self.index_name\n\n        documents: list[Document] = []\n\n        ids = [convert_id(_id) for _id in ids]\n        records = self.client.retrieve(\n            collection_name=index,\n            ids=ids,\n            with_payload=True,\n            with_vectors=True,\n        )\n\n        for record in records:\n            documents.append(\n                convert_qdrant_point_to_dynamiq_document(\n                    record,\n                    use_sparse_embeddings=self.use_sparse_embeddings,\n                    content_key=content_key or self.content_key,\n                )\n            )\n        return documents\n\n    def _query_by_sparse(\n        self,\n        query_sparse_embedding: SparseEmbedding,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        scale_score: bool = False,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Queries Qdrant using a sparse embedding and returns the most relevant documents.\n\n        Args:\n            query_sparse_embedding: Sparse embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            scale_score: Whether to scale the scores of the retrieved documents.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            List of documents that are most similar to `query_sparse_embedding`.\n\n        Raises:\n            QdrantStoreError: If the Document Store was initialized with `use_sparse_embeddings=False`.\n        \"\"\"\n\n        if not self.use_sparse_embeddings:\n            message = (\n                \"You are trying to query using sparse embeddings, but the Document Store \"\n                \"was initialized with `use_sparse_embeddings=False`. \"\n            )\n            raise QdrantStoreError(message)\n\n        qdrant_filters = convert_filters_to_qdrant(filters)\n        query_indices = query_sparse_embedding.indices\n        query_values = query_sparse_embedding.values\n        response = self._execute_with_payload_index_retry(\n            lambda: self.client.query_points(\n                collection_name=self.index_name,\n                query=rest.SparseVector(\n                    indices=query_indices,\n                    values=query_values,\n                ),\n                using=SPARSE_VECTORS_NAME,\n                query_filter=qdrant_filters,\n                limit=top_k,\n                with_vectors=return_embedding,\n                score_threshold=score_threshold,\n            )\n        )\n        points = response.points\n        results = [\n            convert_qdrant_point_to_dynamiq_document(\n                point, use_sparse_embeddings=self.use_sparse_embeddings, content_key=content_key or self.content_key\n            )\n            for point in points\n        ]\n        if scale_score:\n            for document in results:\n                score = document.score\n                score = float(1 / (1 + np.exp(-score / 100)))\n                document.score = score\n        return results\n\n    def _query_by_embedding(\n        self,\n        query_embedding: list[float],\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        scale_score: bool = False,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Queries Qdrant using a dense embedding and returns the most relevant documents.\n\n        Args:\n            query_embedding: Dense embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            scale_score: Whether to scale the scores of the retrieved documents.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            List of documents that are most similar to `query_embedding`.\n        \"\"\"\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        response = self._execute_with_payload_index_retry(\n            lambda: self.client.query_points(\n                collection_name=self.index_name,\n                query=query_embedding,\n                using=DENSE_VECTORS_NAME if self.use_sparse_embeddings else None,\n                query_filter=qdrant_filters,\n                limit=top_k,\n                with_vectors=return_embedding,\n                score_threshold=score_threshold,\n            )\n        )\n        points = response.points\n        results = [\n            convert_qdrant_point_to_dynamiq_document(\n                point, use_sparse_embeddings=self.use_sparse_embeddings, content_key=content_key or self.content_key\n            )\n            for point in points\n        ]\n        if scale_score:\n            for document in results:\n                score = document.score\n                if str(self.metric).lower() == \"cosine\":\n                    score = (score + 1) / 2\n                else:\n                    score = float(1 / (1 + np.exp(-score / 100)))\n                document.score = score\n        return results\n\n    def _query_hybrid(\n        self,\n        query_embedding: list[float],\n        query_sparse_embedding: SparseEmbedding,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Retrieves documents based on dense and sparse embeddings and fuses the results using Reciprocal Rank Fusion.\n\n        This method is not part of the public interface of `QdrantDocumentStore` and shouldn't be used directly. Use the\n        `QdrantHybridRetriever` instead.\n\n        Args:\n            query_embedding: Dense embedding of the query.\n            query_sparse_embedding: Sparse embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            List of Document that are most similar to `query_embedding` and `query_sparse_embedding`.\n\n        Raises:\n            QdrantStoreError: If the Document Store was initialized with `use_sparse_embeddings=False`.\n        \"\"\"\n\n        # This implementation is based on the code from the Python Qdrant client:\n        # https://github.com/qdrant/qdrant-client/blob/8e3ea58f781e4110d11c0a6985b5e6bb66b85d33/qdrant_client/qdrant_fastembed.py#L519\n        if not self.use_sparse_embeddings:\n            message = (\n                \"You are trying to query using sparse embeddings, but the Document Store \"\n                \"was initialized with `use_sparse_embeddings=False`. \"\n            )\n            raise QdrantStoreError(message)\n\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        try:\n            response = self._execute_with_payload_index_retry(\n                lambda: self.client.query_points(\n                    collection_name=self.index_name,\n                    prefetch=[\n                        rest.Prefetch(\n                            query=rest.SparseVector(\n                                indices=query_sparse_embedding.indices,\n                                values=query_sparse_embedding.values,\n                            ),\n                            using=SPARSE_VECTORS_NAME,\n                            filter=qdrant_filters,\n                        ),\n                        rest.Prefetch(\n                            query=query_embedding,\n                            using=DENSE_VECTORS_NAME,\n                            filter=qdrant_filters,\n                        ),\n                    ],\n                    query=rest.FusionQuery(fusion=rest.Fusion.RRF),\n                    limit=top_k,\n                    score_threshold=score_threshold,\n                    with_payload=True,\n                    with_vectors=return_embedding,\n                )\n            )\n            points = response.points\n        except Exception as e:\n            msg = \"Error during hybrid search\"\n            raise QdrantStoreError(msg) from e\n\n        results = [\n            convert_qdrant_point_to_dynamiq_document(\n                point, use_sparse_embeddings=True, content_key=content_key or self.content_key\n            )\n            for point in points\n        ]\n\n        return results\n\n    def get_distance(self, similarity: str | QdrantSimilarityMetric) -&gt; rest.Distance:\n        \"\"\"Retrieves the distance metric for the specified similarity measure.\n\n        Args:\n            similarity: The similarity measure to retrieve the distance.\n\n        Returns:\n            The corresponding rest.Distance object.\n\n        Raises:\n            QdrantStoreError: If the provided similarity measure is not supported.\n        \"\"\"\n        if isinstance(similarity, str):\n            try:\n                similarity = QdrantSimilarityMetric(similarity.lower())\n            except ValueError as exc:\n                msg = (\n                    f\"Provided similarity '{similarity}' is not supported by Qdrant document store. \"\n                    f\"Please choose one of the options: {', '.join(metric.value for metric in QdrantSimilarityMetric)}\"\n                )\n                raise QdrantStoreError(msg) from exc\n\n        try:\n            return self.DISTANCE_BY_SIMILARITY[similarity]\n        except KeyError as ke:\n            msg = (\n                f\"Provided similarity '{similarity}' is not supported by Qdrant \"\n                f\"document store. Please choose one of the options: \"\n                f\"{', '.join(metric.value for metric in QdrantSimilarityMetric)}\"\n            )\n            raise QdrantStoreError(msg) from ke\n\n    def _create_payload_index(self, collection_name: str, payload_fields_to_index: list[dict] | None = None):\n        \"\"\"Create payload index for the collection if payload_fields_to_index is provided.\"\"\"\n\n        if not payload_fields_to_index:\n            return\n\n        for payload_index in payload_fields_to_index:\n            field_name = payload_index[\"field_name\"]\n            field_schema = payload_index[\"field_schema\"]\n\n            if field_name in self._indexed_payload_fields:\n                continue\n\n            try:\n                self.client.create_payload_index(\n                    collection_name=collection_name,\n                    field_name=field_name,\n                    field_schema=field_schema,\n                    wait=True,\n                )\n            except UnexpectedResponse as exc:\n                status_code = getattr(exc, \"status_code\", None)\n                content = exc.content.decode(\"utf-8\", \"ignore\") if getattr(exc, \"content\", None) else \"\"\n                if status_code in {400, 409} and \"already exists\" in content.lower():\n                    logger.debug(\n                        \"Payload index for field %s already exists in collection %s\", field_name, collection_name\n                    )\n                else:\n                    raise\n\n            self._indexed_payload_fields.add(field_name)\n            self._auto_index_attempted.add(field_name)\n\n    def _execute_with_payload_index_retry(self, operation: Callable[[], T]) -&gt; T:\n        \"\"\"Execute and auto-create missing payload indexes, retrying a few times if needed.\"\"\"\n\n        attempts = 0\n        last_exc: UnexpectedResponse | None = None\n\n        while True:\n            try:\n                return operation()\n            except UnexpectedResponse as exc:\n                last_exc = exc\n                attempts += 1\n\n                if not self._try_create_missing_payload_index(exc):\n                    raise\n\n                if attempts &gt; 5:\n                    raise last_exc\n\n    def _extract_error_text(self, exc: UnexpectedResponse) -&gt; str:\n        \"\"\"Extract the error text from a Qdrant UnexpectedResponse.\"\"\"\n\n        if not getattr(exc, \"content\", None):\n            return \"\"\n\n        payload = exc.content.decode(\"utf-8\", \"ignore\")\n\n        try:\n            data = json.loads(payload)\n        except Exception:\n            return payload\n\n        if isinstance(data, dict):\n            status = data.get(\"status\")\n            if isinstance(status, dict):\n                error = status.get(\"error\")\n                if isinstance(error, str):\n                    return error\n        return payload\n\n    def _try_create_missing_payload_index(self, exc: UnexpectedResponse) -&gt; bool:\n        \"\"\"Inspect an error and create the required payload index if the server requests it.\"\"\"\n\n        if getattr(exc, \"status_code\", None) != 400 or not getattr(exc, \"content\", None):\n            return False\n\n        message = self._extract_error_text(exc).replace('\"', '\"')\n        logger.debug(\"Qdrant query error: %s\", message)\n\n        match = self._MISSING_INDEX_PATTERN.search(message)\n        if not match:\n            logger.debug(\"No payload index hint found in error message\")\n            return False\n\n        field_name = match.group(\"field\")\n        if field_name in self._auto_index_attempted:\n            return False\n\n        type_candidates = [candidate.strip().strip(\"\\\"' \") for candidate in match.group(\"types\").split(\",\")]\n        schema = self._resolve_payload_schema(type_candidates)\n\n        logger.info(\n            \"Creating payload index for missing field %s in collection %s (schema=%s)\",\n            field_name,\n            self.index_name,\n            schema.value,\n        )\n\n        self._auto_index_attempted.add(field_name)\n        self._create_payload_index(\n            collection_name=self.index_name,\n            payload_fields_to_index=[{\"field_name\": field_name, \"field_schema\": schema}],\n        )\n        return True\n\n    def _resolve_payload_schema(self, type_candidates: list[str]) -&gt; rest.PayloadSchemaType:\n        \"\"\"Resolve the payload schema type, defaulting to KEYWORD when unknown.\"\"\"\n\n        for candidate in type_candidates:\n            schema = self._PAYLOAD_SCHEMA_BY_NAME.get(candidate.lower())\n            if schema:\n                return schema\n        return rest.PayloadSchemaType.KEYWORD\n\n    def _set_up_collection(\n        self,\n        collection_name: str,\n        embedding_dim: int,\n        create_if_not_exist: bool,\n        recreate_collection: bool,\n        similarity: str,\n        use_sparse_embeddings: bool,\n        sparse_idf: bool,\n        on_disk: bool = False,\n        payload_fields_to_index: list[dict] | None = None,\n    ):\n        \"\"\"Sets up the Qdrant collection with the specified parameters.\n\n        Args:\n            collection_name: The name of the collection to set up.\n            embedding_dim: The dimension of the embeddings.\n            recreate_collection: Whether to recreate the collection if it already exists.\n            similarity: The similarity measure to use.\n            use_sparse_embeddings: Whether to use sparse embeddings.\n            sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n                for BM42.\n            on_disk: Whether to store the collection on disk.\n            payload_fields_to_index: List of payload fields to index.\n\n        Raises:\n            QdrantStoreError: If the collection exists with incompatible settings.\n            ValueError: If the collection exists with a different similarity measure or embedding dimension.\n        \"\"\"\n        distance = self.get_distance(similarity)\n        collection_exists = self._collection_exists(collection_name)\n\n        should_create = (not collection_exists and create_if_not_exist) or recreate_collection\n\n        if not collection_exists and not create_if_not_exist:\n            msg = f\"Collection '{collection_name}' does not exist in Qdrant.\"\n            raise QdrantStoreError(msg)\n\n        if should_create:\n            logger.info(f\"{'Creating' if not collection_exists else 'Recreating'} collection {collection_name}\")\n            self.recreate_collection(\n                collection_name=collection_name,\n                distance=distance,\n                embedding_dim=embedding_dim,\n                on_disk=on_disk,\n                use_sparse_embeddings=use_sparse_embeddings,\n                sparse_idf=sparse_idf,\n            )\n            self._track_collection(collection_name)\n            if payload_fields_to_index:\n                self._create_payload_index(collection_name, payload_fields_to_index)\n            return\n\n        collection_info = self.client.get_collection(collection_name)\n\n        has_named_vectors = (\n            isinstance(collection_info.config.params.vectors, dict)\n            and DENSE_VECTORS_NAME in collection_info.config.params.vectors\n        )\n\n        if self.use_sparse_embeddings and not has_named_vectors:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it has been originally created without sparse embedding vectors. \"\n                f\"If you want to use that collection, you can set `use_sparse_embeddings=False`. \"\n                f\"To use sparse embeddings, you need to recreate the collection or migrate the existing one. \"\n                f\"See `migrate_to_sparse_embeddings_support` function in \"\n                f\"`dynamiq_integrations.document_stores.qdrant`.\"\n            )\n            raise QdrantStoreError(msg)\n\n        elif not self.use_sparse_embeddings and has_named_vectors:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it has been originally created with sparse embedding vectors.\"\n                f\"If you want to use that collection, please set `use_sparse_embeddings=True`.\"\n            )\n            raise QdrantStoreError(msg)\n\n        if self.use_sparse_embeddings:\n            current_distance = collection_info.config.params.vectors[DENSE_VECTORS_NAME].distance\n            current_vector_size = collection_info.config.params.vectors[DENSE_VECTORS_NAME].size\n        else:\n            current_distance = collection_info.config.params.vectors.distance\n            current_vector_size = collection_info.config.params.vectors.size\n\n        if current_distance != distance:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it is configured with a similarity '{current_distance.name}'. \"\n                f\"If you want to use that collection, but with a different \"\n                f\"similarity, please set `recreate_collection=True` argument.\"\n            )\n            raise ValueError(msg)\n\n        if current_vector_size != embedding_dim:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it is configured with a vector size '{current_vector_size}'. \"\n                f\"If you want to use that collection, but with a different \"\n                f\"vector size, please set `recreate_collection=True` argument.\"\n            )\n            raise ValueError(msg)\n\n    def recreate_collection(\n        self,\n        collection_name: str,\n        distance,\n        embedding_dim: int,\n        on_disk: bool | None = None,\n        use_sparse_embeddings: bool | None = None,\n        sparse_idf: bool = False,\n    ):\n        \"\"\"Recreates the Qdrant collection with the specified parameters.\n\n        Args:\n            collection_name: The name of the collection to recreate.\n            distance: The distance metric to use for the collection.\n            embedding_dim: The dimension of the embeddings.\n            on_disk: Whether to store the collection on disk.\n            use_sparse_embeddings: Whether to use sparse embeddings.\n            sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n                for BM42.\n        \"\"\"\n        if on_disk is None:\n            on_disk = self.on_disk\n\n        if use_sparse_embeddings is None:\n            use_sparse_embeddings = self.use_sparse_embeddings\n\n        # dense vectors configuration\n        vectors_config = rest.VectorParams(size=embedding_dim, on_disk=on_disk, distance=distance)\n\n        # Reset cached payload index information when recreating a collection\n        self._indexed_payload_fields.clear()\n        self._auto_index_attempted.clear()\n\n        if use_sparse_embeddings:\n            # in this case, we need to define named vectors\n            vectors_config = {DENSE_VECTORS_NAME: vectors_config}\n\n            sparse_vectors_config = {\n                SPARSE_VECTORS_NAME: rest.SparseVectorParams(\n                    index=rest.SparseIndexParams(\n                        on_disk=on_disk,\n                    ),\n                    modifier=rest.Modifier.IDF if sparse_idf else None,\n                ),\n            }\n\n        if self._collection_exists(collection_name):\n            self.client.delete_collection(collection_name)\n\n        try:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=vectors_config,\n                sparse_vectors_config=sparse_vectors_config if use_sparse_embeddings else None,\n                shard_number=self.shard_number,\n                replication_factor=self.replication_factor,\n                write_consistency_factor=self.write_consistency_factor,\n                on_disk_payload=self.on_disk_payload,\n                hnsw_config=self.hnsw_config,\n                optimizers_config=self.optimizers_config,\n                wal_config=self.wal_config,\n                quantization_config=self.quantization_config,\n                init_from=self.init_from,\n            )\n        except UnexpectedResponse as exc:\n            if getattr(exc, \"status_code\", None) == 404:\n                msg = (\n                    f\"Failed to create collection '{collection_name}'. Qdrant returned 404 for the create API. \"\n                    f\"Double-check the service URL or required prefix for your deployment.\"\n                )\n                raise QdrantStoreError(msg) from exc\n            raise\n\n    def _handle_duplicate_documents(\n        self,\n        documents: list[Document],\n        index: str | None = None,\n        policy: DuplicatePolicy = None,\n    ):\n        \"\"\"Checks whether any of the passed documents is already existing in the chosen index and returns a list of\n        documents that are not in the index yet.\n\n        Args:\n            documents: A list of Dynamiq Document objects.\n            index: Name of the index.\n            policy: The duplicate policy to use when writing documents.\n\n        Returns:\n            A list of Dynamiq Document objects.\n        \"\"\"\n\n        index = index or self.index_name\n        if policy in (DuplicatePolicy.SKIP, DuplicatePolicy.FAIL):\n            documents = self._drop_duplicate_documents(documents, index)\n            documents_found = self.get_documents_by_id(ids=[doc.id for doc in documents], index=index)\n            ids_exist_in_db: list[str] = [doc.id for doc in documents_found]\n\n            if len(ids_exist_in_db) &gt; 0 and policy == DuplicatePolicy.FAIL:\n                msg = f\"Document with ids '{', '.join(ids_exist_in_db)} already exists in index = '{index}'.\"\n                raise DuplicateDocumentError(msg)\n\n            documents = list(filter(lambda doc: doc.id not in ids_exist_in_db, documents))\n\n        return documents\n\n    def _drop_duplicate_documents(self, documents: list[Document], index: str | None = None) -&gt; list[Document]:\n        \"\"\"Drop duplicate documents based on same hash ID.\n\n        Args:\n            documents: A list of Dynamiq Document objects.\n            index: Name of the index.\n\n        Returns:\n            A list of Dynamiq Document objects.\n        \"\"\"\n        _hash_ids: set = set()\n        _documents: list[Document] = []\n\n        for document in documents:\n            if document.id in _hash_ids:\n                logger.info(\n                    \"Duplicate Documents: Document with id '%s' already exists in index '%s'\",\n                    document.id,\n                    index or self.index_name,\n                )\n                continue\n            _documents.append(document)\n            _hash_ids.add(document.id)\n\n        return _documents\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.__init__","title":"<code>__init__(connection=None, client=None, location=None, url=None, port=6333, grpc_port=6334, prefer_grpc=False, https=None, api_key=None, prefix=None, timeout=None, host=None, path=None, force_disable_check_same_thread=False, index_name='Document', dimension=1536, on_disk=False, use_sparse_embeddings=False, sparse_idf=False, metric=QdrantSimilarityMetric.COSINE, return_embedding=False, create_if_not_exist=False, recreate_index=False, shard_number=None, replication_factor=None, write_consistency_factor=None, on_disk_payload=None, hnsw_config=None, optimizers_config=None, wal_config=None, quantization_config=None, init_from=None, wait_result_from_api=True, metadata=None, write_batch_size=100, scroll_size=10000, payload_fields_to_index=None, content_key='content', dry_run_config=None)</code>","text":"<p>Initializes the QdrantDocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str | None</code> <p>If <code>memory</code> - use in-memory Qdrant instance. If <code>str</code> - use it as a URL parameter. If <code>None</code> - use default values for host and port.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>Either host or str of <code>Optional[scheme], host, Optional[port], Optional[prefix]</code>.</p> <code>None</code> <code>port</code> <code>int</code> <p>Port of the REST API interface.</p> <code>6333</code> <code>grpc_port</code> <code>int</code> <p>Port of the gRPC interface.</p> <code>6334</code> <code>prefer_grpc</code> <code>bool</code> <p>If <code>True</code> - use gRPC interface whenever possible in custom methods.</p> <code>False</code> <code>https</code> <code>bool | None</code> <p>If <code>True</code> - use HTTPS(SSL) protocol.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for authentication in Qdrant Cloud.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>If not <code>None</code> - add prefix to the REST URL path. Example: service/v1 will result in http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>Timeout for REST and gRPC API requests.</p> <code>None</code> <code>host</code> <code>str | None</code> <p>Host name of Qdrant service. If <code>url</code> and <code>host</code> are <code>None</code>, set to <code>localhost</code>.</p> <code>None</code> <code>path</code> <code>str | None</code> <p>Persistence path for QdrantLocal.</p> <code>None</code> <code>force_disable_check_same_thread</code> <code>bool</code> <p>For QdrantLocal, force disable check_same_thread. Only use this if you can guarantee that you can resolve the thread safety outside QdrantClient.</p> <code>False</code> <code>index_name</code> <code>str</code> <p>Name of the index.</p> <code>'Document'</code> <code>dimension</code> <code>int</code> <p>Dimension of the embeddings.</p> <code>1536</code> <code>on_disk</code> <code>bool</code> <p>Whether to store the collection on disk.</p> <code>False</code> <code>use_sparse_embedding</code> <p>If set to <code>True</code>, enables support for sparse embeddings.</p> required <code>sparse_idf</code> <code>bool</code> <p>If set to <code>True</code>, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It is required to use techniques like BM42. It is ignored if <code>use_sparse_embeddings</code> is <code>False</code>.</p> <code>False</code> <code>metric</code> <code>QdrantSimilarityMetric</code> <p>The similarity metric to use.</p> <code>COSINE</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return embeddings in the search results.</p> <code>False</code> <code>recreate_index</code> <code>bool</code> <p>Whether to recreate the index.</p> <code>False</code> <code>shard_number</code> <code>int | None</code> <p>Number of shards in the collection.</p> <code>None</code> <code>replication_factor</code> <code>int | None</code> <p>Replication factor for the collection. Defines how many copies of each shard will be created. Effective only in distributed mode.</p> <code>None</code> <code>write_consistency_factor</code> <code>int | None</code> <p>Write consistency factor for the collection. Minimum value is 1. Defines how many replicas should apply to the operation for it to be considered successful. Increasing this number makes the collection more resilient to inconsistencies but will cause failures if not enough replicas are available. Effective only in distributed mode.</p> <code>None</code> <code>on_disk_payload</code> <code>bool | None</code> <p>If <code>True</code>, the point's payload will not be stored in memory and will be read from the disk every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed payload values remain in RAM.</p> <code>None</code> <code>hnsw_config</code> <code>dict | None</code> <p>Params for HNSW index.</p> <code>None</code> <code>optimizers_config</code> <code>dict | None</code> <p>Params for optimizer.</p> <code>None</code> <code>wal_config</code> <code>dict | None</code> <p>Params for Write-Ahead-Log.</p> <code>None</code> <code>quantization_config</code> <code>dict | None</code> <p>Params for quantization. If <code>None</code>, quantization will be disabled.</p> <code>None</code> <code>init_from</code> <code>dict | None</code> <p>Use data stored in another collection to initialize this collection.</p> <code>None</code> <code>wait_result_from_api</code> <code>bool</code> <p>Whether to wait for the result from the API after each request.</p> <code>True</code> <code>metadata</code> <code>dict | None</code> <p>Additional metadata to include with the documents.</p> <code>None</code> <code>write_batch_size</code> <code>int</code> <p>The batch size for writing documents.</p> <code>100</code> <code>scroll_size</code> <code>int</code> <p>The scroll size for reading documents.</p> <code>10000</code> <code>payload_fields_to_index</code> <code>list[dict] | None</code> <p>List of payload fields to index.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>'content'</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def __init__(\n    self,\n    connection: QdrantConnection | None = None,\n    client: Optional[\"QdrantClient\"] = None,\n    location: str | None = None,\n    url: str | None = None,\n    port: int = 6333,\n    grpc_port: int = 6334,\n    prefer_grpc: bool = False,\n    https: bool | None = None,\n    api_key: str | None = None,\n    prefix: str | None = None,\n    timeout: int | None = None,\n    host: str | None = None,\n    path: str | None = None,\n    force_disable_check_same_thread: bool = False,\n    index_name: str = \"Document\",\n    dimension: int = 1536,\n    on_disk: bool = False,\n    use_sparse_embeddings: bool = False,\n    sparse_idf: bool = False,\n    metric: QdrantSimilarityMetric = QdrantSimilarityMetric.COSINE,\n    return_embedding: bool = False,\n    create_if_not_exist: bool = False,\n    recreate_index: bool = False,\n    shard_number: int | None = None,\n    replication_factor: int | None = None,\n    write_consistency_factor: int | None = None,\n    on_disk_payload: bool | None = None,\n    hnsw_config: dict | None = None,\n    optimizers_config: dict | None = None,\n    wal_config: dict | None = None,\n    quantization_config: dict | None = None,\n    init_from: dict | None = None,\n    wait_result_from_api: bool = True,\n    metadata: dict | None = None,\n    write_batch_size: int = 100,\n    scroll_size: int = 10_000,\n    payload_fields_to_index: list[dict] | None = None,\n    content_key: str = \"content\",\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"Initializes the QdrantDocumentStore.\n\n    Args:\n        location: If `memory` - use in-memory Qdrant instance. If `str` - use it as a URL parameter. If `None` - use\n            default values for host and port.\n        url: Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.\n        port: Port of the REST API interface.\n        grpc_port: Port of the gRPC interface.\n        prefer_grpc: If `True` - use gRPC interface whenever possible in custom methods.\n        https: If `True` - use HTTPS(SSL) protocol.\n        api_key: API key for authentication in Qdrant Cloud.\n        prefix: If not `None` - add prefix to the REST URL path. Example: service/v1 will result in\n            http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\n        timeout: Timeout for REST and gRPC API requests.\n        host: Host name of Qdrant service. If `url` and `host` are `None`, set to `localhost`.\n        path: Persistence path for QdrantLocal.\n        force_disable_check_same_thread: For QdrantLocal, force disable check_same_thread. Only use this if you can\n            guarantee that you can resolve the thread safety outside QdrantClient.\n        index_name: Name of the index.\n        dimension: Dimension of the embeddings.\n        on_disk: Whether to store the collection on disk.\n        use_sparse_embedding: If set to `True`, enables support for sparse embeddings.\n        sparse_idf: If set to `True`, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It\n            is required to use techniques like BM42. It is ignored if `use_sparse_embeddings` is `False`.\n        metric: The similarity metric to use.\n        return_embedding: Whether to return embeddings in the search results.\n        recreate_index: Whether to recreate the index.\n        shard_number: Number of shards in the collection.\n        replication_factor: Replication factor for the collection. Defines how many copies of each shard will be\n            created. Effective only in distributed mode.\n        write_consistency_factor: Write consistency factor for the collection. Minimum value is 1. Defines how many\n            replicas should apply to the operation for it to be considered successful. Increasing this number makes\n            the collection more resilient to inconsistencies but will cause failures if not enough replicas are\n            available. Effective only in distributed mode.\n        on_disk_payload: If `True`, the point's payload will not be stored in memory and will be read from the disk\n            every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed\n            payload values remain in RAM.\n        hnsw_config: Params for HNSW index.\n        optimizers_config: Params for optimizer.\n        wal_config: Params for Write-Ahead-Log.\n        quantization_config: Params for quantization. If `None`, quantization will be disabled.\n        init_from: Use data stored in another collection to initialize this collection.\n        wait_result_from_api: Whether to wait for the result from the API after each request.\n        metadata: Additional metadata to include with the documents.\n        write_batch_size: The batch size for writing documents.\n        scroll_size: The scroll size for reading documents.\n        payload_fields_to_index: List of payload fields to index.\n        content_key (Optional[str]): The field used to store content in the storage.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode. Defaults to None.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    self._client = client\n    if self._client is None:\n        connection = connection or QdrantConnection()\n        self._client = connection.connect()\n\n    # Store the Qdrant client specific attributes\n    self.location = location\n    self.url = url\n    self.port = port\n    self.grpc_port = grpc_port\n    self.prefer_grpc = prefer_grpc\n    self.https = https\n    self.api_key = api_key\n    self.prefix = prefix\n    self.timeout = timeout\n    self.host = host\n    self.path = path\n    self.force_disable_check_same_thread = force_disable_check_same_thread\n    self.metadata = metadata or {}\n    self.api_key = api_key\n\n    # Store the Qdrant collection specific attributes\n    self.shard_number = shard_number\n    self.replication_factor = replication_factor\n    self.write_consistency_factor = write_consistency_factor\n    self.on_disk_payload = on_disk_payload\n    self.hnsw_config = hnsw_config\n    self.optimizers_config = optimizers_config\n    self.wal_config = wal_config\n    self.quantization_config = quantization_config\n    self.init_from = init_from\n    self.wait_result_from_api = wait_result_from_api\n    self.create_if_not_exist = create_if_not_exist\n    self.recreate_index = recreate_index\n    self.payload_fields_to_index = payload_fields_to_index\n    self.use_sparse_embeddings = use_sparse_embeddings\n    self.sparse_idf = use_sparse_embeddings and sparse_idf\n    self.dimension = dimension\n    self.on_disk = on_disk\n    self.metric = metric\n    self.index_name = index_name\n    self.return_embedding = return_embedding\n    self.write_batch_size = write_batch_size\n    self.scroll_size = scroll_size\n    self.content_key = content_key\n    self._indexed_payload_fields: set[str] = set()\n    self._auto_index_attempted: set[str] = set()\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Returns the number of documents present in the Document Store.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of documents in the Document Store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"Returns the number of documents present in the Document Store.\n\n    Returns:\n        The number of documents in the Document Store.\n    \"\"\"\n    try:\n        response = self.client.count(\n            collection_name=self.index_name,\n        )\n        return response.count\n    except (UnexpectedResponse, ValueError):\n        # Qdrant local raises ValueError if the collection is not found, but\n        # with the remote server UnexpectedResponse is raised. Until that's unified,\n        # we need to catch both.\n        return 0\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete a Qdrant collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None):\n    \"\"\"\n    Delete a Qdrant collection.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.delete_collection(collection_name=collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Deletes documents that match the provided <code>document_ids</code> from the document store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str] | None</code> <p>The document ids to delete.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"Deletes documents that match the provided `document_ids` from the document store.\n\n    Args:\n        document_ids: The document ids to delete.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all:\n        self.client.delete_collection(collection_name=self.index_name)\n    elif document_ids:\n        ids = [convert_id(_id) for _id in document_ids]\n        try:\n            self.client.delete(\n                collection_name=self.index_name,\n                points_selector=ids,\n                wait=self.wait_result_from_api,\n            )\n        except KeyError:\n            logger.warning(\n                \"Called QdrantDocumentStore.delete_documents() on a non-existing ID\",\n            )\n    else:\n        raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the DocumentStore based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>The filters to apply to the document list.</p> required Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided filters.\n\n    Args:\n        filters (dict[str, Any]): The filters to apply to the document list.\n    \"\"\"\n    if filters:\n        documents = self.filter_documents(filters=filters)\n        document_ids = [doc.id for doc in documents]\n        self.delete_documents(document_ids=document_ids)\n    else:\n        raise ValueError(\"No filters provided to delete documents.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Returns the documents that match the provided filters.</p> <p>For a detailed specification of the filters, refer to the documentation</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | Filter | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of documents that match the given filters.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def filter_documents(\n    self,\n    filters: dict[str, Any] | rest.Filter | None = None,\n) -&gt; list[Document]:\n    \"\"\"Returns the documents that match the provided filters.\n\n    For a detailed specification of the filters, refer to the\n    [documentation](https://docs.dynamiq.deepset.ai/docs/metadata-filtering)\n\n    Args:\n        filters: The filters to apply to the document list.\n\n    Returns:\n        A list of documents that match the given filters.\n    \"\"\"\n    if filters and not isinstance(filters, dict) and not isinstance(filters, rest.Filter):\n        msg = \"Filter must be a dictionary or an instance of `qdrant_client.http.models.Filter`\"\n        raise ValueError(msg)\n\n    if filters and not isinstance(filters, rest.Filter) and \"operator\" not in filters:\n        raise ValueError(\"Filter must contain an 'operator' key\")\n\n    return list(\n        self.get_documents_generator(\n            filters,\n        )\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_distance","title":"<code>get_distance(similarity)</code>","text":"<p>Retrieves the distance metric for the specified similarity measure.</p> <p>Parameters:</p> Name Type Description Default <code>similarity</code> <code>str | QdrantSimilarityMetric</code> <p>The similarity measure to retrieve the distance.</p> required <p>Returns:</p> Type Description <code>Distance</code> <p>The corresponding rest.Distance object.</p> <p>Raises:</p> Type Description <code>QdrantStoreError</code> <p>If the provided similarity measure is not supported.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_distance(self, similarity: str | QdrantSimilarityMetric) -&gt; rest.Distance:\n    \"\"\"Retrieves the distance metric for the specified similarity measure.\n\n    Args:\n        similarity: The similarity measure to retrieve the distance.\n\n    Returns:\n        The corresponding rest.Distance object.\n\n    Raises:\n        QdrantStoreError: If the provided similarity measure is not supported.\n    \"\"\"\n    if isinstance(similarity, str):\n        try:\n            similarity = QdrantSimilarityMetric(similarity.lower())\n        except ValueError as exc:\n            msg = (\n                f\"Provided similarity '{similarity}' is not supported by Qdrant document store. \"\n                f\"Please choose one of the options: {', '.join(metric.value for metric in QdrantSimilarityMetric)}\"\n            )\n            raise QdrantStoreError(msg) from exc\n\n    try:\n        return self.DISTANCE_BY_SIMILARITY[similarity]\n    except KeyError as ke:\n        msg = (\n            f\"Provided similarity '{similarity}' is not supported by Qdrant \"\n            f\"document store. Please choose one of the options: \"\n            f\"{', '.join(metric.value for metric in QdrantSimilarityMetric)}\"\n        )\n        raise QdrantStoreError(msg) from ke\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_documents_by_id","title":"<code>get_documents_by_id(ids, index=None, content_key=None)</code>","text":"<p>Retrieves documents from Qdrant by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>A list of document IDs to retrieve.</p> required <code>index</code> <code>str | None</code> <p>The name of the index to retrieve documents from.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of documents.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_documents_by_id(\n    self, ids: list[str], index: str | None = None, content_key: str | None = None\n) -&gt; list[Document]:\n    \"\"\"Retrieves documents from Qdrant by their IDs.\n\n    Args:\n        ids: A list of document IDs to retrieve.\n        index: The name of the index to retrieve documents from.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        A list of documents.\n    \"\"\"\n    index = index or self.index_name\n\n    documents: list[Document] = []\n\n    ids = [convert_id(_id) for _id in ids]\n    records = self.client.retrieve(\n        collection_name=index,\n        ids=ids,\n        with_payload=True,\n        with_vectors=True,\n    )\n\n    for record in records:\n        documents.append(\n            convert_qdrant_point_to_dynamiq_document(\n                record,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                content_key=content_key or self.content_key,\n            )\n        )\n    return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_documents_generator","title":"<code>get_documents_generator(filters=None, include_embeddings=False, content_key=None)</code>","text":"<p>Returns a generator that yields documents from Qdrant based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | Filter | None</code> <p>Filters applied to the retrieved documents.</p> <code>None</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include the embeddings of the retrieved documents.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>A generator that yields documents retrieved from Qdrant.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_documents_generator(\n    self,\n    filters: dict[str, Any] | rest.Filter | None = None,\n    include_embeddings: bool = False,\n    content_key: str | None = None,\n) -&gt; Generator[Document, None, None]:\n    \"\"\"Returns a generator that yields documents from Qdrant based on the provided filters.\n\n    Args:\n        filters: Filters applied to the retrieved documents.\n        include_embeddings: Whether to include the embeddings of the retrieved documents.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        A generator that yields documents retrieved from Qdrant.\n    \"\"\"\n\n    index = self.index_name\n    qdrant_filters = convert_filters_to_qdrant(filters)\n\n    next_offset = None\n    stop_scrolling = False\n    while not stop_scrolling:\n        records, next_offset = self._execute_with_payload_index_retry(\n            lambda: self.client.scroll(\n                collection_name=index,\n                scroll_filter=qdrant_filters,\n                limit=self.scroll_size,\n                offset=next_offset,\n                with_payload=True,\n                with_vectors=include_embeddings,\n            )\n        )\n        stop_scrolling = next_offset is None or (\n            isinstance(next_offset, grpc.PointId) and next_offset.num == 0 and next_offset.uuid == \"\"\n        )\n\n        for record in records:\n            yield convert_qdrant_point_to_dynamiq_document(\n                record,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                content_key=content_key or self.content_key,\n            )\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False, content_key=None)</code>","text":"<p>Returns a list of all documents in the Document Store.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include the embeddings of the retrieved documents.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of all documents in the Document Store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n    \"\"\"Returns a list of all documents in the Document Store.\n\n    Args:\n        include_embeddings: Whether to include the embeddings of the retrieved documents.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        A list of all documents in the Document Store.\n    \"\"\"\n    return list(\n        self.get_documents_generator(\n            include_embeddings=include_embeddings, content_key=content_key or self.content_key\n        )\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.recreate_collection","title":"<code>recreate_collection(collection_name, distance, embedding_dim, on_disk=None, use_sparse_embeddings=None, sparse_idf=False)</code>","text":"<p>Recreates the Qdrant collection with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to recreate.</p> required <code>distance</code> <p>The distance metric to use for the collection.</p> required <code>embedding_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>on_disk</code> <code>bool | None</code> <p>Whether to store the collection on disk.</p> <code>None</code> <code>use_sparse_embeddings</code> <code>bool | None</code> <p>Whether to use sparse embeddings.</p> <code>None</code> <code>sparse_idf</code> <code>bool</code> <p>Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required for BM42.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def recreate_collection(\n    self,\n    collection_name: str,\n    distance,\n    embedding_dim: int,\n    on_disk: bool | None = None,\n    use_sparse_embeddings: bool | None = None,\n    sparse_idf: bool = False,\n):\n    \"\"\"Recreates the Qdrant collection with the specified parameters.\n\n    Args:\n        collection_name: The name of the collection to recreate.\n        distance: The distance metric to use for the collection.\n        embedding_dim: The dimension of the embeddings.\n        on_disk: Whether to store the collection on disk.\n        use_sparse_embeddings: Whether to use sparse embeddings.\n        sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n            for BM42.\n    \"\"\"\n    if on_disk is None:\n        on_disk = self.on_disk\n\n    if use_sparse_embeddings is None:\n        use_sparse_embeddings = self.use_sparse_embeddings\n\n    # dense vectors configuration\n    vectors_config = rest.VectorParams(size=embedding_dim, on_disk=on_disk, distance=distance)\n\n    # Reset cached payload index information when recreating a collection\n    self._indexed_payload_fields.clear()\n    self._auto_index_attempted.clear()\n\n    if use_sparse_embeddings:\n        # in this case, we need to define named vectors\n        vectors_config = {DENSE_VECTORS_NAME: vectors_config}\n\n        sparse_vectors_config = {\n            SPARSE_VECTORS_NAME: rest.SparseVectorParams(\n                index=rest.SparseIndexParams(\n                    on_disk=on_disk,\n                ),\n                modifier=rest.Modifier.IDF if sparse_idf else None,\n            ),\n        }\n\n    if self._collection_exists(collection_name):\n        self.client.delete_collection(collection_name)\n\n    try:\n        self.client.create_collection(\n            collection_name=collection_name,\n            vectors_config=vectors_config,\n            sparse_vectors_config=sparse_vectors_config if use_sparse_embeddings else None,\n            shard_number=self.shard_number,\n            replication_factor=self.replication_factor,\n            write_consistency_factor=self.write_consistency_factor,\n            on_disk_payload=self.on_disk_payload,\n            hnsw_config=self.hnsw_config,\n            optimizers_config=self.optimizers_config,\n            wal_config=self.wal_config,\n            quantization_config=self.quantization_config,\n            init_from=self.init_from,\n        )\n    except UnexpectedResponse as exc:\n        if getattr(exc, \"status_code\", None) == 404:\n            msg = (\n                f\"Failed to create collection '{collection_name}'. Qdrant returned 404 for the create API. \"\n                f\"Double-check the service URL or required prefix for your deployment.\"\n            )\n            raise QdrantStoreError(msg) from exc\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.FAIL, content_key=None)</code>","text":"<p>Writes documents to Qdrant using the specified policy.</p> <p>The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are: - <code>FAIL</code>: The operation will raise an error if any document already exists. - <code>OVERWRITE</code>: Existing documents will be overwritten with the new ones. - <code>SKIP</code>: Existing documents will be skipped, and only new documents will be added.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>A list of Document objects to write to Qdrant.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>The policy for handling duplicate documents.</p> <code>FAIL</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of documents written to the document store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def write_documents(\n    self,\n    documents: list[Document],\n    policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n    content_key: str | None = None,\n) -&gt; int:\n    \"\"\"Writes documents to Qdrant using the specified policy.\n\n    The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are:\n    - `FAIL`: The operation will raise an error if any document already exists.\n    - `OVERWRITE`: Existing documents will be overwritten with the new ones.\n    - `SKIP`: Existing documents will be skipped, and only new documents will be added.\n\n    Args:\n        documents: A list of Document objects to write to Qdrant.\n        policy: The policy for handling duplicate documents.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        The number of documents written to the document store.\n    \"\"\"\n    if not self._collection_exists(self.index_name):\n        if self.create_if_not_exist:\n            logger.info(f\"Collection {self.index_name} doesn't exist. Creating...\")\n            self._set_up_collection(\n                collection_name=self.index_name,\n                embedding_dim=self.dimension,\n                create_if_not_exist=True,\n                recreate_collection=self.recreate_index,\n                similarity=self.metric,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                sparse_idf=self.sparse_idf,\n                on_disk=self.on_disk,\n            )\n        else:\n            raise QdrantStoreError(f\"Collection {self.index_name} doesn't exist\")\n    for doc in documents:\n        if not isinstance(doc, Document):\n            msg = f\"DocumentStore.write_documents() expects a list of Documents but got an element of {type(doc)}.\"\n            raise ValueError(msg)\n\n    if len(documents) == 0:\n        logger.warning(\"Calling QdrantDocumentStore.write_documents() with empty list\")\n        return 0\n\n    document_objects = self._handle_duplicate_documents(\n        documents=documents,\n        index=self.index_name,\n        policy=policy,\n    )\n\n    batched_documents = get_batches_from_generator(document_objects, self.write_batch_size)\n    for document_batch in batched_documents:\n        batch = convert_dynamiq_documents_to_qdrant_points(\n            document_batch,\n            use_sparse_embeddings=self.use_sparse_embeddings,\n            content_key=content_key or self.content_key,\n        )\n\n        self.client.upsert(\n            collection_name=self.index_name,\n            points=batch,\n            wait=self.wait_result_from_api,\n        )\n\n    self._track_documents([doc.id for doc in documents])\n\n    return len(document_objects)\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.SparseEmbedding","title":"<code>SparseEmbedding</code>","text":"<p>Class representing a sparse embedding.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>class SparseEmbedding:\n    \"\"\"Class representing a sparse embedding.\"\"\"\n\n    indices: list[int]\n    values: list[float]\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.get_batches_from_generator","title":"<code>get_batches_from_generator(iterable, n)</code>","text":"<p>Batch elements of an iterable into fixed-length chunks or blocks.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <p>The iterable to batch.</p> required <code>n</code> <p>The size of each batch.</p> required <p>Yields:</p> Type Description <p>Batches of the iterable.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_batches_from_generator(iterable, n):\n    \"\"\"Batch elements of an iterable into fixed-length chunks or blocks.\n\n    Args:\n        iterable: The iterable to batch.\n        n: The size of each batch.\n\n    Yields:\n        Batches of the iterable.\n    \"\"\"\n    it = iter(iterable)\n    x = tuple(islice(it, n))\n    while x:\n        yield x\n        x = tuple(islice(it, n))\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/weaviate/filters/#dynamiq.storages.vector.weaviate.filters.convert_filters","title":"<code>convert_filters(filters)</code>","text":"<p>Convert filters from dynamiq format to Weaviate format.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters in dynamiq format.</p> required <p>Returns:</p> Name Type Description <code>FilterReturn</code> <code>FilterReturn</code> <p>Filters in Weaviate format.</p> <p>Raises:</p> Type Description <code>VectorStoreFilterException</code> <p>If filters are not a dictionary.</p> Source code in <code>dynamiq/storages/vector/weaviate/filters.py</code> <pre><code>def convert_filters(filters: dict[str, Any]) -&gt; FilterReturn:\n    \"\"\"\n    Convert filters from dynamiq format to Weaviate format.\n\n    Args:\n        filters (dict[str, Any]): Filters in dynamiq format.\n\n    Returns:\n        FilterReturn: Filters in Weaviate format.\n\n    Raises:\n        VectorStoreFilterException: If filters are not a dictionary.\n    \"\"\"\n    if not isinstance(filters, dict):\n        msg = \"Filters must be a dictionary\"\n        raise VectorStoreFilterException(msg)\n\n    if \"field\" in filters:\n        return Filter.all_of([_parse_comparison_condition(filters)])\n    return _parse_logical_condition(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateRetrieverVectorStoreParams","title":"<code>WeaviateRetrieverVectorStoreParams</code>","text":"<p>               Bases: <code>BaseVectorStoreParams</code></p> <p>Parameters for using existing Weaviate collections with tenant context.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>class WeaviateRetrieverVectorStoreParams(BaseVectorStoreParams):\n    \"\"\"Parameters for using existing Weaviate collections with tenant context.\"\"\"\n    alpha: float = 0.5\n    tenant_name: str | None = None\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore","title":"<code>WeaviateVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code>, <code>DryRunMixin</code></p> <p>A Document Store for Weaviate.</p> <p>This class can be used with Weaviate Cloud Services or self-hosted instances.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>class WeaviateVectorStore(BaseVectorStore, DryRunMixin):\n    \"\"\"\n    A Document Store for Weaviate.\n\n    This class can be used with Weaviate Cloud Services or self-hosted instances.\n    \"\"\"\n\n    PATTERN_COLLECTION_NAME = re.compile(r\"^[A-Z][_0-9A-Za-z]*$\")\n    PATTERN_PROPERTY_NAME = re.compile(r\"^[_A-Za-z][_0-9A-Za-z]*$\")\n    _PROPERTY_DATA_TYPES: dict[str, str] = {\n        \"content\": DataType.TEXT,\n        \"message_content\": DataType.TEXT,\n        \"message_role\": DataType.TEXT,\n        \"message_timestamp\": DataType.NUMBER,\n        \"message_id\": DataType.TEXT,\n        \"user_id\": DataType.TEXT,\n        \"session_id\": DataType.TEXT,\n    }\n\n    def _get_property_type(self, property_name: str) -&gt; str:\n        \"\"\"Gets the Weaviate data type for a known property, defaults to TEXT.\"\"\"\n        if property_name == self.content_key:\n            return DataType.TEXT\n        return self._PROPERTY_DATA_TYPES.get(property_name, DataType.TEXT)\n\n    @staticmethod\n    def is_valid_collection_name(name: str) -&gt; bool:\n        return bool(WeaviateVectorStore.PATTERN_COLLECTION_NAME.fullmatch(name))\n\n    @staticmethod\n    def is_valid_property_name(name: str) -&gt; bool:\n        \"\"\"\n        Check if a property name is valid according to Weaviate naming rules.\n\n        Args:\n            name (str): The property name to check\n\n        Returns:\n            bool: True if the name is valid, False otherwise\n        \"\"\"\n        return bool(WeaviateVectorStore.PATTERN_PROPERTY_NAME.fullmatch(name))\n\n    @classmethod\n    def _fix_and_validate_index_name(cls, index_name: str) -&gt; str:\n        \"\"\"\n        Fix the index name if it starts with a lowercase letter and then validate it.\n        Logs a warning if the index name is corrected.\n        \"\"\"\n        if index_name and index_name[0].islower():\n            fixed_name = index_name[0].upper() + index_name[1:]\n            logger.warning(\n                f\"Index name '{index_name}' starts with a lowercase letter. \"\n                f\"Automatically updating it to '{fixed_name}'.\"\n            )\n            index_name = fixed_name\n        if not cls.is_valid_collection_name(index_name):\n            msg = (\n                f\"Collection name '{index_name}' is invalid. It must match the pattern \"\n                f\"{cls.PATTERN_COLLECTION_NAME.pattern}\"\n            )\n            raise ValueError(msg)\n        return index_name\n\n    def __init__(\n        self,\n        connection: Weaviate | None = None,\n        client: Optional[\"WeaviateClient\"] = None,\n        index_name: str = \"default\",\n        create_if_not_exist: bool = False,\n        content_key: str = \"content\",\n        tenant_name: str | None = None,\n        alpha: float = 0.5,\n        dry_run_config: DryRunConfig | None = None,\n    ):\n        \"\"\"\n        Initialize a new instance of WeaviateDocumentStore and connect to the\n        Weaviate instance.\n\n        Args:\n            connection (Weaviate | None): A Weaviate connection object. If None, a\n                new one is created.\n            client (Optional[WeaviateClient]): A Weaviate client. If None, one is\n                created from the connection.\n            index_name (str): The name of the index to use. Defaults to \"default\".\n            content_key (Optional[str]): The field used to store content in the\n                storage.\n            tenant_name (str | None): The name of the tenant to use for all operations.\n                If provided, multi-tenancy will be enabled for the collection.\n            alpha (float): The alpha value used for hybrid retrieval operations. Controls\n                the balance between keyword and vector search. Defaults to 0.5.\n            dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode.\n        \"\"\"\n        super().__init__(dry_run_config=dry_run_config)\n\n        # Validate and normalize the index name\n        index_name = self._fix_and_validate_index_name(index_name)\n        collection_name = index_name\n\n        # Initialize client\n        self.client = client\n        if self.client is None:\n            if connection is None:\n                connection = Weaviate()\n            self.client = connection.connect()\n\n        # Store multi-tenancy configuration\n        self._multi_tenancy_enabled = tenant_name is not None\n        self.content_key = content_key\n        self.alpha = alpha\n\n        # Create collection if needed or validate existing collection\n        if not self.client.collections.exists(collection_name):\n            if create_if_not_exist:\n                self._create_collection(collection_name, tenant_name)\n                self._track_collection(collection_name)\n            else:\n                raise ValueError(\n                    f\"Collection '{collection_name}' does not exist. Set 'create_if_not_exist' to True to create it.\"\n                )\n\n        # Get the base collection\n        base_collection = self.client.collections.get(collection_name)\n\n        # Get the actual multi-tenancy configuration from the collection\n        self._update_multi_tenancy_status(base_collection)\n\n        # Set up the collection - either with tenant context or without\n        self._setup_collection(base_collection, collection_name, tenant_name)\n\n    def _create_collection(\n        self, collection_name: str, tenant_name: str | None, properties_to_define: list[str] | None = None\n    ):\n        \"\"\"\n        Create a new Weaviate collection with appropriate configuration and properties.\n\n        Args:\n            collection_name: Name of the collection to create\n            tenant_name: Optional tenant name to enable multi-tenancy\n            properties_to_define: List of property names to explicitly define in the schema.\n        \"\"\"\n        collection_config_params = {\n            \"name\": collection_name,\n            \"inverted_index_config\": Configure.inverted_index(index_null_state=True),\n            \"vector_index_config\": Configure.VectorIndex.hnsw(),\n        }\n\n        if tenant_name is not None:\n            collection_config_params[\"multi_tenancy_config\"] = Configure.multi_tenancy(enabled=True)\n\n        properties = []\n        all_props_to_define = set(properties_to_define or [])\n        all_props_to_define.add(self.content_key)\n        all_props_to_define.add(\"_original_id\")\n\n        for prop_name in all_props_to_define:\n            if self.is_valid_property_name(prop_name):\n                prop_type = self._get_property_type(prop_name)\n                properties.append(Property(name=prop_name, data_type=prop_type))\n                logger.debug(f\"Prepared property definition: {prop_name} (Type: {prop_type})\")\n            else:\n                logger.warning(f\"Skipping definition for invalid property name: '{prop_name}'\")\n\n        if properties:\n            collection_config_params[\"properties\"] = properties\n        try:\n            self.client.collections.create(**collection_config_params)\n            logger.info(f\"Created collection '{collection_name}' with defined properties.\")\n        except ObjectAlreadyExistsException:\n            logger.warning(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n        except Exception as e:\n            logger.error(f\"Failed to create collection '{collection_name}': {e}\")\n            raise\n\n    def delete_collection(self, collection_name: str | None = None):\n        \"\"\"\n        Delete a Weaviate collection.\n\n        Args:\n            collection_name (str | None): Name of the collection to delete.\n        \"\"\"\n        try:\n            collection_to_delete = collection_name or self.index_name\n            self.client.collections.delete(collection_to_delete)\n            logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n        except Exception as e:\n            logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n            raise\n\n    def ensure_properties_exist(self, properties_to_ensure: list[str]):\n        \"\"\"\n        Checks if properties exist in the schema and adds them if they don't.\n\n        Args:\n            properties_to_ensure: A list of property names to check/add.\n        \"\"\"\n        if not properties_to_ensure:\n            return\n\n        try:\n            collection_config = self._collection.config.get()\n            existing_properties = {prop.name for prop in collection_config.properties}\n\n            for prop_name in properties_to_ensure:\n                if prop_name not in existing_properties and self.is_valid_property_name(prop_name):\n                    prop_type = self._get_property_type(prop_name)\n                    try:\n                        self._collection.config.add_property(Property(name=prop_name, data_type=prop_type))\n                        logger.info(\n                            f\"Added missing property '{prop_name}' \"\n                            f\"(Type: {prop_type}) to collection \"\n                            f\"'{self._collection.name}' schema.\"\n                        )\n                    except Exception as add_err:\n                        logger.error(f\"Failed to add property '{prop_name}' to schema: {add_err}\")\n                elif prop_name in existing_properties:\n                    logger.debug(f\"Property '{prop_name}' already exists in schema.\")\n                elif not self.is_valid_property_name(prop_name):\n                    logger.warning(f\"Cannot ensure invalid property name: '{prop_name}'\")\n\n        except Exception as e:\n            logger.error(f\"Failed to check or update schema properties for collection '{self._collection.name}': {e}\")\n\n    def _update_multi_tenancy_status(self, collection):\n        \"\"\"\n        Update the multi-tenancy status based on the actual collection configuration.\n\n        Args:\n            collection: The Weaviate collection\n        \"\"\"\n        try:\n            collection_config = collection.config.get()\n            actual_multi_tenancy_enabled = False\n\n            if hasattr(collection_config, \"multi_tenancy_config\") and collection_config.multi_tenancy_config:\n                actual_multi_tenancy_enabled = collection_config.multi_tenancy_config.enabled\n\n            # Update instance variable to reflect actual configuration\n            self._multi_tenancy_enabled = actual_multi_tenancy_enabled\n\n        except Exception as e:\n            logger.warning(f\"Failed to retrieve multi-tenancy configuration: {str(e)}\")\n            # Keep the inferred multi-tenancy setting as fallback\n\n    def _setup_collection(self, base_collection, collection_name, tenant_name):\n        \"\"\"\n        Set up the collection with or without tenant context.\n\n        Args:\n            base_collection: The base Weaviate collection\n            collection_name: Name of the collection\n            tenant_name: Optional tenant name to use\n        \"\"\"\n        # No tenant specified - use the base collection\n        if not tenant_name:\n            self._collection = base_collection\n            self._tenant_name = None\n            return\n\n        # Tenant specified but multi-tenancy is disabled in the collection\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\n                f\"Collection '{collection_name}' has multi-tenancy disabled, \"\n                f\"but tenant_name '{tenant_name}' was provided. \"\n                f\"To use a tenant, create the collection with a tenant_name parameter.\"\n            )\n\n        # Tenant specified and multi-tenancy is enabled\n        self._ensure_tenant_exists(base_collection, tenant_name)\n        self._collection = base_collection.with_tenant(tenant_name)\n        self._tenant_name = tenant_name\n\n    def _ensure_tenant_exists(self, collection, tenant_name):\n        \"\"\"\n        Check if the tenant exists and create it if it doesn't.\n\n        Args:\n            collection: The Weaviate collection\n            tenant_name: Name of the tenant to check/create\n        \"\"\"\n        try:\n            # Use get_by_name method from Weaviate client if available\n            tenant = None\n            try:\n                # Modern Weaviate client approach\n                tenant = collection.tenants.get_by_name(tenant_name)\n            except AttributeError:\n                # Fallback for older clients - search in the list\n                tenants = collection.tenants.get()\n\n                # Handle different response formats\n                if isinstance(tenants, dict) and tenant_name in tenants:\n                    tenant = tenants[tenant_name]\n                elif isinstance(tenants, list):\n                    for t in tenants:\n                        if (\n                            (isinstance(t, dict) and t.get(\"name\") == tenant_name)\n                            or (hasattr(t, \"name\") and t.name == tenant_name)\n                            or (str(t) == tenant_name)\n                        ):\n                            tenant = t\n                            break\n\n            # Create tenant if it doesn't exist\n            if not tenant:\n                logger.info(f\"Creating new tenant '{tenant_name}' in collection\")\n                collection.tenants.create(tenants=[Tenant(name=tenant_name)])\n                logger.info(f\"Successfully created tenant '{tenant_name}'\")\n            else:\n                logger.info(f\"Tenant '{tenant_name}' already exists, no need to create\")\n\n        except Exception as e:\n            logger.warning(\n                f\"Error while checking/creating tenant '{tenant_name}': {str(e)}\\n\"\n                f\"Error type: {type(e).__name__}\\n\"\n                f\"Will continue with the assumption that the tenant exists.\"\n            )\n\n    def close(self):\n        \"\"\"Close the connection to Weaviate.\"\"\"\n        if self.client:\n            self.client.close()\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the DocumentStore.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        total = self._collection.aggregate.over_all(total_count=True).total_count\n        return total if total else 0\n\n    def _to_data_object(self, document: Document, content_key: str | None = None) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert a Document to a Weaviate data object ready to be saved.\n\n        Args:\n            document (Document): The document to convert.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            dict[str, Any]: A dictionary representing the Weaviate data object.\n\n        Raises:\n            ValueError: If any property name is invalid according to Weaviate naming rules\n        \"\"\"\n        data = document.to_dict()\n        data[content_key or self.content_key] = data.pop(\"content\", \"\")\n        data[\"_original_id\"] = data.pop(\"id\")\n        metadata = data.get(\"metadata\", {})\n\n        # Validate and add metadata properties\n        for key, val in metadata.items():\n            if not self.is_valid_property_name(key):\n                raise ValueError(\n                    f\"Invalid property name: '{key}'. Property names must match the pattern: [_A-Za-z][_0-9A-Za-z]*\"\n                )\n            data[key] = val\n\n        # Ensure all property names in the data object are valid\n        invalid_props = []\n        for key in data:\n            if key not in [\"_original_id\", \"embedding\", \"metadata\"] and not self.is_valid_property_name(key):\n                invalid_props.append(key)\n\n        if invalid_props:\n            raise ValueError(\n                f\"Invalid property names: {invalid_props}. \"\n                \"Property names must match the pattern: [_A-Za-z][_0-9A-Za-z]*\"\n            )\n\n        del data[\"embedding\"]\n        del data[\"metadata\"]\n\n        return data\n\n    def _to_document(\n        self,\n        data: \"DataObject[dict[str, Any], None]\",\n        content_key: str | None = None,\n    ) -&gt; Document:\n        \"\"\"\n        Convert a data object read from Weaviate into a Document.\n\n        Args:\n            data (DataObject[dict[str, Any], None]): The data object from Weaviate.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            Document: The converted Document object.\n        \"\"\"\n        document_data = data.properties\n        document_id = document_data.pop(\"_original_id\")\n\n        content = document_data.pop(content_key or self.content_key) or \"\"\n\n        if isinstance(data.vector, list):\n            document_data[\"embedding\"] = data.vector\n        elif isinstance(data.vector, dict):\n            document_data[\"embedding\"] = data.vector.get(\"default\")\n        else:\n            document_data[\"embedding\"] = None\n\n        for key, value in document_data.items():\n            if isinstance(value, datetime.datetime):\n                document_data[key] = value.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        if weaviate_meta := getattr(data, \"metadata\", None):\n            if weaviate_meta.score is not None:\n                document_data[\"score\"] = weaviate_meta.score\n            elif weaviate_meta.certainty is not None:\n                document_data[\"score\"] = weaviate_meta.certainty\n\n        score = document_data.pop(\"score\", None)\n        embedding = document_data.pop(\"embedding\", None)\n\n        data = {\n            \"id\": str(document_id),\n            \"content\": content,\n            \"metadata\": document_data,\n            \"score\": score,\n            \"embedding\": embedding,\n        }\n\n        logger.debug(f\"Document loaded from Weaviate: {data}\")\n\n        return Document(**data)\n\n    def add_tenants(self, tenant_names: list[str]) -&gt; None:\n        \"\"\"\n        Add new tenants to the collection.\n\n        Args:\n            tenant_names (list[str]): List of tenant names to add.\n        \"\"\"\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n        tenants = [Tenant(name=name) for name in tenant_names]\n        self._collection.tenants.create(tenants=tenants)\n\n    def remove_tenants(self, tenant_names: list[str]) -&gt; None:\n        \"\"\"\n        Remove tenants from the collection.\n\n        Args:\n            tenant_names (list[str]): List of tenant names to remove.\n        \"\"\"\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n        self._collection.tenants.remove(tenant_names)\n\n    def list_tenants(self) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        List all tenants in the collection.\n\n        Returns:\n            list[dict[str, Any]]: List of tenant information.\n        \"\"\"\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n        return self._collection.tenants.get()\n\n    def get_tenant(self, tenant_name: str) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Get information about a specific tenant.\n\n        Args:\n            tenant_name (str): Name of the tenant to get.\n\n        Returns:\n            dict[str, Any] | None: Tenant information or None if tenant doesn't exist.\n        \"\"\"\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n        try:\n            # Try using direct tenant lookup if available in the client version\n            try:\n                return self._collection.tenants.get_by_name(tenant_name)\n            except AttributeError:\n                # Fallback for older client versions\n                tenants = self.list_tenants()\n\n                # Search through the list of tenants\n                for tenant in tenants:\n                    if isinstance(tenant, dict) and tenant.get(\"name\") == tenant_name:\n                        return tenant\n                    elif hasattr(tenant, \"name\") and tenant.name == tenant_name:\n                        return tenant\n\n                # Not found\n                return None\n        except Exception as e:\n            logger.warning(f\"Error getting tenant '{tenant_name}': {str(e)}\")\n            return None\n\n    def update_tenant_status(self, tenant_name: str, status: TenantActivityStatus) -&gt; None:\n        \"\"\"\n        Update the activity status of a tenant.\n\n        Args:\n            tenant_name (str): Name of the tenant to update.\n            status (TenantActivityStatus): New activity status (ACTIVE, INACTIVE, or OFFLOADED).\n\n        Raises:\n            ValueError: If multi-tenancy is not enabled or the tenant doesn't exist.\n        \"\"\"\n        if not self._multi_tenancy_enabled:\n            raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n        # Check if tenant exists\n        tenant = self.get_tenant(tenant_name)\n        if not tenant:\n            raise ValueError(f\"Tenant '{tenant_name}' does not exist\")\n\n        try:\n            self._collection.tenants.update(tenants=[Tenant(name=tenant_name, activity_status=status)])\n            logger.info(f\"Updated tenant '{tenant_name}' status to {status}\")\n        except Exception as e:\n            logger.error(f\"Failed to update tenant '{tenant_name}' status: {str(e)}\")\n            raise\n\n    def _query(self) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Query all documents from Weaviate.\n\n        Returns:\n            list[dict[str, Any]]: A list of all documents in the store.\n\n        Raises:\n            VectorStoreException: If the query fails.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n        try:\n            result = self._collection.iterator(\n                include_vector=True, return_properties=properties\n            )\n        except WeaviateQueryError as e:\n            msg = f\"Failed to query documents in Weaviate. Error: {e.message}\"\n            raise VectorStoreException(msg) from e\n        return result\n\n    def _query_with_filters(self, filters: dict[str, Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Query documents from Weaviate with filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the query.\n\n        Returns:\n            list[dict[str, Any]]: A list of documents matching the filters.\n\n        Raises:\n            VectorStoreException: If the query fails.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n\n        offset = 0\n        partial_result = None\n        result = []\n        while partial_result is None or len(partial_result.objects) == DEFAULT_QUERY_LIMIT:\n            try:\n                partial_result = self._collection.query.fetch_objects(\n                    filters=convert_filters(filters),\n                    include_vector=True,\n                    limit=DEFAULT_QUERY_LIMIT,\n                    offset=offset,\n                    return_properties=properties,\n                )\n            except WeaviateQueryError as e:\n                msg = f\"Failed to query documents in Weaviate. Error: {e.message}\"\n                raise VectorStoreException(msg) from e\n            result.extend(partial_result.objects)\n            offset += DEFAULT_QUERY_LIMIT\n        return result\n\n    def filter_documents(self, filters: dict[str, Any] | None = None, content_key: str | None = None) -&gt; list[Document]:\n        \"\"\"\n        Filter documents based on the provided filters.\n\n        Args:\n            filters (dict[str, Any] | None): The filters to apply to the document list.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: A list of Documents that match the given filters.\n        \"\"\"\n        if filters:\n            result = self._query_with_filters(filters)\n        else:\n            result = self._query()\n        return [self._to_document(doc, content_key=content_key) for doc in result]\n\n    def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the DocumentStore.\n\n        Args:\n            include_embeddings (bool): Whether to include document embeddings in the result.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: A list of all documents in the store.\n        \"\"\"\n        documents = []\n        for item in self._collection.iterator(include_vector=include_embeddings):\n            document = self._to_document(item, content_key=content_key or self.content_key)\n            documents.append(document)\n        return documents\n\n    def _batch_write(self, documents: list[Document], content_key: str | None = None) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate in batches.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            int: The number of documents written.\n\n        Raises:\n            ValueError: If any of the input is not a Document.\n            VectorStoreException: If the write operation fails.\n        \"\"\"\n        with self.client.batch.dynamic() as batch:\n            for doc in documents:\n                if not isinstance(doc, Document):\n                    msg = f\"Expected a Document, got '{type(doc)}' instead.\"\n                    raise ValueError(msg)\n\n                # Create the batch add parameters\n                batch_params = {\n                    \"properties\": self._to_data_object(doc, content_key=content_key),\n                    \"collection\": self._collection.name,\n                    \"uuid\": generate_uuid5(doc.id),\n                    \"vector\": doc.embedding,\n                }\n\n                # Add tenant parameter if multi-tenancy is enabled and tenant is specified\n                if self._multi_tenancy_enabled and self._tenant_name:\n                    batch_params[\"tenant\"] = self._tenant_name\n\n                # Add the object with the appropriate parameters\n                batch.add_object(**batch_params)\n\n        if failed_objects := self.client.batch.failed_objects:\n            mapped_objects = {}\n            for obj in failed_objects:\n                properties = obj.object_.properties or {}\n                id_ = properties.get(\"_original_id\", obj.object_.uuid)\n                mapped_objects[id_] = obj.message if hasattr(obj, \"message\") else str(obj)\n\n            msg = \"\\n\".join(\n                [\n                    f\"Failed to write object with id '{id_}'. Error: '{message}'\"\n                    for id_, message in mapped_objects.items()\n                ]\n            )\n            raise VectorStoreException(msg)\n\n        return len(documents)\n\n    def _write(self, documents: list[Document], policy: DuplicatePolicy, content_key: str | None = None) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate using the specified policy.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n            policy (DuplicatePolicy): The policy to use for handling duplicates.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            int: The number of documents written.\n\n        Raises:\n            ValueError: If any of the input is not a Document.\n            VectorStoreDuplicateDocumentException: If duplicates are found with FAIL policy.\n        \"\"\"\n        written = 0\n        duplicate_errors_ids = []\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = f\"Expected a Document, got '{type(doc)}' instead.\"\n                raise ValueError(msg)\n\n            if policy == DuplicatePolicy.SKIP and self._collection.data.exists(uuid=generate_uuid5(doc.id)):\n                continue\n\n            try:\n                # Create the insert parameters\n                insert_params = {\n                    \"uuid\": generate_uuid5(doc.id),\n                    \"properties\": self._to_data_object(doc, content_key=content_key),\n                    \"vector\": doc.embedding,\n                }\n\n                # Add tenant parameter if multi-tenancy is enabled and tenant is specified\n                # Note: This shouldn't be necessary when using self._collection with tenant context,\n                # but added for consistency with _batch_write\n                if self._multi_tenancy_enabled and self._tenant_name:\n                    insert_params[\"tenant\"] = self._tenant_name\n\n                self._collection.data.insert(**insert_params)\n                written += 1\n            except UnexpectedStatusCodeError:\n                if policy == DuplicatePolicy.FAIL:\n                    duplicate_errors_ids.append(doc.id)\n        if duplicate_errors_ids:\n            msg = f\"IDs '{', '.join(duplicate_errors_ids)}' already exist in the document store.\"\n            raise VectorStoreDuplicateDocumentException(msg)\n        return written\n\n    def write_documents(\n        self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.NONE, content_key: str | None = None\n    ) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate using the specified policy.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n            policy (DuplicatePolicy): The policy to use for handling duplicates.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            int: The number of documents written.\n        \"\"\"\n        self._track_documents([doc.id for doc in documents])\n\n        if policy in [DuplicatePolicy.NONE, DuplicatePolicy.OVERWRITE]:\n            return self._batch_write(documents, content_key=content_key)\n\n        return self._write(documents, policy, content_key=content_key)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore.\n\n        Args:\n            document_ids (list[str], optional): The IDs of documents to delete.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n\n        Raises:\n            ValueError: If neither document_ids nor delete_all is provided.\n        \"\"\"\n        if delete_all:\n            weaviate_ids = [item.uuid for item in self._collection.iterator()]\n        elif document_ids:\n            weaviate_ids = [generate_uuid5(doc_id) for doc_id in document_ids]\n        else:\n            msg = \"Either 'document_ids' or 'delete_all' must be set.\"\n            raise ValueError(msg)\n        self._collection.data.delete_many(\n            where=Filter.by_id().contains_any(weaviate_ids)\n        )\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the document list.\n        \"\"\"\n        if filters:\n            self._collection.data.delete_many(where=convert_filters(filters))\n        else:\n            raise ValueError(\"No filters provided to delete documents.\")\n\n    def _keyword_retrieval(\n        self,\n        query: str,\n        filters: dict[str, Any] | None = None,\n        top_k: int | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform BM25 retrieval on the documents.\n\n        Args:\n            query (str): The query string.\n            filters (dict[str, Any] | None): Filters to apply to the query.\n            top_k (int | None): The number of top results to return.\n\n        Returns:\n            list[Document]: A list of retrieved documents.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n        result = self._collection.query.bm25(\n            query=query,\n            filters=convert_filters(filters) if filters else None,\n            limit=top_k,\n            include_vector=True,\n            query_properties=[\"content\"],\n            return_properties=properties,\n            return_metadata=[\"score\"],\n        )\n\n        return [self._to_document(doc) for doc in result.objects]\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        filters: dict[str, Any] | None = None,\n        top_k: int | None = None,\n        exclude_document_embeddings=True,\n        distance: float | None = None,\n        certainty: float | None = None,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform embedding-based retrieval on the documents.\n\n        Args:\n            query_embedding (list[float]): The query embedding.\n            filters (dict[str, Any] | None): Filters to apply to the query.\n            top_k (int | None): The number of top results to return.\n            exclude_document_embeddings (bool): Whether to exclude document embeddings in the result.\n            distance (float | None): The maximum distance for retrieval.\n            certainty (float | None): The minimum certainty for retrieval.\n            content_key (Optional[str]): The field used to store content in the storage.\n\n        Returns:\n            list[Document]: A list of retrieved documents.\n\n        Raises:\n            ValueError: If both distance and certainty are provided.\n        \"\"\"\n        if distance is not None and certainty is not None:\n            msg = \"Can't use 'distance' and 'certainty' parameters together\"\n            raise ValueError(msg)\n\n        properties = [p.name for p in self._collection.config.get().properties]\n        result = self._collection.query.near_vector(\n            near_vector=query_embedding,\n            distance=distance,\n            certainty=certainty,\n            include_vector=not exclude_document_embeddings,\n            filters=convert_filters(filters) if filters else None,\n            limit=top_k,\n            return_properties=properties,\n            return_metadata=[\"certainty\"],\n        )\n\n        return [self._to_document(doc, content_key=content_key) for doc in result.objects]\n\n    def _hybrid_retrieval(\n        self,\n        query_embedding: list[float],\n        query: str,\n        filters: dict[str, Any] | None = None,\n        top_k: int | None = None,\n        exclude_document_embeddings=True,\n        alpha: float = 0.5,\n        fusion_type: HybridFusion = HybridFusion.RELATIVE_SCORE,\n        content_key: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform hybrid retrieval on the documents.\n\n        Args:\n            query (str): The query string.\n            filters (dict[str, Any] | None): Filters to apply to the query.\n            top_k (int | None): The number of top results to return.\n\n        Returns:\n            list[Document]: A list of retrieved documents.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n\n        query_alpha = self.alpha if alpha is None else alpha\n        result = self._collection.query.hybrid(\n            query=query,\n            vector=query_embedding,\n            filters=convert_filters(filters) if filters else None,\n            limit=top_k,\n            include_vector=not exclude_document_embeddings,\n            query_properties=[content_key or self.content_key],\n            return_properties=properties,\n            return_metadata=[\"score\"],\n            alpha=query_alpha,\n            fusion_type=fusion_type,\n        )\n\n        return [self._to_document(doc) for doc in result.objects]\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', create_if_not_exist=False, content_key='content', tenant_name=None, alpha=0.5, dry_run_config=None)</code>","text":"<p>Initialize a new instance of WeaviateDocumentStore and connect to the Weaviate instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Weaviate | None</code> <p>A Weaviate connection object. If None, a new one is created.</p> <code>None</code> <code>client</code> <code>Optional[WeaviateClient]</code> <p>A Weaviate client. If None, one is created from the connection.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>The name of the index to use. Defaults to \"default\".</p> <code>'default'</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>'content'</code> <code>tenant_name</code> <code>str | None</code> <p>The name of the tenant to use for all operations. If provided, multi-tenancy will be enabled for the collection.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The alpha value used for hybrid retrieval operations. Controls the balance between keyword and vector search. Defaults to 0.5.</p> <code>0.5</code> <code>dry_run_config</code> <code>Optional[DryRunConfig]</code> <p>Configuration for dry run mode.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def __init__(\n    self,\n    connection: Weaviate | None = None,\n    client: Optional[\"WeaviateClient\"] = None,\n    index_name: str = \"default\",\n    create_if_not_exist: bool = False,\n    content_key: str = \"content\",\n    tenant_name: str | None = None,\n    alpha: float = 0.5,\n    dry_run_config: DryRunConfig | None = None,\n):\n    \"\"\"\n    Initialize a new instance of WeaviateDocumentStore and connect to the\n    Weaviate instance.\n\n    Args:\n        connection (Weaviate | None): A Weaviate connection object. If None, a\n            new one is created.\n        client (Optional[WeaviateClient]): A Weaviate client. If None, one is\n            created from the connection.\n        index_name (str): The name of the index to use. Defaults to \"default\".\n        content_key (Optional[str]): The field used to store content in the\n            storage.\n        tenant_name (str | None): The name of the tenant to use for all operations.\n            If provided, multi-tenancy will be enabled for the collection.\n        alpha (float): The alpha value used for hybrid retrieval operations. Controls\n            the balance between keyword and vector search. Defaults to 0.5.\n        dry_run_config (Optional[DryRunConfig]): Configuration for dry run mode.\n    \"\"\"\n    super().__init__(dry_run_config=dry_run_config)\n\n    # Validate and normalize the index name\n    index_name = self._fix_and_validate_index_name(index_name)\n    collection_name = index_name\n\n    # Initialize client\n    self.client = client\n    if self.client is None:\n        if connection is None:\n            connection = Weaviate()\n        self.client = connection.connect()\n\n    # Store multi-tenancy configuration\n    self._multi_tenancy_enabled = tenant_name is not None\n    self.content_key = content_key\n    self.alpha = alpha\n\n    # Create collection if needed or validate existing collection\n    if not self.client.collections.exists(collection_name):\n        if create_if_not_exist:\n            self._create_collection(collection_name, tenant_name)\n            self._track_collection(collection_name)\n        else:\n            raise ValueError(\n                f\"Collection '{collection_name}' does not exist. Set 'create_if_not_exist' to True to create it.\"\n            )\n\n    # Get the base collection\n    base_collection = self.client.collections.get(collection_name)\n\n    # Get the actual multi-tenancy configuration from the collection\n    self._update_multi_tenancy_status(base_collection)\n\n    # Set up the collection - either with tenant context or without\n    self._setup_collection(base_collection, collection_name, tenant_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.add_tenants","title":"<code>add_tenants(tenant_names)</code>","text":"<p>Add new tenants to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_names</code> <code>list[str]</code> <p>List of tenant names to add.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def add_tenants(self, tenant_names: list[str]) -&gt; None:\n    \"\"\"\n    Add new tenants to the collection.\n\n    Args:\n        tenant_names (list[str]): List of tenant names to add.\n    \"\"\"\n    if not self._multi_tenancy_enabled:\n        raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n    tenants = [Tenant(name=name) for name in tenant_names]\n    self._collection.tenants.create(tenants=tenants)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.close","title":"<code>close()</code>","text":"<p>Close the connection to Weaviate.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection to Weaviate.\"\"\"\n    if self.client:\n        self.client.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the DocumentStore.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the DocumentStore.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    total = self._collection.aggregate.over_all(total_count=True).total_count\n    return total if total else 0\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_collection","title":"<code>delete_collection(collection_name=None)</code>","text":"<p>Delete a Weaviate collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str | None</code> <p>Name of the collection to delete.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_collection(self, collection_name: str | None = None):\n    \"\"\"\n    Delete a Weaviate collection.\n\n    Args:\n        collection_name (str | None): Name of the collection to delete.\n    \"\"\"\n    try:\n        collection_to_delete = collection_name or self.index_name\n        self.client.collections.delete(collection_to_delete)\n        logger.info(f\"Deleted collection '{collection_to_delete}'.\")\n    except Exception as e:\n        logger.error(f\"Failed to delete collection '{collection_to_delete}': {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the DocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>The IDs of documents to delete.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither document_ids nor delete_all is provided.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore.\n\n    Args:\n        document_ids (list[str], optional): The IDs of documents to delete.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n\n    Raises:\n        ValueError: If neither document_ids nor delete_all is provided.\n    \"\"\"\n    if delete_all:\n        weaviate_ids = [item.uuid for item in self._collection.iterator()]\n    elif document_ids:\n        weaviate_ids = [generate_uuid5(doc_id) for doc_id in document_ids]\n    else:\n        msg = \"Either 'document_ids' or 'delete_all' must be set.\"\n        raise ValueError(msg)\n    self._collection.data.delete_many(\n        where=Filter.by_id().contains_any(weaviate_ids)\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the DocumentStore based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>The filters to apply to the document list.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided filters.\n\n    Args:\n        filters (dict[str, Any]): The filters to apply to the document list.\n    \"\"\"\n    if filters:\n        self._collection.data.delete_many(where=convert_filters(filters))\n    else:\n        raise ValueError(\"No filters provided to delete documents.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.ensure_properties_exist","title":"<code>ensure_properties_exist(properties_to_ensure)</code>","text":"<p>Checks if properties exist in the schema and adds them if they don't.</p> <p>Parameters:</p> Name Type Description Default <code>properties_to_ensure</code> <code>list[str]</code> <p>A list of property names to check/add.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def ensure_properties_exist(self, properties_to_ensure: list[str]):\n    \"\"\"\n    Checks if properties exist in the schema and adds them if they don't.\n\n    Args:\n        properties_to_ensure: A list of property names to check/add.\n    \"\"\"\n    if not properties_to_ensure:\n        return\n\n    try:\n        collection_config = self._collection.config.get()\n        existing_properties = {prop.name for prop in collection_config.properties}\n\n        for prop_name in properties_to_ensure:\n            if prop_name not in existing_properties and self.is_valid_property_name(prop_name):\n                prop_type = self._get_property_type(prop_name)\n                try:\n                    self._collection.config.add_property(Property(name=prop_name, data_type=prop_type))\n                    logger.info(\n                        f\"Added missing property '{prop_name}' \"\n                        f\"(Type: {prop_type}) to collection \"\n                        f\"'{self._collection.name}' schema.\"\n                    )\n                except Exception as add_err:\n                    logger.error(f\"Failed to add property '{prop_name}' to schema: {add_err}\")\n            elif prop_name in existing_properties:\n                logger.debug(f\"Property '{prop_name}' already exists in schema.\")\n            elif not self.is_valid_property_name(prop_name):\n                logger.warning(f\"Cannot ensure invalid property name: '{prop_name}'\")\n\n    except Exception as e:\n        logger.error(f\"Failed to check or update schema properties for collection '{self._collection.name}': {e}\")\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.filter_documents","title":"<code>filter_documents(filters=None, content_key=None)</code>","text":"<p>Filter documents based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Documents that match the given filters.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def filter_documents(self, filters: dict[str, Any] | None = None, content_key: str | None = None) -&gt; list[Document]:\n    \"\"\"\n    Filter documents based on the provided filters.\n\n    Args:\n        filters (dict[str, Any] | None): The filters to apply to the document list.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        list[Document]: A list of Documents that match the given filters.\n    \"\"\"\n    if filters:\n        result = self._query_with_filters(filters)\n    else:\n        result = self._query()\n    return [self._to_document(doc, content_key=content_key) for doc in result]\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.get_tenant","title":"<code>get_tenant(tenant_name)</code>","text":"<p>Get information about a specific tenant.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_name</code> <code>str</code> <p>Name of the tenant to get.</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict[str, Any] | None: Tenant information or None if tenant doesn't exist.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def get_tenant(self, tenant_name: str) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Get information about a specific tenant.\n\n    Args:\n        tenant_name (str): Name of the tenant to get.\n\n    Returns:\n        dict[str, Any] | None: Tenant information or None if tenant doesn't exist.\n    \"\"\"\n    if not self._multi_tenancy_enabled:\n        raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n    try:\n        # Try using direct tenant lookup if available in the client version\n        try:\n            return self._collection.tenants.get_by_name(tenant_name)\n        except AttributeError:\n            # Fallback for older client versions\n            tenants = self.list_tenants()\n\n            # Search through the list of tenants\n            for tenant in tenants:\n                if isinstance(tenant, dict) and tenant.get(\"name\") == tenant_name:\n                    return tenant\n                elif hasattr(tenant, \"name\") and tenant.name == tenant_name:\n                    return tenant\n\n            # Not found\n            return None\n    except Exception as e:\n        logger.warning(f\"Error getting tenant '{tenant_name}': {str(e)}\")\n        return None\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.is_valid_property_name","title":"<code>is_valid_property_name(name)</code>  <code>staticmethod</code>","text":"<p>Check if a property name is valid according to Weaviate naming rules.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The property name to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the name is valid, False otherwise</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>@staticmethod\ndef is_valid_property_name(name: str) -&gt; bool:\n    \"\"\"\n    Check if a property name is valid according to Weaviate naming rules.\n\n    Args:\n        name (str): The property name to check\n\n    Returns:\n        bool: True if the name is valid, False otherwise\n    \"\"\"\n    return bool(WeaviateVectorStore.PATTERN_PROPERTY_NAME.fullmatch(name))\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False, content_key=None)</code>","text":"<p>List all documents in the DocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include document embeddings in the result.</p> <code>False</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of all documents in the store.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False, content_key: str | None = None) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the DocumentStore.\n\n    Args:\n        include_embeddings (bool): Whether to include document embeddings in the result.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        list[Document]: A list of all documents in the store.\n    \"\"\"\n    documents = []\n    for item in self._collection.iterator(include_vector=include_embeddings):\n        document = self._to_document(item, content_key=content_key or self.content_key)\n        documents.append(document)\n    return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.list_tenants","title":"<code>list_tenants()</code>","text":"<p>List all tenants in the collection.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of tenant information.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def list_tenants(self) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    List all tenants in the collection.\n\n    Returns:\n        list[dict[str, Any]]: List of tenant information.\n    \"\"\"\n    if not self._multi_tenancy_enabled:\n        raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n    return self._collection.tenants.get()\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.remove_tenants","title":"<code>remove_tenants(tenant_names)</code>","text":"<p>Remove tenants from the collection.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_names</code> <code>list[str]</code> <p>List of tenant names to remove.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def remove_tenants(self, tenant_names: list[str]) -&gt; None:\n    \"\"\"\n    Remove tenants from the collection.\n\n    Args:\n        tenant_names (list[str]): List of tenant names to remove.\n    \"\"\"\n    if not self._multi_tenancy_enabled:\n        raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n    self._collection.tenants.remove(tenant_names)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.update_tenant_status","title":"<code>update_tenant_status(tenant_name, status)</code>","text":"<p>Update the activity status of a tenant.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_name</code> <code>str</code> <p>Name of the tenant to update.</p> required <code>status</code> <code>TenantActivityStatus</code> <p>New activity status (ACTIVE, INACTIVE, or OFFLOADED).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If multi-tenancy is not enabled or the tenant doesn't exist.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def update_tenant_status(self, tenant_name: str, status: TenantActivityStatus) -&gt; None:\n    \"\"\"\n    Update the activity status of a tenant.\n\n    Args:\n        tenant_name (str): Name of the tenant to update.\n        status (TenantActivityStatus): New activity status (ACTIVE, INACTIVE, or OFFLOADED).\n\n    Raises:\n        ValueError: If multi-tenancy is not enabled or the tenant doesn't exist.\n    \"\"\"\n    if not self._multi_tenancy_enabled:\n        raise ValueError(\"Multi-tenancy is not enabled for this collection\")\n\n    # Check if tenant exists\n    tenant = self.get_tenant(tenant_name)\n    if not tenant:\n        raise ValueError(f\"Tenant '{tenant_name}' does not exist\")\n\n    try:\n        self._collection.tenants.update(tenants=[Tenant(name=tenant_name, activity_status=status)])\n        logger.info(f\"Updated tenant '{tenant_name}' status to {status}\")\n    except Exception as e:\n        logger.error(f\"Failed to update tenant '{tenant_name}' status: {str(e)}\")\n        raise\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.NONE, content_key=None)</code>","text":"<p>Write documents to Weaviate using the specified policy.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The list of documents to write.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>The policy to use for handling duplicates.</p> <code>NONE</code> <code>content_key</code> <code>Optional[str]</code> <p>The field used to store content in the storage.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents written.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def write_documents(\n    self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.NONE, content_key: str | None = None\n) -&gt; int:\n    \"\"\"\n    Write documents to Weaviate using the specified policy.\n\n    Args:\n        documents (list[Document]): The list of documents to write.\n        policy (DuplicatePolicy): The policy to use for handling duplicates.\n        content_key (Optional[str]): The field used to store content in the storage.\n\n    Returns:\n        int: The number of documents written.\n    \"\"\"\n    self._track_documents([doc.id for doc in documents])\n\n    if policy in [DuplicatePolicy.NONE, DuplicatePolicy.OVERWRITE]:\n        return self._batch_write(documents, content_key=content_key)\n\n    return self._write(documents, policy, content_key=content_key)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateWriterVectorStoreParams","title":"<code>WeaviateWriterVectorStoreParams</code>","text":"<p>               Bases: <code>BaseWriterVectorStoreParams</code></p> <p>Parameters for creating and managing Weaviate collections with multi-tenancy.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>class WeaviateWriterVectorStoreParams(BaseWriterVectorStoreParams):\n    \"\"\"Parameters for creating and managing Weaviate collections with multi-tenancy.\"\"\"\n    tenant_name: str | None = None\n</code></pre>"},{"location":"dynamiq/types/constants/","title":"Constants","text":""},{"location":"dynamiq/types/document/","title":"Document","text":""},{"location":"dynamiq/types/document/#dynamiq.types.document.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document class for Dynamiq.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Callable[[], Any] | str | None</code> <p>Unique identifier. Defaults to UUID4 hex.</p> <code>content</code> <code>str</code> <p>Main content of the document.</p> <code>metadata</code> <code>dict | None</code> <p>Additional metadata. Defaults to None.</p> <code>embedding</code> <code>list | None</code> <p>Vector representation. Defaults to None.</p> <code>score</code> <code>float | None</code> <p>Relevance or similarity score. Defaults to None.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"Document class for Dynamiq.\n\n    Attributes:\n        id (Callable[[], Any] | str | None): Unique identifier. Defaults to UUID4 hex.\n        content (str): Main content of the document.\n        metadata (dict | None): Additional metadata. Defaults to None.\n        embedding (list | None): Vector representation. Defaults to None.\n        score (float | None): Relevance or similarity score. Defaults to None.\n    \"\"\"\n    id: Callable[[], Any] | str | None = Field(default_factory=lambda: uuid.uuid4().hex)\n    content: str\n    metadata: dict | None = None\n    embedding: list | None = None\n    score: float | None = None\n\n    def to_dict(self, for_tracing: bool = False, truncate_limit: int = TRUNCATE_EMBEDDINGS_LIMIT, **kwargs) -&gt; dict:\n        \"\"\"Convert the Document object to a dictionary.\n\n        Returns:\n            dict: Dictionary representation of the Document.\n        \"\"\"\n        data = self.model_dump(exclude={\"embedding\"}, **kwargs)\n\n        if for_tracing and self.embedding is not None and len(self.embedding) &gt; truncate_limit:\n            data[\"embedding\"] = self.embedding[:truncate_limit]\n        else:\n            data[\"embedding\"] = self.embedding\n\n        return data\n</code></pre>"},{"location":"dynamiq/types/document/#dynamiq.types.document.Document.to_dict","title":"<code>to_dict(for_tracing=False, truncate_limit=TRUNCATE_EMBEDDINGS_LIMIT, **kwargs)</code>","text":"<p>Convert the Document object to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of the Document.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>def to_dict(self, for_tracing: bool = False, truncate_limit: int = TRUNCATE_EMBEDDINGS_LIMIT, **kwargs) -&gt; dict:\n    \"\"\"Convert the Document object to a dictionary.\n\n    Returns:\n        dict: Dictionary representation of the Document.\n    \"\"\"\n    data = self.model_dump(exclude={\"embedding\"}, **kwargs)\n\n    if for_tracing and self.embedding is not None and len(self.embedding) &gt; truncate_limit:\n        data[\"embedding\"] = self.embedding[:truncate_limit]\n    else:\n        data[\"embedding\"] = self.embedding\n\n    return data\n</code></pre>"},{"location":"dynamiq/types/document/#dynamiq.types.document.DocumentCreationMode","title":"<code>DocumentCreationMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for document creation modes.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>class DocumentCreationMode(str, enum.Enum):\n    \"\"\"Enumeration for document creation modes.\"\"\"\n    ONE_DOC_PER_FILE = \"one-doc-per-file\"\n    ONE_DOC_PER_PAGE = \"one-doc-per-page\"\n    ONE_DOC_PER_ELEMENT = \"one-doc-per-element\"\n</code></pre>"},{"location":"dynamiq/types/dry_run/","title":"Dry run","text":""},{"location":"dynamiq/types/dry_run/#dynamiq.types.dry_run.DryRunConfig","title":"<code>DryRunConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for dry run.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether dry run is enabled. Defaults to False.</p> <code>delete_documents</code> <code>bool</code> <p>Whether to delete the ingested documents after the dry run. Defaults to True.</p> <code>delete_collection</code> <code>bool</code> <p>Whether to delete the created collection after the dry run. Defaults to False.</p> Source code in <code>dynamiq/types/dry_run.py</code> <pre><code>class DryRunConfig(BaseModel):\n    \"\"\"Configuration for dry run.\n\n    Attributes:\n        enabled (bool): Whether dry run is enabled. Defaults to False.\n        delete_documents: Whether to delete the ingested documents after the dry run. Defaults to True.\n        delete_collection: Whether to delete the created collection after the dry run. Defaults to False.\n    \"\"\"\n\n    enabled: bool = False\n    delete_documents: bool = Field(default=True, description=\"Delete the ingested documents\")\n    delete_collection: bool = Field(default=True, description=\"Delete the created collection\")\n</code></pre>"},{"location":"dynamiq/types/feedback/","title":"Feedback","text":""},{"location":"dynamiq/types/llm_tool/","title":"Llm tool","text":""},{"location":"dynamiq/types/streaming/","title":"Streaming","text":""},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.AgentReasoningEventMessageData","title":"<code>AgentReasoningEventMessageData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for agent reasoning streaming event data.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class AgentReasoningEventMessageData(BaseModel):\n    \"\"\"Model for agent reasoning streaming event data.\"\"\"\n\n    tool_run_id: str\n    thought: str\n    action: str\n    tool: AgentToolData\n    action_input: Any\n    loop_num: int\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.AgentToolData","title":"<code>AgentToolData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for tool information in agent reasoning events.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class AgentToolData(BaseModel):\n    \"\"\"Model for tool information in agent reasoning events.\"\"\"\n\n    name: str\n    type: str\n    action_type: str | None = None\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.AgentToolResultEventMessageData","title":"<code>AgentToolResultEventMessageData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for agent tool result streaming event data.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class AgentToolResultEventMessageData(BaseModel):\n    \"\"\"Model for agent tool result streaming event data.\"\"\"\n\n    tool_run_id: str\n    name: str\n    tool: AgentToolData\n    input: Any\n    result: Any\n    files: list = Field(default_factory=list)\n    loop_num: int\n    output: dict[str, Any] | None = None\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingConfig","title":"<code>StreamingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for streaming.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether streaming is enabled. Defaults to False.</p> <code>event</code> <code>str</code> <p>Event name. Defaults to \"streaming\".</p> <code>timeout</code> <code>float | None</code> <p>Timeout for streaming. Defaults to 600 seconds.</p> <code>input_queue</code> <code>Queue | None</code> <p>Input queue for streaming. Defaults to None.</p> <code>input_queue_done_event</code> <code>Event | None</code> <p>Event to signal input queue completion. Defaults to None.</p> <code>input_queue_poll_interval</code> <code>float</code> <p>Poll interval for checking done_event during input queue wait. Shorter interval allows faster response to cancellation. Defaults to 5.0 second.</p> <code>mode</code> <code>StreamingMode</code> <p>Streaming mode. Defaults to StreamingMode.ANSWER.</p> <code>include_usage</code> <code>bool</code> <p>Whether to include usage information. Defaults to False.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingConfig(BaseModel):\n    \"\"\"Configuration for streaming.\n\n    Attributes:\n        enabled (bool): Whether streaming is enabled. Defaults to False.\n        event (str): Event name. Defaults to \"streaming\".\n        timeout (float | None): Timeout for streaming. Defaults to 600 seconds.\n        input_queue (Queue | None): Input queue for streaming. Defaults to None.\n        input_queue_done_event (Event | None): Event to signal input queue completion. Defaults to None.\n        input_queue_poll_interval (float): Poll interval for checking done_event during input queue wait.\n            Shorter interval allows faster response to cancellation. Defaults to 5.0 second.\n        mode (StreamingMode): Streaming mode. Defaults to StreamingMode.ANSWER.\n        include_usage (bool): Whether to include usage information. Defaults to False.\n    \"\"\"\n    enabled: bool = False\n    event: str = STREAMING_EVENT\n    timeout: PositiveFloat | None = 600.0\n    input_queue: Queue | None = None\n    input_queue_done_event: Event | None = None\n    input_queue_poll_interval: PositiveFloat = 5.0\n    mode: StreamingMode = StreamingMode.FINAL\n    include_usage: bool = False\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @cached_property\n    def input_streaming_enabled(self) -&gt; bool:\n        \"\"\"Check if input streaming is enabled.\n\n        Returns:\n            bool: True if input streaming is enabled, False otherwise.\n        \"\"\"\n        return self.enabled and self.input_queue\n\n    def to_dict(self, for_tracing: bool = False, **kwargs) -&gt; dict:\n        if for_tracing and not self.enabled:\n            return {\"enabled\": False}\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingConfig.input_streaming_enabled","title":"<code>input_streaming_enabled: bool</code>  <code>cached</code> <code>property</code>","text":"<p>Check if input streaming is enabled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if input streaming is enabled, False otherwise.</p>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage","title":"<code>StreamingEventMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message for streaming events.</p> <p>Attributes:</p> Name Type Description <code>run_id</code> <code>str | None</code> <p>Run ID.</p> <code>wf_run_id</code> <code>str | None</code> <p>Workflow run ID. Defaults to a generated UUID.</p> <code>entity_id</code> <code>str</code> <p>Entity ID.</p> <code>data</code> <code>Any</code> <p>Data associated with the event.</p> <code>event</code> <code>str | None</code> <p>Event name. Defaults to \"streaming\".</p> <code>source</code> <code>StreamingEntitySource | None</code> <p>Entity details.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingEventMessage(BaseModel):\n    \"\"\"Message for streaming events.\n\n    Attributes:\n        run_id (str | None): Run ID.\n        wf_run_id (str | None): Workflow run ID. Defaults to a generated UUID.\n        entity_id (str): Entity ID.\n        data (Any): Data associated with the event.\n        event (str | None): Event name. Defaults to \"streaming\".\n        source (StreamingEntitySource | None): Entity details.\n    \"\"\"\n\n    run_id: str | None = None\n    wf_run_id: str | None = Field(default_factory=generate_uuid)\n    entity_id: str | None = None\n    data: Any\n    event: str | None = None\n    source: StreamingEntitySource | None = None\n\n    @field_validator(\"event\")\n    @classmethod\n    def set_event(cls, value: str | None) -&gt; str:\n        \"\"\"Set the event name.\n\n        Args:\n            value (str | None): Event name.\n\n        Returns:\n            str: Event name or default.\n        \"\"\"\n        return value or STREAMING_EVENT\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert to dictionary.\n\n        Returns:\n            dict: Dictionary representation.\n        \"\"\"\n        return self.model_dump(**kwargs)\n\n    def to_json(self, **kwargs) -&gt; str:\n        \"\"\"Convert to JSON string.\n\n        Returns:\n            str: JSON string representation.\n        \"\"\"\n        return self.model_dump_json(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.set_event","title":"<code>set_event(value)</code>  <code>classmethod</code>","text":"<p>Set the event name.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>Event name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Event name or default.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>@field_validator(\"event\")\n@classmethod\ndef set_event(cls, value: str | None) -&gt; str:\n    \"\"\"Set the event name.\n\n    Args:\n        value (str | None): Event name.\n\n    Returns:\n        str: Event name or default.\n    \"\"\"\n    return value or STREAMING_EVENT\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert to dictionary.\n\n    Returns:\n        dict: Dictionary representation.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.to_json","title":"<code>to_json(**kwargs)</code>","text":"<p>Convert to JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string representation.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>def to_json(self, **kwargs) -&gt; str:\n    \"\"\"Convert to JSON string.\n\n    Returns:\n        str: JSON string representation.\n    \"\"\"\n    return self.model_dump_json(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingMode","title":"<code>StreamingMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for streaming modes.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingMode(str, Enum):\n    \"\"\"Enumeration for streaming modes.\"\"\"\n\n    FINAL = \"final\"  # Streams only final output in agents nodes.\n    ALL = \"all\"  # Streams all intermediate steps and final output in agents and llms nodes.\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingThought","title":"<code>StreamingThought</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for reasoning/thought streaming chunks.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingThought(BaseModel):\n    \"\"\"Model for reasoning/thought streaming chunks.\"\"\"\n\n    thought: str\n    loop_num: int\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/utils/chat/","title":"Chat","text":""},{"location":"dynamiq/utils/chat/#dynamiq.utils.chat.format_chat_history","title":"<code>format_chat_history(chat_history)</code>","text":"<p>Format chat history for the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>list[dict[str, str]]</code> <p>List of chat entries.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted chat history.</p> Source code in <code>dynamiq/utils/chat.py</code> <pre><code>def format_chat_history(chat_history: list[dict[str, str]]) -&gt; str:\n    \"\"\"Format chat history for the orchestrator.\n\n    Args:\n        chat_history (list[dict[str, str]]): List of chat entries.\n\n    Returns:\n        str: Formatted chat history.\n    \"\"\"\n    formatted_history = \"\"\n    for entry in chat_history:\n        role = entry[\"role\"].title()\n        content = entry[\"content\"]\n        formatted_history += f\"{role}: {content}\\n\"\n    return formatted_history\n</code></pre>"},{"location":"dynamiq/utils/duration/","title":"Duration","text":""},{"location":"dynamiq/utils/duration/#dynamiq.utils.duration.format_duration","title":"<code>format_duration(start, end)</code>","text":"<p>Format the duration between two datetime objects into a human-readable string.</p> <p>This function calculates the time difference between the start and end datetimes and returns a formatted string representing the duration in milliseconds, seconds, minutes, or hours, depending on the length of the duration.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>The starting datetime.</p> required <code>end</code> <code>datetime</code> <p>The ending datetime.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string representing the duration.  - For durations less than 1 second: \"Xms\" (milliseconds)  - For durations between 1 second and 1 minute: \"Xs\" (seconds)  - For durations between 1 minute and 1 hour: \"Xm\" (minutes)  - For durations of 1 hour or more: \"Xh\" (hours)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; start = datetime(2023, 1, 1, 12, 0, 0)\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(milliseconds=500)))\n500ms\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(seconds=45)))\n45.0s\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(minutes=30)))\n30.0m\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(hours=2)))\n2.0h\n</code></pre> Source code in <code>dynamiq/utils/duration.py</code> <pre><code>def format_duration(start: datetime, end: datetime) -&gt; str:\n    \"\"\"\n    Format the duration between two datetime objects into a human-readable string.\n\n    This function calculates the time difference between the start and end datetimes and\n    returns a formatted string representing the duration in milliseconds, seconds, minutes,\n    or hours, depending on the length of the duration.\n\n    Args:\n        start (datetime): The starting datetime.\n        end (datetime): The ending datetime.\n\n    Returns:\n        str: A formatted string representing the duration.\n             - For durations less than 1 second: \"Xms\" (milliseconds)\n             - For durations between 1 second and 1 minute: \"Xs\" (seconds)\n             - For durations between 1 minute and 1 hour: \"Xm\" (minutes)\n             - For durations of 1 hour or more: \"Xh\" (hours)\n\n    Examples:\n        &gt;&gt;&gt; from datetime import datetime, timedelta\n        &gt;&gt;&gt; start = datetime(2023, 1, 1, 12, 0, 0)\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(milliseconds=500)))\n        500ms\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(seconds=45)))\n        45.0s\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(minutes=30)))\n        30.0m\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(hours=2)))\n        2.0h\n    \"\"\"\n    delta = end - start\n    total_seconds = delta.total_seconds()\n\n    if total_seconds &lt; 1:\n        return f\"{total_seconds * 1000:.0f}ms\"\n    elif total_seconds &lt; 60:\n        return f\"{round(total_seconds, 1)}s\"\n    elif total_seconds &lt; 3600:\n        return f\"{round(total_seconds / 60, 1)}m\"\n    else:\n        return f\"{round(total_seconds / 3600, 1)}h\"\n</code></pre>"},{"location":"dynamiq/utils/env/","title":"Env","text":""},{"location":"dynamiq/utils/env/#dynamiq.utils.env.get_env_var","title":"<code>get_env_var(var_name, default_value=None)</code>","text":"<p>Retrieves the value of an environment variable.</p> <p>This function attempts to retrieve the value of the specified environment variable. If the variable is not found and no default value is provided, it raises a ValueError.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>The name of the environment variable to retrieve.</p> required <code>default_value</code> <code>str</code> <p>The default value to return if the environment variable is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The value of the environment variable.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the environment variable is not found and no default value is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_env_var(\"HOME\")\n'/home/user'\n&gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\", \"default\")\n'default'\n&gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\")\nTraceback (most recent call last):\n    ...\nValueError: Environment variable 'NONEXISTENT_VAR' not found.\n</code></pre> Source code in <code>dynamiq/utils/env.py</code> <pre><code>def get_env_var(var_name: str, default_value: Any = None):\n    \"\"\"Retrieves the value of an environment variable.\n\n    This function attempts to retrieve the value of the specified environment variable. If the\n    variable is not found and no default value is provided, it raises a ValueError.\n\n    Args:\n        var_name (str): The name of the environment variable to retrieve.\n        default_value (str, optional): The default value to return if the environment variable\n            is not found. Defaults to None.\n\n    Returns:\n        str: The value of the environment variable.\n\n    Raises:\n        ValueError: If the environment variable is not found and no default value is provided.\n\n    Examples:\n        &gt;&gt;&gt; get_env_var(\"HOME\")\n        '/home/user'\n        &gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\", \"default\")\n        'default'\n        &gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\")\n        Traceback (most recent call last):\n            ...\n        ValueError: Environment variable 'NONEXISTENT_VAR' not found.\n    \"\"\"\n    value = os.environ.get(var_name, default_value)\n\n    if value is None:\n        logger.warning(f\"Environment variable '{var_name}' not found\")\n\n    return value\n</code></pre>"},{"location":"dynamiq/utils/feedback/","title":"Feedback","text":""},{"location":"dynamiq/utils/feedback/#dynamiq.utils.feedback.send_message","title":"<code>send_message(event_message, config, feedback_method=FeedbackMethod.STREAM)</code>","text":"<p>Emits message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>StreamingEventMessage</code> <p>Message to send.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> required <code>feedback_method</code> <code>FeedbackMethod</code> <p>Sets up where message is sent. Defaults to \"stream\".</p> <code>STREAM</code> Source code in <code>dynamiq/utils/feedback.py</code> <pre><code>def send_message(\n    event_message: StreamingEventMessage,\n    config: RunnableConfig,\n    feedback_method: FeedbackMethod = FeedbackMethod.STREAM,\n) -&gt; None:\n    \"\"\"Emits message\n\n    Args:\n        message (StreamingEventMessage): Message to send.\n        config (RunnableConfig): Configuration for the runnable.\n        feedback_method (FeedbackMethod, optional): Sets up where message is sent. Defaults to \"stream\".\n    \"\"\"\n\n    match feedback_method:\n        case FeedbackMethod.CONSOLE:\n            print(event_message.data)\n        case FeedbackMethod.STREAM:\n            for callback in config.callbacks:\n                callback.on_node_execute_stream({}, event=event_message)\n</code></pre>"},{"location":"dynamiq/utils/file_types/","title":"File types","text":""},{"location":"dynamiq/utils/json_parser/","title":"Json parser","text":""},{"location":"dynamiq/utils/json_parser/#dynamiq.utils.json_parser.clean_json_string","title":"<code>clean_json_string(json_str)</code>","text":"<p>Clean a JSON-like string so that it can be parsed by the built-in <code>json</code> module.</p> <ol> <li>Remove single-line (//...) and multi-line (/.../) comments outside of    string literals.</li> <li>Convert single-quoted string literals to double-quoted ones, preserving    internal apostrophes.</li> <li>Replace Python-specific boolean/null literals (True, False, None) with their    JSON equivalents (true, false, null).</li> <li>Remove trailing commas in objects and arrays.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The raw JSON-like string to clean.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A cleaned JSON string suitable for json.loads.</p> Source code in <code>dynamiq/utils/json_parser.py</code> <pre><code>def clean_json_string(json_str: str) -&gt; str:\n    \"\"\"\n    Clean a JSON-like string so that it can be parsed by the built-in `json` module.\n\n    1. Remove single-line (//...) and multi-line (/*...*/) comments outside of\n       string literals.\n    2. Convert single-quoted string literals to double-quoted ones, preserving\n       internal apostrophes.\n    3. Replace Python-specific boolean/null literals (True, False, None) with their\n       JSON equivalents (true, false, null).\n    4. Remove trailing commas in objects and arrays.\n\n    Args:\n        json_str: The raw JSON-like string to clean.\n\n    Returns:\n        A cleaned JSON string suitable for json.loads.\n    \"\"\"\n    # 1. Remove comments\n    json_str = _remove_comments_outside_strings(json_str)\n\n    # 2. Convert single\u2010quoted string literals -&gt; double\u2010quoted\n    pattern = r\"'((?:\\\\'|[^'])*)'\"\n    json_str = re.sub(pattern, single_quoted_replacer, json_str)\n\n    # 3. Replace Python-specific boolean/null with JSON equivalents\n    json_str = re.sub(r\"\\bTrue\\b\", \"true\", json_str)\n    json_str = re.sub(r\"\\bFalse\\b\", \"false\", json_str)\n    json_str = re.sub(r\"\\bNone\\b\", \"null\", json_str)\n\n    # 4. Remove trailing commas before a closing bracket or brace\n    json_str = re.sub(r\",\\s*(\\]|\\})\", r\"\\1\", json_str)\n\n    return json_str\n</code></pre>"},{"location":"dynamiq/utils/json_parser/#dynamiq.utils.json_parser.extract_json_string","title":"<code>extract_json_string(s)</code>","text":"<p>Extract the first JSON object or array from the string by balancing brackets. The function looks for '{' or '[' and keeps track of nested brackets until they are balanced, returning the substring that contains the complete JSON.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string potentially containing a JSON object or array.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The extracted JSON string if found and balanced, otherwise None.</p> Source code in <code>dynamiq/utils/json_parser.py</code> <pre><code>def extract_json_string(s: str) -&gt; str | None:\n    \"\"\"\n    Extract the first JSON object or array from the string by balancing brackets.\n    The function looks for '{' or '[' and keeps track of nested brackets until\n    they are balanced, returning the substring that contains the complete JSON.\n\n    Args:\n        s: The input string potentially containing a JSON object or array.\n\n    Returns:\n        The extracted JSON string if found and balanced, otherwise None.\n    \"\"\"\n    bracket_stack: list[str] = []\n    start_index: int | None = None\n    in_string = False\n    escape = False\n\n    for i, char in enumerate(s):\n        # Toggle in_string when encountering an unescaped double quote\n        if char == '\"' and not escape:\n            in_string = not in_string\n        elif char == \"\\\\\" and not escape:\n            escape = True\n            continue\n\n        if not in_string:\n            if char in \"{[\":\n                if not bracket_stack:\n                    start_index = i\n                bracket_stack.append(char)\n            elif char in \"}]\":\n                if bracket_stack:\n                    opening_bracket = bracket_stack.pop()\n                    if (opening_bracket == \"{\" and char != \"}\") or (opening_bracket == \"[\" and char != \"]\"):\n                        # Mismatched brackets\n                        return None\n                    # If stack is empty, we've balanced everything\n                    if not bracket_stack and start_index is not None:\n                        return s[start_index : i + 1]\n                else:\n                    # Found a closing bracket without a matching opener\n                    return None\n        escape = False\n\n    # If brackets never fully balanced, return None\n    return None\n</code></pre>"},{"location":"dynamiq/utils/json_parser/#dynamiq.utils.json_parser.parse_llm_json_output","title":"<code>parse_llm_json_output(response)</code>","text":"<p>Attempt to parse the received LLM output into a JSON object or array. If direct parsing fails, looks for the first balanced JSON substring, then tries corrections.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The raw output from the LLM.</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | list[Any]</code> <p>A Python dict or list representing the parsed JSON.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the output cannot be parsed into valid JSON.</p> Source code in <code>dynamiq/utils/json_parser.py</code> <pre><code>def parse_llm_json_output(response: str) -&gt; dict[str, Any] | list[Any]:\n    \"\"\"\n    Attempt to parse the received LLM output into a JSON object or array.\n    If direct parsing fails, looks for the first balanced JSON substring,\n    then tries corrections.\n\n    Args:\n        response: The raw output from the LLM.\n\n    Returns:\n        A Python dict or list representing the parsed JSON.\n\n    Raises:\n        ValueError: If the output cannot be parsed into valid JSON.\n    \"\"\"\n    # Try directly parsing\n    try:\n        return json.loads(response)\n    except json.JSONDecodeError:\n        pass\n\n    # Attempt bracket extraction\n    json_str = extract_json_string(response)\n    if not json_str:\n        raise ValueError(f\"Response from LLM is not valid JSON: {response}\")\n\n    # Try parsing the extracted substring\n    try:\n        return json.loads(json_str)\n    except json.JSONDecodeError:\n        pass\n\n    # Clean and try again\n    cleaned_json_str = clean_json_string(json_str)\n    try:\n        return json.loads(cleaned_json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Failed to parse JSON after corrections: {e}\")\n</code></pre>"},{"location":"dynamiq/utils/json_parser/#dynamiq.utils.json_parser.single_quoted_replacer","title":"<code>single_quoted_replacer(match)</code>","text":"<p>A helper function for clean_json_string to replace single-quoted JSON-like string literals with double-quoted equivalents, preserving internal apostrophes.</p> <p>Parameters:</p> Name Type Description Default <code>match</code> <code>Match[str]</code> <p>The regular expression match object for a single-quoted string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The corresponding double-quoted string literal.</p> Source code in <code>dynamiq/utils/json_parser.py</code> <pre><code>def single_quoted_replacer(match: Match[str]) -&gt; str:\n    \"\"\"\n    A helper function for clean_json_string to replace single-quoted JSON-like\n    string literals with double-quoted equivalents, preserving internal\n    apostrophes.\n\n    Args:\n        match: The regular expression match object for a single-quoted string.\n\n    Returns:\n        The corresponding double-quoted string literal.\n    \"\"\"\n    content = match.group(1)\n    # Convert escaped \\' to an actual apostrophe\n    content = content.replace(\"\\\\'\", \"'\")\n    # Escape any double quotes inside\n    content = content.replace('\"', '\\\\\"')\n    return f'\"{content}\"'\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/","title":"Jsonpath","text":""},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.filter","title":"<code>filter(json, expression_filter)</code>","text":"<p>Filter a JSON object based on a JSONPath expression.</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>dict</code> <p>The input JSON object to be filtered.</p> required <code>expression_filter</code> <code>str</code> <p>A JSONPath expression used to filter the JSON object.</p> required <p>Returns:</p> Type Description <p>The filtered data, which can be a single value, a list of values, or None if no match is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filter is not a valid JSONPath expression or if there's an error in parsing.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def filter(json: dict, expression_filter: str):\n    \"\"\"\n    Filter a JSON object based on a JSONPath expression.\n\n    Args:\n        json (dict): The input JSON object to be filtered.\n        expression_filter (str): A JSONPath expression used to filter the JSON object.\n\n    Returns:\n        The filtered data, which can be a single value, a list of values, or None if no match is found.\n\n    Raises:\n        ValueError: If the filter is not a valid JSONPath expression or if there's an error in parsing.\n    \"\"\"\n    if not expression_filter:\n        return json\n    if not is_jsonpath(expression_filter):\n        raise ValueError(f\"Invalid `expression_filter` {expression_filter}: must be a jsonpath\")\n\n    filtered_data = None\n    try:\n        value = parse(expression_filter).find(json)\n        if value:\n            filtered_data = [v.value for v in value]\n            if len(filtered_data) == 1:\n                filtered_data = filtered_data[0]\n    except Exception as e:\n        raise ValueError(f\"Error in path during parsing with `expression_filter` {expression_filter}: {e}\")\n\n    return filtered_data\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.filter_all","title":"<code>filter_all(json, expression_filter)</code>","text":"<p>Filter a JSON object and return all matched values as a list.</p> <p>Unlike <code>filter</code>, this function always returns a list of matched values, making it possible to distinguish between \"no match\", \"single match\" (even if the value is a list or None), and \"multiple matches\" by checking len().</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>dict | list</code> <p>The input JSON object or list to be filtered.</p> required <code>expression_filter</code> <code>str</code> <p>A JSONPath expression used to filter the JSON object.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of all matched values (empty list if no matches found).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filter is not a valid JSONPath expression or if there's an error in parsing.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def filter_all(json: dict | list, expression_filter: str) -&gt; list:\n    \"\"\"Filter a JSON object and return all matched values as a list.\n\n    Unlike `filter`, this function always returns a list of matched values,\n    making it possible to distinguish between \"no match\", \"single match\"\n    (even if the value is a list or None), and \"multiple matches\" by checking len().\n\n    Args:\n        json: The input JSON object or list to be filtered.\n        expression_filter: A JSONPath expression used to filter the JSON object.\n\n    Returns:\n        A list of all matched values (empty list if no matches found).\n\n    Raises:\n        ValueError: If the filter is not a valid JSONPath expression or if there's an error in parsing.\n    \"\"\"\n    if not expression_filter:\n        return [json]\n    if not is_jsonpath(expression_filter):\n        raise ValueError(f\"Invalid `expression_filter` {expression_filter}: must be a jsonpath\")\n\n    try:\n        matches = parse(expression_filter).find(json)\n        return [m.value for m in matches]\n    except Exception as e:\n        raise ValueError(f\"Error in path during parsing with `expression_filter` {expression_filter}: {e}\")\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.is_jsonpath","title":"<code>is_jsonpath(path)</code>","text":"<p>Check if the given string is a valid JSONPath expression.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The string to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is a valid JSONPath expression, False otherwise.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def is_jsonpath(path: str) -&gt; bool:\n    \"\"\"\n    Check if the given string is a valid JSONPath expression.\n\n    Args:\n        path (str): The string to be checked.\n\n    Returns:\n        bool: True if the string is a valid JSONPath expression, False otherwise.\n    \"\"\"\n    try:\n        parse(path)\n        return True\n    except JsonPathParserError:\n        return False\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.mapper","title":"<code>mapper(json, expression_map, expression_prefixes=JSONPATH_EXPRESSION_PREFIXES, use_expression_as_value=True)</code>","text":"<p>Map values from a JSON object or list to a new dictionary based on a mapping configuration.</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>dict | list</code> <p>The input JSON object or list to be mapped.</p> required <code>expression_map</code> <code>dict</code> <p>A dictionary defining the mapping configuration.</p> required <code>expression_prefixes</code> <code>tuple</code> <p>The prefixes of the JSONPath expressions.</p> <code>JSONPATH_EXPRESSION_PREFIXES</code> <code>use_expression_as_value</code> <code>bool</code> <p>Whether to use the expression as value if the value is not found.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A new dictionary with mapped values according to the provided configuration.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the map is not a dictionary or if the json is neither a dictionary nor a list.</p> <code>ValueError</code> <p>If there's an error in JSONPath parsing.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def mapper(\n    json: dict | list,\n    expression_map: dict,\n    expression_prefixes: tuple = JSONPATH_EXPRESSION_PREFIXES,\n    use_expression_as_value: bool = True,\n) -&gt; dict:\n    \"\"\"\n    Map values from a JSON object or list to a new dictionary based on a mapping configuration.\n\n    Args:\n        json (dict | list): The input JSON object or list to be mapped.\n        expression_map (dict): A dictionary defining the mapping configuration.\n        expression_prefixes (tuple): The prefixes of the JSONPath expressions.\n        use_expression_as_value (bool): Whether to use the expression as value if the value is not found.\n\n    Returns:\n        dict: A new dictionary with mapped values according to the provided configuration.\n\n    Raises:\n        TypeError: If the map is not a dictionary or if the json is neither a dictionary nor a list.\n        ValueError: If there's an error in JSONPath parsing.\n    \"\"\"\n    if not expression_map:\n        return json\n    if not isinstance(expression_map, dict):\n        raise TypeError(\"Invalid `expression_map`: must be a dictionary\")\n    if not isinstance(json, dict) and not isinstance(json, list):\n        raise TypeError(\"Invalid `json`: must be a dictionary or a list\")\n\n    new_json = {}\n    for key, path in expression_map.items():\n        if not is_jsonpath(path):\n            new_json[key] = path\n            continue\n        try:\n            found = parse(path).find(json)\n            if not found:\n                if use_expression_as_value and not path.startswith(expression_prefixes):\n                    new_json[key] = path\n                else:\n                    new_json[key] = None\n            elif len(found) == 1:\n                new_json[key] = found[0].value\n            else:\n                new_json[key] = [v.value for v in found]\n        except Exception as e:\n            raise ValueError(f\"Error in jsonpath during parsing: {e}\")\n\n    return new_json\n</code></pre>"},{"location":"dynamiq/utils/logger/","title":"Logger","text":""},{"location":"dynamiq/utils/utils/","title":"Utils","text":""},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.JsonWorkflowEncoder","title":"<code>JsonWorkflowEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>A custom JSON encoder for handling specific object types in workflow serialization.</p> <p>This encoder extends the default JSONEncoder to provide custom serialization for Enum, UUID, and datetime objects.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>class JsonWorkflowEncoder(JSONEncoder):\n    \"\"\"\n    A custom JSON encoder for handling specific object types in workflow serialization.\n\n    This encoder extends the default JSONEncoder to provide custom serialization for Enum, UUID,\n    and datetime objects.\n    \"\"\"\n\n    def default(self, obj: Any) -&gt; Any:\n        \"\"\"\n        Encode the given object into a JSON-serializable format.\n\n        Args:\n            obj (Any): The object to be encoded.\n\n        Returns:\n            Any: A JSON-serializable representation of the object.\n\n        Raises:\n            TypeError: If the object type is not handled by this encoder or the default encoder.\n        \"\"\"\n        encoded_value = encode(obj)\n        if encoded_value is obj:\n            encoded_value = JSONEncoder.default(self, obj)\n\n        return encoded_value\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.JsonWorkflowEncoder.default","title":"<code>default(obj)</code>","text":"<p>Encode the given object into a JSON-serializable format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to be encoded.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A JSON-serializable representation of the object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object type is not handled by this encoder or the default encoder.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n    \"\"\"\n    Encode the given object into a JSON-serializable format.\n\n    Args:\n        obj (Any): The object to be encoded.\n\n    Returns:\n        Any: A JSON-serializable representation of the object.\n\n    Raises:\n        TypeError: If the object type is not handled by this encoder or the default encoder.\n    \"\"\"\n    encoded_value = encode(obj)\n    if encoded_value is obj:\n        encoded_value = JSONEncoder.default(self, obj)\n\n    return encoded_value\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.TruncationMethod","title":"<code>TruncationMethod</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for text truncation methods.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>class TruncationMethod(str, Enum):\n    \"\"\"Enum for text truncation methods.\"\"\"\n\n    START = \"START\"\n    END = \"END\"\n    MIDDLE = \"MIDDLE\"\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.clear_annotation","title":"<code>clear_annotation(annotation)</code>","text":"<p>Returns the first non-None type if the annotation allows multiple types; otherwise, returns the annotation itself.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>Any</code> <p>Provided annotation.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cleared annotation.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def clear_annotation(annotation: Any) -&gt; Any:\n    \"\"\"\n    Returns the first non-None type if the annotation allows multiple types;\n    otherwise, returns the annotation itself.\n\n    Args:\n        annotation (Any): Provided annotation.\n\n    Returns:\n        Any: Cleared annotation.\n    \"\"\"\n    if get_origin(annotation) in (Union, UnionType):\n        first_non_none = next((t for t in get_args(annotation) if t is not NoneType), None)\n        return first_non_none\n    return annotation\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.deep_merge","title":"<code>deep_merge(source, destination)</code>","text":"<p>Recursively merge dictionaries with proper override behavior.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>dict</code> <p>Source dictionary with higher priority values</p> required <code>destination</code> <code>dict</code> <p>Destination dictionary with lower priority values</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Merged dictionary where source values override destination values,   and lists are concatenated when both source and destination have lists</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def deep_merge(source: dict, destination: dict) -&gt; dict:\n    \"\"\"\n    Recursively merge dictionaries with proper override behavior.\n\n    Args:\n        source: Source dictionary with higher priority values\n        destination: Destination dictionary with lower priority values\n\n    Returns:\n        dict: Merged dictionary where source values override destination values,\n              and lists are concatenated when both source and destination have lists\n    \"\"\"\n    result = destination.copy()\n    for key, value in source.items():\n        if key in result:\n            if isinstance(value, dict) and isinstance(result[key], dict):\n                result[key] = deep_merge(value, result[key])\n            elif isinstance(value, list) and isinstance(result[key], list):\n                result[key] = result[key] + value\n            else:\n                result[key] = value\n        else:\n            result[key] = value\n    return result\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.encode","title":"<code>encode(value)</code>","text":"<p>Encode a value into a JSON/YAML-serializable format.</p> <p>Handles specific object types like Enum, UUID, datetime objects, and other complex types to convert them into serializable formats.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be encoded.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A serializable representation of the value.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def encode(value: Any) -&gt; Any:\n    \"\"\"\n    Encode a value into a JSON/YAML-serializable format.\n\n    Handles specific object types like Enum, UUID, datetime objects, and other complex types\n    to convert them into serializable formats.\n\n    Args:\n        value (Any): The value to be encoded.\n\n    Returns:\n        Any: A serializable representation of the value.\n    \"\"\"\n    if isinstance(value, Enum):\n        return value.value\n    if isinstance(value, UUID):\n        return str(value)\n    if isinstance(value, (datetime, date)):\n        return value.isoformat()\n    if isinstance(value, (BytesIO, bytes, Exception)) or callable(value):\n        return format_value(value)\n    return value\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.encode_bytes","title":"<code>encode_bytes(value)</code>","text":"<p>Encode a bytes object to an encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The bytes object to be encoded.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>encoded string representation of the bytes object.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def encode_bytes(value: bytes) -&gt; str:\n    \"\"\"\n    Encode a bytes object to an encoded string.\n\n    Args:\n        value (bytes): The bytes object to be encoded.\n\n    Returns:\n        str: encoded string representation of the bytes object.\n    \"\"\"\n    try:\n        return value.decode()\n    except UnicodeDecodeError:\n        return base64.b64encode(value).decode()\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.format_value","title":"<code>format_value(value, skip_format_types=None, force_format_types=None, for_tracing=False, truncate_limit=TRUNCATE_LIST_LIMIT, **kwargs)</code>","text":"<p>Format a value for serialization.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to format.</p> required <code>skip_format_types</code> <code>set</code> <p>Types to skip formatting.</p> <code>None</code> <code>force_format_types</code> <code>set</code> <p>Types to force formatting.</p> <code>None</code> <code>for_tracing</code> <code>bool</code> <p>Whether to format value regarding tracing standard.</p> <code>False</code> <code>truncate_limit</code> <code>int</code> <p>The maximum allowed length for the value; if exceeded, the value will be truncated.</p> <code>TRUNCATE_LIST_LIMIT</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Formatted value.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def format_value(\n    value: Any,\n    skip_format_types: set = None,\n    force_format_types: set = None,\n    for_tracing: bool = False,\n    truncate_limit: int = TRUNCATE_LIST_LIMIT,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Format a value for serialization.\n\n    Args:\n        value (Any): The value to format.\n        skip_format_types (set, optional): Types to skip formatting.\n        force_format_types (set, optional): Types to force formatting.\n        for_tracing (bool, optional): Whether to format value regarding tracing standard.\n        truncate_limit (int): The maximum allowed length for the value; if exceeded, the value will be truncated.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Formatted value.\n    \"\"\"\n    from dynamiq.nodes.tools.python import PythonInputSchema\n    from dynamiq.runnables import RunnableResult\n\n    if skip_format_types is None:\n        skip_format_types = set()\n    if force_format_types is None:\n        force_format_types = set()\n\n    truncate_metadata = {}\n    if not isinstance(value, tuple(force_format_types)) and isinstance(\n        value, tuple(skip_format_types)\n    ):\n        return value\n\n    if isinstance(value, BytesIO):\n        return getattr(value, \"name\", None) or encode_bytes(value.getvalue())\n    if isinstance(value, bytes):\n        return encode_bytes(value)\n\n    path = kwargs.get(\"path\", \"\")\n    if isinstance(value, dict):\n        formatted_dict = {}\n        for k, v in value.items():\n            new_path = f\"{path}.{k}\" if path else k\n            formatted_v = format_value(\n                v,\n                skip_format_types,\n                force_format_types,\n                for_tracing,\n                path=new_path,\n                truncate_limit=truncate_limit,\n            )\n            formatted_dict[k] = formatted_v\n        return formatted_dict\n\n    if for_tracing and isinstance(value, list) and len(value) &gt; truncate_limit:\n        value = value[:truncate_limit]\n\n    if isinstance(value, (list, tuple, set)):\n        formatted_list = []\n        for i, v in enumerate(value):\n            new_path = f\"{path}[{i}]\"\n            formatted_v = format_value(\n                v,\n                skip_format_types,\n                force_format_types,\n                for_tracing,\n                path=new_path,\n                truncate_limit=truncate_limit,\n            )\n            formatted_list.append(formatted_v)\n\n        return type(value)(formatted_list)\n\n    if isinstance(value, (RunnableResult, PythonInputSchema)):\n        return value.to_dict(skip_format_types=skip_format_types, force_format_types=force_format_types)\n    if isinstance(value, BaseModel):\n        dict_kwargs = {\"for_tracing\": for_tracing} if for_tracing else {}\n        if hasattr(value, \"to_dict\"):\n            try:\n                base_dict = value.to_dict(**dict_kwargs)\n            except Exception:\n                base_dict = value.to_dict()\n        else:\n            base_dict = value.model_dump()\n\n        return base_dict\n    if isinstance(value, Exception):\n        recoverable = bool(kwargs.get(\"recoverable\"))\n        return {\n            \"message\": f\"{str(value)}\",\n            \"type\": type(value).__name__,\n            \"recoverable\": recoverable,\n        }, truncate_metadata\n    if callable(value):\n        return f\"func: {getattr(value, '__name__', str(value))}\"\n\n    try:\n        return RootModel[type(value)](value).model_dump()\n    except PydanticUserError:\n        return str(value)\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.generate_uuid","title":"<code>generate_uuid()</code>","text":"<p>Generate a UUID4 string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of a UUID4.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def generate_uuid() -&gt; str:\n    \"\"\"\n    Generate a UUID4 string.\n\n    Returns:\n        str: A string representation of a UUID4.\n    \"\"\"\n    return str(uuid4())\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.is_called_from_async_context","title":"<code>is_called_from_async_context()</code>","text":"<p>Attempt to detect if the function is being called from an async context.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if called from an async context, False otherwise</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def is_called_from_async_context() -&gt; bool:\n    \"\"\"\n    Attempt to detect if the function is being called from an async context.\n\n    Returns:\n        bool: True if called from an async context, False otherwise\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_flags &amp; inspect.CO_COROUTINE:\n                return True\n            frame = frame.f_back\n        return False\n    except Exception:\n        return False\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.merge","title":"<code>merge(a, b)</code>","text":"<p>Merge two dictionaries or objects.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Any</code> <p>The first dictionary or object.</p> required <code>b</code> <code>Any</code> <p>The second dictionary or object.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A new dictionary containing the merged key-value pairs from both inputs.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def merge(a: Any, b: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Merge two dictionaries or objects.\n\n    Args:\n        a (Any): The first dictionary or object.\n        b (Any): The second dictionary or object.\n\n    Returns:\n        dict[str, Any]: A new dictionary containing the merged key-value pairs from both inputs.\n    \"\"\"\n    return {**a, **b}\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.orjson_encode","title":"<code>orjson_encode(obj)</code>","text":"<p>orjson-compatible default function that replicates dynamiq JsonWorkflowEncoder behavior.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def orjson_encode(obj: Any) -&gt; Any:\n    \"\"\"\n    orjson-compatible default function that replicates dynamiq JsonWorkflowEncoder behavior.\n    \"\"\"\n    encoded_value = encode(value=obj)\n    if encoded_value is obj:\n        try:\n            formatted_value, _ = format_value(obj)\n            return formatted_value\n        except Exception:\n            return str(obj)\n    else:\n        return encoded_value\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to a JSON-compatible dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to be serialized.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary representation of the object, suitable for JSON serialization.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def serialize(obj: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize an object to a JSON-compatible dictionary.\n\n    Args:\n        obj (Any): The object to be serialized.\n\n    Returns:\n        dict[str, Any]: A dictionary representation of the object, suitable for JSON serialization.\n    \"\"\"\n    import jsonpickle\n\n    return loads(jsonpickle.encode(obj, unpicklable=False))\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.truncate_text_for_embedding","title":"<code>truncate_text_for_embedding(text, max_tokens=8192, truncation_method=TruncationMethod.MIDDLE, truncation_message='...[truncated for embedding]...')</code>","text":"<p>Truncate text for embedding models to prevent token limit exceeded errors.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to potentially truncate</p> required <code>max_tokens</code> <code>int</code> <p>Maximum allowed token count (default: 8192 for most embedding models)</p> <code>8192</code> <code>truncation_method</code> <code>TruncationMethod | str</code> <p>Method to use for truncation (TruncationMethod.START/END/MIDDLE)</p> <code>MIDDLE</code> <code>truncation_message</code> <code>str</code> <p>Message to insert when truncating</p> <code>'...[truncated for embedding]...'</code> <p>Returns:</p> Type Description <code>str</code> <p>Truncated text that should fit within the embedding model's token limits</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def truncate_text_for_embedding(\n    text: str,\n    max_tokens: int = 8192,\n    truncation_method: TruncationMethod | str = TruncationMethod.MIDDLE,\n    truncation_message: str = \"...[truncated for embedding]...\",\n) -&gt; str:\n    \"\"\"\n    Truncate text for embedding models to prevent token limit exceeded errors.\n\n    Args:\n        text: The text to potentially truncate\n        max_tokens: Maximum allowed token count (default: 8192 for most embedding models)\n        truncation_method: Method to use for truncation (TruncationMethod.START/END/MIDDLE)\n        truncation_message: Message to insert when truncating\n\n    Returns:\n        Truncated text that should fit within the embedding model's token limits\n    \"\"\"\n    if not text:\n        return text\n\n    max_chars = max_tokens * CHARS_PER_TOKEN\n\n    if len(text) &lt;= max_chars:\n        return text\n\n    truncation_msg_len = len(truncation_message)\n\n    if max_chars &lt;= truncation_msg_len:\n        simple_msg = \"...[truncated]...\"\n        if max_chars &lt;= len(simple_msg):\n            return text[:max_chars]\n        return simple_msg\n\n    if truncation_method == TruncationMethod.START or truncation_method == \"START\":\n        return truncation_message + text[-(max_chars - truncation_msg_len) :]\n    elif truncation_method == TruncationMethod.END or truncation_method == \"END\":\n        return text[: max_chars - truncation_msg_len] + truncation_message\n    else:\n        half_length = (max_chars - truncation_msg_len) // 2\n        return text[:half_length] + truncation_message + text[-half_length:]\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/mock_knowledgebase/","title":"Mock knowledgebase","text":""},{"location":"dynamiq/utils/workflow_generation/mock_knowledgebase/#dynamiq.utils.workflow_generation.mock_knowledgebase.KnowledgebaseRetriever","title":"<code>KnowledgebaseRetriever</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Mock Node for representing a knowledge base</p> Source code in <code>dynamiq/utils/workflow_generation/mock_knowledgebase.py</code> <pre><code>class KnowledgebaseRetriever(Node):\n    \"\"\"\n    A Mock Node for representing a knowledge base\n    \"\"\"\n\n    name: str = \"KnowledgebaseRetriever\"\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    description: str = \"A node for retrieving relevant documents from a knowledge base.\"\n    input_schema: ClassVar[type[KnowledgebaseRetrieverInputSchema]] = KnowledgebaseRetrieverInputSchema\n\n    def execute(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs) -&gt; Any:\n        return\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/","title":"Node generation","text":""},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.add_connection","title":"<code>add_connection(node, data)</code>","text":"<p>Add connections generation data.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>Target node for adding a connection.</p> required <code>data</code> <code>dict[str, Any]</code> <p>Generation data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>New connection id.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def add_connection(node: Node, data: dict[str, Any]) -&gt; str:\n    \"\"\"Add connections generation data.\n\n    Args:\n        node (Node): Target node for adding a connection.\n        data (dict[str, Any]): Generation data.\n\n    Returns:\n        str: New connection id.\n    \"\"\"\n    node_connection_annotation = node.model_fields[\"connection\"].annotation\n    if type(node_connection_annotation) in (Union, types.UnionType):\n        connection = next((x for x in get_args(node_connection_annotation) if x is not None), None)\n    else:\n        connection = node_connection_annotation\n\n    connection_type = connection(api_key=\"\").type\n    connection_id = generate_uuid()\n\n    if \"connections\" not in data:\n        data[\"connections\"] = {connection_id: {\"type\": connection_type, \"api_key\": \"\"}}\n    else:\n        data[\"connections\"][connection_id] = {\"type\": connection_type, \"api_key\": \"\"}\n\n    return connection_id\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.generate_boolean","title":"<code>generate_boolean()</code>","text":"<p>Generate a random boolean value (True or False).</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>A randomly chosen boolean value.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def generate_boolean() -&gt; bool:\n    \"\"\"\n    Generate a random boolean value (True or False).\n\n    Returns:\n        bool: A randomly chosen boolean value.\n    \"\"\"\n    return random.choice([False, True])  # nosec\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.generate_data_from_schema","title":"<code>generate_data_from_schema(schema)</code>","text":"<p>Recursive function that generates mock data based on provided schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict[str, Any]</code> <p>Schema for data generation</p> required Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def generate_data_from_schema(schema: dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Recursive function that generates mock data based on provided schema.\n\n    Args:\n        schema (dict[str, Any]): Schema for data generation\n    \"\"\"\n    schema_type = schema.get(\"type\")\n\n    if schema_type == \"string\":\n        return schema.get(\"enum\", [\"mocked_data\"])[0]\n\n    elif schema_type == \"integer\":\n        return generate_integer(minimum=schema.get(\"minimum\", 0), maximum=schema.get(\"maximum\", 100))\n    elif schema_type == \"number\":\n        return generate_number(minimum=schema.get(\"minimum\", 0), maximum=schema.get(\"maximum\", 1))\n\n    elif schema_type == \"boolean\":\n        return generate_boolean()\n\n    elif schema_type == \"object\":\n        obj = {}\n        props = schema.get(\"properties\", {})\n        required = schema.get(\"required\", [])\n        for key, value_schema in props.items():\n            if key in required:\n                obj[key] = generate_data_from_schema(value_schema)\n        return obj\n\n    elif schema_type == \"array\":\n        item_schema = schema.get(\"items\", {})\n        if list_elements := item_schema.get(\"anyOf\"):\n            return [generate_data_from_schema(list_elements[0])]\n        return [generate_data_from_schema(item_schema)]\n\n    elif any_object := schema.get(\"anyOf\"):\n        return generate_data_from_schema(any_object[0])\n\n    return None\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.generate_integer","title":"<code>generate_integer(minimum, maximum)</code>","text":"<p>Generate a random integer between the specified minimum and maximum values (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>minimum</code> <code>int</code> <p>The lower bound of the range.</p> required <code>maximum</code> <code>int</code> <p>The upper bound of the range.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>A integer between minimum and maximum.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def generate_integer(minimum: int, maximum: int) -&gt; int:\n    \"\"\"\n    Generate a random integer between the specified minimum and maximum values (inclusive).\n\n    Args:\n        minimum (int): The lower bound of the range.\n        maximum (int): The upper bound of the range.\n\n    Returns:\n        int: A integer between minimum and maximum.\n    \"\"\"\n    return random.randint(minimum, maximum)  # nosec\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.generate_node","title":"<code>generate_node(node_cls, node_info, taken_names)</code>","text":"<p>Generates instance of node with unique name.</p> <p>Supported Nodes: Simple (non-nested) nodes and agents.</p> <p>Parameters:</p> Name Type Description Default <code>node_cls</code> <code>type[Node]</code> <p>Class of node to generate.</p> required <code>node_info</code> <code>dict[str, Any]</code> <p>Node details.</p> required <code>taken_names</code> <code>list[str]</code> <p>List of taken names.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Generated id of the node.</p> <code>Node</code> <p>Generated node.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def generate_node(node_cls: type[Node], node_info: dict[str, Any], taken_names: list[str]):\n    \"\"\"\n    Generates instance of node with unique name.\n\n    Supported Nodes: Simple (non-nested) nodes and agents.\n\n    Args:\n        node_cls (type[Node]): Class of node to generate.\n        node_info (dict[str, Any]): Node details.\n        taken_names (list[str]): List of taken names.\n\n    Returns:\n        str: Generated id of the node.\n        Node: Generated node.\n    \"\"\"\n\n    node_id, data = generate_yaml_data(node_cls, node_info)\n    result = WorkflowYAMLLoader.parse(data)\n    node = result.nodes[node_id]\n\n    if isinstance(node, Agent):\n        node.llm.name = node.llm.name.lower().replace(\" \", \"-\")\n        for tool in node.tools:\n            tool.name = tool.name.lower().replace(\" \", \"-\")\n    node.name = node.name.lower().replace(\" \", \"-\")\n\n    if node.input_transformer.selector:\n        if isinstance(node, BaseLLM) and node.prompt:\n            validate_input_transformer(node.prompt.messages, node)\n\n        elif isinstance(node, Agent):\n            validate_input_transformer([Message(role=MessageRole.USER, content=f\"{{{{input}}}} {node.role}\")], node)\n    if node.name in taken_names:\n        raise ValueError(f\"Name {node.name} is already taken.\")\n\n    return node_id, node\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.generate_number","title":"<code>generate_number(minimum, maximum)</code>","text":"<p>Generate a random floating-point number between the specified minimum and maximum values.</p> <p>Parameters:</p> Name Type Description Default <code>minimum</code> <code>float</code> <p>The lower bound of the range.</p> required <code>maximum</code> <code>float</code> <p>The upper bound of the range.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A random float between minimum and maximum.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def generate_number(minimum: float, maximum: float) -&gt; float:\n    \"\"\"\n    Generate a random floating-point number between the specified minimum and maximum values.\n\n    Args:\n        minimum (float): The lower bound of the range.\n        maximum (float): The upper bound of the range.\n\n    Returns:\n        float: A random float between minimum and maximum.\n    \"\"\"\n    return random.uniform(minimum, maximum)  # nosec\n</code></pre>"},{"location":"dynamiq/utils/workflow_generation/node_generation/#dynamiq.utils.workflow_generation.node_generation.validate_input_transformer","title":"<code>validate_input_transformer(messages, node_data)</code>","text":"<p>Validates input transformer for Agents and LLM nodes</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Input message node accepts</p> required <code>node_data</code> <code>dict[str, Any]</code> <p>Generated node data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Empty string if node information was successfully validated. Error message if not.</p> Source code in <code>dynamiq/utils/workflow_generation/node_generation.py</code> <pre><code>def validate_input_transformer(messages: list[Message], node_data: Node) -&gt; str:\n    \"\"\"\n    Validates input transformer for Agents and LLM nodes\n\n    Args:\n        messages (list[Message]): Input message node accepts\n        node_data (dict[str, Any]): Generated node data.\n\n    Returns:\n        str: Empty string if node information was successfully validated. Error message if not.\n    \"\"\"\n    prompt = Prompt(messages=messages)\n    required_parameters = prompt.get_required_parameters()\n\n    provided_parameters = {element for element in list(node_data.input_transformer.selector.keys())}\n    provided_parameters.discard(\"files\")\n\n    if required_parameters != provided_parameters:\n        raise ValueError(\n            f\"Invalid parameters provided in node data. Required parameters: {list(required_parameters)}. \"\n            f\"Provided parameters in InputTransformer {list(provided_parameters)}.\"\n        )\n</code></pre>"},{"location":"dynamiq/workflow/workflow/","title":"Workflow","text":""},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow","title":"<code>Workflow</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code></p> <p>A container for a flow that manages its lifecycle, YAML serialization, versioning, metadata, callbacks, and configuration.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of workflow.</p> <code>id</code> <code>str</code> <p>Unique identifier for the workflow.</p> <code>flow</code> <code>BaseFlow</code> <p>The flow associated with the workflow.</p> <code>version</code> <code>str | None</code> <p>Version of the workflow.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>class Workflow(BaseModel, Runnable):\n    \"\"\"\n    A container for a flow that manages its lifecycle, YAML serialization,\n    versioning, metadata, callbacks, and configuration.\n\n    Attributes:\n        name (str): Name of workflow.\n        id (str): Unique identifier for the workflow.\n        flow (BaseFlow): The flow associated with the workflow.\n        version (str | None): Version of the workflow.\n    \"\"\"\n\n    name: str = \"Workflow\"\n    id: str = Field(default_factory=generate_uuid)\n    flow: BaseFlow = Field(default_factory=Flow)\n    version: str | None = None\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return \"dynamiq.workflows.Workflow\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    @classmethod\n    def from_yaml_file(\n        cls,\n        file_path: str,\n        wf_id: str = None,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ):\n        \"\"\"Load workflow from a YAML file.\n\n        Args:\n            file_path (str): Path to the YAML file.\n            wf_id (str, optional): Workflow ID. Defaults to None.\n            connection_manager (ConnectionManager | None, optional): Connection manager. Defaults to None.\n            init_components (bool, optional): Whether to initialize components. Defaults to False.\n\n        Returns:\n            Workflow: Loaded workflow instance.\n        \"\"\"\n        from dynamiq.serializers.loaders.yaml import WorkflowYAMLLoader\n\n        try:\n            wf_data = WorkflowYAMLLoader.load(\n                file_path, connection_manager, init_components\n            )\n        except Exception as e:\n            logger.error(f\"Failed to load workflow from YAML. {e}\")\n            raise\n\n        return cls.from_yaml_file_data(wf_data, wf_id)\n\n    @classmethod\n    def from_yaml_file_data(cls, file_data: \"WorkflowYamlData\", wf_id: str = None):\n        \"\"\"Load workflow from YAML file data.\n\n        Args:\n            file_data (WorkflowYamlData): Data loaded from the YAML file.\n            wf_id (str, optional): Workflow ID. Defaults to None.\n\n        Returns:\n            Workflow: Loaded workflow instance.\n        \"\"\"\n        try:\n            if wf_id is None:\n                if len(file_data.workflows) &gt; 1:\n                    raise ValueError(\n                        \"Multiple workflows found in YAML. Please specify 'wf_id'\"\n                    )\n                return list(file_data.workflows.values())[0]\n\n            if wf := file_data.workflows.get(wf_id):\n                return wf\n            else:\n                raise ValueError(f\"Workflow '{wf_id}' not found in YAML\")\n        except Exception as e:\n            logger.error(f\"Failed to load workflow from YAML. {e}\")\n            raise\n\n    @property\n    def to_dict_exclude_params(self):\n        return {\"flow\": True}\n\n    def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n        for_tracing: bool = kwargs.pop(\"for_tracing\", False)\n        data = self.model_dump(\n            exclude=exclude,\n            serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n            **kwargs,\n        )\n        data[\"flow\"] = self.flow.to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n        return data\n\n    def to_yaml_file_data(self) -&gt; \"WorkflowYamlData\":\n        \"\"\"Dump the workflow to a YAML file data.\n\n        Returns:\n            WorkflowYamlData: Data for the YAML dump.\n        \"\"\"\n        from dynamiq.serializers.loaders.yaml import WorkflowYamlData\n\n        return WorkflowYamlData(\n            workflows={self.id: self},\n            flows={self.flow.id: self.flow},\n            nodes={node.id: node for node in self.flow.nodes},\n            connections={},\n        )\n\n    def to_yaml_file(self, file_path: str | PathLike | IO[Any]):\n        \"\"\"\n        Dump the workflow to a YAML file.\n\n        Args:\n            file_path(str | PathLike | IO[Any]): Path to the YAML file.\n        \"\"\"\n        from dynamiq.serializers.dumpers.yaml import WorkflowYAMLDumper\n\n        yaml_file_data = self.to_yaml_file_data()\n\n        try:\n            WorkflowYAMLDumper.dump(file_path, yaml_file_data)\n        except Exception as e:\n            logger.error(f\"Failed to dump workflow to YAML. {e}\")\n            raise\n\n    def run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"Run the workflow synchronously with given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the workflow execution.\n        \"\"\"\n        run_id = uuid4()\n        logger.info(f\"Workflow {self.id}: execution started.\")\n\n        # update kwargs with run_id\n        merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n        self.run_on_workflow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        result = self.flow.run_sync(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n        if result.status == RunnableStatus.SUCCESS:\n            self.run_on_workflow_end(result.output, config, **merged_kwargs)\n            logger.info(f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\")\n        else:\n            error = result.error.type(result.error.message)\n            failed_nodes: list[RunnableFailedNodeInfo] = result.error.failed_nodes\n            self.run_on_workflow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n\n        return RunnableResult(status=result.status, input=input_data, output=result.output, error=result.error)\n\n    async def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n        \"\"\"Run the workflow asynchronously with given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the workflow execution.\n        \"\"\"\n        run_id = uuid4()\n        logger.info(f\"Workflow {self.id}: execution started.\")\n\n        # update kwargs with run_id\n        merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n        self.run_on_workflow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        result = await self.flow.run_async(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n        if result.status == RunnableStatus.SUCCESS:\n            self.run_on_workflow_end(result.output, config, **merged_kwargs)\n            logger.info(\n                f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n            )\n        else:\n            if result.error:\n                error = result.error.type(result.error.message)\n                failed_nodes: list[RunnableFailedNodeInfo] = result.error.failed_nodes\n            else:\n                error = RuntimeError(\"Workflow execution failed with unknown error\")\n                failed_nodes: list[RunnableFailedNodeInfo] = []\n            self.run_on_workflow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n            logger.error(\n                f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\"\n            )\n\n        return RunnableResult(status=result.status, input=input_data, output=result.output, error=result.error)\n\n    def run_on_workflow_start(self, input_data: Any, config: RunnableConfig = None, **kwargs: Any):\n        \"\"\"Run callbacks on workflow start.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_workflow_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n\n    def run_on_workflow_end(\n        self, output: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"Run callbacks on workflow end.\n\n        Args:\n            output (Any): Output data from the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_workflow_end(self.to_dict(**dict_kwargs), output, **kwargs)\n\n    def run_on_workflow_error(\n        self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"Run callbacks on workflow error.\n\n        Args:\n            error (BaseException): The error that occurred.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                dict_kwargs = {}\n                if isinstance(callback, TracingCallbackHandler):\n                    dict_kwargs[\"for_tracing\"] = True\n                callback.on_workflow_error(self.to_dict(**dict_kwargs), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.from_yaml_file","title":"<code>from_yaml_file(file_path, wf_id=None, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Load workflow from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the YAML file.</p> required <code>wf_id</code> <code>str</code> <p>Workflow ID. Defaults to None.</p> <code>None</code> <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Connection manager. Defaults to None.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Whether to initialize components. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Workflow</code> <p>Loaded workflow instance.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>@classmethod\ndef from_yaml_file(\n    cls,\n    file_path: str,\n    wf_id: str = None,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n):\n    \"\"\"Load workflow from a YAML file.\n\n    Args:\n        file_path (str): Path to the YAML file.\n        wf_id (str, optional): Workflow ID. Defaults to None.\n        connection_manager (ConnectionManager | None, optional): Connection manager. Defaults to None.\n        init_components (bool, optional): Whether to initialize components. Defaults to False.\n\n    Returns:\n        Workflow: Loaded workflow instance.\n    \"\"\"\n    from dynamiq.serializers.loaders.yaml import WorkflowYAMLLoader\n\n    try:\n        wf_data = WorkflowYAMLLoader.load(\n            file_path, connection_manager, init_components\n        )\n    except Exception as e:\n        logger.error(f\"Failed to load workflow from YAML. {e}\")\n        raise\n\n    return cls.from_yaml_file_data(wf_data, wf_id)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.from_yaml_file_data","title":"<code>from_yaml_file_data(file_data, wf_id=None)</code>  <code>classmethod</code>","text":"<p>Load workflow from YAML file data.</p> <p>Parameters:</p> Name Type Description Default <code>file_data</code> <code>WorkflowYamlData</code> <p>Data loaded from the YAML file.</p> required <code>wf_id</code> <code>str</code> <p>Workflow ID. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Workflow</code> <p>Loaded workflow instance.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>@classmethod\ndef from_yaml_file_data(cls, file_data: \"WorkflowYamlData\", wf_id: str = None):\n    \"\"\"Load workflow from YAML file data.\n\n    Args:\n        file_data (WorkflowYamlData): Data loaded from the YAML file.\n        wf_id (str, optional): Workflow ID. Defaults to None.\n\n    Returns:\n        Workflow: Loaded workflow instance.\n    \"\"\"\n    try:\n        if wf_id is None:\n            if len(file_data.workflows) &gt; 1:\n                raise ValueError(\n                    \"Multiple workflows found in YAML. Please specify 'wf_id'\"\n                )\n            return list(file_data.workflows.values())[0]\n\n        if wf := file_data.workflows.get(wf_id):\n            return wf\n        else:\n            raise ValueError(f\"Workflow '{wf_id}' not found in YAML\")\n    except Exception as e:\n        logger.error(f\"Failed to load workflow from YAML. {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_async","title":"<code>run_async(input_data, config=None, **kwargs)</code>  <code>async</code>","text":"<p>Run the workflow asynchronously with given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the workflow execution.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>async def run_async(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"Run the workflow asynchronously with given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the workflow execution.\n    \"\"\"\n    run_id = uuid4()\n    logger.info(f\"Workflow {self.id}: execution started.\")\n\n    # update kwargs with run_id\n    merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n    self.run_on_workflow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    result = await self.flow.run_async(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n    if result.status == RunnableStatus.SUCCESS:\n        self.run_on_workflow_end(result.output, config, **merged_kwargs)\n        logger.info(\n            f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n        )\n    else:\n        if result.error:\n            error = result.error.type(result.error.message)\n            failed_nodes: list[RunnableFailedNodeInfo] = result.error.failed_nodes\n        else:\n            error = RuntimeError(\"Workflow execution failed with unknown error\")\n            failed_nodes: list[RunnableFailedNodeInfo] = []\n        self.run_on_workflow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n        logger.error(\n            f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\"\n        )\n\n    return RunnableResult(status=result.status, input=input_data, output=result.output, error=result.error)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_end","title":"<code>run_on_workflow_end(output, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow end.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Output data from the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_end(\n    self, output: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"Run callbacks on workflow end.\n\n    Args:\n        output (Any): Output data from the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_workflow_end(self.to_dict(**dict_kwargs), output, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_error","title":"<code>run_on_workflow_error(error, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow error.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_error(\n    self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"Run callbacks on workflow error.\n\n    Args:\n        error (BaseException): The error that occurred.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_workflow_error(self.to_dict(**dict_kwargs), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_start","title":"<code>run_on_workflow_start(input_data, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow start.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_start(self, input_data: Any, config: RunnableConfig = None, **kwargs: Any):\n    \"\"\"Run callbacks on workflow start.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            dict_kwargs = {}\n            if isinstance(callback, TracingCallbackHandler):\n                dict_kwargs[\"for_tracing\"] = True\n            callback.on_workflow_start(self.to_dict(**dict_kwargs), input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_sync","title":"<code>run_sync(input_data, config=None, **kwargs)</code>","text":"<p>Run the workflow synchronously with given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the workflow execution.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_sync(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; RunnableResult:\n    \"\"\"Run the workflow synchronously with given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the workflow execution.\n    \"\"\"\n    run_id = uuid4()\n    logger.info(f\"Workflow {self.id}: execution started.\")\n\n    # update kwargs with run_id\n    merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n    self.run_on_workflow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    result = self.flow.run_sync(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n    if result.status == RunnableStatus.SUCCESS:\n        self.run_on_workflow_end(result.output, config, **merged_kwargs)\n        logger.info(f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\")\n    else:\n        error = result.error.type(result.error.message)\n        failed_nodes: list[RunnableFailedNodeInfo] = result.error.failed_nodes\n        self.run_on_workflow_error(error, config, failed_nodes=failed_nodes, **merged_kwargs)\n        logger.error(f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\")\n\n    return RunnableResult(status=result.status, input=input_data, output=result.output, error=result.error)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.to_dict","title":"<code>to_dict(include_secure_params=False, **kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def to_dict(self, include_secure_params: bool = False, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    exclude = kwargs.pop(\"exclude\", self.to_dict_exclude_params)\n    for_tracing: bool = kwargs.pop(\"for_tracing\", False)\n    data = self.model_dump(\n        exclude=exclude,\n        serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n        **kwargs,\n    )\n    data[\"flow\"] = self.flow.to_dict(include_secure_params=include_secure_params, for_tracing=for_tracing, **kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.to_yaml_file","title":"<code>to_yaml_file(file_path)</code>","text":"<p>Dump the workflow to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path(str</code> <code>| PathLike | IO[Any]</code> <p>Path to the YAML file.</p> required Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def to_yaml_file(self, file_path: str | PathLike | IO[Any]):\n    \"\"\"\n    Dump the workflow to a YAML file.\n\n    Args:\n        file_path(str | PathLike | IO[Any]): Path to the YAML file.\n    \"\"\"\n    from dynamiq.serializers.dumpers.yaml import WorkflowYAMLDumper\n\n    yaml_file_data = self.to_yaml_file_data()\n\n    try:\n        WorkflowYAMLDumper.dump(file_path, yaml_file_data)\n    except Exception as e:\n        logger.error(f\"Failed to dump workflow to YAML. {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.to_yaml_file_data","title":"<code>to_yaml_file_data()</code>","text":"<p>Dump the workflow to a YAML file data.</p> <p>Returns:</p> Name Type Description <code>WorkflowYamlData</code> <code>WorkflowYamlData</code> <p>Data for the YAML dump.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def to_yaml_file_data(self) -&gt; \"WorkflowYamlData\":\n    \"\"\"Dump the workflow to a YAML file data.\n\n    Returns:\n        WorkflowYamlData: Data for the YAML dump.\n    \"\"\"\n    from dynamiq.serializers.loaders.yaml import WorkflowYamlData\n\n    return WorkflowYamlData(\n        workflows={self.id: self},\n        flows={self.flow.id: self.flow},\n        nodes={node.id: node for node in self.flow.nodes},\n        connections={},\n    )\n</code></pre>"},{"location":"tutorials/agents/","title":"Agents Tutorial","text":""},{"location":"tutorials/agents/#simple-agent","title":"Simple Agent","text":"<p>An agent that has access to the E2B Code Interpreter and is capable of solving complex coding tasks.</p>"},{"location":"tutorials/agents/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection, E2B as E2BConnection\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\n</code></pre> <p>Initialize the E2B Tool</p> <p>Set up the E2B tool with the necessary API key.</p> <pre><code>e2b_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"$API_KEY\")\n)\n</code></pre> <p>Setup Your LLM</p> <p>Configure the Large Language Model (LLM) with the necessary parameters such as the model, temperature, and maximum tokens.</p> <pre><code>llm = OpenAI(\n    id=\"openai\",\n    connection=OpenAIConnection(api_key=\"$API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.3,\n    max_tokens=1000,\n)\n</code></pre> <p>Create the Agent</p> <p>Create an agent that uses the LLM and the E2B tool to solve coding tasks.</p> <pre><code>agent = Agent(\n    name=\"react-agent\",\n    llm=llm,\n    tools=[e2b_tool],\n    role=\"Senior Data Scientist\",\n    max_loops=10,\n)\n</code></pre> <p>Run the Agent with an Input</p> <p>Execute the agent with a specific input task.</p> <pre><code>result = agent.run(\n    input_data={\n        \"input\": \"Add the first 10 numbers and tell if the result is prime.\",\n    }\n)\n\nprint(result.output.get(\"content\"))\n</code></pre>"},{"location":"tutorials/agents/#manager-led-multi-agent-workflow","title":"Manager-led Multi-Agent Workflow","text":"<p>This pattern treats specialist agents as tools that a manager agent can call, allowing you to parallelize work and keep responsibilities focused.</p>"},{"location":"tutorials/agents/#step-by-step-guide_1","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq import Workflow\nfrom dynamiq.connections import OpenAI as OpenAIConnection, ScaleSerp as ScaleSerpConnection\nfrom dynamiq.flows import Flow\nfrom dynamiq.nodes.agents import Agent\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.nodes.tools.scale_serp import ScaleSerpTool\nfrom dynamiq.nodes.types import Behavior, InferenceMode\n</code></pre> <p>Initialize Tools</p> <p>Set up any external tools your specialists need. Here we use a web search tool to gather fresh market information.</p> <pre><code>search_tool = ScaleSerpTool(\n    connection=ScaleSerpConnection(api_key=\"$SCALESERP_API_KEY\")\n)\n</code></pre> <p>Initialize LLM</p> <p>Configure the shared Large Language Model that all agents will use.</p> <pre><code>llm = OpenAI(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n</code></pre> <p>Define Specialist Agents</p> <p>Create the agents that perform research and writing. Each specializes in a single task and expects inputs under the <code>\"input\"</code> key when invoked as a tool.</p> <pre><code>research_agent = Agent(\n    name=\"Research Analyst\",\n    role=\"Find recent market news and provide referenced highlights.\",\n    llm=llm,\n    tools=[search_tool],\n    inference_mode=InferenceMode.XML,\n    max_loops=6,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n\nwriter_agent = Agent(\n    name=\"Brief Writer\",\n    role=\"Turn research highlights into a concise executive brief.\",\n    llm=llm,\n    inference_mode=InferenceMode.XML,\n    max_loops=4,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n</code></pre> <p>Define the Manager Agent</p> <p>The manager agent coordinates the specialists, calls them as tools, and assembles the final answer.</p> <pre><code>manager_agent = Agent(\n    name=\"Manager\",\n    role=(\n        \"Delegate research and writing to sub-agents.\\n\"\n        \"Always call tools with {'input': '&lt;task&gt;'} payloads and assemble the final brief.\"\n    ),\n    llm=llm,\n    tools=[research_agent, writer_agent],\n    inference_mode=InferenceMode.XML,\n    parallel_tool_calls_enabled=True,\n    max_loops=8,\n    behaviour_on_max_loops=Behavior.RETURN,\n)\n</code></pre> <p>Create and Run the Workflow</p> <p>Wrap the manager agent in a workflow and provide an input brief. The manager will delegate work to the sub-agents and return a synthesized result.</p> <pre><code>workflow = Workflow(flow=Flow(nodes=[manager_agent]))\n\nresult = workflow.run(\n    input_data={\"input\": \"Summarize the latest developments in battery technology for investors.\"},\n)\n\nprint(result.output[manager_agent.id][\"output\"][\"content\"])\n</code></pre> <p>This tutorial provides a comprehensive guide to setting up and running agents using Dynamiq. By following these steps, you can create individual agents to tackle complex tasks and combine specialists under a manager to deliver richer multi-agent workflows.</p>"},{"location":"tutorials/quickstart/","title":"Quickstart Tutorial","text":""},{"location":"tutorials/quickstart/#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's how you can get started with Dynamiq:</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<p>First, let's get Dynamiq installed. You'll need Python, so make sure that's set up on your machine. Then run:</p> <pre><code>pip install dynamiq\n</code></pre> <p>Or build the Python package from the source code:</p> <pre><code>git clone https://github.com/dynamiq-ai/dynamiq.git\ncd dynamiq\npoetry install\n</code></pre>"},{"location":"tutorials/quickstart/#examples","title":"Examples","text":""},{"location":"tutorials/quickstart/#simple-llm-flow","title":"Simple LLM Flow","text":"<p>Here's a simple example to get you started with Dynamiq:</p> <p>Import Necessary Libraries</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq import Workflow\nfrom dynamiq.prompts import Prompt, Message\n</code></pre> <p>Define the Prompt Template for Translation</p> <p>Create a template for the prompt that will be used to translate text into English.</p> <pre><code>prompt_template = \"\"\"\nTranslate the following text into English: {{ text }}\n\"\"\"\n</code></pre> <p>Create a Prompt Object with the Defined Template</p> <pre><code>prompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n</code></pre> <p>Setup Your LLM (Large Language Model) Node</p> <p>Configure the LLM node with the necessary parameters such as the model, temperature, and maximum tokens.</p> <pre><code>llm = OpenAI(\n    id=\"openai\",  # Unique identifier for the node\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),  # Connection using API key\n    model=\"gpt-4o\",  # Model to be used\n    temperature=0.3,  # Sampling temperature for the model\n    max_tokens=1000,  # Maximum number of tokens in the output\n    prompt=prompt  # Prompt to be used for the model\n)\n</code></pre> <p>Create a Workflow Object</p> <p>Initialize a workflow to manage the nodes and their execution.</p> <pre><code>workflow = Workflow()\n</code></pre> <p>Add the LLM Node to the Workflow</p> <p>Add the configured LLM node to the workflow.</p> <pre><code>workflow.flow.add_nodes(llm)\n</code></pre> <p>Run the Workflow with the Input Data</p> <p>Execute the workflow with the input data that needs to be translated.</p> <pre><code>result = workflow.run(\n    input_data={\n        \"text\": \"Hola Mundo!\"  # Text to be translated\n    }\n)\n</code></pre> <p>Print the Result of the Translation</p> <p>Output the result of the translation to the console.</p> <pre><code>print(result.output)\n</code></pre> <p>This tutorial provides a quick and easy way to get started with Dynamiq. By following these steps, you can set up a simple workflow to translate text using a large language model.</p>"},{"location":"tutorials/rag/","title":"RAG Tutorial","text":""},{"location":"tutorials/rag/#rag-document-indexing-flow","title":"RAG - Document Indexing Flow","text":"<p>This workflow takes input PDF files, pre-processes them, converts them to vector embeddings, and stores them in a vector database (Pinecone, Elasticsearch, etc.).</p>"},{"location":"tutorials/rag/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from io import BytesIO\nfrom dynamiq import Workflow\nfrom dynamiq.nodes import InputTransformer\nfrom dynamiq.connections import (\n    OpenAI as OpenAIConnection,\n    Pinecone as PineconeConnection,\n    Elasticsearch as ElasticsearchConnection\n)\nfrom dynamiq.nodes.converters import PyPDFConverter\nfrom dynamiq.nodes.splitters.document import DocumentSplitter\nfrom dynamiq.nodes.embedders import OpenAIDocumentEmbedder\nfrom dynamiq.nodes.writers import PineconeDocumentWriter, ElasticsearchDocumentWriter\n</code></pre> <p>Initialize the RAG Workflow</p> <pre><code>rag_wf = Workflow()\n</code></pre> <p>PyPDF Document Converter</p> <p>Convert the PDF documents into a format suitable for processing.</p> <pre><code>converter = PyPDFConverter(document_creation_mode=\"one-doc-per-page\")\nrag_wf.flow.add_nodes(converter)  # Add node to the DAG\n</code></pre> <p>Document Splitter</p> <p>Split the documents into smaller chunks for better processing.</p> <pre><code>document_splitter = DocumentSplitter(\n    split_by=\"sentence\",\n    split_length=10,\n    split_overlap=1,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[converter.id]}.output.documents\",\n        },  # Map output of the previous node to the expected input of the current node\n    ),\n).depends_on(converter)\nrag_wf.flow.add_nodes(document_splitter)\n</code></pre> <p>OpenAI Vector Embeddings</p> <p>Convert the document chunks into vector embeddings using OpenAI.</p> <pre><code>embedder = OpenAIDocumentEmbedder(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"text-embedding-3-small\",\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[document_splitter.id]}.output.documents\",\n        },\n    ),\n).depends_on(document_splitter)\nrag_wf.flow.add_nodes(embedder)\n</code></pre> <p>Vector Storage Options</p> <p>You can choose between different vector stores for document storage. Here are examples for both Pinecone and Elasticsearch:</p>"},{"location":"tutorials/rag/#option-1-pinecone-vector-storage","title":"Option 1: Pinecone Vector Storage","text":"<p>Store the vector embeddings in the Pinecone vector database.</p> <pre><code>vector_store = PineconeDocumentWriter(\n    connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n    index_name=\"default\",\n    dimension=1536,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[embedder.id]}.output.documents\",\n        },\n    ),\n).depends_on(embedder)\nrag_wf.flow.add_nodes(vector_store)\n</code></pre> <p>If you don't have an index in the database and want to create it programmatically, you need to specify the parameter <code>create_if_not_exist=True</code> and, depending on your deployment type, specify the additional parameters needed for index creation.</p> <p>If you have a <code>serverless</code> Pinecone deployment, your vector store initialization might look like this:</p> <pre><code># Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(),\n        index_name=\"quickstart\",\n        dimension=1536,\n        create_if_not_exist=True,\n        index_type=\"serverless\",\n        cloud=\"aws\",\n        region=\"us-east-1\"\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\n</code></pre> <p>If you have a pod-based deployment, your vector store initialization could look like this:</p> <pre><code># Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(),\n        index_name=\"quickstart\",\n        dimension=1536,\n        create_if_not_exist=True,\n        index_type=\"pod\",\n        environment=\"us-west1-gcp\",\n        pod_type=\"p1.x1\",\n        pods=1\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\n</code></pre>"},{"location":"tutorials/rag/#option-2-elasticsearch-vector-storage","title":"Option 2: Elasticsearch Vector Storage","text":"<p>Store the vector embeddings in Elasticsearch.</p> <p>For local setup: <pre><code>vector_store = ElasticsearchDocumentWriter(\n    connection=ElasticsearchConnection(\n        url=\"$ELASTICSEARCH_URL\",\n        api_key=\"$ELASTICSEARCH_API_KEY\",\n    ),\n    index_name=\"documents\",\n    dimension=1536,\n    similarity=\"cosine\",\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[embedder.id]}.output.documents\",\n        },\n    ),\n).depends_on(embedder)\nrag_wf.flow.add_nodes(vector_store)\n</code></pre></p> <p>For Elastic Cloud deployment:</p> <pre><code>vector_store = ElasticsearchDocumentWriter(\n    connection=ElasticsearchConnection(\n        username=\"$ELASTICSEARCH_USERNAME\",\n        password=\"$ELASTICSEARCH_PASSWORD\",\n        cloud_id=\"$ELASTICSEARCH_CLOUD_ID\",\n    ),\n    index_name=\"documents\",\n    dimension=1536,\n    create_if_not_exist=True,\n    index_settings={\n        \"number_of_shards\": 1,\n        \"number_of_replicas\": 1\n    },\n    mapping_settings={\n        \"dynamic\": \"strict\"\n    }\n).depends_on(embedder)\n</code></pre> <p>Prepare Input PDF Files</p> <p>Prepare the input PDF files for processing.</p> <pre><code>file_paths = [\"example.pdf\"]\ninput_data = {\n    \"files\": [\n        BytesIO(open(path, \"rb\").read()) for path in file_paths\n    ],\n    \"metadata\": [\n        {\"filename\": path} for path in file_paths\n    ],\n}\n</code></pre> <p>Run RAG Indexing Flow</p> <p>Execute the workflow to process and store the documents.</p> <pre><code>rag_wf.run(input_data=input_data)\n</code></pre>"},{"location":"tutorials/rag/#rag-document-retrieval-flow","title":"RAG - Document Retrieval Flow","text":"<p>This simple retrieval RAG flow searches for relevant documents and answers the original user question using the retrieved documents.</p>"},{"location":"tutorials/rag/#step-by-step-guide_1","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq import Workflow\nfrom dynamiq.nodes import InputTransformer\nfrom dynamiq.connections import (\n    OpenAI as OpenAIConnection,\n    Pinecone as PineconeConnection,\n    Elasticsearch as ElasticsearchConnection\n)\nfrom dynamiq.nodes.embedders import OpenAITextEmbedder\nfrom dynamiq.nodes.retrievers import PineconeDocumentRetriever, ElasticsearchDocumentRetriever\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.prompts import Message, Prompt\n</code></pre> <p>Initialize the RAG Retrieval Workflow</p> <pre><code>retrieval_wf = Workflow()\n</code></pre> <p>Shared OpenAI Connection</p> <p>Set up a shared connection to OpenAI.</p> <pre><code>openai_connection = OpenAIConnection(api_key=\"$OPENAI_API_KEY\")\n</code></pre> <p>OpenAI Text Embedder for Query Embedding</p> <p>Embed the user query into a vector format.</p> <pre><code>embedder = OpenAITextEmbedder(\n    connection=openai_connection,\n    model=\"text-embedding-3-small\",\n)\nretrieval_wf.flow.add_nodes(embedder)\n</code></pre> <p>Document Retriever Options</p> <p>You can choose between different retrievers. Here are examples for both Pinecone and Elasticsearch:</p>"},{"location":"tutorials/rag/#option-1-pinecone-document-retriever","title":"Option 1: Pinecone Document Retriever","text":"<pre><code>document_retriever = PineconeDocumentRetriever(\n    connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n    index_name=\"default\",\n    dimension=1536,\n    top_k=5,\n    input_transformer=InputTransformer(\n        selector={\n            \"embedding\": f\"${[embedder.id]}.output.embedding\",\n        },\n    ),\n).depends_on(embedder)\nretrieval_wf.flow.add_nodes(document_retriever)\n</code></pre>"},{"location":"tutorials/rag/#option-2-elasticsearch-document-retriever","title":"Option 2: Elasticsearch Document Retriever","text":"<p>For local setup:</p> <pre><code># Vector similarity search with Elasticsearch\ndocument_retriever = ElasticsearchDocumentRetriever(\n    connection=ElasticsearchConnection(\n        url=\"$ELASTICSEARCH_URL\",\n        api_key=\"$ELASTICSEARCH_API_KEY\",\n    ),\n    index_name=\"documents\",\n    top_k=5,\n    input_transformer=InputTransformer(\n        selector={\n            \"query\": f\"${[embedder.id]}.output.embedding\",  # Vector query for similarity search\n        },\n    ),\n).depends_on(embedder)\nretrieval_wf.flow.add_nodes(document_retriever)\n</code></pre> <p>For cloud deployment with score normalization:</p> <pre><code>document_retriever = ElasticsearchDocumentRetriever(\n    connection=ElasticsearchConnection(\n        username=\"$ELASTICSEARCH_USERNAME\",\n        password=\"$ELASTICSEARCH_PASSWORD\",\n        cloud_id=\"$ELASTICSEARCH_CLOUD_ID\",\n    ),\n    index_name=\"documents\",\n    top_k=5,\n    scale_scores=True,  # Scale scores to 0-1 range\n    input_transformer=InputTransformer(\n        selector={\n            \"query\": f\"${[embedder.id]}.output.embedding\",  # Vector query for similarity search\n        },\n    ),\n).depends_on(embedder)\n</code></pre> <p>Define the Prompt Template</p> <p>Create a template for generating answers based on the retrieved documents.</p> <pre><code>prompt_template = \"\"\"\nPlease answer the question based on the provided context.\n\nQuestion: {{ query }}\n\nContext:\n{% for document in documents %}\n- {{ document.content }}\n{% endfor %}\n\"\"\"\n</code></pre> <p>OpenAI LLM for Answer Generation</p> <p>Generate an answer to the user query using OpenAI's language model.</p> <pre><code>prompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\nanswer_generator = OpenAI(\n    connection=openai_connection,\n    model=\"gpt-4o\",\n    prompt=prompt,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[document_retriever.id]}.output.documents\",\n            \"query\": f\"${[embedder.id]}.output.query\",\n        },  # Take documents from the vector store node and query from the embedder\n    ),\n).depends_on([embedder, document_retriever])\nretrieval_wf.flow.add_nodes(answer_generator)\n</code></pre> <p>Run the RAG Retrieval Flow</p> <p>Execute the workflow to retrieve and answer the user query.</p> <pre><code>question = \"What are the line items provided in the invoice?\"\nresult = retrieval_wf.run(input_data={\"query\": question})\n\n# Print the answer\nanswer = result.output.get(answer_generator.id).get(\"output\", {}).get(\"content\")\nprint(answer)\n</code></pre>"}]}