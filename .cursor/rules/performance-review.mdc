---
description: Performance analysis and optimization review for Dynamiq AI orchestration framework
globs:
  - "dynamiq/**/*.py"
  - "tests/**/*.py"
  - "examples/**/*.py"
alwaysApply: false
---

# Dynamiq Performance Review

Comprehensive performance analysis command for the Dynamiq AI orchestration framework. Use this to identify bottlenecks, optimization opportunities, and performance anti-patterns.

## Framework Architecture Context

Dynamiq is a DAG-based workflow orchestration framework for agentic AI applications:

- **Workflows** contain **Flows** which manage **Nodes** in a directed acyclic graph
- **TopologicalSorter** determines node execution order based on dependencies
- **ThreadExecutor** (default) or **ProcessExecutor** handles concurrent node execution
- **ConnectionManager** manages external service client lifecycle
- **RunnableConfig** carries callbacks, cache config, and execution parameters

## Performance Analysis Checklist

### 1. DAG Execution Efficiency

<analysis>
name: dag_execution_analysis
description: Analyze workflow DAG structure for execution bottlenecks
check_for:
  - Sequential bottlenecks where parallelizable nodes are chained unnecessarily
  - Deep dependency chains that limit parallelism (`node.depends_on()` chains)
  - Nodes with `is_postponed_component_init=True` that could be pre-initialized
  - Excessive `reset_run_state()` calls in Flow that recreate TopologicalSorter
  - Large `max_node_workers` values exceeding actual parallelism potential
  
patterns_to_review:
  - `node.depends_on([node1, node2])` - Ensure dependencies are truly required
  - `flow.add_nodes()` - Check if nodes can be batched vs added individually
  - `wf.run_sync()` vs `wf.run_async()` - Use async for I/O-bound workflows
</analysis>

### 2. Executor and Threading Analysis

<analysis>
name: executor_performance
description: Review thread/process pool configuration and usage
check_for:
  - Default `MAX_WORKERS_THREAD_POOL_EXECUTOR = 8` may be suboptimal for I/O workloads
  - ProcessExecutor serialization overhead with `jsonpickle` for complex nodes
  - Missing `executor.shutdown(wait=True)` causing resource leaks
  - Thread pool exhaustion from nested agent tool execution
  - `time.sleep(0.003)` polling loops in Flow._get_nodes_ready_to_run()
  
recommendations:
  - For I/O-bound workflows: increase max_workers, prefer ThreadExecutor
  - For CPU-bound workflows: use ProcessExecutor with appropriate max_workers
  - Monitor `futures.wait(return_when=FIRST_COMPLETED)` patterns
  - Agent parallel_tool_calls use ThreadPoolExecutor internally - watch for nesting
</analysis>

### 3. Connection and Client Management

<analysis>
name: connection_lifecycle
description: Analyze connection pooling and client reuse
check_for:
  - Nodes creating clients in `execute()` instead of `init_components()`
  - Missing `_connection_manager` storage preventing automatic reconnection
  - Closed client detection via `is_client_closed()` not implemented for custom clients
  - Connection objects recreated per-request instead of reused via ConnectionManager
  - Missing `ensure_client()` override for clients with non-standard closed state
  
hot_paths:
  - `ConnectionNode.init_components()` - Must call super() and store connection_manager
  - `VectorStoreNode.connect_to_vector_store()` - Called on every reconnection
  - `connection.connect()` - Expensive for services like Neo4j, Elasticsearch, Weaviate
</analysis>

### 4. Caching Strategy

<analysis>
name: caching_analysis
description: Review node and workflow caching effectiveness
check_for:
  - Nodes with `caching.enabled = False` that have deterministic outputs
  - Cache key collisions from `WorkflowCacheManager.get_key()` hash function
  - Large output data causing Redis/cache serialization overhead
  - Missing cache invalidation for nodes with mutable external state
  - `@cache_wf_entity` decorator overhead on fast operations
  
optimization_opportunities:
  - Enable caching for expensive LLM calls and embedder operations
  - Use `model_dump(exclude=...)` to reduce cached data size
  - Consider cache TTL for time-sensitive data
  - Profile `hashlib.sha256` cost for large input dictionaries
</analysis>

### 5. LLM and Agent Performance

<analysis>
name: llm_agent_optimization
description: Analyze LLM node and agent execution efficiency
check_for:
  - Agent `max_loops` too high (default 15) causing unnecessary iterations
  - Missing `stop_sequences` causing LLM to generate beyond needed output
  - `parallel_tool_calls_enabled=True` creating ThreadPoolExecutor per loop
  - Summarization `context_usage_ratio` threshold triggering premature summarization
  - Streaming callbacks (`AgentStreamingParserCallback`) adding parsing overhead
  - `_tool_cache` not utilized effectively for repeated tool calls
  
llm_specific:
  - `litellm` timeout configuration for slow providers
  - Token counting with `prompt.count_tokens()` on every loop iteration
  - Model-specific prompt managers adding template rendering overhead
  - Function calling schema generation in `generate_function_calling_schemas()`
</analysis>

### 6. Async/Sync Pattern Analysis

<analysis>
name: async_patterns
description: Identify blocking operations in async context
check_for:
  - `asyncio.to_thread()` wrapping already async-capable operations
  - Blocking `time.sleep()` instead of `asyncio.sleep()` in async paths
  - `requests.*` in async context instead of httpx/aiohttp
  - File I/O without `aiofiles` in async node execution
  - `run_async` defaulting to `asyncio.to_thread(run_sync)` for all nodes
  
async_bottlenecks:
  - `Flow.run_async()` uses `asyncio.gather()` but individual nodes may block
  - Callback handlers may perform blocking I/O in async workflow
  - `_cleanup_dry_run_async()` runs cleanup in parallel with `asyncio.gather()`
</analysis>

### 7. Memory and Object Lifecycle

<analysis>
name: memory_analysis
description: Review memory usage and object lifecycle patterns
check_for:
  - Node `clone()` creating deep copies of large objects unnecessarily
  - `_intermediate_steps` dictionary growing unboundedly in long agent runs
  - `RunnableResult` storing full input/output data instead of references
  - Prompt message history accumulation without `summarization_config`
  - `BytesIO` and large file objects not released after processing
  
memory_hotspots:
  - `Agent._prompt.messages` list grows with each loop iteration
  - `Flow._results` dictionary holds all node outputs until workflow completes
  - `to_dict(for_tracing=True)` creating many intermediate dictionaries
  - Document splitters holding entire documents in memory
</analysis>

### 8. Callback and Tracing Overhead

<analysis>
name: callback_overhead
description: Analyze callback system performance impact
check_for:
  - Multiple callbacks iterating over same events (O(n) per callback list)
  - `TracingCallbackHandler` serializing large node state on every event
  - `to_dict(for_tracing=True)` called multiple times per node execution
  - Streaming callbacks processing every chunk individually
  - `run_on_node_*` methods catching and logging exceptions silently
  
optimization_targets:
  - Batch callback invocations where possible
  - Use `for_tracing=True` consistently to minimize serialization
  - Consider callback filtering by event type
  - Profile `StreamingQueueCallbackHandler` queue operations
</analysis>

### 9. Serialization Performance

<analysis>
name: serialization_analysis
description: Review YAML/JSON serialization bottlenecks
check_for:
  - `WorkflowYAMLDumper.dump()` serializing entire workflow graph
  - Nested node serialization in `to_dict()` creating deep recursion
  - `model_dump()` with `serialize_as_any=True` causing type introspection
  - `format_value()` recursively processing complex objects
  - `jsonpickle.encode()` for ProcessExecutor node serialization
  
yaml_specific:
  - `WorkflowYAMLLoader.load()` with `init_components=True` initializing all connections
  - Connection extraction to separate YAML section creating multiple passes
  - Nested node type resolution via NodeManager registry lookups
</analysis>

### 10. Vector Store and Retriever Performance

<analysis>
name: vector_store_performance
description: Analyze vector storage and retrieval efficiency
check_for:
  - Missing batch operations for document indexing (single document writes)
  - Embedder called per-document instead of batch embedding
  - Retriever `top_k` too high returning unused results
  - Filter construction overhead in `*_filters.py` modules
  - Vector store client reconnection on every query
  
store_specific:
  - Qdrant: `qdrant_client.search()` parameters optimization
  - Pinecone: Index selection and namespace configuration
  - Milvus: Collection loading and index building overhead
  - Weaviate: GraphQL query complexity
  - Elasticsearch: Query DSL construction efficiency
</analysis>

## Performance Review Commands

When asked to perform a performance review, analyze the code against ALL checklist items above and provide:

1. **Critical Issues**: Blocking problems that significantly impact performance
2. **Optimization Opportunities**: Improvements that could yield measurable gains
3. **Best Practices Violations**: Deviations from Dynamiq performance patterns
4. **Metrics to Monitor**: Specific measurements to track improvement

## Example Analysis Output

```markdown
## Performance Analysis Report

### Critical Issues
- [ ] Agent in `agent_searcher.py` has `max_loops=50` with no early termination
- [ ] Nodes creating PostgreSQL connections in `execute()` method

### Optimization Opportunities  
- [ ] Enable caching on OpenAI embedder nodes (deterministic for same input)
- [ ] Batch document writes in `qdrant_writer_flow.py`

### Metrics to Monitor
- Workflow execution time: `format_duration(time_start, datetime.now())`
- Node execution count per flow run
- Cache hit ratio for LLM nodes
- Connection reconnection frequency
```

## File Patterns to Prioritize

- `dynamiq/flows/flow.py` - Core execution engine
- `dynamiq/nodes/agents/agent.py` - Agent loop performance
- `dynamiq/executors/pool.py` - Thread/process pool management
- `dynamiq/connections/managers.py` - Connection lifecycle
- `dynamiq/cache/managers/workflow.py` - Caching implementation
- `dynamiq/nodes/llms/*.py` - LLM provider integrations
- `dynamiq/nodes/retrievers/*.py` - Vector retrieval
- `dynamiq/nodes/writers/*.py` - Vector write operations
